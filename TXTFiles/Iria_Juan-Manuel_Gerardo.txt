Nom du fichier :
			Iria_Juan-Manuel_Gerardo.pdf
Titre :
			On the Development of the RST Spanish Treebank
Nombre d'auteur :
 			3
Auteurs :
			Iria da Cunha
			Juan-Manuel Torres-Moreno
			Gerardo Sierra
Abstract :
			In this article we present the RST Spanish Treebank, the first corpus annotated with rhetorical relations for this language. We describe the characteristics of the corpus, the annotation criteria, the annotation procedure, the inter-annotator agreement, and other related aspects. Moreover, we show the interface that we have developed to carry out searches over the corpus' annotated texts.

Intro :
			null			Extensive experiments on query-oriented multi- 
			document summarization have been carried out 
			over the past few years. Most of the strategies 
			to produce summaries are based on an extrac- 
			tion method, which identifies salient textual seg- 
			ments, most often sentences, in documents. Sen- 
			tences containing the most salient concepts are se- 
			lected, ordered and assembled according to their 
			relevance to produce summaries (also called ex- 
			tracts) (Mani and Maybury, 1999). 
			Recently emerged from the Document Under- 
			standing Conference (DUC) 20071, update sum- 
			marization attempts to enhance summarization 
			when more information about knowledge acquired 
			by the user is available. It asks the following ques- 
			tion: has the user already read documents on the 
			topic? In the case of a positive answer, producing 
			an extract focusing on only new facts is of inter- 
			est. In this way, an important issue is introduced: 
			c 2008. Licensed under the Creative Commons 
			Attribution-Noncommercial-Share Alike 3.0 Unported li- 
			cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). 
			Some rights reserved. 
			1Document Understanding Conferences are conducted 
			since 2000 by the National Institute of Standards and Tech- 
			nology (NIST), http://www-nlpir.nist.gov 
			redundancy with previously read documents (his- 
			tory) has to be removed from the extract. 
			A natural way to go about update summarization 
			would be extracting temporal tags (dates, elapsed 
			times, temporal expressions...) (Mani and Wilson, 
			2000) or to automatically construct the timeline 
			from documents (Swan and Allan, 2000). These 
			temporal marks could be used to focus extracts on 
			the most recently written facts. However, most re- 
			cently written facts are not necessarily new facts. 
			Machine Reading (MR) was used by (Hickl et 
			al., 2007) to construct knowledge representations 
			from clusters of documents. Sentences contain- 
			ing "new" information (i.e. that could not be in- 
			ferred by any previously considered document) 
			are selected to generate summary. However, this 
			highly efficient approach (best system in DUC 
			2007 update) requires large linguistic resources. 
			(Witte et al., 2007) propose a rule-based system 
			based on fuzzy coreference cluster graphs. Again, 
			this approach requires to manually write the sen- 
			tence ranking scheme. Several strategies remain- 
			ing on post-processing redundancy removal tech- 
			niques have been suggested. Extracts constructed 
			from history were used by (Boudin and Torres- 
			Moreno, 2007) to minimize history's redundancy. 
			(Lin et al., 2007) have proposed a modified Max- 
			imal Marginal Relevance (MMR) (Carbonell and 
			Goldstein, 1998) re-ranker during sentence selec- 
			tion, constructing the summary by incrementally 
			re-ranking sentences. 
			In this paper, we propose a scalable sentence 
			scoring method for update summarization derived 
			from MMR. Motivated by the need for relevant 
			novelty, candidate sentences are selected accord- 
			ing to a combined criterion of query relevance and 
			dissimilarity with previously read sentences. The 
			rest of the paper is organized as follows. Section 2 
			23 
			introduces our proposed sentence scoring method 
			and Section 3 presents experiments and evaluates 
			Extensive experiments on query-oriented multi- 
			document summarization have been carried out 
			over the past few years. Most of the strategies 
			to produce summaries are based on an extrac- 
			tion method, which identifies salient textual seg- 
			ments, most often sentences, in documents. Sen- 
			tences containing the most salient concepts are se- 
			lected, ordered and assembled according to their 
			relevance to produce summaries (also called ex- 
			tracts) (Mani and Maybury, 1999). 
			Recently emerged from the Document Under- 
			standing Conference (DUC) 20071, update sum- 
			marization attempts to enhance summarization 
			when more information about knowledge acquired 
			by the user is available. It asks the following ques- 
			tion: has the user already read documents on the 
			topic? In the case of a positive answer, producing 
			an extract focusing on only new facts is of inter- 
			est. In this way, an important issue is introduced: 
			c 2008. Licensed under the Creative Commons 
			Attribution-Noncommercial-Share Alike 3.0 Unported li- 
			cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). 
			Some rights reserved. 
			1Document Understanding Conferences are conducted 
			since 2000 by the National Institute of Standards and Tech- 
			nology (NIST), http://www-nlpir.nist.gov 
			redundancy with previously read documents (his- 
			tory) has to be removed from the extract. 
			A natural way to go about update summarization 
			would be extracting temporal tags (dates, elapsed 
			times, temporal expressions...) (Mani and Wilson, 
			2000) or to automatically construct the timeline 
			from documents (Swan and Allan, 2000). These 
			temporal marks could be used to focus extracts on 
			the most recently written facts. However, most re- 
			cently written facts are not necessarily new facts. 
			Machine Reading (MR) was used by (Hickl et 
			al., 2007) to construct knowledge representations 
			from clusters of documents. Sentences contain- 
			ing "new" information (i.e. that could not be in- 
			ferred by any previously considered document) 
			are selected to generate summary. However, this 
			highly efficient approach (best system in DUC 
			2007 update) requires large linguistic resources. 
			(Witte et al., 2007) propose a rule-based system 
			based on fuzzy coreference cluster graphs. Again, 
			this approach requires to manually write the sen- 
			tence ranking scheme. Several strategies remain- 
			ing on post-processing redundancy removal tech- 
			niques have been suggested. Extracts constructed 
			from history were used by (Boudin and Torres- 
			Moreno, 2007) to minimize history's redundancy. 
			(Lin et al., 2007) have proposed a modified Max- 
			imal Marginal Relevance (MMR) (Carbonell and 
			Goldstein, 1998) re-ranker during sentence selec- 
			tion, constructing the summary by incrementally 
			re-ranking sentences. 
			In this paper, we propose a scalable sentence 
			scoring method for update summarization derived 
			from MMR. Motivated by the need for relevant 
			novelty, candidate sentences are selected accord- 
			ing to a combined criterion of query relevance and 
			dissimilarity with previously read sentences. The 
			rest of the paper is organized as follows. Section 2 
			23 
			introduces our proposed sentence scoring method 
			and Section 3 presents experiments and evaluates 
			The subfield of summarization has been investigated by the NLP community for 
			nearly the last half century. Radev et al. (2002) define a summary as "a text that 
			is produced from one or more texts, that conveys important information in the 
			original text(s), and that is no longer than half of the original text(s) and usually 
			significantly less than that". This simple definition captures three important aspects 
			that characterize research on automatic summarization: 
			* Summaries may be produced from a single document or multiple documents, 
			* Summaries should preserve important information, 
			* Summaries should be short. 
			Even if we agree unanimously on these points, it seems from the literature that 
			any attempt to provide a more elaborate definition for the task would result in 
			disagreement within the community. In fact, many approaches differ on the manner 
			of their problem formulations. We start by introducing some common terms in the 
			1 
			summarization dialect: extraction is the procedure of identifying important sections 
			of the text and producing them verbatim; abstraction aims to produce important 
			material in a new way; fusion combines extracted parts coherently; and compression 
			aims to throw out unimportant sections of the text (Radev et al., 2002). 
			Earliest instances of research on summarizing scientific documents proposed 
			paradigms for extracting salient sentences from text using features like word and 
			phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key 
			phrases (Edmundson, 1969). Various work published since then has concentrated on 
			other domains, mostly on newswire data. Many approaches addressed the problem 
			by building systems depending of the type of the required summary. While extractive 
			summarization is mainly concerned with what the summary content should be, usu- 
			ally relying solely on extraction of sentences, abstractive summarization puts strong 
			emphasis on the form, aiming to produce a grammatical summary, which usually 
			requires advanced language generation techniques. In a paradigm more tuned to 
			information retrieval (IR), one can also consider topic-driven summarization, that 
			assumes that the summary content depends on the preference of the user and can 
			be assessed via a query, making the final summary focused on a particular topic. 
			A crucial issue that will certainly drive future research on summarization is 
			evaluation. During the last fifteen years, many system evaluation competitions like 
			TREC,1 DUC2 and MUC3 have created sets of training material and have estab- 
			lished baselines for performance levels. However, a universal strategy to evaluate 
			summarization systems is still absent. 
			In this survey, we primarily aim to investigate how empirical methods have been 
			used to build summarization systems. The rest of the paper is organized as fol- 
			lows: Section 2 describes single-document summarization, focusing on extractive 
			techniques. Section 3 progresses to discuss the area of multi-document summariza- 
			tion, where a few abstractive approaches that pioneered the field are also considered. 
			Section 4 briefly discusses some unconventional approaches that we believe can be 
			useful in the future of summarization research. Section 5 elaborates a few eval- 
			uation techniques and describes some of the standards for evaluating summaries 
			The subfield of summarization has been investigated by the NLP community for 
			nearly the last half century. Radev et al. (2002) define a summary as "a text that 
			is produced from one or more texts, that conveys important information in the 
			original text(s), and that is no longer than half of the original text(s) and usually 
			significantly less than that". This simple definition captures three important aspects 
			that characterize research on automatic summarization: 
			* Summaries may be produced from a single document or multiple documents, 
			* Summaries should preserve important information, 
			* Summaries should be short. 
			Even if we agree unanimously on these points, it seems from the literature that 
			any attempt to provide a more elaborate definition for the task would result in 
			disagreement within the community. In fact, many approaches differ on the manner 
			of their problem formulations. We start by introducing some common terms in the 
			1 
			summarization dialect: extraction is the procedure of identifying important sections 
			of the text and producing them verbatim; abstraction aims to produce important 
			material in a new way; fusion combines extracted parts coherently; and compression 
			aims to throw out unimportant sections of the text (Radev et al., 2002). 
			Earliest instances of research on summarizing scientific documents proposed 
			paradigms for extracting salient sentences from text using features like word and 
			phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key 
			phrases (Edmundson, 1969). Various work published since then has concentrated on 
			other domains, mostly on newswire data. Many approaches addressed the problem 
			by building systems depending of the type of the required summary. While extractive 
			summarization is mainly concerned with what the summary content should be, usu- 
			ally relying solely on extraction of sentences, abstractive summarization puts strong 
			emphasis on the form, aiming to produce a grammatical summary, which usually 
			requires advanced language generation techniques. In a paradigm more tuned to 
			information retrieval (IR), one can also consider topic-driven summarization, that 
			assumes that the summary content depends on the preference of the user and can 
			be assessed via a query, making the final summary focused on a particular topic. 
			A crucial issue that will certainly drive future research on summarization is 
			evaluation. During the last fifteen years, many system evaluation competitions like 
			TREC,1 DUC2 and MUC3 have created sets of training material and have estab- 
			lished baselines for performance levels. However, a universal strategy to evaluate 
			summarization systems is still absent. 
			In this survey, we primarily aim to investigate how empirical methods have been 
			used to build summarization systems. The rest of the paper is organized as fol- 
			lows: Section 2 describes single-document summarization, focusing on extractive 
			techniques. Section 3 progresses to discuss the area of multi-document summariza- 
			tion, where a few abstractive approaches that pioneered the field are also considered. 
			Section 4 briefly discusses some unconventional approaches that we believe can be 
			useful in the future of summarization research. Section 5 elaborates a few eval- 
			uation techniques and describes some of the standards for evaluating summaries 
			The goal of Automatic Speech Recognition (ASR) is to transform spoken data 
			into a written representation, thus enabling natural human-machine interaction 
			[33] with further Natural Language Processing (NLP) tasks. Machine transla- 
			tion, question answering, semantic parsing, POS tagging, sentiment analysis and 
			automatic text summarization; originally developed to work with formal writ- 
			ten texts, can be applied over the transcripts made by ASR systems [2,25,31]. 
			However, before applying any of these NLP tasks a segmentation process called 
			Sentence Boundary Detection (SBD) should be performed over ASR transcripts 
			to reach a minimal syntactic information in the text. 
			To measure the performance of a SBD system, the automatically segmented 
			transcript is evaluated against a single reference normally done by a human. But 
			c Springer Nature Switzerland AG 2018 
			I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119-131, 2018. 
			https://doi.org/10.1007/978-3-030-04497-8_10 
			120 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			given a transcript, does it exist a unique reference? Or, is it possible that the 
			same transcript could be segmented in five different ways by five different people 
			in the same conditions? If so, which one is correct; and more important, how 
			to fairly evaluate the automatically segmented transcript? These questions are 
			the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a 
			new semi-supervised metric for evaluating SBD systems based on multi-reference 
			(dis)agreement. 
			The rest of this article is organized as follows. In Sect. 2 we set the frame of 
			SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3, 
			followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE 
			and discussion over the method and alternative multi-reference evaluation is 
			The goal of Automatic Speech Recognition (ASR) is to transform spoken data 
			into a written representation, thus enabling natural human-machine interaction 
			[33] with further Natural Language Processing (NLP) tasks. Machine transla- 
			tion, question answering, semantic parsing, POS tagging, sentiment analysis and 
			automatic text summarization; originally developed to work with formal writ- 
			ten texts, can be applied over the transcripts made by ASR systems [2,25,31]. 
			However, before applying any of these NLP tasks a segmentation process called 
			Sentence Boundary Detection (SBD) should be performed over ASR transcripts 
			to reach a minimal syntactic information in the text. 
			To measure the performance of a SBD system, the automatically segmented 
			transcript is evaluated against a single reference normally done by a human. But 
			c Springer Nature Switzerland AG 2018 
			I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119-131, 2018. 
			https://doi.org/10.1007/978-3-030-04497-8_10 
			120 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			given a transcript, does it exist a unique reference? Or, is it possible that the 
			same transcript could be segmented in five different ways by five different people 
			in the same conditions? If so, which one is correct; and more important, how 
			to fairly evaluate the automatically segmented transcript? These questions are 
			the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a 
			new semi-supervised metric for evaluating SBD systems based on multi-reference 
			(dis)agreement. 
			The rest of this article is organized as follows. In Sect. 2 we set the frame of 
			SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3, 
			followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE 
			and discussion over the method and alternative multi-reference evaluation is 
			The Rhetorical Structure Theory (RST) (Mann and 
			Thompson, 1988) is a language independent theory 
			based on the idea that a text can be segmented into 
			Elementary Discourse Units (EDUs) linked by 
			means of nucleus-satellite or multinuclear 
			rhetorical relations. In the first case, the satellite 
			gives additional information about the other one, 
			the nucleus, on which it depends (ex. Result, 
			Condition, Elaboration or Concession). In the 
			second case, several elements, all nuclei, are 
			connected at the same level, that is, there are no 
			elements dependent on others and they all have the 
			same importance with regard to the intentions of 
			the author of the text (ex. Contrast, List, Joint or 
			Sequence). The rhetorical analysis of a text by 
			means of RST includes 3 phases: segmentation, 
			detection of relations and building of hierarchical 
			rhetorical trees. For more information about RST 
			we recommend the original article of Mann and 
			Thompson (1988), the web site of RST1 and the 
			RST review by Taboada and Mann (2006a). 
			RST has been used to develop several 
			applications, like automatic summarization, 
			information extraction (IE), text generation, 
			question-answering, automatic translation, etc. 
			(Taboada and Mann, 2006b). Nevertheless, most of 
			these works have been developed for English, 
			German or Portuguese. This is due to the fact that 
			at present corpora annotated with RST relations are 
			available only for these languages (for English: 
			Carlson et al., 2002, Taboada and Renkema, 2008; 
			for German: Stede, 2004; for Portuguese: Pardo et 
			al., 2008) and there are automatic RST parsers for 
			two of them (for English: Marcu, 2000; for 
			Portuguese: Pardo et al., 2008) or automatic RST 
			segmenters (for English: Tofiloski et al., 2009). 
			Scientific community working on RST applied to 
			Spanish is very small. For example, Bouayad-Agha 
			et al. (2006) apply RST to text generation in 
			several languages, Spanish among them. Da Cunha 
			et al. (2007) develop a summarization system for 
			medical texts in Spanish based on RST. Da Cunha 
			and Iruskieta (2010) perform a contrastive analysis 
			of Spanish and Basque texts. Romera (2004) 
			analyzes coherence relations by means of RST in 
			spoken Spanish. Taboada (2004) applies RST to 
			analyze the resources used by speakers to elaborate 
			conversations in English and Spanish. 
			We consider that it is necessary to build a 
			Spanish corpus annotated by means of RST. This 
			corpus should be useful for the development of a 
			rhetorical parser for this language and several other 
			applications related to computational linguistics, 
			like those developed for other languages 
			1 http://www.sfu.ca/rst/index.html 
			1 
			(automatic translation, automatic summarization, 
			IE, etc.). And that is what we pretend to achieve 
			with our work. We present the development of the 
			RST Spanish Treebank, the first Spanish corpus 
			annotated by means of RST. 
			In Section 2, we present the state of the art 
			about RST annotated corpora. In Section 3, we 
			explain the characteristics of the RST Spanish 
			Treebank. In Section 4, we show the search 
			interface we have developed. In Section 5, we 
			The Rhetorical Structure Theory (RST) (Mann and 
			Thompson, 1988) is a language independent theory 
			based on the idea that a text can be segmented into 
			Elementary Discourse Units (EDUs) linked by 
			means of nucleus-satellite or multinuclear 
			rhetorical relations. In the first case, the satellite 
			gives additional information about the other one, 
			the nucleus, on which it depends (ex. Result, 
			Condition, Elaboration or Concession). In the 
			second case, several elements, all nuclei, are 
			connected at the same level, that is, there are no 
			elements dependent on others and they all have the 
			same importance with regard to the intentions of 
			the author of the text (ex. Contrast, List, Joint or 
			Sequence). The rhetorical analysis of a text by 
			means of RST includes 3 phases: segmentation, 
			detection of relations and building of hierarchical 
			rhetorical trees. For more information about RST 
			we recommend the original article of Mann and 
			Thompson (1988), the web site of RST1 and the 
			RST review by Taboada and Mann (2006a). 
			RST has been used to develop several 
			applications, like automatic summarization, 
			information extraction (IE), text generation, 
			question-answering, automatic translation, etc. 
			(Taboada and Mann, 2006b). Nevertheless, most of 
			these works have been developed for English, 
			German or Portuguese. This is due to the fact that 
			at present corpora annotated with RST relations are 
			available only for these languages (for English: 
			Carlson et al., 2002, Taboada and Renkema, 2008; 
			for German: Stede, 2004; for Portuguese: Pardo et 
			al., 2008) and there are automatic RST parsers for 
			two of them (for English: Marcu, 2000; for 
			Portuguese: Pardo et al., 2008) or automatic RST 
			segmenters (for English: Tofiloski et al., 2009). 
			Scientific community working on RST applied to 
			Spanish is very small. For example, Bouayad-Agha 
			et al. (2006) apply RST to text generation in 
			several languages, Spanish among them. Da Cunha 
			et al. (2007) develop a summarization system for 
			medical texts in Spanish based on RST. Da Cunha 
			and Iruskieta (2010) perform a contrastive analysis 
			of Spanish and Basque texts. Romera (2004) 
			analyzes coherence relations by means of RST in 
			spoken Spanish. Taboada (2004) applies RST to 
			analyze the resources used by speakers to elaborate 
			conversations in English and Spanish. 
			We consider that it is necessary to build a 
			Spanish corpus annotated by means of RST. This 
			corpus should be useful for the development of a 
			rhetorical parser for this language and several other 
			applications related to computational linguistics, 
			like those developed for other languages 
			1 http://www.sfu.ca/rst/index.html 
			1 
			(automatic translation, automatic summarization, 
			IE, etc.). And that is what we pretend to achieve 
			with our work. We present the development of the 
			RST Spanish Treebank, the first Spanish corpus 
			annotated by means of RST. 
			In Section 2, we present the state of the art 
			about RST annotated corpora. In Section 3, we 
			explain the characteristics of the RST Spanish 
			Treebank. In Section 4, we show the search 
			interface we have developed. In Section 5, we 


Corps :			null			2 Method 
			The underlying idea of our method is that as the 
			number of sentences in the history increases, the 
			likelihood to have redundant information within 
			candidate sentences also increases. We propose 
			a scalable sentence scoring method derived from 
			MMR that, as the size of the history increases, 
			gives more importance to non-redundancy that to 
			query relevance. We define H to represent the pre- 
			viously read documents (history), Q to represent 
			the query and s the candidate sentence. The fol- 
			lowing subsections formally define the similarity 
			measures and the scalable MMR scoring method. 
			2.1 A query-oriented multi-document 
			summarizer 
			We have first started by implementing a simple 
			summarizer for which the task is to produce query- 
			focused summaries from clusters of documents. 
			Each document is pre-processed: documents are 
			segmented into sentences, sentences are filtered 
			(words which do not carry meaning are removed 
			such as functional words or common words) and 
			normalized using a lemmas database (i.e. inflected 
			forms "go", "goes", "went", "gone"... are replaced 
			by "go"). An N-dimensional term-space , where 
			N is the number of different terms found in the 
			cluster, is constructed. Sentences are represented 
			in  by vectors in which each component is the 
			term frequency within the sentence. Sentence scor- 
			ing can be seen as a passage retrieval task in Infor- 
			mation Retrieval (IR). Each sentence s is scored by 
			computing a combination of two similarity mea- 
			sures between the sentence and the query. The first 
			measure is the well known cosine angle (Salton et 
			al., 1975) between the sentence and the query vec- 
			torial representations in  (denoted respectively s 
			and Q). The second similarity measure is based 
			on the Jaro-Winkler distance (Winkler, 1999). The 
			original Jaro-Winkler measure, denoted JW, uses 
			the number of matching characters and transposi- 
			tions to compute a similarity score between two 
			terms, giving more favourable ratings to terms that 
			match from the beginning. We have extended this 
			measure to calculate the similarity between the 
			sentence s and the query Q: 
			JWe(s, Q) = 
			1 
			|Q| 
			* 
			qQ 
			max 
			mS 
			JW(q, m) (1) 
			where S is the term set of s in which the terms 
			m that already have maximized JW(q, m) are re- 
			moved. The use of JWe smooths normalization and 
			misspelling errors. Each sentence s is scored using 
			the linear combination: 
			Sim1(s, Q) =  * cosine(s, Q) 
			+ (1 - ) * JWe(s, Q) (2) 
			where  = 0.7, optimally tuned on the past DUCs 
			data (2005 and 2006). The system produces a list 
			of ranked sentences from which the summary is 
			constructed by arranging the high scored sentences 
			until the desired size is reached. 
			2.2 A scalable MMR approach 
			MMR re-ranking algorithm has been successfully 
			used in query-oriented summarization (Ye et al., 
			2005). It strives to reduce redundancy while main- 
			taining query relevance in selected sentences. The 
			summary is constructed incrementally from a list 
			of ranked sentences, at each iteration the sentence 
			which maximizes MMR is chosen: 
			MMR = arg max 
			sS 
			[  * Sim1(s, Q) 
			- (1 - ) * max 
			sjE 
			Sim2(s, sj) ] (3) 
			where S is the set of candidates sentences and E 
			is the set of selected sentences.  represents an 
			interpolation coefficient between sentence's rele- 
			vance and non-redundancy. Sim2(s, sj) is a nor- 
			malized Longest Common Substring (LCS) mea- 
			sure between sentences s and sj. Detecting sen- 
			tence rehearsals, LCS is well adapted for redun- 
			dancy removal. 
			We propose an interpretation of MMR to tackle 
			the update summarization issue. Since Sim1 and 
			Sim2 are ranged in [0, 1], they can be seen as prob- 
			abilities even though they are not. Just as rewriting 
			(3) as (NR stands for Novelty Relevance): 
			NR = arg max 
			sS 
			[  * Sim1(s, Q) 
			+ (1 - ) * (1 - max 
			shH 
			Sim2(s, sh)) ] (4) 
			We can understand that (4) equates to an OR com- 
			bination. But as we are looking for a more intu- 
			itive AND and since the similarities are indepen- 
			dent, we have to use the product combination. The 
			24 
			scoring method defined in (2) is modified into a 
			double maximization criterion in which the best 
			ranked sentence will be the most relevant to the 
			query AND the most different to the sentences in 
			H. 
			SMMR(s) = Sim1(s, Q) 
			* 1 - max 
			shH 
			Sim2(s, sh) 
			f(H) 
			(5) 
			Decreasing  in (3) with the length of the sum- 
			mary was suggested by (Murray et al., 2005) and 
			successfully used in the DUC 2005 by (Hachey 
			et al., 2005), thereby emphasizing the relevance 
			at the outset but increasingly prioritizing redun- 
			dancy removal as the process continues. Sim- 
			ilarly, we propose to follow this assumption in 
			SMMR using a function denoted f that as the 
			amount of data in history increases, prioritize non- 
			redundancy (f(H)  0). 
			3 Experiments 
			The method described in the previous section has 
			been implemented and evaluated by using the 
			DUC 2007 update corpus2. The following subsec- 
			tions present details of the different experiments 
			we have conducted. 
			3.1 The DUC 2007 update corpus 
			We used for our experiments the DUC 2007 up- 
			date competition data set. The corpus is composed 
			of 10 topics, with 25 documents per topic. The up- 
			date task goal was to produce short (100 words) 
			multi-document update summaries of newswire ar- 
			ticles under the assumption that the user has al- 
			ready read a set of earlier articles. The purpose 
			of each update summary will be to inform the 
			reader of new information about a particular topic. 
			Given a DUC topic and its 3 document clusters: A 
			(10 documents), B (8 documents) and C (7 doc- 
			uments), the task is to create from the documents 
			three brief, fluent summaries that contribute to sat- 
			isfying the information need expressed in the topic 
			statement. 
			1. A summary of documents in cluster A. 
			2. An update summary of documents in B, un- 
			der the assumption that the reader has already 
			read documents in A. 
			2More information about the DUC 2007 corpus is avail- 
			able at http://duc.nist.gov/. 
			3. An update summary of documents in C, un- 
			der the assumption that the reader has already 
			read documents in A and B. 
			Within a topic, the document clusters must be pro- 
			cessed in chronological order. Our system gener- 
			ates a summary for each cluster by arranging the 
			high ranked sentences until the limit of 100 words 
			is reached. 
			3.2 Evaluation 
			Most existing automated evaluation methods work 
			by comparing the generated summaries to one or 
			more reference summaries (ideally, produced by 
			humans). To evaluate the quality of our generated 
			summaries, we choose to use the ROUGE3 (Lin, 
			2004) evaluation toolkit, that has been found to be 
			highly correlated with human judgments. ROUGE- 
			N is a n-gram recall measure calculated between 
			a candidate summary and a set of reference sum- 
			maries. In our experiments ROUGE-1, ROUGE-2 
			and ROUGE-SU4 will be computed. 
			3.3 Results 
			Table 1 reports the results obtained on the DUC 
			2007 update data set for different sentence scor- 
			ing methods. cosine + JWe stands for the scor- 
			ing method defined in (2) and NR improves it 
			with sentence re-ranking defined in equation (4). 
			SMMR is the combined adaptation we have pro- 
			posed in (5). The function f(H) used in SMMR is 
			the simple rational function 1 
			H 
			, where H increases 
			with the number of previous clusters (f(H) = 1 
			for cluster A, 1 
			2 
			for cluster B and 1 
			3 
			for cluster C). 
			This function allows to simply test the assumption 
			that non-redundancy have to be favoured as the 
			size of history grows. Baseline results are obtained 
			on summaries generated by taking the leading sen- 
			tences of the most recent documents of the cluster, 
			up to 100 words (official baseline of DUC). The 
			table also lists the three top performing systems at 
			DUC 2007 and the lowest scored human reference. 
			As we can see from these results, SMMR out- 
			performs the other sentence scoring methods. By 
			ways of comparison our system would have been 
			ranked second at the DUC 2007 update competi- 
			tion. Moreover, no post-processing was applied to 
			the selected sentences leaving an important margin 
			of progress. Another interesting result is the high 
			performance of the non-update specific method 
			(cosine + JWe) that could be due to the small size 
			3ROUGE is available at http://haydn.isi.edu/ROUGE/. 
			25 
			of the corpus (little redundancy between clusters). 
			ROUGE-1 ROUGE-2 ROUGE-SU4 
			Baseline 0.26232 0.04543 0.08247 
			3rd system 0.35715 0.09622 0.13245 
			2nd system 0.36965 0.09851 0.13509 
			cosine + JWe 
			0.35905 0.10161 0.13701 
			NR 0.36207 0.10042 0.13781 
			SMMR 0.36323 0.10223 0.13886 
			1st system 0.37032 0.11189 0.14306 
			Worst human 0.40497 0.10511 0.14779 
			Table 1: ROUGE average recall scores computed 
			on the DUC 2007 update corpus. 
			2 Method 
			The underlying idea of our method is that as the 
			number of sentences in the history increases, the 
			likelihood to have redundant information within 
			candidate sentences also increases. We propose 
			a scalable sentence scoring method derived from 
			MMR that, as the size of the history increases, 
			gives more importance to non-redundancy that to 
			query relevance. We define H to represent the pre- 
			viously read documents (history), Q to represent 
			the query and s the candidate sentence. The fol- 
			lowing subsections formally define the similarity 
			measures and the scalable MMR scoring method. 
			2.1 A query-oriented multi-document 
			summarizer 
			We have first started by implementing a simple 
			summarizer for which the task is to produce query- 
			focused summaries from clusters of documents. 
			Each document is pre-processed: documents are 
			segmented into sentences, sentences are filtered 
			(words which do not carry meaning are removed 
			such as functional words or common words) and 
			normalized using a lemmas database (i.e. inflected 
			forms "go", "goes", "went", "gone"... are replaced 
			by "go"). An N-dimensional term-space , where 
			N is the number of different terms found in the 
			cluster, is constructed. Sentences are represented 
			in  by vectors in which each component is the 
			term frequency within the sentence. Sentence scor- 
			ing can be seen as a passage retrieval task in Infor- 
			mation Retrieval (IR). Each sentence s is scored by 
			computing a combination of two similarity mea- 
			sures between the sentence and the query. The first 
			measure is the well known cosine angle (Salton et 
			al., 1975) between the sentence and the query vec- 
			torial representations in  (denoted respectively s 
			and Q). The second similarity measure is based 
			on the Jaro-Winkler distance (Winkler, 1999). The 
			original Jaro-Winkler measure, denoted JW, uses 
			the number of matching characters and transposi- 
			tions to compute a similarity score between two 
			terms, giving more favourable ratings to terms that 
			match from the beginning. We have extended this 
			measure to calculate the similarity between the 
			sentence s and the query Q: 
			JWe(s, Q) = 
			1 
			|Q| 
			* 
			qQ 
			max 
			mS 
			JW(q, m) (1) 
			where S is the term set of s in which the terms 
			m that already have maximized JW(q, m) are re- 
			moved. The use of JWe smooths normalization and 
			misspelling errors. Each sentence s is scored using 
			the linear combination: 
			Sim1(s, Q) =  * cosine(s, Q) 
			+ (1 - ) * JWe(s, Q) (2) 
			where  = 0.7, optimally tuned on the past DUCs 
			data (2005 and 2006). The system produces a list 
			of ranked sentences from which the summary is 
			constructed by arranging the high scored sentences 
			until the desired size is reached. 
			2.2 A scalable MMR approach 
			MMR re-ranking algorithm has been successfully 
			used in query-oriented summarization (Ye et al., 
			2005). It strives to reduce redundancy while main- 
			taining query relevance in selected sentences. The 
			summary is constructed incrementally from a list 
			of ranked sentences, at each iteration the sentence 
			which maximizes MMR is chosen: 
			MMR = arg max 
			sS 
			[  * Sim1(s, Q) 
			- (1 - ) * max 
			sjE 
			Sim2(s, sj) ] (3) 
			where S is the set of candidates sentences and E 
			is the set of selected sentences.  represents an 
			interpolation coefficient between sentence's rele- 
			vance and non-redundancy. Sim2(s, sj) is a nor- 
			malized Longest Common Substring (LCS) mea- 
			sure between sentences s and sj. Detecting sen- 
			tence rehearsals, LCS is well adapted for redun- 
			dancy removal. 
			We propose an interpretation of MMR to tackle 
			the update summarization issue. Since Sim1 and 
			Sim2 are ranged in [0, 1], they can be seen as prob- 
			abilities even though they are not. Just as rewriting 
			(3) as (NR stands for Novelty Relevance): 
			NR = arg max 
			sS 
			[  * Sim1(s, Q) 
			+ (1 - ) * (1 - max 
			shH 
			Sim2(s, sh)) ] (4) 
			We can understand that (4) equates to an OR com- 
			bination. But as we are looking for a more intu- 
			itive AND and since the similarities are indepen- 
			dent, we have to use the product combination. The 
			24 
			scoring method defined in (2) is modified into a 
			double maximization criterion in which the best 
			ranked sentence will be the most relevant to the 
			query AND the most different to the sentences in 
			H. 
			SMMR(s) = Sim1(s, Q) 
			* 1 - max 
			shH 
			Sim2(s, sh) 
			f(H) 
			(5) 
			Decreasing  in (3) with the length of the sum- 
			mary was suggested by (Murray et al., 2005) and 
			successfully used in the DUC 2005 by (Hachey 
			et al., 2005), thereby emphasizing the relevance 
			at the outset but increasingly prioritizing redun- 
			dancy removal as the process continues. Sim- 
			ilarly, we propose to follow this assumption in 
			SMMR using a function denoted f that as the 
			amount of data in history increases, prioritize non- 
			redundancy (f(H)  0). 
			3 Experiments 
			The method described in the previous section has 
			been implemented and evaluated by using the 
			DUC 2007 update corpus2. The following subsec- 
			tions present details of the different experiments 
			we have conducted. 
			3.1 The DUC 2007 update corpus 
			We used for our experiments the DUC 2007 up- 
			date competition data set. The corpus is composed 
			of 10 topics, with 25 documents per topic. The up- 
			date task goal was to produce short (100 words) 
			multi-document update summaries of newswire ar- 
			ticles under the assumption that the user has al- 
			ready read a set of earlier articles. The purpose 
			of each update summary will be to inform the 
			reader of new information about a particular topic. 
			Given a DUC topic and its 3 document clusters: A 
			(10 documents), B (8 documents) and C (7 doc- 
			uments), the task is to create from the documents 
			three brief, fluent summaries that contribute to sat- 
			isfying the information need expressed in the topic 
			statement. 
			1. A summary of documents in cluster A. 
			2. An update summary of documents in B, un- 
			der the assumption that the reader has already 
			read documents in A. 
			2More information about the DUC 2007 corpus is avail- 
			able at http://duc.nist.gov/. 
			3. An update summary of documents in C, un- 
			der the assumption that the reader has already 
			read documents in A and B. 
			Within a topic, the document clusters must be pro- 
			cessed in chronological order. Our system gener- 
			ates a summary for each cluster by arranging the 
			high ranked sentences until the limit of 100 words 
			is reached. 
			3.2 Evaluation 
			Most existing automated evaluation methods work 
			by comparing the generated summaries to one or 
			more reference summaries (ideally, produced by 
			humans). To evaluate the quality of our generated 
			summaries, we choose to use the ROUGE3 (Lin, 
			2004) evaluation toolkit, that has been found to be 
			highly correlated with human judgments. ROUGE- 
			N is a n-gram recall measure calculated between 
			a candidate summary and a set of reference sum- 
			maries. In our experiments ROUGE-1, ROUGE-2 
			and ROUGE-SU4 will be computed. 
			3.3 Results 
			Table 1 reports the results obtained on the DUC 
			2007 update data set for different sentence scor- 
			ing methods. cosine + JWe stands for the scor- 
			ing method defined in (2) and NR improves it 
			with sentence re-ranking defined in equation (4). 
			SMMR is the combined adaptation we have pro- 
			posed in (5). The function f(H) used in SMMR is 
			the simple rational function 1 
			H 
			, where H increases 
			with the number of previous clusters (f(H) = 1 
			for cluster A, 1 
			2 
			for cluster B and 1 
			3 
			for cluster C). 
			This function allows to simply test the assumption 
			that non-redundancy have to be favoured as the 
			size of history grows. Baseline results are obtained 
			on summaries generated by taking the leading sen- 
			tences of the most recent documents of the cluster, 
			up to 100 words (official baseline of DUC). The 
			table also lists the three top performing systems at 
			DUC 2007 and the lowest scored human reference. 
			As we can see from these results, SMMR out- 
			performs the other sentence scoring methods. By 
			ways of comparison our system would have been 
			ranked second at the DUC 2007 update competi- 
			tion. Moreover, no post-processing was applied to 
			the selected sentences leaving an important margin 
			of progress. Another interesting result is the high 
			performance of the non-update specific method 
			(cosine + JWe) that could be due to the small size 
			3ROUGE is available at http://haydn.isi.edu/ROUGE/. 
			25 
			of the corpus (little redundancy between clusters). 
			ROUGE-1 ROUGE-2 ROUGE-SU4 
			Baseline 0.26232 0.04543 0.08247 
			3rd system 0.35715 0.09622 0.13245 
			2nd system 0.36965 0.09851 0.13509 
			cosine + JWe 
			0.35905 0.10161 0.13701 
			NR 0.36207 0.10042 0.13781 
			SMMR 0.36323 0.10223 0.13886 
			1st system 0.37032 0.11189 0.14306 
			Worst human 0.40497 0.10511 0.14779 
			Table 1: ROUGE average recall scores computed 
			on the DUC 2007 update corpus. 
			2 Single-Document Summarization 
			Usually, the flow of information in a given document is not uniform, which means 
			that some parts are more important than others. The major challenge in summa- 
			rization lies in distinguishing the more informative parts of a document from the 
			less ones. Though there have been instances of research describing the automatic 
			creation of abstracts, most work presented in the literature relies on verbatim ex- 
			traction of sentences to address the problem of single-document summarization. In 
			1See http://trec.nist.gov/. 
			2See http://duc.nist.gov/. 
			3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/. 
			muc 7 toc.html 
			2 
			this section, we describe some eminent extractive techniques. First, we look at early 
			work from the 1950s and 60s that kicked off research on summarization. Second, 
			we concentrate on approaches involving machine learning techniques published in 
			the 1990s to today. Finally, we briefly describe some techniques that use a more 
			complex natural language analysis to tackle the problem. 
			2.1 Early Work 
			Most early work on single-document summarization focused on technical documents. 
			Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de- 
			scribes research done at IBM in the 1950s. In his work, Luhn proposed that the 
			frequency of a particular word in an article provides an useful measure of its sig- 
			nificance. There are several key ideas put forward in this paper that have assumed 
			importance in later work on summarization. As a first step, words were stemmed to 
			their root forms, and stop words were deleted. Luhn then compiled a list of content 
			words sorted by decreasing frequency, the index providing a significance measure of 
			the word. On a sentence level, a significance factor was derived that reflects the 
			number of occurrences of significant words within a sentence, and the linear distance 
			between them due to the intervention of non-significant words. All sentences are 
			ranked in order of their significance factor, and the top ranking sentences are finally 
			selected to form the auto-abstract. 
			Related work (Baxendale, 1958), also done at IBM and published in the same 
			journal, provides early insight on a particular feature helpful in finding salient parts 
			of documents: the sentence position. Towards this goal, the author examined 200 
			paragraphs to find that in 85% of the paragraphs the topic sentence came as the first 
			one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate 
			way to select a topic sentence would be to choose one of these two. This positional 
			feature has since been used in many complex machine learning based systems. 
			Edmundson (1969) describes a system that produces document extracts. His 
			primary contribution was the development of a typical structure for an extractive 
			summarization experiment. At first, the author developed a protocol for creating 
			manual extracts, that was applied in a set of 400 technical documents. The two 
			features of word frequency and positional importance were incorporated from the 
			previous two works. Two other features were used: the presence of cue words 
			(presence of words like significant, or hardly), and the skeleton of the document 
			(whether the sentence is a title or heading). Weights were attached to each of these 
			features manually to score each sentence. During evaluation, it was found that about 
			44% of the auto-extracts matched the manual extracts. 
			2.2 Machine Learning Methods 
			In the 1990s, with the advent of machine learning techniques in NLP, a series of semi- 
			nal publications appeared that employed statistical techniques to produce document 
			extracts. While initially most systems assumed feature independence and relied on 
			naive-Bayes methods, others have focused on the choice of appropriate features and 
			3 
			on learning algorithms that make no independence assumptions. Other significant 
			approaches involved hidden Markov models and log-linear models to improve ex- 
			tractive summarization. A very recent paper, in contrast, used neural networks and 
			third party features (like common words in search engine queries) to improve purely 
			extractive single document summarization. We next describe all these approaches 
			in more detail. 
			2.2.1 Naive-Bayes Methods 
			Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able 
			to learn from data. The classification function categorizes each sentence as worthy 
			of extraction or not, using a naive-Bayes classifier. Let s be a particular sentence, 
			S the set of sentences that make up the summary, and F1, . . . , Fk the features. 
			Assuming independence of the features: 
			P(s  S | F1, F2, ..Fk) = 
			k 
			i=1 
			P(Fi | s  S) * P(s  S) 
			k 
			i=1 
			P(Fi) 
			(1) 
			The features were compliant to (Edmundson, 1969), but additionally included the 
			sentence length and the presence of uppercase words. Each sentence was given a 
			score according to (1), and only the n top sentences were extracted. To evaluate 
			the system, a corpus of technical documents with manual abstracts was used in 
			the following way: for each sentence in the manual abstract, the authors manually 
			analyzed its match with the actual document sentences and created a mapping 
			(e.g. exact match with a sentence, matching a join of two sentences, not matchable, 
			etc.). The auto-extracts were then evaluated against this mapping. Feature analysis 
			revealed that a system using only the position and the cue features, along with the 
			sentence length sentence feature, performed best. 
			Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richer 
			features. They describe a system called DimSum that made use of features like 
			term frequency (tf ) and inverse document frequency (idf) to derive signature words.4 
			The idf was computed from a large corpus of the same domain as the concerned 
			documents. Statistically derived two-noun word collocations were used as units for 
			counting, along with single words. A named-entity tagger was used and each entity 
			was considered as a single token. They also employed some shallow discourse analysis 
			like reference to same entities in the text, maintaining cohesion. The references 
			were resolved at a very shallow level by linking name aliases within a document 
			like "U.S." to "United States", or "IBM" for "International Business Machines". 
			Synonyms and morphological variants were also merged while considering lexical 
			terms, the former being identified by using Wordnet (Miller, 1995). The corpora 
			used in the experiments were from newswire, some of which belonged to the TREC 
			evaluations. 
			4Words that indicate key concepts in a document. 
			4 
			2.2.2 Rich Features and Decision Trees 
			Lin and Hovy (1997) studied the importance of a single feature, sentence position. 
			Just weighing a sentence by its position in text, which the authors term as the 
			"position method", arises from the idea that texts generally follow a predictable 
			discourse structure, and that the sentences of greater topic centrality tend to occur in 
			certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse 
			structure significantly varies over domains, the position method cannot be defined 
			as naively as in (Baxendale, 1958). The paper makes an important contribution by 
			investigating techniques of tailoring the position method towards optimality over a 
			genre and how it can be evaluated for effectiveness. A newswire corpus was used, the 
			collection of Ziff-Davis texts produced from the TIPSTER5 program; it consists of 
			text about computer and related hardware, accompanied by a set of key topic words 
			and a small abstract of six sentences. For each document in the corpus, the authors 
			measured the yield of each sentence position against the topic keywords. They then 
			ranked the sentence positions by their average yield to produce the Optimal Position 
			Policy (OPP) for topic positions for the genre. 
			Two kinds of evaluation were performed. Previously unseen text was used for 
			testing whether the same procedure would work in a different domain. The first 
			evaluation showed contours exactly like the training documents. In the second eval- 
			uation, word overlap of manual abstracts with the extracted sentences was measured. 
			Windows in abstracts were compared with windows on the selected sentences and 
			corresponding precision and recall values were measured. A high degree of coverage 
			indicated the effectiveness of the position method. 
			In later work, Lin (1999) broke away from the assumption that features are 
			independent of each other and tried to model the problem of sentence extraction 
			using decision trees, instead of a naive-Bayes classifier. He examined a lot of fea- 
			tures and their effect on sentence extraction. The data used in this work is a 
			publicly available collection of texts, classified into various topics, provided by the 
			TIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems. 
			The dataset contains essential text fragments (phrases, clauses, and sentences) which 
			must be included in summaries to answer some TREC topics. These fragments were 
			each evaluated by a human judge. The experiments described in the paper are with 
			the SUMMARIST system developed at the University of Southern California. The 
			system extracted sentences from the documents and those were matched against 
			human extracts, like most early work on extractive summarization. 
			Some novel features were the query signature (normalized score given to sen- 
			tences depending on number of query words that they contain), IR signature (the 
			m most salient words in the corpus, similar to the signature words of (Aone et al., 
			1999)), numerical data (boolean value 1 given to sentences that contained a num- 
			ber in them), proper name (boolean value 1 given to sentences that contained a 
			proper name in them), pronoun or adjective (boolean value 1 given to sentences 
			5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. 
			6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html. 
			5 
			that contained a pronoun or adjective in them), weekday or month (similar as pre- 
			vious feature) and quotation (similar as previous feature). It is worth noting that 
			some features like the query signature are question-oriented because of the setting 
			of the evaluation, unlike a generalized summarization framework. 
			The author experimented with various baselines, like using only the positional 
			feature, or using a simple combination of all features by adding their values. When 
			evaluated by matching machine extracted and human extracted sentences, the deci- 
			sion tree classifier was clearly the winner for the whole dataset, but for three topics, 
			a naive combination of features beat it. Lin conjectured that this happened because 
			some of the features were independent of each other. Feature analysis suggested 
			that the IR signature was a valuable feature, corroborating the early findings of 
			Luhn (1958). 
			2.2.3 Hidden Markov Models 
			In contrast with previous approaches, that were mostly feature-based and non- 
			sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentence 
			from a document using a hidden Markov model (HMM). The basic motivation for 
			using a sequential model is to account for local dependencies between sentences. 
			Only three features were used: position of the sentence in the document (built into 
			the state structure of the HMM), number of terms in the sentence, and likeliness of 
			the sentence terms given the document terms. 
			no 3 
			2 
			1 no 
			no 
			no 
			Figure 1: Markov model to extract to three summary sentences from a document 
			(Conroy and O'leary, 2001). 
			The HMM was structured as follows: it contained 2s + 1 states, alternating be- 
			tween s summary states and s+1 nonsummary states. The authors allowed "hesita- 
			tion" only in nonsummary states and "skipping next state" only in summary states. 
			Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the 
			TREC dataset as training corpus, the authors obtained the maximum-likelihood 
			estimate for each transition probability, forming the transition matrix estimate  
			M, 
			whose element (i, j) is the empirical probability of transitioning from state i to j. 
			Associated with each state i was an output function, bi(O) = Pr(O | state i) where 
			O is an observed vector of features. They made a simplifying assumption that the 
			features are multivariate normal. The output function for each state was thus esti- 
			mated by using the training data to compute the maximum likelihood estimate of 
			its mean and covariance matrix. They estimated 2s+1 means, but assumed that all 
			of the output functions shared a common covariance matrix. Evaluation was done 
			6 
			by comparing with human generated extracts. 
			2.2.4 Log-Linear Models 
			Osborne (2002) claims that existing approaches to summarization have always as- 
			sumed feature independence. The author used log-linear models to obviate this 
			assumption and showed empirically that the system produced better extracts than 
			a naive-Bayes model, with a prior appended to both models. Let c be a label, s 
			the item we are interested in labeling, fi the i-th feature, and i the corresponding 
			feature weight. The conditional log-linear model used by Osborne (2002) can be 
			stated as follows: 
			P(c | s) = 
			1 
			Z(s) 
			exp 
			i 
			ifi(c, s) , (2) 
			where Z(s) = c 
			exp ( i 
			ifi(c, s)). In this domain, there are only two possible 
			labels: either the sentence is to be extracted or it is not. The weights were trained 
			by conjugate gradient descent. The authors added a non-uniform prior to the model, 
			claiming that a log-linear model tends to reject too many sentences for inclusion in 
			a summary. The same prior was also added to a naive-Bayes model for comparison. 
			The classification took place as follows: 
			label(s) = arg max 
			cC 
			P(c) * P(s, c) = arg max 
			cC 
			log P(c) + 
			i 
			ifi(c, s) . (3) 
			The authors optimized the prior using the f2 score of the classifier as an objective 
			function on a part of the dataset (in the technical domain). The summaries were 
			evaluated using the standard f2 score where f2 = 2pr 
			p+r 
			, where the precision and recall 
			measures were measured against human generated extracts. The features included 
			word pairs (pairs of words with all words truncated to ten characters), sentence 
			length, sentence position, and naive discourse features like inside introduction or 
			inside conclusion. With respect to f2 score, the log-linear model outperformed the 
			naive-Bayes classifier with the prior, exhibiting the former's effectiveness. 
			2.2.5 Neural Networks and Third Party Features 
			In 2001-02, DUC issued a task of creating a 100-word summary of a single news 
			article. However, the best performing systems in the evaluations could not outper- 
			form the baseline with statistical significance. This extremely strong baseline has 
			been analyzed by Nenkova (2005) and corresponds to the selection of the first n 
			sentences of a newswire article. This surprising result has been attributed to the 
			journalistic convention of putting the most important part of an article in the initial 
			paragraphs. After 2002, the task of single-document summarization for newswire 
			was dropped from DUC. Svore et al. (2007) propose an algorithm based on neu- 
			ral nets and the use of third party datasets to tackle the problem of extractive 
			summarization, outperforming the baseline with statistical significance. 
			7 
			The authors used a dataset containing 1365 documents gathered from CNN.com, 
			each consisting of the title, timestamp, three or four human generated story high- 
			lights and the article text. They considered the task of creating three machine 
			highlights. The human generated highlights were not verbatim extractions from the 
			article itself. The authors evaluated their system using two metrics: the first one 
			concatenated the three highlights produced by the system, concatenated the three 
			human generated highlights, and compared these two blocks; the second metric con- 
			sidered the ordering and compared the sentences on an individual level. 
			Svore et al. (2007) trained a model from the labels and the features for each 
			sentence of an article, that could infer the proper ranking of sentences in a test 
			document. The ranking was accomplished using RankNet (Burges et al., 2005), a 
			pair-based neural network algorithm designed to rank a set of inputs that uses the 
			gradient descent method for training. For the training set, they used ROUGE-1 
			(Lin, 2004) to score the similarity of a human written highlight and a sentence 
			in the document. These similarity scores were used as soft labels during training, 
			contrasting with other approaches where sentences are "hard-labeled", as selected 
			or not. 
			Some of the used features based on position or n-grams frequencies have been 
			observed in previous work. However, the novelty of the framework lay in the use 
			of features that derived information from query logs from Microsoft's news search 
			engine7 and Wikipedia8 entries. The authors conjecture that if a document sentence 
			contained keywords used in the news search engine, or entities found in Wikipedia 
			articles, then there is a greater chance of having that sentence in the highlight. The 
			extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically 
			significant improvements over the baseline of selecting the first three sentences in a 
			document. 
			2.3 Deep Natural Language Analysis Methods 
			In this subsection, we describe a set of papers that detail approaches towards single- 
			document summarization involving complex natural language analysis techniques. 
			None of these papers solve the problem using machine learning, but rather use a set 
			of heuristics to create document extracts. Most of these techniques try to model the 
			text's discourse structure. 
			Barzilay and Elhadad (1997) describe a work that used considerable amount of 
			linguistic analysis for performing the task of summarization. For a better under- 
			standing of their method, we need to define a lexical chain: it is a sequence of related 
			words in a text, spanning short (adjacent words or sentences) or long distances (en- 
			tire text). The authors' method progressed with the following steps: segmentation 
			of the text, identification of lexical chains, and using strong lexical chains to identify 
			the sentences worthy of extraction. They tried to reach a middle ground between 
			(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep 
			7See http://search.live.com/news. 
			8See http://en.wikipedia.org. 
			8 
			semantic structure of the text, while the latter relied on word statistics of the doc- 
			uments. The authors describe the notion of cohesion in text as a means of sticking 
			together different parts of the text. Lexical cohesion is a notable example where 
			semantically related words are used. For example, let us take a look at the following 
			sentence.9 
			John bought a Jag. He loves the car. (4) 
			Here, the word car refers to the word Jag in the previous sentence, and exemplifies 
			lexical cohesion. The phenomenon of cohesion occurs not only at the word level, 
			but at word sequences too, resulting in lexical chains, which the authors used as 
			a source representation for summarization. Semantically related words and word 
			sequences were identified in the document, and several chains were extracted, that 
			form a representation of the document. To find out lexical chains, the authors used 
			Wordnet (Miller, 1995), applying three generic steps: 
			1. Selecting a set of candidate words. 
			2. For each candidate word, finding an appropriate chain relying on a relatedness 
			criterion among members of the chains, 
			3. If it is found, inserting the word in the chain and updating it accordingly. 
			The relatedness was measured in terms of Wordnet distance. Simple nouns and 
			noun compounds were used as starting point to find the set of candidates. In the 
			final steps, strong lexical chains were used to create the summaries. The chains were 
			scored by their length and homogeneity. Then the authors used a few heuristics to 
			select the significant sentences. 
			In another paper, Ono et al. (1994) put forward a computational model of dis- 
			course for Japanese expository writings, where they elaborate a practical procedure 
			for extracting the discourse rhetorical structure, a binary tree representing relations 
			between chunks of sentences (rhetorical structure trees are used more intensively in 
			(Marcu, 1998a), as we will see below). This structure was extracted using a series 
			of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can- 
			didate generation and preference judgement. Evaluation was based on the relative 
			importance of rhetorical relations. In the following step, the nodes of the rhetori- 
			cal structure tree were pruned to reduce the sentence, keeping its important parts. 
			Same was done for paragraphs to finally produce the summary. Evaluation was done 
			with respect to sentence coverage and 30 editorial articles of a Japanese newspaper 
			were used as the dataset. The articles had corresponding sets of key sentences and 
			most important key sentences judged by human subjects. The key sentence coverage 
			was about 51% and the most important key sentence coverage was 74%, indicating 
			encouraging results. 
			Marcu (1998a) describes a unique approach towards summarization that, unlike 
			most other previous work, does not assume that the sentences in a document form 
			a flat sequence. This paper used discourse based heuristics with the traditional 
			9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html. 
			9 
			features that have been used in the summarization literature. The discourse theory 
			used in this paper is the Rhetorical Structure Theory (RST) that holds between 
			two non-overlapping pieces of text spans: the nucleus and the satellite. The author 
			mentions that the distinction between nuclei and satellites comes from the empir- 
			ical observation that the nucleus expresses what is more essential to the writer's 
			purpose than the satellite; and that the nucleus of a rhetorical relation is compre- 
			hensible independent of the satellite, but not vice versa. Marcu (1998b) describes 
			the details of a rhetorical parser producing a discourse tree. Figure 2 shows an 
			example discourse tree for a text example detailed in the paper. Once such a dis- 
			Antithesis 
			2 
			Elaboration 
			Elaboration 
			2 
			2 
			Elaboration 
			3 
			Justification 
			8 
			Exemplification 
			1 2 3 4 5 7 8 
			4 5 
			8 10 
			9 10 
			5 6 
			Contrast 
			Evidence 
			Concession 
			Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the 
			nodes denote sentence numbers from the text example. The text below the number 
			in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and 
			the normals ones are the NUCLEI. 
			course structure is created, a partial ordering of important units can be developed 
			from the tree. Each equivalence class in the partial ordering is derived from the 
			new sentences at a particular level of the discourse tree. In Figure 2, we observe 
			that sentence 2 is at the root, followed by sentence 8 in the second level. In the 
			third level, sentence 3 and 10 are observed, and so forth. The equivalence classes 
			are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6. 
			If it is specified that the summary should contain the top k% of the text, the first 
			k% of the units in the partial ordering can be selected to produce the summary. The 
			author talks about a summarization system based just on this method in (Marcu, 
			1998b) and in one of his earlier papers. In this paper, he merged the discourse 
			based heuristics with traditional heuristics. The metrics used were clustering based 
			10 
			metric (each node in the discourse tree was assigned a cluster score; for leaves the 
			score was 0, for the internal nodes it was given by the similarity of the immediate 
			children; discourse tree A was chosen to be better than B if its clustering score 
			was higher), marker based metric (a discourse structure A was chosen to be better 
			than a discourse structure B if A used more rhetorical relations than B), rhetorical 
			clustering based technique (measured the similarity between salient units of two text 
			spans), shape based metric (preferred a discourse tree A over B if A was more skewed 
			towards the right than B), title based metric, position based metric, connectedness 
			based metric (cosine similarity of an unit to all other text units, a discourse structure 
			A was chosen to be better than B if its connectedness measure was more than B). 
			A weighted linear combination of all these scores gave the score of a discourse 
			structure. To find the best combination of heuristics, the author computed the 
			weights that maximized the F-score on the training dataset, which was constituted 
			by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al., 
			1992) that performed a greedy search in a seven dimensional space of the metrics. 
			For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved 
			for the 10% summaries which was 3.5% higher than a baseline lead based algorithm, 
			which was very encouraging. 
			3 Multi-Document Summarization 
			Extraction of a single summary from multiple documents has gained interest since 
			mid 1990s, most applications being in the domain of news articles. Several Web- 
			based news clustering systems were inspired by research on multi-document summa- 
			rization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12 
			This departs from single-document summarization since the problem involves mul- 
			tiple sources of information that overlap and supplement each other, being contra- 
			dictory at occasions. So the key tasks are not only identifying and coping with 
			redundancy across documents, but also recognizing novelty and ensuring that the 
			final summary is both coherent and complete. 
			The field seems to have been pioneered by the NLP group at Columbia University 
			(McKeown and Radev, 1995), where a summarization system called SUMMONS13 
			was developed by extending already existing technology for template-driven message 
			understanding systems. Although in that early stage multi-document summariza- 
			tion was mainly seen as a task requiring substantial capabilities of both language 
			interpretation and generation, it later gained autonomy, as people coming from dif- 
			ferent communities added new perspectives to the problem. Extractive techniques 
			have been applied, making use of similarity measures between pairs of sentences. 
			Approaches vary on how these similarities are used: some identify common themes 
			through clustering and then select one sentence to represent each cluster (McKeown 
			10See http://news.google.com. 
			11See http://newsblaster.cs.columbia.edu. 
			12See http://NewsInEssence.com. 
			13SUMMarizing Online NewS articles. 
			11 
			et al., 1999; Radev et al., 2000), others generate a composite sentence from each 
			cluster (Barzilay et al., 1999), while some approaches work dynamically by includ- 
			ing each candidate passage only if it is considered novel with respect to the previous 
			included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998). 
			Some recent work extends multi-document summarization to multilingual environ- 
			ments (Evans, 2005). 
			The way the problem is posed has also varied over time. While in some pub- 
			lications it is claimed that extractive techniques would not be effective for multi- 
			document summarization (McKeown and Radev, 1995; McKeown et al., 1999), some 
			years later that claim was overturned, as extractive systems like MEAD14 (Radev 
			et al., 2000) achieved good performance in large scale summarization of news arti- 
			cles. This can be explained by the fact that summarization systems often distinguish 
			among themselves about what their goal actually is. While some systems, like SUM- 
			MONS, are designed to work in strict domains, aiming to build a sort of briefing 
			that highlights differences and updates accross different news reports, putting much 
			emphasis on how information is presented to the user, others, like MEAD, are large 
			scale systems that intend to work in general domains, being more concerned with 
			information content rather than form. Consequently, systems of the former kind re- 
			quire a strong effort on language generation to produce a grammatical and coherent 
			summary, while latter systems are probably more close to the information retrieval 
			paradigm. Abstractive systems like SUMMONS are difficult to replicate, as they 
			heavily rely on the adaptation of internal tools to perform information extraction 
			and language generation. On the other hand, extractive systems are generally easy 
			to implement from scratch, and this makes them appealing when sophisticated NLP 
			tools are not available. 
			3.1 Abstraction and Information Fusion 
			As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown, 
			1998) is the first historical example of a multi-document summarization system. It 
			tackles single events about a narrow domain (news articles about terrorism) and 
			produces a briefing merging relevant information about each event and how reports 
			by different news agencies have evolved over time. The whole thread of reports is 
			then presented, as illustrated in the following example of a "good" summary: 
			"In the afternoon of February 26, 1993, Reuters reported that a suspect 
			bomb killed at least five people in the World Trade Center. However, 
			Associated Press announced that exactly five people were killed in the 
			blast. Finally, Associated Press announced that Arab terrorists were 
			possibly responsible for the terrorist act." 
			Rather than working with raw text, SUMMONS reads a database previously 
			built by a template-based message understanding system. A full multi-document 
			14Available for download at http://www.summarization.com/mead/. 
			12 
			summarizer is built by concatenating the two systems, first processing full text as 
			input and filling template slots, and then synthesizing a summary from the extracted 
			information. The architecture of SUMMONS consists of two major components: a 
			content planner that selects the information to include in the summary through 
			combination of the input templates, and a linguistic generator that selects the right 
			words to express the information in grammatical and coherent text. The latter 
			component was devised by adapting existing language generation tools, namely the 
			FUF/SURGE system15. Content planning, on the other hand, is made through 
			summary operators, a set of heuristic rules that perform operations like "change of 
			perspective", "contradiction", "refinement", etc. Some of these operations require 
			resolving conflicts, i.e., contradictory information among different sources or time 
			instants; others complete pieces of information that are included in some articles 
			and not in others, combining them into a single template. At the end, the linguis- 
			tic generator gathers all the combined information and uses connective phrases to 
			synthesize a summary. 
			While this framework seems promising when the domain is narrow enough so that 
			the templates can be designed by hand, a generalization for broader domains would 
			be problematic. This was improved later by McKeown et al. (1999) and Barzilay 
			et al. (1999), where the input is now a set of related documents in raw text, like 
			those retrieved by a standard search engine in response to a query. The system starts 
			by identifying themes, i.e., sets of similar text units (usually paragraphs). This is 
			formulated as a clustering problem. To compute a similarity measure between text 
			units, these are mapped to vectors of features, that include single words weighted 
			by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet 
			database and a database of semantic classes of verbs. For each pair of paragraphs, a 
			vector is computed that represents matches on the different features. Decision rules 
			that were learned from data are then used to classify each pair of text units either 
			as similar or dissimilar; this in turn feeds a subsequent algorithm that places the 
			most related paragraphs in the same theme. 
			Once themes are identified, the system enters its second stage: information fu- 
			sion. The goal is to decide which sentences of a theme should be included in the 
			summary. Rather than just picking a sentence that is a group representative, the 
			authors propose an algorithm which compares and intersects predicate argument 
			structures of the phrases within each theme to determine which are repeated often 
			enough to be included in the summary. This is done as follows: first, sentences are 
			parsed through Collins' statistical parser (Collins, 1999) and converted into depen- 
			dency trees, which allows capturing the predicate-argument structure and identify 
			functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence 
			representation. 
			The comparison algorithm then traverses these dependency trees recursively, 
			adding identical nodes to the output tree. Once full phrases (a verb with at least 
			two constituents) are found, they are marked to be included in the summary. If two 
			15FUF, SURGE, and other tools developed by the Columbia NLP group are available at 
			http://www1.cs.columbia.edu/nlp/tools.cgi. 
			13 
			and Kan 1998]. We match two verbs that share the same 
			semantic class in this classifi cation. 
			In addition to the above primitive features that all com- 
			pare single items from each text unit, we use composite fea- 
			tures that combine pairs of primitive features. Our compos- 
			ite features impose particular constraints on the order of the 
			two elements in the pair, on the maximum distance between 
			the two elements, and on the syntactic classes that the two 
			elements come from. They can vary from a simple com- 
			bination (e.g., "two text units must share two words to be 
			similar") to complex cases with many conditions (e.g., "two 
			text units must have matching noun phrases that appear in 
			the same order and with relative difference in position no 
			more than fi ve"). In this manner, we capture information 
			on how similarly related elements are spaced out in the two 
			text units, as well as syntactic information on word combi- 
			nations. Matches on composite features indicate combined 
			evidence for the similarity of the two units. 
			To determine whether the units match overall, we employ 
			a machine learning algorithm [Cohen 1996] that induces de- 
			cision rules using the features that really make a difference. 
			A set of pairs of units already marked as similar or not by a 
			human is used for training the classifi er. We have manually 
			marked a set of 8,225 paragraph comparisons from the TDT 
			corpus for training and evaluating our similarity classifi er. 
			For comparison, we also use an implementation of the 
			TF*IDF method which is standard for matching texts in in- 
			formation retrieval. We compute the total frequency (TF) of 
			words in each text unit and the number of units in our train- 
			ing set each word appears in (DF, or document frequency). 
			Then each text unit is represented as a vector of TF*IDF 
			scores, calculated as 
			TF(wordi 
			) * log Total number of units 
			DF(wordi 
			) 
			Similarity between text units is measured by the cosine of 
			the angle between the corresponding two vectors (i.e., the 
			normalized inner product of the two vectors), and the opti- 
			mal value of a threshold for judging two units as similar is 
			computed from the training set. 
			After all pairwise similarities between text units have 
			been calculated, we utilize a clustering algorithm to iden- 
			tify themes. As a paragraph may belong to multiple themes, 
			most standard clustering algorithms, which partition their 
			input set, are not suitable for our task. We use a greedy, 
			one-pass algorithm that fi rst constructs groups from the most 
			similar paragraphs, seeding the groups with the fully con- 
			nected subcomponents of the graph that the similarity rela- 
			tionship induces over the set of paragraphs, and then places 
			additional paragraphs within a group if the fraction of the 
			members of the group they are similar to exceeds a preset 
			threshold. 
			Language Generation 
			Given a group of similar paragraphs--a theme--the prob- 
			lem is to create a concise and fluent fusion of information in 
			this theme, reflecting facts common to all paragraphs. A 
			straightforward method would be to pick a representative 
			subject 
			class: noun 
			27 
			class: cardinal 
			bombing 
			class: noun 
			McVeigh with 
			class: preposition 
			definite: yes 
			charge 
			class: verb voice :passive 
			polarity: + 
			tense: past 
			Figure 4: Dependency grammar representation of the sen- 
			tence "McVeigh, 27, was charged with the bombing". 
			sentence that meets some criteria (e.g., a threshold number 
			of common content words). In practice, however, any repre- 
			sentative sentence will usually include embedded phrase(s) 
			containing information that is not common to all sentences 
			in the theme. Furthermore, other sentences in the theme of- 
			ten contain additional information not presented in the rep- 
			resentative sentence. Our approach, therefore, uses inter- 
			section among theme sentences to identify phrases common 
			to most paragraphs and then generates a new sentence from 
			identifi ed phrases. 
			Intersection among Theme Sentences 
			Intersection is carried out in the content planner, which uses 
			a parser for interpreting the input sentences, with our new 
			work focusing on the comparison of phrases. Theme sen- 
			tences are fi rst run through a statistical parser[Collins 1996] 
			and then, in order to identify functional roles (e.g., subject, 
			object), are converted to a dependency grammar representa- 
			tion [Kittredge and Mel' 
			cuk 1983], which makes predicate- 
			argument structure explicit. 
			We developed a rule-based component to produce func- 
			tional roles, which transforms the phrase-structure output of 
			Collins' parser to dependency grammar; function words (de- 
			terminers and auxiliaries) are eliminated from the tree and 
			corresponding syntactic features are updated. An example 
			of a theme sentence and its dependency grammar represen- 
			tation are shown in Figure 4. Each non-auxiliary word in the 
			sentence has a node in the representation, and this node is 
			connected to its direct dependents. 
			The comparison algorithm starts with all subtrees rooted 
			at verbs from the input dependency structure, and traverses 
			them recursively: if two nodes are identical, they are added 
			to the output tree, and their children are compared. Once 
			a full phrase (verb with at least two constituents) has been 
			found, it is confi rmed for inclusion in the summary. 
			Diffi culties arise when two nodes are not identical, but are 
			similar. Such phrases may be paraphrases of each other and 
			still convey essentially the same information. Since theme 
			sentences are a priori close semantically, this signifi cantly 
			Figure 3: Dependency tree representing the sentence "McVeigh, 27, was charged 
			with the bombing" (extracted from (McKeown et al., 1999)). 
			phrases, rooted at some node, are not identical but yet similar, the hypothesis that 
			they are paraphrases of each other is considered; to take this into account, corpus- 
			driven paraphrasing rules are written to allow paraphrase intersection.16 Once the 
			summary content (represented as predicate-argument structures) is decided, a gram- 
			matical text is generated by translating those structures into the arguments expected 
			by the FUF/SURGE language generation system. 
			3.2 Topic-driven Summarization and MMR 
			Carbonell and Goldstein (1998) made a major contribution to topic-driven sum- 
			marization by introducing the maximal marginal relevance (MMR) measure. The 
			idea is to combine query relevance with information novelty; it may be applicable 
			in several tasks ranging from text retrieval to topic-driven summarization. MMR 
			simultaneously rewards relevant sentences and penalizes redundant ones by consid- 
			ering a linear combination of two similarity measures. 
			Let Q be a query or user profile and R a ranked list of documents retrieved by 
			a search engine. Consider an incremental procedure that selects documents, one at 
			a time, and adds them to a set S. So let S be the set of already selected documents 
			in a particular step, and R \ S the set of yet unselected documents in R. For each 
			candidate document Di  R \ S, its marginal relevance MR(Di) is computed as: 
			MR(Di) := Sim1(Di, Q) - (1 - ) max 
			DjS 
			Sim2(Di, Dj) (5) 
			where  is a parameter lying in [0, 1] that controls the relative importance given 
			to relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the 
			16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al., 
			1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization 
			in different syntactic categories (e.g. classifier vs. apposition), change in grammatical features 
			(active/passive, time, number, etc.), head omission, transformation from one POS to another, 
			using semantically related words (e.g. synonyms), etc. 
			14 
			experiments both were set to the standard cosine similarity traditionally used in the 
			vector space model, Sim1(x, y) = Sim2(x, y) = x,y 
			x * y 
			. The document achieving the 
			highest marginal relevance, DMMR = arg maxDiR\S 
			MR(Di), is then selected, i.e., 
			added to S, and the procedure continues until a maximum number of documents 
			are selected or a minimum relevance threshold is attained. Carbonell and Goldstein 
			(1998) found experimentally that choosing dynamically the value of  turns out to be 
			more effective than keeping it fixed, namely starting with small values (  0.3) to 
			give more emphasis to novelty, and then increasing it (  0.7) to focus on the most 
			relevant documents. To perform summarization, documents can be first segmented 
			into sentences or paragraphs, and after a query is submitted, the MMR algorithm 
			can be applied followed by a selection of the top ranking passages, reordering them as 
			they appeared in the original documents, and presenting the result as the summary. 
			One of the attractive points in using MMR for summarization is its topic-oriented 
			feature, through its dependency on the query Q, which makes it particularly ap- 
			pealing to generate summaries according to a user profile: as the authors claim, "a 
			different user with different information needs may require a totally different sum- 
			mary of the same document." This assertion was not being taken into account by 
			previous multi-document summarization systems. 
			3.3 Graph Spreading Activation 
			Mani and Bloedorn (1997) describe an information extraction framework for sum- 
			marization, a graph-based method to find similarities and dissimilarities in pairs 
			of documents. Albeit no textual summary is generated, the summary content is 
			represented via entities (concepts) and relations that are displayed respectively as 
			nodes and edges of a graph. Rather than extracting sentences, they detect salient 
			regions of the graph via a spreading activation technique.17 
			This approach shares with the method described in Section 3.2 the property 
			of being topic-driven; there is an additional input that stands for the topic with 
			respect to which the summary is to be generated. The topic is represented through 
			a set of entry nodes in the graph. A document is represented as a graph as follows: 
			each node represents the occurrence of a single word (i.e., one word together with 
			its position in the text). Each node can have several kinds of links: adjacency 
			links (ADJ) to adjacent words in the text, SAME links to other occurrences of the 
			same word, and ALPHA links encoding semantic relationships captured through 
			Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of 
			adjacent nodes which belong to the same phrase, and NAME and COREF links 
			stand for co-referential name occurrences; Fig. 4 shows some of these links. 
			Once the graph is built, topic nodes are identified by stem comparison and be- 
			come the entry nodes. A search for semantically related text is then propagated from 
			these to the other nodes of the graph, in a process called spreading activation. Salient 
			17The name "spreading activation" is borrowed from a method used in information retrieval 
			(Salton and Buckley, 1988) to expand the search vocabulary. 
			18See http://www.netowl.com. 
			15 
			1.39: Aoki, the Japanese ambassador, said in telephone calls to 
			Fujimori. 
			Japanesebroadcaster NHK that the rebels wanted to talk directly to 
			1.43:According to some estimates, only a couple hundred armed 
			followers remain. 
			2.19 They are freeing u 
			not doing us any harm," 
			... 
			2.27:Although the MRTA 
			early days in the mid-198 
			give to the poor, it lost pu 
			turning increasingly to ki 
			billion in damage to the c 
			since 1980. 
			and drug activities. 2.28: 
			Peru have cost at least 3 
			... 
			close ties with Japan. 
			1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and 
			the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea, 
			... 
			... 
			... 
			2.26:The MRTA called T 
			"Breaking The Silence." 
			1.32: President Alberto Fujimori, who is of Japanese ancestry, has had 
			Germany, Austria and Venezuela. Hood-style movement tha 
			negotiations with the gov 
			dawn on Wednesday. 
			... 
			... 
			2.22:The attack was a ma 
			Fujimori's government, w 
			virtual victory in a 16-yea 
			rebels belonging to the M 
			and better-known Maoist 
			... 
			1.28:Many leaders of the Tupac Amaru which is smaller than Peru's 
			was captured in June 1992 and is serving a life sentence, as is his 
			us: `Don't lift your heads up or you will be shot." 
			1.19: 
			hostages," a rebel who did not give his name told a local radio station in 
			a telephone call from inside the compound. 
			"The guerillas stalked around the residence grounds threatening 
			lieutenant, Peter Cardenas. 
			1.25: "We are clear: the liberation of all our comrades, or we die with all the 
			1.30:Other top commanders conceded defeat July 1993. 
			and surrendered in 
			COREF 
			Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay, 
			ADJ 
			1.38:Fujimori whose sister was among the 
			an emergency cabinet meeting today. 
			hostages released, called 
			ALPHA 
			ADJ 
			, the rebels threatened to kill the remaining 
			captives. 
			1.24:Early Wednesday 
			Figure 5: Texts of two related articles. The top 5 salient sentences containing common 
			words in bold face; likewise, the top 5 salient sentences containing unique words have th 
			Figure 4: Examples of nodes and links in the graph for a particular sentence (detail 
			extracted from from a figure in (Mani and Bloedorn, 1997)). 
			words and phrases are initialized according to their TF-IDF score. The weight of 
			neighboring nodes depends on the node link traveled and is an exponentially decay- 
			ing function of the distance of the traversed path. Traveling within a sentence is 
			made cheaper than across sentence boundaries, which in turn is cheaper than across 
			paragraph boundaries. Given a pair of document graphs, common nodes are identi- 
			fied either by sharing the same stem or by being synonyms. Analogously, difference 
			nodes are those that are not common. For each sentence in both documents, two 
			scores are computed: one score that reflects the presence of common nodes, which 
			is computed as the average weight of these nodes; and another score that computes 
			instead the average weights of difference nodes. Both scores are computed after 
			spreading activation. In the end, the sentences that have higher common and dif- 
			ferent scores are highlighted, the user being able to specify the maximal number of 
			common and different sentences to control the output. In the future, the authors 
			expect to use these structure to actually compose abstractive summaries, rather 
			than just highlighting pieces of text. 
			3.4 Centroid-based Summarization 
			Although clustering techniques were already being employed by McKeown et al. 
			(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000) 
			pioneered the use of cluster centroids to play a central role in summarization. A full 
			description of the centroid-based approach that underlies the MEAD system can 
			be found in (Radev et al., 2004); here we sketch briefly the main points. Perhaps 
			the most appealing feature is the fact that it does not make use of any language 
			generation module, unlike most previous systems. All documents are modeled as 
			bags-of-words. The system is also easily scalable and domain-independent. 
			The first stage consists of topic detection, whose goal is to group together news 
			articles that describe the same event. To accomplish this task, an agglomerative 
			clustering algorithm is used that operates over the TF-IDF vector representations 
			of the documents, successively adding documents to clusters and recomputing the 
			16 
			centroids according to 
			cj = dCj 
			 
			d 
			|Cj| 
			(6) 
			where cj is the centroid of the j-th cluster, Cj is the set of documents that belong 
			to that cluster, its cardinality being |Cj|, and  
			d is a "truncated version" of d that 
			vanishes on those words whose TF-IDF scores are below a threshold. Centroids 
			can thus be regarded as pseudo-documents that include those words whose TF- 
			IDF scores are above a threshold in the documents that constitute the cluster. Each 
			event cluster is a collection of (typically 2 to 10) news articles from multiple sources, 
			chronologically ordered, describing an event as it develops over time. 
			The second stage uses the centroids to identify sentences in each cluster that 
			are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics 
			are defined that resemble the two summands in the MMR (see Section 3.2): cluster- 
			based relative utility (CBRU) and cross-sentence informational subsumption (CSIS). 
			The first accounts for how relevant a particular sentence is to the general topic of 
			the entire cluster; the second is a measure of redundancy among sentences. Unlike 
			MMR, these metrics are not query-dependent. Given one cluster C of documents 
			segmented into n sentences, and a compression rate R, a sequence of nR sentences 
			are extracted in the same order as they appear in the original documents, which in 
			turn are ordered chronologically. The selection of the sentences is made by approx- 
			imating their CBRU and CSIS.19 For each sentence si, three different features are 
			used: 
			* Its centroid value (Ci), defined as the sum of the centroid values of all the 
			words in the sentence, 
			* A positional value (Pi), that is used to make leading sentences more important. 
			Let Cmax be the centroid value of the highest ranked sentence in the document. 
			Then Pi = n-i+1 
			n 
			Cmax. 
			* The first-sentence overlap (Fi), defined as the inner product between the word 
			occurrence vector of sentence i and that of the first sentence of the document. 
			The final score of each sentence is a combination of the three scores above minus a 
			redundancy penalty (Rs) for each sentence that overlaps highly ranked sentences. 
			3.5 Multilingual Multi-document Summarization 
			Evans (2005) addresses the task of summarizing documents written in multiple 
			languages; this had already been sketched by Hovy and Lin (1999). Multilingual 
			summarization is still at an early stage, but this framework looks quite useful for 
			newswire applications that need to combine information from foreign news agen- 
			cies. Evans (2005) considered the scenario where there is a preferred language in 
			which the summary is to be written, and multiple documents in the preferred and 
			19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details). 
			17 
			in foreign languages are available. In their experiments, the preferred language was 
			English and the documents are news articles in English and Arabic. The rationale is 
			to summarize the English articles without discarding the information contained in 
			the Arabic documents. The IBM's statistical machine translation system is first ap- 
			plied to translate the Arabic documents to English. Then a search is made, for each 
			translated text unit, to see whether there is a similar sentence or not in the English 
			documents. If so, and if the sentence is found relevant enough to be included in the 
			summary, the similar English sentence is included instead of the Arabic-to-English 
			translation. This way, the final summary is more likely to be grammatical, since 
			machine translation is known to be far from perfect. On the other hand, the result 
			is also expected to have higher coverage than using just the English documents, 
			since the information contained in the Arabic documents can help to decide about 
			the relevance of each sentence. In order to measure similarity between sentences, a 
			tool named SimFinder20 was employed: this is a tool for clustering text based on 
			similarity over a variety of lexical and syntactic features using a log-linear regression 
			model. 
			4 Other Approaches to Summarization 
			This section describes briefly some unconventional approaches that, rather than 
			aiming to build full summarization systems, investigate some details that underlie 
			the summarization process, and that we conjecture to have a role to play in future 
			research on this field. 
			4.1 Short Summaries 
			Witbrock and Mittal (1999) claim that extractive summarization is not very pow- 
			erful in that the extracts are not concise enough when very short summaries are 
			required. They present a system that generated headline style summaries. The cor- 
			pus used in this work was newswire articles from Reuters and the Associated Press, 
			publicly available at the LDC21. The system learned statistical models of the rela- 
			tionship between source text units and headline units. It attempted to model both 
			the order and the likelihood of the appearance of tokens in the target documents. 
			Both the models, one for content selection and the other for surface realization were 
			used to co-constrain each other during the search in the summary generation task. 
			For content selection, the model learned a translation model between a docu- 
			ment and its summary (Brown et al., 1993). This model in the simplest case can be 
			thought as a mapping between a word in the document and the likelihood of some 
			word appearing in the summary. To simplify the model, the authors assumed that 
			the probability of a word appearing in a summary is independent of its structure. 
			This mapping boils down to the fact that the probability of a particular summary 
			20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder. 
			21See http://ldc.upenn.edu. 
			18 
			candidate is the product of the probabilities of the summary content and that con- 
			tent being expressed using a particular structure. 
			The surface realization model used was a bigram model. Viterbi beam search 
			was used to efficiently find a near-optimal summary. The Markov assumption was 
			violated by using backtracking at every state to strongly discourage paths that 
			repeated terms, since bigrams that start repeating often seem to pathologically 
			overwhelm the search otherwise. To evaluate the system, the authors compared 
			its output against the actual headlines for a set of input newswire stories. Since 
			phrasing could not be compared, they compared the generated headlines against 
			the actual headlines, as well as the top ranked summary sentence of the story. Since 
			the system did not have a mechanism to determine the optimal length of a headline, 
			six headlines for each story were generated, ranging in length from 4 to 10 words 
			and they measured the term-overlap between each of the generated headlines and 
			the test. For headline length 4, there was 0.89 overlap in the headline and there was 
			0.91 overlap amongst the top scored sentence, indicating useful results. 
			4.2 Sentence Compression 
			Knight and Marcu (2000) introduced a statistical approach to sentence compression. 
			The authors believe that understanding the simpler task of compressing a sentence 
			may be a fruitful first step to later tackle the problems of single and multi-document 
			summarization. 
			Sentence compression is defined as follows: given a sequence of words W = 
			w1w2 . . . wn that constitute a sentence, find a subsequence wi1 
			wi2 
			. . . wik 
			, with 
			1  i1 < i2 < . . . ik  n, that is a compressed version of W. Note that there 
			are 2n possibilities of output. Knight and Marcu (2000) considered two different 
			approaches: one that is inspired by the noisy-channel model, and another one based 
			on decision trees. Due to its simplicity and elegance, we describe the first approach 
			here. 
			The noisy-channel model considers that one starts with a short summary s, 
			drawn according to the source model P(s), which is then subject to channel noise to 
			become the full sentence t, in a process guided by the channel model P(t|s). When 
			the string t is observed, one wants to recover the original summary according to: 
			 
			s = arg max 
			s 
			P(s|t) = arg max 
			s 
			P(s)P(t|s). (7) 
			This model has the advantage of decoupling the goals of producing a short text that 
			looks grammatical (incorporated in the source model) and of preserving important 
			information (which is done through the channel model). In (Knight and Marcu, 
			2000), the source and channel models are simple models inspired by probabilistic 
			context-free grammars (PCFGs). The following probability mass functions are de- 
			fined over parse trees rather than strings: Ptree(s), the probability of a parse tree 
			that generates s, and Pexpand tree(t|s), the probability that a small parse tree that 
			generates s is expanded to a longer one that generates t. 
			19 
			The sentence t is first parsed by using Collins' parser (Collins, 1999). Then, 
			rather than computing Ptree(s) over all the 2n hypotheses for s, which would be 
			exponential in the sentence length, a shaded-forest structure is used: the parse 
			tree of t is traversed and the grammar (learned from the Penn Treebank22) is used 
			to check recursively which nodes may be removed from each production in order 
			to achieve another valid production. This algorithm allows to compute efficiently 
			Ptree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually, 
			the noisy channel model works the other way around: summaries are the original 
			strings that are expanded via expansion templates. Expansion operations have the 
			effect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and 
			Pexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram 
			distribution over the leaves of the tree (i.e. the words). In the end, the log probability 
			is (heuristically) divided by the length of the sentence s in order not to penalize 
			excessively longer sentences (this is done commonly in speech recognition). 
			More recently, Daum 
			e III and Marcu (2002) extended this approach to document 
			compression by using rhetorical structure theory as in Marcu (1998a), where the 
			entire document is represented as a tree, hence allowing not only to compress relevant 
			sentences, but also to drop irrelevant ones. In this framework, Daum 
			e III and Marcu 
			(2004) employed kernel methods to decide for each node in the tree whether or not 
			it should be kept. 
			4.3 Sequential document representation 
			We conclude this section by mentioning some recent work that concerns document 
			representation, with applications in summarization. In the bag-of-words representa- 
			tion (Salton et al., 1975) each document is represented as a sparse vector in a very 
			large Euclidean space, indexed by words in the vocabulary V . A well-known tech- 
			nique in information retrieval to capture word correlation is latent semantic indexing 
			(LSI), that aims to find a linear subspace of dimension k  |V | where documents 
			may be approximately represented by their projections. 
			These classical approaches assume by convenience that Euclidean geometry is 
			a proper model for text documents. As an alternative, Gous (1999) and Hall and 
			Hofmann (2000) used the framework of information geometry (Amari and Nagaoka, 
			2001) to generalize LSI to the multinomial manifold, which can be identified with 
			the probability simplex 
			Pn-1 = x  Rn | 
			n 
			i=1 
			xi = 1, xi  0 for i = 1, . . . , n . (8) 
			Instead of finding a linear subspace, as in the Euclidean case, they learn a subman- 
			ifold of Pn-1. To illustrate this idea, Gous (1999) split a book (Machiavelli's The 
			Prince) into several text blocks (its numbered pages), considered each page as a 
			point in P|V |-1, and projected data into a 2-dimensional submanifold. The result is 
			22See http://www.cis.upenn.edu/~treebank/. 
			20 
			the representation of the book as a sequential path in R2, tracking the evolution of 
			the subject matter of the book over the course of its pages (see Fig. 5). Inspired by 
			Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex- 
			tracted from (Gous, 1999)). The inflection around page 85 reflects a real change in 
			the subject matter, where the book shifts from political theory to a more biograph- 
			ical discourse. 
			this framework, Lebanon et al. (2007) suggested representing a document as a sim- 
			plicial curve (i.e. a curve in the probability simplex), yielding the locally weighted 
			bag-of-words (lowbow) model. According to this representation, a length-normalized 
			document is a function x : [0, 1] x V  R+ such that 
			wjV 
			x(t, wj) = 1, for any t  [0, 1]. (9) 
			We can regard the document as a continuous signal, and x(t, wj) as expressing 
			the relevance of word wj at instant t. This generalizes both the pure sequential 
			representation and the (global) bag-of-words model. Let y = (y1, . . . , yn)  V n be 
			a n-length document. The pure sequential representation of y arises by defining 
			x = xseq with: 
			xseq(t, wj) = 
			1, if wj = y tn 
			0, if wj = y tn 
			, 
			(10) 
			where a denotes the smallest integer greater than a. The global bag-of-words 
			representation of x corresponds to defining x = xbow, where 
			xbow(, wj) = 
			1 
			0 
			xseq(t, wj)dt,   [0, 1], j = 1, . . . , |V |. (11) 
			In this case, the curve degenerates into a single point in the simplex, which is 
			the maximum likelihood estimate of the multinomial parameters. An intermediate 
			21 
			representation arises by smoothing (10) via a function f, : [0, 1]  R++, where 
			  [0, 1] and   R++ are respectively a location and a scale parameter. An 
			example of such a smoothing function is the truncated Gaussian defined in [0, 1] 
			and normalized. This allows defining the lowbow representation at  of the n-lenght 
			document (y1, . . . , yn)  V n as the function x : [0, 1] x V  R+ such that: 
			x(, wj) = 
			1 
			0 
			xseq(t, wj)f,(t)dt. (12) 
			The scale of the smoothing function controls the amount of locality/globality in 
			the document representation (see Fig. 6): when    we recover the global bow 
			representation (11); when   0, we approach the pure sequential representation 
			(10). 
			Figure 6: The lowbow representation of a document with |V | = 3, for several values 
			of the scale parameter  (extracted from (Lebanon, 2006)). 
			Representing a document as a simplicial curve allows us to characterize geomet- 
			rically several properties of the document. For example, the tangent vector field 
			along the curve describes sequential "topic trends" and their change; the curvature 
			measures the amount of wigglyness or deviation from a geodesic path. This prop- 
			erties can be useful for tasks like text segmentation or summarization; for example 
			plotting the velocity of the curve ||  
			x()|| along time offers a visualization of the doc- 
			ument where local maxima tend to correspond to topic boundaries (see (Lebanon 
			et al., 2007) for more information). 
			22 
			5 Evaluation 
			Evaluating a summary is a difficult task because there does not exist an ideal sum- 
			mary for a given document or set of documents. From papers surveyed in the previ- 
			ous sections and elsewhere in literature, it has been found that agreement between 
			human summarizers is quite low, both for evaluating and generating summaries. 
			More than the form of the summary, it is difficult to evaluate the summary con- 
			tent. Another important problem in summary evaluation is the widespread use of 
			disparate metrics. The absence of a standard human or automatic evaluation met- 
			ric makes it very hard to compare different systems and establish a baseline. This 
			problem is not present in other NLP problems, like parsing. Besides this, manual 
			evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation 
			of summaries as in the DUC conferences would require over 3000 hours of human ef- 
			forts. Hence, an evaluation metric having high correlation with human scores would 
			obviate the process of manual evaluation. In this section, we would look at some im- 
			portant recent papers that have been able to create standards in the summarization 
			community. 
			5.1 Human and Automatic Evaluation 
			Lin and Hovy (2002) describe and compare various human and automatic metrics to 
			evaluate summaries. They focus on the evaluation procedure used in the Document 
			Understanding Conference 2001 (DUC-2001), where the Summary Evaluation En- 
			vironment (SEE) interface was used to support the human evaluation part. NIST 
			assessors in DUC-2001 compared manually written ideal summaries with summaries 
			generated automatically by summarization systems and baseline summaries. Each 
			text was decomposed into a list of units (sentences) and displayed in separate win- 
			dows in SEE. To measure the content of summaries, assessors stepped through each 
			model unit (MU) from the ideal summaries and marked all system units (SU) shar- 
			ing content with the current model unit, rating them with scores in the range 1 - 4 
			to specify that the marked system units express all (4), most (3), some (2) or hardly 
			any (1) of the content of the current model unit. Grammaticality, cohesion, and co- 
			herence were also rated similarly by the assessors. The weighted recall at threshold 
			t (where t range from 1 to 4) is then defined as 
			Recallt = 
			Number of MUs marked at or above t 
			Number of MUs in the model summary 
			. (13) 
			An interesting study is presented that shows how unstable the human markings 
			for overlapping units are. For multiple systems, the coverage scores assigned to the 
			same units were different by human assessors 18% of the time for the single document 
			task and 7.6% of the time for multi-document task. The authors also observe that 
			inter-human agreement is quite low in creating extracts from documents ( 40% for 
			single-documents and  29% for multi-documents). To overcome the instability of 
			human evaluations, they proposed using automatic metrics for summary evaluation. 
			23 
			Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001), 
			they outline an accumulative n-gram matching score (which they call NAMS), 
			NAMS = a1 * NAM1 + a2 * NAM2 + a3 * NAM3 + a4 * NAM4, (14) 
			where the NAMn n-gram hit ratio is defined as: 
			# of matched n-grams between MU and S 
			total # of n-grams in MU 
			(15) 
			with S denoting here the whole system summary, and where only content words 
			were used in forming the n-grams. Different configurations of ai were tried; the 
			best correlation with human judgement (using Spearman's rank order correlation 
			coefficient) was achieved using a configuration giving 2/3 weight to bigram matches 
			and 1/3 to unigrams matches with stemming done by the Porter stemmer. 
			5.2 ROUGE 
			Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist- 
			ing Evaluation (ROUGE)23 that have become standards of automatic evaluation of 
			summaries. 
			In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be 
			a summary generated automatically by some system. Let n(d) be a binary vector 
			representing the n-grams contained in a document d; the i-th component i 
			n 
			(d) is 1 
			if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an 
			n-gram recall based statistic that can be computed as follows: 
			ROUGE-N(s) = rR 
			n(r), n(s) 
			rR 
			n(r), n(r) 
			, (16) 
			where ., . denotes the usual inner product of vectors. This measure is closely related 
			to BLEU which is a precision related measure. Unlike other measures previously 
			considered, ROUGE-N can be used for multiple reference summaries, which is quite 
			useful in practical situations. An alternative is taking the most similar summary in 
			the reference set, 
			ROUGE-Nmulti(s) = max 
			rR 
			n(r), n(s) 
			n(r), n(r) 
			. (17) 
			Another metric in (Lin, 2004) applies the concept of longest common subse- 
			quences24 (LCS). The rationale is: the longer the LCS between two summary sen- 
			tences, the more similar they are. Let r1, . . . , ru be the reference sentences of the 
			documents in R, and s a candidate summary (considered as a concatenation of 
			sentences). The ROUGE-L is defined as an LCS based F-measure: 
			ROUGE-L(s) = 
			(1 + 2)RLCSPLCS 
			RLCS + 2PLCS 
			(18) 
			23See http://openrouge.com/default.aspx. 
			24A subsequence of a string s = s1 
			. . . sn 
			is a string of the form si1 
			. . . sin 
			where 1  i1 
			< . . . in 
			 n. 
			24 
			where RLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			Pu 
			i=1 
			|ri| 
			, PLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			|s| 
			, |x| denotes the length of 
			sentence x, LCS(x, y) denotes the length of the LCS between sentences x and y, 
			and  is a (usually large) parameter to balance precision and recall. Notice that 
			the LCS function may be computed by a simple dynamic programming approach. 
			The metric (18) is further refined by including weights that penalize subsequence 
			matches that are not consecutive, yielding a new measure denoted ROUGE-W. 
			Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen 
			as a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let 2(d) 
			be a binary vector indexed by ordered pairs of words; the i-th component i 
			2 
			(d) is 
			1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is 
			computed as follows: 
			ROUGE-S(s) = 
			(1 + 2)RSPS 
			RS + 2PS 
			(19) 
			where RS(s) = Pu 
			i=1 
			2(ri),2(s) 
			Pu 
			i=1 
			2(ri),2(ri) 
			and PS(s) = Pu 
			i=1 
			2(ri),2(s) 
			2(s),2(s) 
			. 
			The various versions of ROUGE were evaluated by computing the correlation 
			coefficient between ROUGE scores and human judgement scores. ROUGE-2 per- 
			formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and 
			ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How- 
			ever, correlation achieved with human judgement for multi-document summarization 
			was not as high as single-document ones; improvement on this side of the paradigm 
			is an open research topic. 
			5.3 Information-theoretic Evaluation of Summaries 
			A very recent approach (Lin et al., 2006) proposes to use an information-theoretic 
			method to automatic evaluation of summaries. The central idea is to use a diver- 
			gence measure between a pair of probability distributions, in this case the Jensen- 
			Shannon divergence, where the first distribution is derived from an automatic sum- 
			mary and the second from a set of reference summaries. This approach has the 
			advantage of suiting both the single-document and the multi-document summariza- 
			tion scenarios. 
			Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton 
			set in the case of single-document summarization). Assume that a distribution 
			parameterized by R generates reference summaries of the documents in D. The 
			task of summarization can be seen as that of estimating R. Analogously, assume 
			that every summarization system is governed by some distribution parameterized 
			by A. Then, we may define a good summarizer as one for which A is close to R. 
			One information-theoretic measure between distributions that is adequate for this 
			is the KL divergence (Cover and Thomas, 1991), 
			KL(pA ||pR ) = 
			m 
			i=1 
			pA 
			i 
			log 
			pA 
			i 
			pR 
			i 
			. (20) 
			However, the KL divergence is unbounded and goes to infinity whenever pA 
			i 
			vanishes 
			25 
			and pR 
			i 
			does not, which requires using some kind of smoothing when estimating the 
			distributions. Lin et al. (2006) claims that the measure used here should also be 
			symmetric,25 another thing that the KL divergence is not. Hence, they propose to 
			use the Jensen-Shannon divergence which is bounded and symmetric:26 
			JS(pA ||pR ) = 
			1 
			2 
			KL(pA ||r) + 
			1 
			2 
			KL(pR ||r) = 
			= H(r) - 
			1 
			2 
			H(pA ) - 
			1 
			2 
			H(pA ), (21) 
			where r = 1 
			2 
			pA + 1 
			2 
			pR is the average distribution. 
			To evaluate a summary SA given a reference summary SR, the authors propose 
			to use the negative JS divergence between the estimates of pA and pR given the 
			summaries, 
			score(SA|SR) = -JS(p 
			A ||p 
			R ) (22) 
			The parameters are estimated via a posteriori maximization assuming a multi- 
			nomial generation model for each summary (which means that they are modeled as 
			bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial 
			family). So: 
			 
			A = arg max 
			A 
			p(SA|A)p(A), (23) 
			where (m being the number of distinct words, a1, . . . , am being the word counts in 
			the summary, a0 = m 
			i=1 
			ai) 
			p(SA|A) = 
			(a0 + 1) 
			m 
			i=1 
			(ai + 1) 
			m 
			i=1 
			A,i 
			ai (24) 
			and 
			p(A) = 
			(0) 
			m 
			i=1 
			(i) 
			m 
			i=1 
			A,i 
			i-1 (25) 
			where i are hyper-parameters and 0 = m 
			i=1 
			i. After some algebra, we get 
			 
			A,i = 
			ai + i - 1 
			a0 + 0 - m 
			(26) 
			which is similar to MLE with smoothing.27  
			R is estimated analogously using the 
			reference summary SR. Not surprisingly, if we have more than one reference sum- 
			mary, the MAP estimation given all summaries equals MAP estimation given their 
			concatenation into a single summary. 
			25However, the authors do not give much support for this claim. In our view, there is no reason 
			to require symmetry. 
			26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfies 
			the axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora 
			of properties that are presented elsewhere, but this is out of scope of this survey. 
			27In particular if i 
			= 1 it is just maximum likelihood estimation (MLE). 
			26 
			The authors experimented three automatic evaluation schemes (JS with smooth- 
			ing, JS without smoothing, and KL divergence) against manual evaluation; the best 
			performance was achieved by JS without smoothing. This is not surprising since, as 
			seen above, the JS divergence is bounded, unlike the KL divergence, and so it does 
			not require smoothing. Smoothing has the effect of pulling the two distributions 
			more close to the uniform distribution. 
			2 Single-Document Summarization 
			Usually, the flow of information in a given document is not uniform, which means 
			that some parts are more important than others. The major challenge in summa- 
			rization lies in distinguishing the more informative parts of a document from the 
			less ones. Though there have been instances of research describing the automatic 
			creation of abstracts, most work presented in the literature relies on verbatim ex- 
			traction of sentences to address the problem of single-document summarization. In 
			1See http://trec.nist.gov/. 
			2See http://duc.nist.gov/. 
			3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/. 
			muc 7 toc.html 
			2 
			this section, we describe some eminent extractive techniques. First, we look at early 
			work from the 1950s and 60s that kicked off research on summarization. Second, 
			we concentrate on approaches involving machine learning techniques published in 
			the 1990s to today. Finally, we briefly describe some techniques that use a more 
			complex natural language analysis to tackle the problem. 
			2.1 Early Work 
			Most early work on single-document summarization focused on technical documents. 
			Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de- 
			scribes research done at IBM in the 1950s. In his work, Luhn proposed that the 
			frequency of a particular word in an article provides an useful measure of its sig- 
			nificance. There are several key ideas put forward in this paper that have assumed 
			importance in later work on summarization. As a first step, words were stemmed to 
			their root forms, and stop words were deleted. Luhn then compiled a list of content 
			words sorted by decreasing frequency, the index providing a significance measure of 
			the word. On a sentence level, a significance factor was derived that reflects the 
			number of occurrences of significant words within a sentence, and the linear distance 
			between them due to the intervention of non-significant words. All sentences are 
			ranked in order of their significance factor, and the top ranking sentences are finally 
			selected to form the auto-abstract. 
			Related work (Baxendale, 1958), also done at IBM and published in the same 
			journal, provides early insight on a particular feature helpful in finding salient parts 
			of documents: the sentence position. Towards this goal, the author examined 200 
			paragraphs to find that in 85% of the paragraphs the topic sentence came as the first 
			one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate 
			way to select a topic sentence would be to choose one of these two. This positional 
			feature has since been used in many complex machine learning based systems. 
			Edmundson (1969) describes a system that produces document extracts. His 
			primary contribution was the development of a typical structure for an extractive 
			summarization experiment. At first, the author developed a protocol for creating 
			manual extracts, that was applied in a set of 400 technical documents. The two 
			features of word frequency and positional importance were incorporated from the 
			previous two works. Two other features were used: the presence of cue words 
			(presence of words like significant, or hardly), and the skeleton of the document 
			(whether the sentence is a title or heading). Weights were attached to each of these 
			features manually to score each sentence. During evaluation, it was found that about 
			44% of the auto-extracts matched the manual extracts. 
			2.2 Machine Learning Methods 
			In the 1990s, with the advent of machine learning techniques in NLP, a series of semi- 
			nal publications appeared that employed statistical techniques to produce document 
			extracts. While initially most systems assumed feature independence and relied on 
			naive-Bayes methods, others have focused on the choice of appropriate features and 
			3 
			on learning algorithms that make no independence assumptions. Other significant 
			approaches involved hidden Markov models and log-linear models to improve ex- 
			tractive summarization. A very recent paper, in contrast, used neural networks and 
			third party features (like common words in search engine queries) to improve purely 
			extractive single document summarization. We next describe all these approaches 
			in more detail. 
			2.2.1 Naive-Bayes Methods 
			Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able 
			to learn from data. The classification function categorizes each sentence as worthy 
			of extraction or not, using a naive-Bayes classifier. Let s be a particular sentence, 
			S the set of sentences that make up the summary, and F1, . . . , Fk the features. 
			Assuming independence of the features: 
			P(s  S | F1, F2, ..Fk) = 
			k 
			i=1 
			P(Fi | s  S) * P(s  S) 
			k 
			i=1 
			P(Fi) 
			(1) 
			The features were compliant to (Edmundson, 1969), but additionally included the 
			sentence length and the presence of uppercase words. Each sentence was given a 
			score according to (1), and only the n top sentences were extracted. To evaluate 
			the system, a corpus of technical documents with manual abstracts was used in 
			the following way: for each sentence in the manual abstract, the authors manually 
			analyzed its match with the actual document sentences and created a mapping 
			(e.g. exact match with a sentence, matching a join of two sentences, not matchable, 
			etc.). The auto-extracts were then evaluated against this mapping. Feature analysis 
			revealed that a system using only the position and the cue features, along with the 
			sentence length sentence feature, performed best. 
			Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richer 
			features. They describe a system called DimSum that made use of features like 
			term frequency (tf ) and inverse document frequency (idf) to derive signature words.4 
			The idf was computed from a large corpus of the same domain as the concerned 
			documents. Statistically derived two-noun word collocations were used as units for 
			counting, along with single words. A named-entity tagger was used and each entity 
			was considered as a single token. They also employed some shallow discourse analysis 
			like reference to same entities in the text, maintaining cohesion. The references 
			were resolved at a very shallow level by linking name aliases within a document 
			like "U.S." to "United States", or "IBM" for "International Business Machines". 
			Synonyms and morphological variants were also merged while considering lexical 
			terms, the former being identified by using Wordnet (Miller, 1995). The corpora 
			used in the experiments were from newswire, some of which belonged to the TREC 
			evaluations. 
			4Words that indicate key concepts in a document. 
			4 
			2.2.2 Rich Features and Decision Trees 
			Lin and Hovy (1997) studied the importance of a single feature, sentence position. 
			Just weighing a sentence by its position in text, which the authors term as the 
			"position method", arises from the idea that texts generally follow a predictable 
			discourse structure, and that the sentences of greater topic centrality tend to occur in 
			certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse 
			structure significantly varies over domains, the position method cannot be defined 
			as naively as in (Baxendale, 1958). The paper makes an important contribution by 
			investigating techniques of tailoring the position method towards optimality over a 
			genre and how it can be evaluated for effectiveness. A newswire corpus was used, the 
			collection of Ziff-Davis texts produced from the TIPSTER5 program; it consists of 
			text about computer and related hardware, accompanied by a set of key topic words 
			and a small abstract of six sentences. For each document in the corpus, the authors 
			measured the yield of each sentence position against the topic keywords. They then 
			ranked the sentence positions by their average yield to produce the Optimal Position 
			Policy (OPP) for topic positions for the genre. 
			Two kinds of evaluation were performed. Previously unseen text was used for 
			testing whether the same procedure would work in a different domain. The first 
			evaluation showed contours exactly like the training documents. In the second eval- 
			uation, word overlap of manual abstracts with the extracted sentences was measured. 
			Windows in abstracts were compared with windows on the selected sentences and 
			corresponding precision and recall values were measured. A high degree of coverage 
			indicated the effectiveness of the position method. 
			In later work, Lin (1999) broke away from the assumption that features are 
			independent of each other and tried to model the problem of sentence extraction 
			using decision trees, instead of a naive-Bayes classifier. He examined a lot of fea- 
			tures and their effect on sentence extraction. The data used in this work is a 
			publicly available collection of texts, classified into various topics, provided by the 
			TIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems. 
			The dataset contains essential text fragments (phrases, clauses, and sentences) which 
			must be included in summaries to answer some TREC topics. These fragments were 
			each evaluated by a human judge. The experiments described in the paper are with 
			the SUMMARIST system developed at the University of Southern California. The 
			system extracted sentences from the documents and those were matched against 
			human extracts, like most early work on extractive summarization. 
			Some novel features were the query signature (normalized score given to sen- 
			tences depending on number of query words that they contain), IR signature (the 
			m most salient words in the corpus, similar to the signature words of (Aone et al., 
			1999)), numerical data (boolean value 1 given to sentences that contained a num- 
			ber in them), proper name (boolean value 1 given to sentences that contained a 
			proper name in them), pronoun or adjective (boolean value 1 given to sentences 
			5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. 
			6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html. 
			5 
			that contained a pronoun or adjective in them), weekday or month (similar as pre- 
			vious feature) and quotation (similar as previous feature). It is worth noting that 
			some features like the query signature are question-oriented because of the setting 
			of the evaluation, unlike a generalized summarization framework. 
			The author experimented with various baselines, like using only the positional 
			feature, or using a simple combination of all features by adding their values. When 
			evaluated by matching machine extracted and human extracted sentences, the deci- 
			sion tree classifier was clearly the winner for the whole dataset, but for three topics, 
			a naive combination of features beat it. Lin conjectured that this happened because 
			some of the features were independent of each other. Feature analysis suggested 
			that the IR signature was a valuable feature, corroborating the early findings of 
			Luhn (1958). 
			2.2.3 Hidden Markov Models 
			In contrast with previous approaches, that were mostly feature-based and non- 
			sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentence 
			from a document using a hidden Markov model (HMM). The basic motivation for 
			using a sequential model is to account for local dependencies between sentences. 
			Only three features were used: position of the sentence in the document (built into 
			the state structure of the HMM), number of terms in the sentence, and likeliness of 
			the sentence terms given the document terms. 
			no 3 
			2 
			1 no 
			no 
			no 
			Figure 1: Markov model to extract to three summary sentences from a document 
			(Conroy and O'leary, 2001). 
			The HMM was structured as follows: it contained 2s + 1 states, alternating be- 
			tween s summary states and s+1 nonsummary states. The authors allowed "hesita- 
			tion" only in nonsummary states and "skipping next state" only in summary states. 
			Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the 
			TREC dataset as training corpus, the authors obtained the maximum-likelihood 
			estimate for each transition probability, forming the transition matrix estimate  
			M, 
			whose element (i, j) is the empirical probability of transitioning from state i to j. 
			Associated with each state i was an output function, bi(O) = Pr(O | state i) where 
			O is an observed vector of features. They made a simplifying assumption that the 
			features are multivariate normal. The output function for each state was thus esti- 
			mated by using the training data to compute the maximum likelihood estimate of 
			its mean and covariance matrix. They estimated 2s+1 means, but assumed that all 
			of the output functions shared a common covariance matrix. Evaluation was done 
			6 
			by comparing with human generated extracts. 
			2.2.4 Log-Linear Models 
			Osborne (2002) claims that existing approaches to summarization have always as- 
			sumed feature independence. The author used log-linear models to obviate this 
			assumption and showed empirically that the system produced better extracts than 
			a naive-Bayes model, with a prior appended to both models. Let c be a label, s 
			the item we are interested in labeling, fi the i-th feature, and i the corresponding 
			feature weight. The conditional log-linear model used by Osborne (2002) can be 
			stated as follows: 
			P(c | s) = 
			1 
			Z(s) 
			exp 
			i 
			ifi(c, s) , (2) 
			where Z(s) = c 
			exp ( i 
			ifi(c, s)). In this domain, there are only two possible 
			labels: either the sentence is to be extracted or it is not. The weights were trained 
			by conjugate gradient descent. The authors added a non-uniform prior to the model, 
			claiming that a log-linear model tends to reject too many sentences for inclusion in 
			a summary. The same prior was also added to a naive-Bayes model for comparison. 
			The classification took place as follows: 
			label(s) = arg max 
			cC 
			P(c) * P(s, c) = arg max 
			cC 
			log P(c) + 
			i 
			ifi(c, s) . (3) 
			The authors optimized the prior using the f2 score of the classifier as an objective 
			function on a part of the dataset (in the technical domain). The summaries were 
			evaluated using the standard f2 score where f2 = 2pr 
			p+r 
			, where the precision and recall 
			measures were measured against human generated extracts. The features included 
			word pairs (pairs of words with all words truncated to ten characters), sentence 
			length, sentence position, and naive discourse features like inside introduction or 
			inside conclusion. With respect to f2 score, the log-linear model outperformed the 
			naive-Bayes classifier with the prior, exhibiting the former's effectiveness. 
			2.2.5 Neural Networks and Third Party Features 
			In 2001-02, DUC issued a task of creating a 100-word summary of a single news 
			article. However, the best performing systems in the evaluations could not outper- 
			form the baseline with statistical significance. This extremely strong baseline has 
			been analyzed by Nenkova (2005) and corresponds to the selection of the first n 
			sentences of a newswire article. This surprising result has been attributed to the 
			journalistic convention of putting the most important part of an article in the initial 
			paragraphs. After 2002, the task of single-document summarization for newswire 
			was dropped from DUC. Svore et al. (2007) propose an algorithm based on neu- 
			ral nets and the use of third party datasets to tackle the problem of extractive 
			summarization, outperforming the baseline with statistical significance. 
			7 
			The authors used a dataset containing 1365 documents gathered from CNN.com, 
			each consisting of the title, timestamp, three or four human generated story high- 
			lights and the article text. They considered the task of creating three machine 
			highlights. The human generated highlights were not verbatim extractions from the 
			article itself. The authors evaluated their system using two metrics: the first one 
			concatenated the three highlights produced by the system, concatenated the three 
			human generated highlights, and compared these two blocks; the second metric con- 
			sidered the ordering and compared the sentences on an individual level. 
			Svore et al. (2007) trained a model from the labels and the features for each 
			sentence of an article, that could infer the proper ranking of sentences in a test 
			document. The ranking was accomplished using RankNet (Burges et al., 2005), a 
			pair-based neural network algorithm designed to rank a set of inputs that uses the 
			gradient descent method for training. For the training set, they used ROUGE-1 
			(Lin, 2004) to score the similarity of a human written highlight and a sentence 
			in the document. These similarity scores were used as soft labels during training, 
			contrasting with other approaches where sentences are "hard-labeled", as selected 
			or not. 
			Some of the used features based on position or n-grams frequencies have been 
			observed in previous work. However, the novelty of the framework lay in the use 
			of features that derived information from query logs from Microsoft's news search 
			engine7 and Wikipedia8 entries. The authors conjecture that if a document sentence 
			contained keywords used in the news search engine, or entities found in Wikipedia 
			articles, then there is a greater chance of having that sentence in the highlight. The 
			extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically 
			significant improvements over the baseline of selecting the first three sentences in a 
			document. 
			2.3 Deep Natural Language Analysis Methods 
			In this subsection, we describe a set of papers that detail approaches towards single- 
			document summarization involving complex natural language analysis techniques. 
			None of these papers solve the problem using machine learning, but rather use a set 
			of heuristics to create document extracts. Most of these techniques try to model the 
			text's discourse structure. 
			Barzilay and Elhadad (1997) describe a work that used considerable amount of 
			linguistic analysis for performing the task of summarization. For a better under- 
			standing of their method, we need to define a lexical chain: it is a sequence of related 
			words in a text, spanning short (adjacent words or sentences) or long distances (en- 
			tire text). The authors' method progressed with the following steps: segmentation 
			of the text, identification of lexical chains, and using strong lexical chains to identify 
			the sentences worthy of extraction. They tried to reach a middle ground between 
			(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep 
			7See http://search.live.com/news. 
			8See http://en.wikipedia.org. 
			8 
			semantic structure of the text, while the latter relied on word statistics of the doc- 
			uments. The authors describe the notion of cohesion in text as a means of sticking 
			together different parts of the text. Lexical cohesion is a notable example where 
			semantically related words are used. For example, let us take a look at the following 
			sentence.9 
			John bought a Jag. He loves the car. (4) 
			Here, the word car refers to the word Jag in the previous sentence, and exemplifies 
			lexical cohesion. The phenomenon of cohesion occurs not only at the word level, 
			but at word sequences too, resulting in lexical chains, which the authors used as 
			a source representation for summarization. Semantically related words and word 
			sequences were identified in the document, and several chains were extracted, that 
			form a representation of the document. To find out lexical chains, the authors used 
			Wordnet (Miller, 1995), applying three generic steps: 
			1. Selecting a set of candidate words. 
			2. For each candidate word, finding an appropriate chain relying on a relatedness 
			criterion among members of the chains, 
			3. If it is found, inserting the word in the chain and updating it accordingly. 
			The relatedness was measured in terms of Wordnet distance. Simple nouns and 
			noun compounds were used as starting point to find the set of candidates. In the 
			final steps, strong lexical chains were used to create the summaries. The chains were 
			scored by their length and homogeneity. Then the authors used a few heuristics to 
			select the significant sentences. 
			In another paper, Ono et al. (1994) put forward a computational model of dis- 
			course for Japanese expository writings, where they elaborate a practical procedure 
			for extracting the discourse rhetorical structure, a binary tree representing relations 
			between chunks of sentences (rhetorical structure trees are used more intensively in 
			(Marcu, 1998a), as we will see below). This structure was extracted using a series 
			of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can- 
			didate generation and preference judgement. Evaluation was based on the relative 
			importance of rhetorical relations. In the following step, the nodes of the rhetori- 
			cal structure tree were pruned to reduce the sentence, keeping its important parts. 
			Same was done for paragraphs to finally produce the summary. Evaluation was done 
			with respect to sentence coverage and 30 editorial articles of a Japanese newspaper 
			were used as the dataset. The articles had corresponding sets of key sentences and 
			most important key sentences judged by human subjects. The key sentence coverage 
			was about 51% and the most important key sentence coverage was 74%, indicating 
			encouraging results. 
			Marcu (1998a) describes a unique approach towards summarization that, unlike 
			most other previous work, does not assume that the sentences in a document form 
			a flat sequence. This paper used discourse based heuristics with the traditional 
			9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html. 
			9 
			features that have been used in the summarization literature. The discourse theory 
			used in this paper is the Rhetorical Structure Theory (RST) that holds between 
			two non-overlapping pieces of text spans: the nucleus and the satellite. The author 
			mentions that the distinction between nuclei and satellites comes from the empir- 
			ical observation that the nucleus expresses what is more essential to the writer's 
			purpose than the satellite; and that the nucleus of a rhetorical relation is compre- 
			hensible independent of the satellite, but not vice versa. Marcu (1998b) describes 
			the details of a rhetorical parser producing a discourse tree. Figure 2 shows an 
			example discourse tree for a text example detailed in the paper. Once such a dis- 
			Antithesis 
			2 
			Elaboration 
			Elaboration 
			2 
			2 
			Elaboration 
			3 
			Justification 
			8 
			Exemplification 
			1 2 3 4 5 7 8 
			4 5 
			8 10 
			9 10 
			5 6 
			Contrast 
			Evidence 
			Concession 
			Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the 
			nodes denote sentence numbers from the text example. The text below the number 
			in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and 
			the normals ones are the NUCLEI. 
			course structure is created, a partial ordering of important units can be developed 
			from the tree. Each equivalence class in the partial ordering is derived from the 
			new sentences at a particular level of the discourse tree. In Figure 2, we observe 
			that sentence 2 is at the root, followed by sentence 8 in the second level. In the 
			third level, sentence 3 and 10 are observed, and so forth. The equivalence classes 
			are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6. 
			If it is specified that the summary should contain the top k% of the text, the first 
			k% of the units in the partial ordering can be selected to produce the summary. The 
			author talks about a summarization system based just on this method in (Marcu, 
			1998b) and in one of his earlier papers. In this paper, he merged the discourse 
			based heuristics with traditional heuristics. The metrics used were clustering based 
			10 
			metric (each node in the discourse tree was assigned a cluster score; for leaves the 
			score was 0, for the internal nodes it was given by the similarity of the immediate 
			children; discourse tree A was chosen to be better than B if its clustering score 
			was higher), marker based metric (a discourse structure A was chosen to be better 
			than a discourse structure B if A used more rhetorical relations than B), rhetorical 
			clustering based technique (measured the similarity between salient units of two text 
			spans), shape based metric (preferred a discourse tree A over B if A was more skewed 
			towards the right than B), title based metric, position based metric, connectedness 
			based metric (cosine similarity of an unit to all other text units, a discourse structure 
			A was chosen to be better than B if its connectedness measure was more than B). 
			A weighted linear combination of all these scores gave the score of a discourse 
			structure. To find the best combination of heuristics, the author computed the 
			weights that maximized the F-score on the training dataset, which was constituted 
			by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al., 
			1992) that performed a greedy search in a seven dimensional space of the metrics. 
			For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved 
			for the 10% summaries which was 3.5% higher than a baseline lead based algorithm, 
			which was very encouraging. 
			3 Multi-Document Summarization 
			Extraction of a single summary from multiple documents has gained interest since 
			mid 1990s, most applications being in the domain of news articles. Several Web- 
			based news clustering systems were inspired by research on multi-document summa- 
			rization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12 
			This departs from single-document summarization since the problem involves mul- 
			tiple sources of information that overlap and supplement each other, being contra- 
			dictory at occasions. So the key tasks are not only identifying and coping with 
			redundancy across documents, but also recognizing novelty and ensuring that the 
			final summary is both coherent and complete. 
			The field seems to have been pioneered by the NLP group at Columbia University 
			(McKeown and Radev, 1995), where a summarization system called SUMMONS13 
			was developed by extending already existing technology for template-driven message 
			understanding systems. Although in that early stage multi-document summariza- 
			tion was mainly seen as a task requiring substantial capabilities of both language 
			interpretation and generation, it later gained autonomy, as people coming from dif- 
			ferent communities added new perspectives to the problem. Extractive techniques 
			have been applied, making use of similarity measures between pairs of sentences. 
			Approaches vary on how these similarities are used: some identify common themes 
			through clustering and then select one sentence to represent each cluster (McKeown 
			10See http://news.google.com. 
			11See http://newsblaster.cs.columbia.edu. 
			12See http://NewsInEssence.com. 
			13SUMMarizing Online NewS articles. 
			11 
			et al., 1999; Radev et al., 2000), others generate a composite sentence from each 
			cluster (Barzilay et al., 1999), while some approaches work dynamically by includ- 
			ing each candidate passage only if it is considered novel with respect to the previous 
			included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998). 
			Some recent work extends multi-document summarization to multilingual environ- 
			ments (Evans, 2005). 
			The way the problem is posed has also varied over time. While in some pub- 
			lications it is claimed that extractive techniques would not be effective for multi- 
			document summarization (McKeown and Radev, 1995; McKeown et al., 1999), some 
			years later that claim was overturned, as extractive systems like MEAD14 (Radev 
			et al., 2000) achieved good performance in large scale summarization of news arti- 
			cles. This can be explained by the fact that summarization systems often distinguish 
			among themselves about what their goal actually is. While some systems, like SUM- 
			MONS, are designed to work in strict domains, aiming to build a sort of briefing 
			that highlights differences and updates accross different news reports, putting much 
			emphasis on how information is presented to the user, others, like MEAD, are large 
			scale systems that intend to work in general domains, being more concerned with 
			information content rather than form. Consequently, systems of the former kind re- 
			quire a strong effort on language generation to produce a grammatical and coherent 
			summary, while latter systems are probably more close to the information retrieval 
			paradigm. Abstractive systems like SUMMONS are difficult to replicate, as they 
			heavily rely on the adaptation of internal tools to perform information extraction 
			and language generation. On the other hand, extractive systems are generally easy 
			to implement from scratch, and this makes them appealing when sophisticated NLP 
			tools are not available. 
			3.1 Abstraction and Information Fusion 
			As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown, 
			1998) is the first historical example of a multi-document summarization system. It 
			tackles single events about a narrow domain (news articles about terrorism) and 
			produces a briefing merging relevant information about each event and how reports 
			by different news agencies have evolved over time. The whole thread of reports is 
			then presented, as illustrated in the following example of a "good" summary: 
			"In the afternoon of February 26, 1993, Reuters reported that a suspect 
			bomb killed at least five people in the World Trade Center. However, 
			Associated Press announced that exactly five people were killed in the 
			blast. Finally, Associated Press announced that Arab terrorists were 
			possibly responsible for the terrorist act." 
			Rather than working with raw text, SUMMONS reads a database previously 
			built by a template-based message understanding system. A full multi-document 
			14Available for download at http://www.summarization.com/mead/. 
			12 
			summarizer is built by concatenating the two systems, first processing full text as 
			input and filling template slots, and then synthesizing a summary from the extracted 
			information. The architecture of SUMMONS consists of two major components: a 
			content planner that selects the information to include in the summary through 
			combination of the input templates, and a linguistic generator that selects the right 
			words to express the information in grammatical and coherent text. The latter 
			component was devised by adapting existing language generation tools, namely the 
			FUF/SURGE system15. Content planning, on the other hand, is made through 
			summary operators, a set of heuristic rules that perform operations like "change of 
			perspective", "contradiction", "refinement", etc. Some of these operations require 
			resolving conflicts, i.e., contradictory information among different sources or time 
			instants; others complete pieces of information that are included in some articles 
			and not in others, combining them into a single template. At the end, the linguis- 
			tic generator gathers all the combined information and uses connective phrases to 
			synthesize a summary. 
			While this framework seems promising when the domain is narrow enough so that 
			the templates can be designed by hand, a generalization for broader domains would 
			be problematic. This was improved later by McKeown et al. (1999) and Barzilay 
			et al. (1999), where the input is now a set of related documents in raw text, like 
			those retrieved by a standard search engine in response to a query. The system starts 
			by identifying themes, i.e., sets of similar text units (usually paragraphs). This is 
			formulated as a clustering problem. To compute a similarity measure between text 
			units, these are mapped to vectors of features, that include single words weighted 
			by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet 
			database and a database of semantic classes of verbs. For each pair of paragraphs, a 
			vector is computed that represents matches on the different features. Decision rules 
			that were learned from data are then used to classify each pair of text units either 
			as similar or dissimilar; this in turn feeds a subsequent algorithm that places the 
			most related paragraphs in the same theme. 
			Once themes are identified, the system enters its second stage: information fu- 
			sion. The goal is to decide which sentences of a theme should be included in the 
			summary. Rather than just picking a sentence that is a group representative, the 
			authors propose an algorithm which compares and intersects predicate argument 
			structures of the phrases within each theme to determine which are repeated often 
			enough to be included in the summary. This is done as follows: first, sentences are 
			parsed through Collins' statistical parser (Collins, 1999) and converted into depen- 
			dency trees, which allows capturing the predicate-argument structure and identify 
			functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence 
			representation. 
			The comparison algorithm then traverses these dependency trees recursively, 
			adding identical nodes to the output tree. Once full phrases (a verb with at least 
			two constituents) are found, they are marked to be included in the summary. If two 
			15FUF, SURGE, and other tools developed by the Columbia NLP group are available at 
			http://www1.cs.columbia.edu/nlp/tools.cgi. 
			13 
			and Kan 1998]. We match two verbs that share the same 
			semantic class in this classifi cation. 
			In addition to the above primitive features that all com- 
			pare single items from each text unit, we use composite fea- 
			tures that combine pairs of primitive features. Our compos- 
			ite features impose particular constraints on the order of the 
			two elements in the pair, on the maximum distance between 
			the two elements, and on the syntactic classes that the two 
			elements come from. They can vary from a simple com- 
			bination (e.g., "two text units must share two words to be 
			similar") to complex cases with many conditions (e.g., "two 
			text units must have matching noun phrases that appear in 
			the same order and with relative difference in position no 
			more than fi ve"). In this manner, we capture information 
			on how similarly related elements are spaced out in the two 
			text units, as well as syntactic information on word combi- 
			nations. Matches on composite features indicate combined 
			evidence for the similarity of the two units. 
			To determine whether the units match overall, we employ 
			a machine learning algorithm [Cohen 1996] that induces de- 
			cision rules using the features that really make a difference. 
			A set of pairs of units already marked as similar or not by a 
			human is used for training the classifi er. We have manually 
			marked a set of 8,225 paragraph comparisons from the TDT 
			corpus for training and evaluating our similarity classifi er. 
			For comparison, we also use an implementation of the 
			TF*IDF method which is standard for matching texts in in- 
			formation retrieval. We compute the total frequency (TF) of 
			words in each text unit and the number of units in our train- 
			ing set each word appears in (DF, or document frequency). 
			Then each text unit is represented as a vector of TF*IDF 
			scores, calculated as 
			TF(wordi 
			) * log Total number of units 
			DF(wordi 
			) 
			Similarity between text units is measured by the cosine of 
			the angle between the corresponding two vectors (i.e., the 
			normalized inner product of the two vectors), and the opti- 
			mal value of a threshold for judging two units as similar is 
			computed from the training set. 
			After all pairwise similarities between text units have 
			been calculated, we utilize a clustering algorithm to iden- 
			tify themes. As a paragraph may belong to multiple themes, 
			most standard clustering algorithms, which partition their 
			input set, are not suitable for our task. We use a greedy, 
			one-pass algorithm that fi rst constructs groups from the most 
			similar paragraphs, seeding the groups with the fully con- 
			nected subcomponents of the graph that the similarity rela- 
			tionship induces over the set of paragraphs, and then places 
			additional paragraphs within a group if the fraction of the 
			members of the group they are similar to exceeds a preset 
			threshold. 
			Language Generation 
			Given a group of similar paragraphs--a theme--the prob- 
			lem is to create a concise and fluent fusion of information in 
			this theme, reflecting facts common to all paragraphs. A 
			straightforward method would be to pick a representative 
			subject 
			class: noun 
			27 
			class: cardinal 
			bombing 
			class: noun 
			McVeigh with 
			class: preposition 
			definite: yes 
			charge 
			class: verb voice :passive 
			polarity: + 
			tense: past 
			Figure 4: Dependency grammar representation of the sen- 
			tence "McVeigh, 27, was charged with the bombing". 
			sentence that meets some criteria (e.g., a threshold number 
			of common content words). In practice, however, any repre- 
			sentative sentence will usually include embedded phrase(s) 
			containing information that is not common to all sentences 
			in the theme. Furthermore, other sentences in the theme of- 
			ten contain additional information not presented in the rep- 
			resentative sentence. Our approach, therefore, uses inter- 
			section among theme sentences to identify phrases common 
			to most paragraphs and then generates a new sentence from 
			identifi ed phrases. 
			Intersection among Theme Sentences 
			Intersection is carried out in the content planner, which uses 
			a parser for interpreting the input sentences, with our new 
			work focusing on the comparison of phrases. Theme sen- 
			tences are fi rst run through a statistical parser[Collins 1996] 
			and then, in order to identify functional roles (e.g., subject, 
			object), are converted to a dependency grammar representa- 
			tion [Kittredge and Mel' 
			cuk 1983], which makes predicate- 
			argument structure explicit. 
			We developed a rule-based component to produce func- 
			tional roles, which transforms the phrase-structure output of 
			Collins' parser to dependency grammar; function words (de- 
			terminers and auxiliaries) are eliminated from the tree and 
			corresponding syntactic features are updated. An example 
			of a theme sentence and its dependency grammar represen- 
			tation are shown in Figure 4. Each non-auxiliary word in the 
			sentence has a node in the representation, and this node is 
			connected to its direct dependents. 
			The comparison algorithm starts with all subtrees rooted 
			at verbs from the input dependency structure, and traverses 
			them recursively: if two nodes are identical, they are added 
			to the output tree, and their children are compared. Once 
			a full phrase (verb with at least two constituents) has been 
			found, it is confi rmed for inclusion in the summary. 
			Diffi culties arise when two nodes are not identical, but are 
			similar. Such phrases may be paraphrases of each other and 
			still convey essentially the same information. Since theme 
			sentences are a priori close semantically, this signifi cantly 
			Figure 3: Dependency tree representing the sentence "McVeigh, 27, was charged 
			with the bombing" (extracted from (McKeown et al., 1999)). 
			phrases, rooted at some node, are not identical but yet similar, the hypothesis that 
			they are paraphrases of each other is considered; to take this into account, corpus- 
			driven paraphrasing rules are written to allow paraphrase intersection.16 Once the 
			summary content (represented as predicate-argument structures) is decided, a gram- 
			matical text is generated by translating those structures into the arguments expected 
			by the FUF/SURGE language generation system. 
			3.2 Topic-driven Summarization and MMR 
			Carbonell and Goldstein (1998) made a major contribution to topic-driven sum- 
			marization by introducing the maximal marginal relevance (MMR) measure. The 
			idea is to combine query relevance with information novelty; it may be applicable 
			in several tasks ranging from text retrieval to topic-driven summarization. MMR 
			simultaneously rewards relevant sentences and penalizes redundant ones by consid- 
			ering a linear combination of two similarity measures. 
			Let Q be a query or user profile and R a ranked list of documents retrieved by 
			a search engine. Consider an incremental procedure that selects documents, one at 
			a time, and adds them to a set S. So let S be the set of already selected documents 
			in a particular step, and R \ S the set of yet unselected documents in R. For each 
			candidate document Di  R \ S, its marginal relevance MR(Di) is computed as: 
			MR(Di) := Sim1(Di, Q) - (1 - ) max 
			DjS 
			Sim2(Di, Dj) (5) 
			where  is a parameter lying in [0, 1] that controls the relative importance given 
			to relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the 
			16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al., 
			1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization 
			in different syntactic categories (e.g. classifier vs. apposition), change in grammatical features 
			(active/passive, time, number, etc.), head omission, transformation from one POS to another, 
			using semantically related words (e.g. synonyms), etc. 
			14 
			experiments both were set to the standard cosine similarity traditionally used in the 
			vector space model, Sim1(x, y) = Sim2(x, y) = x,y 
			x * y 
			. The document achieving the 
			highest marginal relevance, DMMR = arg maxDiR\S 
			MR(Di), is then selected, i.e., 
			added to S, and the procedure continues until a maximum number of documents 
			are selected or a minimum relevance threshold is attained. Carbonell and Goldstein 
			(1998) found experimentally that choosing dynamically the value of  turns out to be 
			more effective than keeping it fixed, namely starting with small values (  0.3) to 
			give more emphasis to novelty, and then increasing it (  0.7) to focus on the most 
			relevant documents. To perform summarization, documents can be first segmented 
			into sentences or paragraphs, and after a query is submitted, the MMR algorithm 
			can be applied followed by a selection of the top ranking passages, reordering them as 
			they appeared in the original documents, and presenting the result as the summary. 
			One of the attractive points in using MMR for summarization is its topic-oriented 
			feature, through its dependency on the query Q, which makes it particularly ap- 
			pealing to generate summaries according to a user profile: as the authors claim, "a 
			different user with different information needs may require a totally different sum- 
			mary of the same document." This assertion was not being taken into account by 
			previous multi-document summarization systems. 
			3.3 Graph Spreading Activation 
			Mani and Bloedorn (1997) describe an information extraction framework for sum- 
			marization, a graph-based method to find similarities and dissimilarities in pairs 
			of documents. Albeit no textual summary is generated, the summary content is 
			represented via entities (concepts) and relations that are displayed respectively as 
			nodes and edges of a graph. Rather than extracting sentences, they detect salient 
			regions of the graph via a spreading activation technique.17 
			This approach shares with the method described in Section 3.2 the property 
			of being topic-driven; there is an additional input that stands for the topic with 
			respect to which the summary is to be generated. The topic is represented through 
			a set of entry nodes in the graph. A document is represented as a graph as follows: 
			each node represents the occurrence of a single word (i.e., one word together with 
			its position in the text). Each node can have several kinds of links: adjacency 
			links (ADJ) to adjacent words in the text, SAME links to other occurrences of the 
			same word, and ALPHA links encoding semantic relationships captured through 
			Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of 
			adjacent nodes which belong to the same phrase, and NAME and COREF links 
			stand for co-referential name occurrences; Fig. 4 shows some of these links. 
			Once the graph is built, topic nodes are identified by stem comparison and be- 
			come the entry nodes. A search for semantically related text is then propagated from 
			these to the other nodes of the graph, in a process called spreading activation. Salient 
			17The name "spreading activation" is borrowed from a method used in information retrieval 
			(Salton and Buckley, 1988) to expand the search vocabulary. 
			18See http://www.netowl.com. 
			15 
			1.39: Aoki, the Japanese ambassador, said in telephone calls to 
			Fujimori. 
			Japanesebroadcaster NHK that the rebels wanted to talk directly to 
			1.43:According to some estimates, only a couple hundred armed 
			followers remain. 
			2.19 They are freeing u 
			not doing us any harm," 
			... 
			2.27:Although the MRTA 
			early days in the mid-198 
			give to the poor, it lost pu 
			turning increasingly to ki 
			billion in damage to the c 
			since 1980. 
			and drug activities. 2.28: 
			Peru have cost at least 3 
			... 
			close ties with Japan. 
			1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and 
			the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea, 
			... 
			... 
			... 
			2.26:The MRTA called T 
			"Breaking The Silence." 
			1.32: President Alberto Fujimori, who is of Japanese ancestry, has had 
			Germany, Austria and Venezuela. Hood-style movement tha 
			negotiations with the gov 
			dawn on Wednesday. 
			... 
			... 
			2.22:The attack was a ma 
			Fujimori's government, w 
			virtual victory in a 16-yea 
			rebels belonging to the M 
			and better-known Maoist 
			... 
			1.28:Many leaders of the Tupac Amaru which is smaller than Peru's 
			was captured in June 1992 and is serving a life sentence, as is his 
			us: `Don't lift your heads up or you will be shot." 
			1.19: 
			hostages," a rebel who did not give his name told a local radio station in 
			a telephone call from inside the compound. 
			"The guerillas stalked around the residence grounds threatening 
			lieutenant, Peter Cardenas. 
			1.25: "We are clear: the liberation of all our comrades, or we die with all the 
			1.30:Other top commanders conceded defeat July 1993. 
			and surrendered in 
			COREF 
			Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay, 
			ADJ 
			1.38:Fujimori whose sister was among the 
			an emergency cabinet meeting today. 
			hostages released, called 
			ALPHA 
			ADJ 
			, the rebels threatened to kill the remaining 
			captives. 
			1.24:Early Wednesday 
			Figure 5: Texts of two related articles. The top 5 salient sentences containing common 
			words in bold face; likewise, the top 5 salient sentences containing unique words have th 
			Figure 4: Examples of nodes and links in the graph for a particular sentence (detail 
			extracted from from a figure in (Mani and Bloedorn, 1997)). 
			words and phrases are initialized according to their TF-IDF score. The weight of 
			neighboring nodes depends on the node link traveled and is an exponentially decay- 
			ing function of the distance of the traversed path. Traveling within a sentence is 
			made cheaper than across sentence boundaries, which in turn is cheaper than across 
			paragraph boundaries. Given a pair of document graphs, common nodes are identi- 
			fied either by sharing the same stem or by being synonyms. Analogously, difference 
			nodes are those that are not common. For each sentence in both documents, two 
			scores are computed: one score that reflects the presence of common nodes, which 
			is computed as the average weight of these nodes; and another score that computes 
			instead the average weights of difference nodes. Both scores are computed after 
			spreading activation. In the end, the sentences that have higher common and dif- 
			ferent scores are highlighted, the user being able to specify the maximal number of 
			common and different sentences to control the output. In the future, the authors 
			expect to use these structure to actually compose abstractive summaries, rather 
			than just highlighting pieces of text. 
			3.4 Centroid-based Summarization 
			Although clustering techniques were already being employed by McKeown et al. 
			(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000) 
			pioneered the use of cluster centroids to play a central role in summarization. A full 
			description of the centroid-based approach that underlies the MEAD system can 
			be found in (Radev et al., 2004); here we sketch briefly the main points. Perhaps 
			the most appealing feature is the fact that it does not make use of any language 
			generation module, unlike most previous systems. All documents are modeled as 
			bags-of-words. The system is also easily scalable and domain-independent. 
			The first stage consists of topic detection, whose goal is to group together news 
			articles that describe the same event. To accomplish this task, an agglomerative 
			clustering algorithm is used that operates over the TF-IDF vector representations 
			of the documents, successively adding documents to clusters and recomputing the 
			16 
			centroids according to 
			cj = dCj 
			 
			d 
			|Cj| 
			(6) 
			where cj is the centroid of the j-th cluster, Cj is the set of documents that belong 
			to that cluster, its cardinality being |Cj|, and  
			d is a "truncated version" of d that 
			vanishes on those words whose TF-IDF scores are below a threshold. Centroids 
			can thus be regarded as pseudo-documents that include those words whose TF- 
			IDF scores are above a threshold in the documents that constitute the cluster. Each 
			event cluster is a collection of (typically 2 to 10) news articles from multiple sources, 
			chronologically ordered, describing an event as it develops over time. 
			The second stage uses the centroids to identify sentences in each cluster that 
			are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics 
			are defined that resemble the two summands in the MMR (see Section 3.2): cluster- 
			based relative utility (CBRU) and cross-sentence informational subsumption (CSIS). 
			The first accounts for how relevant a particular sentence is to the general topic of 
			the entire cluster; the second is a measure of redundancy among sentences. Unlike 
			MMR, these metrics are not query-dependent. Given one cluster C of documents 
			segmented into n sentences, and a compression rate R, a sequence of nR sentences 
			are extracted in the same order as they appear in the original documents, which in 
			turn are ordered chronologically. The selection of the sentences is made by approx- 
			imating their CBRU and CSIS.19 For each sentence si, three different features are 
			used: 
			* Its centroid value (Ci), defined as the sum of the centroid values of all the 
			words in the sentence, 
			* A positional value (Pi), that is used to make leading sentences more important. 
			Let Cmax be the centroid value of the highest ranked sentence in the document. 
			Then Pi = n-i+1 
			n 
			Cmax. 
			* The first-sentence overlap (Fi), defined as the inner product between the word 
			occurrence vector of sentence i and that of the first sentence of the document. 
			The final score of each sentence is a combination of the three scores above minus a 
			redundancy penalty (Rs) for each sentence that overlaps highly ranked sentences. 
			3.5 Multilingual Multi-document Summarization 
			Evans (2005) addresses the task of summarizing documents written in multiple 
			languages; this had already been sketched by Hovy and Lin (1999). Multilingual 
			summarization is still at an early stage, but this framework looks quite useful for 
			newswire applications that need to combine information from foreign news agen- 
			cies. Evans (2005) considered the scenario where there is a preferred language in 
			which the summary is to be written, and multiple documents in the preferred and 
			19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details). 
			17 
			in foreign languages are available. In their experiments, the preferred language was 
			English and the documents are news articles in English and Arabic. The rationale is 
			to summarize the English articles without discarding the information contained in 
			the Arabic documents. The IBM's statistical machine translation system is first ap- 
			plied to translate the Arabic documents to English. Then a search is made, for each 
			translated text unit, to see whether there is a similar sentence or not in the English 
			documents. If so, and if the sentence is found relevant enough to be included in the 
			summary, the similar English sentence is included instead of the Arabic-to-English 
			translation. This way, the final summary is more likely to be grammatical, since 
			machine translation is known to be far from perfect. On the other hand, the result 
			is also expected to have higher coverage than using just the English documents, 
			since the information contained in the Arabic documents can help to decide about 
			the relevance of each sentence. In order to measure similarity between sentences, a 
			tool named SimFinder20 was employed: this is a tool for clustering text based on 
			similarity over a variety of lexical and syntactic features using a log-linear regression 
			model. 
			4 Other Approaches to Summarization 
			This section describes briefly some unconventional approaches that, rather than 
			aiming to build full summarization systems, investigate some details that underlie 
			the summarization process, and that we conjecture to have a role to play in future 
			research on this field. 
			4.1 Short Summaries 
			Witbrock and Mittal (1999) claim that extractive summarization is not very pow- 
			erful in that the extracts are not concise enough when very short summaries are 
			required. They present a system that generated headline style summaries. The cor- 
			pus used in this work was newswire articles from Reuters and the Associated Press, 
			publicly available at the LDC21. The system learned statistical models of the rela- 
			tionship between source text units and headline units. It attempted to model both 
			the order and the likelihood of the appearance of tokens in the target documents. 
			Both the models, one for content selection and the other for surface realization were 
			used to co-constrain each other during the search in the summary generation task. 
			For content selection, the model learned a translation model between a docu- 
			ment and its summary (Brown et al., 1993). This model in the simplest case can be 
			thought as a mapping between a word in the document and the likelihood of some 
			word appearing in the summary. To simplify the model, the authors assumed that 
			the probability of a word appearing in a summary is independent of its structure. 
			This mapping boils down to the fact that the probability of a particular summary 
			20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder. 
			21See http://ldc.upenn.edu. 
			18 
			candidate is the product of the probabilities of the summary content and that con- 
			tent being expressed using a particular structure. 
			The surface realization model used was a bigram model. Viterbi beam search 
			was used to efficiently find a near-optimal summary. The Markov assumption was 
			violated by using backtracking at every state to strongly discourage paths that 
			repeated terms, since bigrams that start repeating often seem to pathologically 
			overwhelm the search otherwise. To evaluate the system, the authors compared 
			its output against the actual headlines for a set of input newswire stories. Since 
			phrasing could not be compared, they compared the generated headlines against 
			the actual headlines, as well as the top ranked summary sentence of the story. Since 
			the system did not have a mechanism to determine the optimal length of a headline, 
			six headlines for each story were generated, ranging in length from 4 to 10 words 
			and they measured the term-overlap between each of the generated headlines and 
			the test. For headline length 4, there was 0.89 overlap in the headline and there was 
			0.91 overlap amongst the top scored sentence, indicating useful results. 
			4.2 Sentence Compression 
			Knight and Marcu (2000) introduced a statistical approach to sentence compression. 
			The authors believe that understanding the simpler task of compressing a sentence 
			may be a fruitful first step to later tackle the problems of single and multi-document 
			summarization. 
			Sentence compression is defined as follows: given a sequence of words W = 
			w1w2 . . . wn that constitute a sentence, find a subsequence wi1 
			wi2 
			. . . wik 
			, with 
			1  i1 < i2 < . . . ik  n, that is a compressed version of W. Note that there 
			are 2n possibilities of output. Knight and Marcu (2000) considered two different 
			approaches: one that is inspired by the noisy-channel model, and another one based 
			on decision trees. Due to its simplicity and elegance, we describe the first approach 
			here. 
			The noisy-channel model considers that one starts with a short summary s, 
			drawn according to the source model P(s), which is then subject to channel noise to 
			become the full sentence t, in a process guided by the channel model P(t|s). When 
			the string t is observed, one wants to recover the original summary according to: 
			 
			s = arg max 
			s 
			P(s|t) = arg max 
			s 
			P(s)P(t|s). (7) 
			This model has the advantage of decoupling the goals of producing a short text that 
			looks grammatical (incorporated in the source model) and of preserving important 
			information (which is done through the channel model). In (Knight and Marcu, 
			2000), the source and channel models are simple models inspired by probabilistic 
			context-free grammars (PCFGs). The following probability mass functions are de- 
			fined over parse trees rather than strings: Ptree(s), the probability of a parse tree 
			that generates s, and Pexpand tree(t|s), the probability that a small parse tree that 
			generates s is expanded to a longer one that generates t. 
			19 
			The sentence t is first parsed by using Collins' parser (Collins, 1999). Then, 
			rather than computing Ptree(s) over all the 2n hypotheses for s, which would be 
			exponential in the sentence length, a shaded-forest structure is used: the parse 
			tree of t is traversed and the grammar (learned from the Penn Treebank22) is used 
			to check recursively which nodes may be removed from each production in order 
			to achieve another valid production. This algorithm allows to compute efficiently 
			Ptree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually, 
			the noisy channel model works the other way around: summaries are the original 
			strings that are expanded via expansion templates. Expansion operations have the 
			effect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and 
			Pexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram 
			distribution over the leaves of the tree (i.e. the words). In the end, the log probability 
			is (heuristically) divided by the length of the sentence s in order not to penalize 
			excessively longer sentences (this is done commonly in speech recognition). 
			More recently, Daum 
			e III and Marcu (2002) extended this approach to document 
			compression by using rhetorical structure theory as in Marcu (1998a), where the 
			entire document is represented as a tree, hence allowing not only to compress relevant 
			sentences, but also to drop irrelevant ones. In this framework, Daum 
			e III and Marcu 
			(2004) employed kernel methods to decide for each node in the tree whether or not 
			it should be kept. 
			4.3 Sequential document representation 
			We conclude this section by mentioning some recent work that concerns document 
			representation, with applications in summarization. In the bag-of-words representa- 
			tion (Salton et al., 1975) each document is represented as a sparse vector in a very 
			large Euclidean space, indexed by words in the vocabulary V . A well-known tech- 
			nique in information retrieval to capture word correlation is latent semantic indexing 
			(LSI), that aims to find a linear subspace of dimension k  |V | where documents 
			may be approximately represented by their projections. 
			These classical approaches assume by convenience that Euclidean geometry is 
			a proper model for text documents. As an alternative, Gous (1999) and Hall and 
			Hofmann (2000) used the framework of information geometry (Amari and Nagaoka, 
			2001) to generalize LSI to the multinomial manifold, which can be identified with 
			the probability simplex 
			Pn-1 = x  Rn | 
			n 
			i=1 
			xi = 1, xi  0 for i = 1, . . . , n . (8) 
			Instead of finding a linear subspace, as in the Euclidean case, they learn a subman- 
			ifold of Pn-1. To illustrate this idea, Gous (1999) split a book (Machiavelli's The 
			Prince) into several text blocks (its numbered pages), considered each page as a 
			point in P|V |-1, and projected data into a 2-dimensional submanifold. The result is 
			22See http://www.cis.upenn.edu/~treebank/. 
			20 
			the representation of the book as a sequential path in R2, tracking the evolution of 
			the subject matter of the book over the course of its pages (see Fig. 5). Inspired by 
			Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex- 
			tracted from (Gous, 1999)). The inflection around page 85 reflects a real change in 
			the subject matter, where the book shifts from political theory to a more biograph- 
			ical discourse. 
			this framework, Lebanon et al. (2007) suggested representing a document as a sim- 
			plicial curve (i.e. a curve in the probability simplex), yielding the locally weighted 
			bag-of-words (lowbow) model. According to this representation, a length-normalized 
			document is a function x : [0, 1] x V  R+ such that 
			wjV 
			x(t, wj) = 1, for any t  [0, 1]. (9) 
			We can regard the document as a continuous signal, and x(t, wj) as expressing 
			the relevance of word wj at instant t. This generalizes both the pure sequential 
			representation and the (global) bag-of-words model. Let y = (y1, . . . , yn)  V n be 
			a n-length document. The pure sequential representation of y arises by defining 
			x = xseq with: 
			xseq(t, wj) = 
			1, if wj = y tn 
			0, if wj = y tn 
			, 
			(10) 
			where a denotes the smallest integer greater than a. The global bag-of-words 
			representation of x corresponds to defining x = xbow, where 
			xbow(, wj) = 
			1 
			0 
			xseq(t, wj)dt,   [0, 1], j = 1, . . . , |V |. (11) 
			In this case, the curve degenerates into a single point in the simplex, which is 
			the maximum likelihood estimate of the multinomial parameters. An intermediate 
			21 
			representation arises by smoothing (10) via a function f, : [0, 1]  R++, where 
			  [0, 1] and   R++ are respectively a location and a scale parameter. An 
			example of such a smoothing function is the truncated Gaussian defined in [0, 1] 
			and normalized. This allows defining the lowbow representation at  of the n-lenght 
			document (y1, . . . , yn)  V n as the function x : [0, 1] x V  R+ such that: 
			x(, wj) = 
			1 
			0 
			xseq(t, wj)f,(t)dt. (12) 
			The scale of the smoothing function controls the amount of locality/globality in 
			the document representation (see Fig. 6): when    we recover the global bow 
			representation (11); when   0, we approach the pure sequential representation 
			(10). 
			Figure 6: The lowbow representation of a document with |V | = 3, for several values 
			of the scale parameter  (extracted from (Lebanon, 2006)). 
			Representing a document as a simplicial curve allows us to characterize geomet- 
			rically several properties of the document. For example, the tangent vector field 
			along the curve describes sequential "topic trends" and their change; the curvature 
			measures the amount of wigglyness or deviation from a geodesic path. This prop- 
			erties can be useful for tasks like text segmentation or summarization; for example 
			plotting the velocity of the curve ||  
			x()|| along time offers a visualization of the doc- 
			ument where local maxima tend to correspond to topic boundaries (see (Lebanon 
			et al., 2007) for more information). 
			22 
			5 Evaluation 
			Evaluating a summary is a difficult task because there does not exist an ideal sum- 
			mary for a given document or set of documents. From papers surveyed in the previ- 
			ous sections and elsewhere in literature, it has been found that agreement between 
			human summarizers is quite low, both for evaluating and generating summaries. 
			More than the form of the summary, it is difficult to evaluate the summary con- 
			tent. Another important problem in summary evaluation is the widespread use of 
			disparate metrics. The absence of a standard human or automatic evaluation met- 
			ric makes it very hard to compare different systems and establish a baseline. This 
			problem is not present in other NLP problems, like parsing. Besides this, manual 
			evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation 
			of summaries as in the DUC conferences would require over 3000 hours of human ef- 
			forts. Hence, an evaluation metric having high correlation with human scores would 
			obviate the process of manual evaluation. In this section, we would look at some im- 
			portant recent papers that have been able to create standards in the summarization 
			community. 
			5.1 Human and Automatic Evaluation 
			Lin and Hovy (2002) describe and compare various human and automatic metrics to 
			evaluate summaries. They focus on the evaluation procedure used in the Document 
			Understanding Conference 2001 (DUC-2001), where the Summary Evaluation En- 
			vironment (SEE) interface was used to support the human evaluation part. NIST 
			assessors in DUC-2001 compared manually written ideal summaries with summaries 
			generated automatically by summarization systems and baseline summaries. Each 
			text was decomposed into a list of units (sentences) and displayed in separate win- 
			dows in SEE. To measure the content of summaries, assessors stepped through each 
			model unit (MU) from the ideal summaries and marked all system units (SU) shar- 
			ing content with the current model unit, rating them with scores in the range 1 - 4 
			to specify that the marked system units express all (4), most (3), some (2) or hardly 
			any (1) of the content of the current model unit. Grammaticality, cohesion, and co- 
			herence were also rated similarly by the assessors. The weighted recall at threshold 
			t (where t range from 1 to 4) is then defined as 
			Recallt = 
			Number of MUs marked at or above t 
			Number of MUs in the model summary 
			. (13) 
			An interesting study is presented that shows how unstable the human markings 
			for overlapping units are. For multiple systems, the coverage scores assigned to the 
			same units were different by human assessors 18% of the time for the single document 
			task and 7.6% of the time for multi-document task. The authors also observe that 
			inter-human agreement is quite low in creating extracts from documents ( 40% for 
			single-documents and  29% for multi-documents). To overcome the instability of 
			human evaluations, they proposed using automatic metrics for summary evaluation. 
			23 
			Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001), 
			they outline an accumulative n-gram matching score (which they call NAMS), 
			NAMS = a1 * NAM1 + a2 * NAM2 + a3 * NAM3 + a4 * NAM4, (14) 
			where the NAMn n-gram hit ratio is defined as: 
			# of matched n-grams between MU and S 
			total # of n-grams in MU 
			(15) 
			with S denoting here the whole system summary, and where only content words 
			were used in forming the n-grams. Different configurations of ai were tried; the 
			best correlation with human judgement (using Spearman's rank order correlation 
			coefficient) was achieved using a configuration giving 2/3 weight to bigram matches 
			and 1/3 to unigrams matches with stemming done by the Porter stemmer. 
			5.2 ROUGE 
			Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist- 
			ing Evaluation (ROUGE)23 that have become standards of automatic evaluation of 
			summaries. 
			In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be 
			a summary generated automatically by some system. Let n(d) be a binary vector 
			representing the n-grams contained in a document d; the i-th component i 
			n 
			(d) is 1 
			if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an 
			n-gram recall based statistic that can be computed as follows: 
			ROUGE-N(s) = rR 
			n(r), n(s) 
			rR 
			n(r), n(r) 
			, (16) 
			where ., . denotes the usual inner product of vectors. This measure is closely related 
			to BLEU which is a precision related measure. Unlike other measures previously 
			considered, ROUGE-N can be used for multiple reference summaries, which is quite 
			useful in practical situations. An alternative is taking the most similar summary in 
			the reference set, 
			ROUGE-Nmulti(s) = max 
			rR 
			n(r), n(s) 
			n(r), n(r) 
			. (17) 
			Another metric in (Lin, 2004) applies the concept of longest common subse- 
			quences24 (LCS). The rationale is: the longer the LCS between two summary sen- 
			tences, the more similar they are. Let r1, . . . , ru be the reference sentences of the 
			documents in R, and s a candidate summary (considered as a concatenation of 
			sentences). The ROUGE-L is defined as an LCS based F-measure: 
			ROUGE-L(s) = 
			(1 + 2)RLCSPLCS 
			RLCS + 2PLCS 
			(18) 
			23See http://openrouge.com/default.aspx. 
			24A subsequence of a string s = s1 
			. . . sn 
			is a string of the form si1 
			. . . sin 
			where 1  i1 
			< . . . in 
			 n. 
			24 
			where RLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			Pu 
			i=1 
			|ri| 
			, PLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			|s| 
			, |x| denotes the length of 
			sentence x, LCS(x, y) denotes the length of the LCS between sentences x and y, 
			and  is a (usually large) parameter to balance precision and recall. Notice that 
			the LCS function may be computed by a simple dynamic programming approach. 
			The metric (18) is further refined by including weights that penalize subsequence 
			matches that are not consecutive, yielding a new measure denoted ROUGE-W. 
			Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen 
			as a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let 2(d) 
			be a binary vector indexed by ordered pairs of words; the i-th component i 
			2 
			(d) is 
			1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is 
			computed as follows: 
			ROUGE-S(s) = 
			(1 + 2)RSPS 
			RS + 2PS 
			(19) 
			where RS(s) = Pu 
			i=1 
			2(ri),2(s) 
			Pu 
			i=1 
			2(ri),2(ri) 
			and PS(s) = Pu 
			i=1 
			2(ri),2(s) 
			2(s),2(s) 
			. 
			The various versions of ROUGE were evaluated by computing the correlation 
			coefficient between ROUGE scores and human judgement scores. ROUGE-2 per- 
			formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and 
			ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How- 
			ever, correlation achieved with human judgement for multi-document summarization 
			was not as high as single-document ones; improvement on this side of the paradigm 
			is an open research topic. 
			5.3 Information-theoretic Evaluation of Summaries 
			A very recent approach (Lin et al., 2006) proposes to use an information-theoretic 
			method to automatic evaluation of summaries. The central idea is to use a diver- 
			gence measure between a pair of probability distributions, in this case the Jensen- 
			Shannon divergence, where the first distribution is derived from an automatic sum- 
			mary and the second from a set of reference summaries. This approach has the 
			advantage of suiting both the single-document and the multi-document summariza- 
			tion scenarios. 
			Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton 
			set in the case of single-document summarization). Assume that a distribution 
			parameterized by R generates reference summaries of the documents in D. The 
			task of summarization can be seen as that of estimating R. Analogously, assume 
			that every summarization system is governed by some distribution parameterized 
			by A. Then, we may define a good summarizer as one for which A is close to R. 
			One information-theoretic measure between distributions that is adequate for this 
			is the KL divergence (Cover and Thomas, 1991), 
			KL(pA ||pR ) = 
			m 
			i=1 
			pA 
			i 
			log 
			pA 
			i 
			pR 
			i 
			. (20) 
			However, the KL divergence is unbounded and goes to infinity whenever pA 
			i 
			vanishes 
			25 
			and pR 
			i 
			does not, which requires using some kind of smoothing when estimating the 
			distributions. Lin et al. (2006) claims that the measure used here should also be 
			symmetric,25 another thing that the KL divergence is not. Hence, they propose to 
			use the Jensen-Shannon divergence which is bounded and symmetric:26 
			JS(pA ||pR ) = 
			1 
			2 
			KL(pA ||r) + 
			1 
			2 
			KL(pR ||r) = 
			= H(r) - 
			1 
			2 
			H(pA ) - 
			1 
			2 
			H(pA ), (21) 
			where r = 1 
			2 
			pA + 1 
			2 
			pR is the average distribution. 
			To evaluate a summary SA given a reference summary SR, the authors propose 
			to use the negative JS divergence between the estimates of pA and pR given the 
			summaries, 
			score(SA|SR) = -JS(p 
			A ||p 
			R ) (22) 
			The parameters are estimated via a posteriori maximization assuming a multi- 
			nomial generation model for each summary (which means that they are modeled as 
			bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial 
			family). So: 
			 
			A = arg max 
			A 
			p(SA|A)p(A), (23) 
			where (m being the number of distinct words, a1, . . . , am being the word counts in 
			the summary, a0 = m 
			i=1 
			ai) 
			p(SA|A) = 
			(a0 + 1) 
			m 
			i=1 
			(ai + 1) 
			m 
			i=1 
			A,i 
			ai (24) 
			and 
			p(A) = 
			(0) 
			m 
			i=1 
			(i) 
			m 
			i=1 
			A,i 
			i-1 (25) 
			where i are hyper-parameters and 0 = m 
			i=1 
			i. After some algebra, we get 
			 
			A,i = 
			ai + i - 1 
			a0 + 0 - m 
			(26) 
			which is similar to MLE with smoothing.27  
			R is estimated analogously using the 
			reference summary SR. Not surprisingly, if we have more than one reference sum- 
			mary, the MAP estimation given all summaries equals MAP estimation given their 
			concatenation into a single summary. 
			25However, the authors do not give much support for this claim. In our view, there is no reason 
			to require symmetry. 
			26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfies 
			the axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora 
			of properties that are presented elsewhere, but this is out of scope of this survey. 
			27In particular if i 
			= 1 it is just maximum likelihood estimation (MLE). 
			26 
			The authors experimented three automatic evaluation schemes (JS with smooth- 
			ing, JS without smoothing, and KL divergence) against manual evaluation; the best 
			performance was achieved by JS without smoothing. This is not surprising since, as 
			seen above, the JS divergence is bounded, unlike the KL divergence, and so it does 
			not require smoothing. Smoothing has the effect of pulling the two distributions 
			more close to the uniform distribution. 
			2 Sentence Boundary Detection 
			Sentence Boundary Detection (SBD) has been a major research topic science 
			ASR moved to more general domains as conversational speech [17,24,26]. Per- 
			formance of ASR systems has improved over the years with the inclusion and 
			combination of new Deep Neural Networks methods [5,9,33]. As a general rule, 
			the output of ASR systems lacks of any syntactic information such as capital- 
			ization and sentence boundaries, showing the interest of ASR systems to obtain 
			the correct sequence of words with almost no concern of the overall structure of 
			the document [8]. 
			Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence 
			Boundary Disambiguation. This task aims to segment a formal written text into 
			well formed sentences based on the existent punctuation marks [11,19,20,29]. In 
			this context a sentence is defined (for English) by the Cambridge Dictionary1 
			as: 
			"a group of words, usually containing a verb, that expresses a thought in 
			the form of a statement, question, instruction, or exclamation and starts 
			with a capital letter when written". 
			PMD carries certain complications, some given the ambiguity of punctuation 
			marks within a sentence. A period can denote an acronym, an abbreviation, the 
			end of the sentence or a combination of them as in the following example: 
			The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. 
			director Christopher A. Wray next Thursday at 8 p.m. 
			However its difficulties, DPM profits of morphological and lexical information 
			to achieve a correct sentence segmentation. By contrast, segmenting an ASR 
			transcript should be done without any (or almost any) lexical information and 
			a flurry definition of sentence. 
			1 https://dictionary.cambridge.org/. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 121 
			The obvious division in spoken language may be considered speaker utter- 
			ances. However, in a normal conversation or even in a monologue, the way ideas 
			are organized differs largely from written text. This differences, added to disflu- 
			encies like revisions, repetitions, restarts, interruptions and hesitations make the 
			definition of a sentence unclear thus complicating the segmentation task [27]. 
			Table 1 exemplifies some of the difficulties that are present when working with 
			spoken language. 
			Table 1. Sentence Boundary Detection example 
			Speech transcript SBD applied to transcript 
			two two women can look out after 
			a kid so bad as a man and a 
			woman can so you can have a you 
			can have a mother and a father 
			that that still don't do right with 
			the kid and you can have to men 
			that can so as long as the love 
			each other as long as they love 
			each other it doesn't matter 
			two // two women can look out 
			after a kid so bad as a man and a 
			woman can // so you can have a 
			// you can have a mother and a 
			father that // that still don't do 
			right with the kid and you can 
			have to men that can // so as 
			long as the love each other // as 
			long as they love each other it 
			doesn't matter// 
			Stolcke and Shriberg [26] considered a set of linguistic structures as segments 
			including the following list: 
			- Complete sentences 
			- Stand-alone sentences 
			- Disfluent sentences aborted in mid-utterance 
			- Interjections 
			- Back-channel responses. 
			In [17], Meteer and Iyer divided speaker utterances into segments, consisting 
			each of a single independent clause. A segment was considered to begin either 
			at the beginning of an utterance, or after the end of the preceding segment. Any 
			dysfluency between the end of the previous segments and the begging of current 
			one was considered part of the current segments. 
			Rott and  
			Cerva [23] aimed to summarize news delivered orally segmenting the 
			transcripts into "something that is similar to sentences". They used a syntactic 
			analyzer to identify the phrases within the text. 
			A wide study focused in unbalanced data for the SBD task was performed 
			by Liu et al. [15]. During this study they followed the segmentation scheme pro- 
			posed by the Linguistic Data Consortium2 on the Simple Metadata Annotation 
			Specification V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in 
			Semantic Units. 
			2 https://www.ldc.upenn.edu/. 
			122 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			A Semantic Unit (SU) is considered to be an atomic element of the transcript 
			that manages to express a complete thought or idea on the part of the speaker 
			[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text, 
			but other times (the most part of them) a SU corresponds to a phrase or a single 
			word. 
			SUs seem to be an inclusive conception of a segment, they embrace different 
			previous segment definitions and are flexible enough to deal with the majority 
			of spoken language troubles. For these reasons we will adopt SUs as our segment 
			definition. 
			2.1 Sentence Boundary Evaluation 
			SBD research has been focused on two different aspects; features and methods. 
			Regarding the features, some work focused on acoustic elements like pauses 
			duration, fundamental frequencies, energy, rate of speech, volume change and 
			speaker turn [10,12,14]. 
			The other kind of features used in SBD are textual or lexical features. They 
			rely on the transcript content to extract features like bag-of-word, POS tags or 
			word embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features 
			have also been explored [1,13,14,32], which is advantageous when both audio 
			signal and transcript are available. 
			With respect to the methods used for SBD, they mostly rely on statisti- 
			cal/neural machine translation [12,22], language models [8,15,18,26], conditional 
			random fields [16,30] and deep neural networks [3,7,29]. 
			Despite their differences in features and/or methodology, almost all previous 
			cited research share a common element; the evaluation methodology. Metrics as 
			Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) 
			are used to evaluate the proposed system against one reference. As discussed 
			in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial 
			to have a good segmentation. But comparing the output of a system against a 
			unique reference will provide a reliable score to decide if the system is good or 
			bad? 
			Bohac et al. [1] compared the human ability to punctuate recognized spon- 
			taneous speech. They asked 10 people (correctors) to punctuate about 30 min of 
			ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks 
			placed by correctors varied between 557 and 801; this means a difference of 244 
			segments for the same transcript. Over all correctors, the absolute consensus for 
			period (.) was only 4.6% caused by the replacement of other punctuation marks 
			as semicolons (;) and exclamation marks (!). These results are understandable if 
			we consider the difficulties presented previously in this section. 
			To our knowledge, the amount of studies that have tried to target the sentence 
			boundary evaluation with a multi-reference approach is very small. In [1], Bohac 
			et al. evaluated the overall punctuation accuracy for Czech in a straightforward 
			multi-reference framework. They considered a period (.) valid if at least five of 
			their 10 correctors agreed on its position. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 123 
			Kol 
			a 
			r and Lamel [13] considered two independent references to evaluate their 
			system and proposed two approaches. The fist one was to calculate the SER for 
			each of one the two available references and then compute their mean. They 
			found this approach to be very strict because for those boundaries where no 
			agreement between references existed, the system was going to be partially wrong 
			even the fact that it has correctly predicted the boundary. Their second app- 
			roach tried to moderate the number of unjust penalizations. For this case, a 
			classification was considered incorrect only if it didn't match either of the two 
			references. 
			These two examples exemplify the real need and some straightforward solu- 
			tions for multi-reference evaluation metrics. However, we think that it is possible 
			to consider in a more inclusive approach the similarities and differences that mul- 
			tiple references could provide into a sentence boundary evaluation protocol. 
			3 Window-Based Sentence Boundary Evaluation 
			Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic 
			multi-reference sentence boundary evaluation protocol which considers the per- 
			formance of a candidate segmentation over a set of segmentation references and 
			the agreement between those references. 
			Let R = {R1 
			, R2 
			, ..., Rm 
			} be the set of all available references given a tran- 
			script T = {t1 
			, t2 
			, ..., tn 
			}, where tj 
			is the jth word in the transcript; a reference 
			Ri 
			is defined as a binary vector in terms of the existent SU boundaries in T. 
			Ri 
			= {b1 
			, b2 
			, ..., bn 
			} (1) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			Given a transcript T, the candidate segmentation CT 
			is defined similar to Ri 
			. 
			CT 
			= {b1 
			, b2 
			, ..., bn 
			} (2) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			3.1 General Reference and Agreement Ratio 
			A General Reference (RG 
			) is then constructed to calculate the agreement ratio 
			between all references in. It is defined by the boundary frequencies of each ref- 
			erence Ri 
			 R. 
			RG 
			= {d1 
			, d2 
			, ..., dn 
			} (3) 
			where 
			124 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			dj 
			= 
			m 
			i=1 
			tij 
			tj 
			 T, dj 
			= [0, m] (4) 
			The Agreement Ratio (RGAR 
			) is needed to get a numerical value of the dis- 
			tribution of SU boundaries over R. A value of RGAR 
			close to 0 means a low 
			agreement between references in R, while RGAR 
			= 1 means a perfect agreement 
			(Ri 
			 R, Ri 
			= Ri+1 
			|i = 1, ..., m - 1) in R. 
			RGAR 
			= 
			RGP B 
			RGHA 
			(5) 
			In the equation above, RGP B 
			corresponds to the ponderated common boundaries 
			of RG 
			and RGHA 
			to its hypothetical maximum agreement. 
			RGP B 
			= 
			n 
			j=1 
			dj 
			[dj 
			 2] (6) 
			RGHA 
			= m x 
			dj 
			RG 
			1 [dj 
			= 0] (7) 
			3.2 Window-Boundaries Reference 
			In Sect. 2 we discussed about how disfluencies complicate SU segmentation. In a 
			multi-reference environment this causes disagreement between references around 
			a same SU boundary. The way WiSeBE handle disagreements produced by dis- 
			fluencies is with a Window-boundaries Reference (RW 
			) defined as: 
			RW 
			= {w1 
			, w2 
			, ..., wp 
			} (8) 
			where each window wk 
			considers one or more boundaries dj 
			from RG 
			with a 
			window separation limit equal to RWl 
			. 
			wk 
			= {dj 
			, dj+1 
			, dj+2 
			, ...} (9) 
			3.3 W iSeBE 
			WiSeBE is a normalized score dependent of (1) the performance of CT 
			over RW 
			and (2) the agreement between all references in R. It is defined as: 
			WiSeBE = F1RW 
			x RGAR 
			WiSeBE = [0, 1] (10) 
			where F1RW 
			corresponds to the harmonic mean of precision and recall of CT 
			with respect to RW 
			(Eq. 11), while RGAR 
			is the agreement ratio defined in (5). 
			RGAR 
			can be interpreted as a scaling factor; a low value will penalize the overall 
			WiSeBE score given the low agreement between references. By contrast, for a 
			high agreement in R (RGAR 
			 1), WiSeBE  F1RW 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 125 
			F1RW 
			= 2 x 
			precisionRW 
			x recallRW 
			precisionRW 
			+ recallRW 
			(11) 
			precisionRW 
			= bj 
			CT 
			1 [bj 
			= 1, bj 
			 w w  RW 
			] 
			bj 
			CT 
			1 [bj 
			= 1] 
			(12) 
			recallRW 
			= wk 
			RW 
			1 [wk 
			b b  CT 
			] 
			p 
			(13) 
			Equations 12 and 13 describe precision and recall of CT 
			with respect to RW 
			. 
			Precision is the number of boundaries bj 
			inside any window wk 
			from RW 
			divided 
			by the total number of boundaries bj 
			in CT 
			. Recall corresponds to the number 
			of windows w with at least one boundary b divided by the number of windows 
			w in RW 
			. 
			4 Evaluating with W iSeBE 
			To exemplify the WiSeBE score we evaluated and compared the performance 
			of two different SBD systems over a set of YouTube videos in a multi-reference 
			environment. The first system (S1) employs a Convolutional Neural Network to 
			determine if the middle word of a sliding window corresponds to a SU bound- 
			ary or not [6]. The second approach (S2) by contrast, introduces a bidirectional 
			Recurrent Neural Network model with attention mechanism for boundary detec- 
			tion [28]. 
			In a first glance we performed the evaluation of the systems against each 
			one of the references independently. Then, we implemented a multi-reference 
			evaluation with WiSeBE. 
			4.1 Dataset 
			We focused evaluation over a small but diversified dataset composed by 10 
			YouTube videos in the English language in the news context. The selected videos 
			cover different topics like technology, human rights, terrorism and politics with 
			a length variation between 2 and 10 min. To encourage the diversity of content 
			format we included newscasts, interviews, reports and round tables. 
			During the transcription phase we opted for a manual transcription process 
			because we observed that using transcripts from an ASR system will difficult 
			in a large degree the manual segmentation process. The number of words per 
			transcript oscilate between 271 and 1,602 with a total number of 8,080. 
			We gave clear instructions to three evaluators (ref1 
			, ref2 
			, ref3 
			) of how seg- 
			mentation was needed to be perform, including the SU concept and how punctu- 
			ation marks were going to be taken into account. Periods (.), question marks (?), 
			exclamation marks (!) and semicolons (;) were considered SU delimiters (bound- 
			aries) while colons (:) and commas (,) were considered as internal SU marks. 
			The number of segments per transcript and reference can be seen in Table 2. An 
			interesting remark is that ref3 
			assigns about 43% less boundaries than the mean 
			of the other two references. 
			126 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 2. Manual dataset segmentation 
			Reference v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			ref1 38 42 17 11 55 87 109 72 55 16 502 
			ref2 33 42 16 14 54 98 92 65 51 20 485 
			ref3 23 20 10 6 39 39 76 30 29 9 281 
			4.2 Evaluation 
			We ran both systems (S1 & S2) over the manually transcribed videos obtaining 
			the number of boundaries shown in Table 3. In general, it can be seen that S1 
			predicts 27% more segments than S2. This difference can affect the performance 
			of S1, increasing its probabilities of false positives. 
			Table 3. Automatic dataset segmentation 
			System v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			S1 53 38 15 13 54 108 106 70 71 11 539 
			S2 38 37 12 11 36 92 86 46 53 13 424 
			Table 4 condenses the performance of both systems evaluated against each 
			one of the references independently. If we focus on F1 scores, performance of both 
			systems varies depending of the reference. For ref1 
			, S1 was better in 5 occasions 
			with respect of S2; S1 was better in 2 occasions only for ref2 
			; S1 overperformed 
			S2 in 3 occasions concerning ref3 
			and in 4 occasions for mean (bold). 
			Also from Table 4 we can observe that ref1 
			has a bigger similarity to S1 in 
			5 occasions compared to other two references, while ref2 
			is more similar to S2 
			in 7 transcripts (underline). 
			After computing the mean F1 scores over the transcripts, it can be concluded 
			that in average S2 had a better performance segmenting the dataset compared 
			to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of 
			the dataset? Regardless all references have been considered, nor agreement or 
			disagreement between them has been taken into account. 
			All values related to the WiSeBE score are displayed in Table 5. The Agree- 
			ment Ratio (RGAR 
			) between references oscillates between 0.525 for v8 
			and 0.767 
			for v5 
			. The lower the RGAR 
			, the bigger the penalization WiSeBE will give to 
			the final score. A good example is S2 for transcript v4 
			where F1RW 
			reaches a 
			value of 0.800, but after considering RGAR 
			the WiSeBE score falls to 0.462. 
			It is feasible to think that if all references are taken into account at the same 
			time during evaluation (F1RW 
			), the score will be bigger compared to an average 
			of independent evaluations (F1mean 
			); however this is not always true. That is 
			the case of S1 in v10, which present a slight decrease for F1RW 
			compared to 
			F1mean 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 127 
			Table 4. Independent multi-reference evaluation 
			Transcript System ref1 ref2 ref3 Mean 
			P R F1 P R F1 P R F1 P R F1 
			v1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432 
			S2 0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480 
			v2 S1 0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578 
			S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549 
			v3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270 
			S2 0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325 
			v4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505 
			S2 0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735 
			v5 S1 0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592 
			S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499 
			v6 S1 0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443 
			S2 0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457 
			v7 S1 0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518 
			S2 0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539 
			v8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429 
			S2 0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487 
			v9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459 
			S2 0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541 
			v10 S1 0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582 
			S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487 
			Mean scores S1 -- 0.520 -- 0.510 -- 0.404 -- 0.481 
			S2 -- 0.543 -- 0.554 -- 0.433 -- 0.510 
			An important remark is the behavior of S1 and S2 concerning v6 
			. If evalu- 
			ated without considering any (dis)agreement between references (F1mean 
			), S2 
			overperforms S1; this is inverted once the systems are evaluated with WiSeBE. 
			2 Sentence Boundary Detection 
			Sentence Boundary Detection (SBD) has been a major research topic science 
			ASR moved to more general domains as conversational speech [17,24,26]. Per- 
			formance of ASR systems has improved over the years with the inclusion and 
			combination of new Deep Neural Networks methods [5,9,33]. As a general rule, 
			the output of ASR systems lacks of any syntactic information such as capital- 
			ization and sentence boundaries, showing the interest of ASR systems to obtain 
			the correct sequence of words with almost no concern of the overall structure of 
			the document [8]. 
			Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence 
			Boundary Disambiguation. This task aims to segment a formal written text into 
			well formed sentences based on the existent punctuation marks [11,19,20,29]. In 
			this context a sentence is defined (for English) by the Cambridge Dictionary1 
			as: 
			"a group of words, usually containing a verb, that expresses a thought in 
			the form of a statement, question, instruction, or exclamation and starts 
			with a capital letter when written". 
			PMD carries certain complications, some given the ambiguity of punctuation 
			marks within a sentence. A period can denote an acronym, an abbreviation, the 
			end of the sentence or a combination of them as in the following example: 
			The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. 
			director Christopher A. Wray next Thursday at 8 p.m. 
			However its difficulties, DPM profits of morphological and lexical information 
			to achieve a correct sentence segmentation. By contrast, segmenting an ASR 
			transcript should be done without any (or almost any) lexical information and 
			a flurry definition of sentence. 
			1 https://dictionary.cambridge.org/. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 121 
			The obvious division in spoken language may be considered speaker utter- 
			ances. However, in a normal conversation or even in a monologue, the way ideas 
			are organized differs largely from written text. This differences, added to disflu- 
			encies like revisions, repetitions, restarts, interruptions and hesitations make the 
			definition of a sentence unclear thus complicating the segmentation task [27]. 
			Table 1 exemplifies some of the difficulties that are present when working with 
			spoken language. 
			Table 1. Sentence Boundary Detection example 
			Speech transcript SBD applied to transcript 
			two two women can look out after 
			a kid so bad as a man and a 
			woman can so you can have a you 
			can have a mother and a father 
			that that still don't do right with 
			the kid and you can have to men 
			that can so as long as the love 
			each other as long as they love 
			each other it doesn't matter 
			two // two women can look out 
			after a kid so bad as a man and a 
			woman can // so you can have a 
			// you can have a mother and a 
			father that // that still don't do 
			right with the kid and you can 
			have to men that can // so as 
			long as the love each other // as 
			long as they love each other it 
			doesn't matter// 
			Stolcke and Shriberg [26] considered a set of linguistic structures as segments 
			including the following list: 
			- Complete sentences 
			- Stand-alone sentences 
			- Disfluent sentences aborted in mid-utterance 
			- Interjections 
			- Back-channel responses. 
			In [17], Meteer and Iyer divided speaker utterances into segments, consisting 
			each of a single independent clause. A segment was considered to begin either 
			at the beginning of an utterance, or after the end of the preceding segment. Any 
			dysfluency between the end of the previous segments and the begging of current 
			one was considered part of the current segments. 
			Rott and  
			Cerva [23] aimed to summarize news delivered orally segmenting the 
			transcripts into "something that is similar to sentences". They used a syntactic 
			analyzer to identify the phrases within the text. 
			A wide study focused in unbalanced data for the SBD task was performed 
			by Liu et al. [15]. During this study they followed the segmentation scheme pro- 
			posed by the Linguistic Data Consortium2 on the Simple Metadata Annotation 
			Specification V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in 
			Semantic Units. 
			2 https://www.ldc.upenn.edu/. 
			122 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			A Semantic Unit (SU) is considered to be an atomic element of the transcript 
			that manages to express a complete thought or idea on the part of the speaker 
			[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text, 
			but other times (the most part of them) a SU corresponds to a phrase or a single 
			word. 
			SUs seem to be an inclusive conception of a segment, they embrace different 
			previous segment definitions and are flexible enough to deal with the majority 
			of spoken language troubles. For these reasons we will adopt SUs as our segment 
			definition. 
			2.1 Sentence Boundary Evaluation 
			SBD research has been focused on two different aspects; features and methods. 
			Regarding the features, some work focused on acoustic elements like pauses 
			duration, fundamental frequencies, energy, rate of speech, volume change and 
			speaker turn [10,12,14]. 
			The other kind of features used in SBD are textual or lexical features. They 
			rely on the transcript content to extract features like bag-of-word, POS tags or 
			word embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features 
			have also been explored [1,13,14,32], which is advantageous when both audio 
			signal and transcript are available. 
			With respect to the methods used for SBD, they mostly rely on statisti- 
			cal/neural machine translation [12,22], language models [8,15,18,26], conditional 
			random fields [16,30] and deep neural networks [3,7,29]. 
			Despite their differences in features and/or methodology, almost all previous 
			cited research share a common element; the evaluation methodology. Metrics as 
			Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) 
			are used to evaluate the proposed system against one reference. As discussed 
			in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial 
			to have a good segmentation. But comparing the output of a system against a 
			unique reference will provide a reliable score to decide if the system is good or 
			bad? 
			Bohac et al. [1] compared the human ability to punctuate recognized spon- 
			taneous speech. They asked 10 people (correctors) to punctuate about 30 min of 
			ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks 
			placed by correctors varied between 557 and 801; this means a difference of 244 
			segments for the same transcript. Over all correctors, the absolute consensus for 
			period (.) was only 4.6% caused by the replacement of other punctuation marks 
			as semicolons (;) and exclamation marks (!). These results are understandable if 
			we consider the difficulties presented previously in this section. 
			To our knowledge, the amount of studies that have tried to target the sentence 
			boundary evaluation with a multi-reference approach is very small. In [1], Bohac 
			et al. evaluated the overall punctuation accuracy for Czech in a straightforward 
			multi-reference framework. They considered a period (.) valid if at least five of 
			their 10 correctors agreed on its position. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 123 
			Kol 
			a 
			r and Lamel [13] considered two independent references to evaluate their 
			system and proposed two approaches. The fist one was to calculate the SER for 
			each of one the two available references and then compute their mean. They 
			found this approach to be very strict because for those boundaries where no 
			agreement between references existed, the system was going to be partially wrong 
			even the fact that it has correctly predicted the boundary. Their second app- 
			roach tried to moderate the number of unjust penalizations. For this case, a 
			classification was considered incorrect only if it didn't match either of the two 
			references. 
			These two examples exemplify the real need and some straightforward solu- 
			tions for multi-reference evaluation metrics. However, we think that it is possible 
			to consider in a more inclusive approach the similarities and differences that mul- 
			tiple references could provide into a sentence boundary evaluation protocol. 
			3 Window-Based Sentence Boundary Evaluation 
			Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic 
			multi-reference sentence boundary evaluation protocol which considers the per- 
			formance of a candidate segmentation over a set of segmentation references and 
			the agreement between those references. 
			Let R = {R1 
			, R2 
			, ..., Rm 
			} be the set of all available references given a tran- 
			script T = {t1 
			, t2 
			, ..., tn 
			}, where tj 
			is the jth word in the transcript; a reference 
			Ri 
			is defined as a binary vector in terms of the existent SU boundaries in T. 
			Ri 
			= {b1 
			, b2 
			, ..., bn 
			} (1) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			Given a transcript T, the candidate segmentation CT 
			is defined similar to Ri 
			. 
			CT 
			= {b1 
			, b2 
			, ..., bn 
			} (2) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			3.1 General Reference and Agreement Ratio 
			A General Reference (RG 
			) is then constructed to calculate the agreement ratio 
			between all references in. It is defined by the boundary frequencies of each ref- 
			erence Ri 
			 R. 
			RG 
			= {d1 
			, d2 
			, ..., dn 
			} (3) 
			where 
			124 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			dj 
			= 
			m 
			i=1 
			tij 
			tj 
			 T, dj 
			= [0, m] (4) 
			The Agreement Ratio (RGAR 
			) is needed to get a numerical value of the dis- 
			tribution of SU boundaries over R. A value of RGAR 
			close to 0 means a low 
			agreement between references in R, while RGAR 
			= 1 means a perfect agreement 
			(Ri 
			 R, Ri 
			= Ri+1 
			|i = 1, ..., m - 1) in R. 
			RGAR 
			= 
			RGP B 
			RGHA 
			(5) 
			In the equation above, RGP B 
			corresponds to the ponderated common boundaries 
			of RG 
			and RGHA 
			to its hypothetical maximum agreement. 
			RGP B 
			= 
			n 
			j=1 
			dj 
			[dj 
			 2] (6) 
			RGHA 
			= m x 
			dj 
			RG 
			1 [dj 
			= 0] (7) 
			3.2 Window-Boundaries Reference 
			In Sect. 2 we discussed about how disfluencies complicate SU segmentation. In a 
			multi-reference environment this causes disagreement between references around 
			a same SU boundary. The way WiSeBE handle disagreements produced by dis- 
			fluencies is with a Window-boundaries Reference (RW 
			) defined as: 
			RW 
			= {w1 
			, w2 
			, ..., wp 
			} (8) 
			where each window wk 
			considers one or more boundaries dj 
			from RG 
			with a 
			window separation limit equal to RWl 
			. 
			wk 
			= {dj 
			, dj+1 
			, dj+2 
			, ...} (9) 
			3.3 W iSeBE 
			WiSeBE is a normalized score dependent of (1) the performance of CT 
			over RW 
			and (2) the agreement between all references in R. It is defined as: 
			WiSeBE = F1RW 
			x RGAR 
			WiSeBE = [0, 1] (10) 
			where F1RW 
			corresponds to the harmonic mean of precision and recall of CT 
			with respect to RW 
			(Eq. 11), while RGAR 
			is the agreement ratio defined in (5). 
			RGAR 
			can be interpreted as a scaling factor; a low value will penalize the overall 
			WiSeBE score given the low agreement between references. By contrast, for a 
			high agreement in R (RGAR 
			 1), WiSeBE  F1RW 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 125 
			F1RW 
			= 2 x 
			precisionRW 
			x recallRW 
			precisionRW 
			+ recallRW 
			(11) 
			precisionRW 
			= bj 
			CT 
			1 [bj 
			= 1, bj 
			 w w  RW 
			] 
			bj 
			CT 
			1 [bj 
			= 1] 
			(12) 
			recallRW 
			= wk 
			RW 
			1 [wk 
			b b  CT 
			] 
			p 
			(13) 
			Equations 12 and 13 describe precision and recall of CT 
			with respect to RW 
			. 
			Precision is the number of boundaries bj 
			inside any window wk 
			from RW 
			divided 
			by the total number of boundaries bj 
			in CT 
			. Recall corresponds to the number 
			of windows w with at least one boundary b divided by the number of windows 
			w in RW 
			. 
			4 Evaluating with W iSeBE 
			To exemplify the WiSeBE score we evaluated and compared the performance 
			of two different SBD systems over a set of YouTube videos in a multi-reference 
			environment. The first system (S1) employs a Convolutional Neural Network to 
			determine if the middle word of a sliding window corresponds to a SU bound- 
			ary or not [6]. The second approach (S2) by contrast, introduces a bidirectional 
			Recurrent Neural Network model with attention mechanism for boundary detec- 
			tion [28]. 
			In a first glance we performed the evaluation of the systems against each 
			one of the references independently. Then, we implemented a multi-reference 
			evaluation with WiSeBE. 
			4.1 Dataset 
			We focused evaluation over a small but diversified dataset composed by 10 
			YouTube videos in the English language in the news context. The selected videos 
			cover different topics like technology, human rights, terrorism and politics with 
			a length variation between 2 and 10 min. To encourage the diversity of content 
			format we included newscasts, interviews, reports and round tables. 
			During the transcription phase we opted for a manual transcription process 
			because we observed that using transcripts from an ASR system will difficult 
			in a large degree the manual segmentation process. The number of words per 
			transcript oscilate between 271 and 1,602 with a total number of 8,080. 
			We gave clear instructions to three evaluators (ref1 
			, ref2 
			, ref3 
			) of how seg- 
			mentation was needed to be perform, including the SU concept and how punctu- 
			ation marks were going to be taken into account. Periods (.), question marks (?), 
			exclamation marks (!) and semicolons (;) were considered SU delimiters (bound- 
			aries) while colons (:) and commas (,) were considered as internal SU marks. 
			The number of segments per transcript and reference can be seen in Table 2. An 
			interesting remark is that ref3 
			assigns about 43% less boundaries than the mean 
			of the other two references. 
			126 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 2. Manual dataset segmentation 
			Reference v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			ref1 38 42 17 11 55 87 109 72 55 16 502 
			ref2 33 42 16 14 54 98 92 65 51 20 485 
			ref3 23 20 10 6 39 39 76 30 29 9 281 
			4.2 Evaluation 
			We ran both systems (S1 & S2) over the manually transcribed videos obtaining 
			the number of boundaries shown in Table 3. In general, it can be seen that S1 
			predicts 27% more segments than S2. This difference can affect the performance 
			of S1, increasing its probabilities of false positives. 
			Table 3. Automatic dataset segmentation 
			System v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			S1 53 38 15 13 54 108 106 70 71 11 539 
			S2 38 37 12 11 36 92 86 46 53 13 424 
			Table 4 condenses the performance of both systems evaluated against each 
			one of the references independently. If we focus on F1 scores, performance of both 
			systems varies depending of the reference. For ref1 
			, S1 was better in 5 occasions 
			with respect of S2; S1 was better in 2 occasions only for ref2 
			; S1 overperformed 
			S2 in 3 occasions concerning ref3 
			and in 4 occasions for mean (bold). 
			Also from Table 4 we can observe that ref1 
			has a bigger similarity to S1 in 
			5 occasions compared to other two references, while ref2 
			is more similar to S2 
			in 7 transcripts (underline). 
			After computing the mean F1 scores over the transcripts, it can be concluded 
			that in average S2 had a better performance segmenting the dataset compared 
			to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of 
			the dataset? Regardless all references have been considered, nor agreement or 
			disagreement between them has been taken into account. 
			All values related to the WiSeBE score are displayed in Table 5. The Agree- 
			ment Ratio (RGAR 
			) between references oscillates between 0.525 for v8 
			and 0.767 
			for v5 
			. The lower the RGAR 
			, the bigger the penalization WiSeBE will give to 
			the final score. A good example is S2 for transcript v4 
			where F1RW 
			reaches a 
			value of 0.800, but after considering RGAR 
			the WiSeBE score falls to 0.462. 
			It is feasible to think that if all references are taken into account at the same 
			time during evaluation (F1RW 
			), the score will be bigger compared to an average 
			of independent evaluations (F1mean 
			); however this is not always true. That is 
			the case of S1 in v10, which present a slight decrease for F1RW 
			compared to 
			F1mean 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 127 
			Table 4. Independent multi-reference evaluation 
			Transcript System ref1 ref2 ref3 Mean 
			P R F1 P R F1 P R F1 P R F1 
			v1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432 
			S2 0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480 
			v2 S1 0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578 
			S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549 
			v3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270 
			S2 0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325 
			v4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505 
			S2 0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735 
			v5 S1 0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592 
			S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499 
			v6 S1 0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443 
			S2 0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457 
			v7 S1 0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518 
			S2 0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539 
			v8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429 
			S2 0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487 
			v9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459 
			S2 0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541 
			v10 S1 0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582 
			S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487 
			Mean scores S1 -- 0.520 -- 0.510 -- 0.404 -- 0.481 
			S2 -- 0.543 -- 0.554 -- 0.433 -- 0.510 
			An important remark is the behavior of S1 and S2 concerning v6 
			. If evalu- 
			ated without considering any (dis)agreement between references (F1mean 
			), S2 
			overperforms S1; this is inverted once the systems are evaluated with WiSeBE. 
			2 State of the Art 
			The most known RST corpus is the RST Discourse 
			Treebank, for English (Carlson et al., 2002a, 
			2002b). It includes 385 texts of the journalistic 
			domain, extracted from the Penn Treebank 
			(Marcus et al., 1993), such as cultural reviews, 
			editorials, economy articles, etc. 347 texts are used 
			as a learning corpus and 38 texts are used as a test 
			corpus. It contains 176,389 words and 21,789 
			EDUs. 13.8% of the texts (that is, 53) were 
			annotated by two people with a list of 78 relations. 
			For annotation, the annotation tool RSTtool 2 
			(O'Donnell, 2000) was used, with some 
			adaptations. The principal advantages of this 
			corpus stand on the high number of annotated texts 
			(for the moment it is the biggest RST corpus) and 
			the clarity of the annotation method (specified in 
			the annotation manual by Carlson and Marcu, 
			2001). However, some drawbacks remain. The 
			corpus is not free, it is not on-line and it only 
			includes texts of one domain (journalistic). 
			For English there is also the Discourse 
			Relations Reference Corpus (Taboada and 
			Renkema, 2008). This corpus includes 65 texts 
			(each one tagged by one annotator) of several types 
			and from several sources: 21 articles from the Wall 
			Street Journal extracted from the RST Discourse 
			Treebank, 30 movies and books' reviews extracted 
			from the epinions.com website, and 14 diverse 
			texts, including letters, webs, magazine articles, 
			newspaper editorials, etc. The tool used for 
			annotation was also the RSTtool. The advantages 
			of this corpus are that it is free and on-line, and it 
			includes texts of several types and domains. The 
			disadvantages are that the amount of texts is not 
			very high, the annotation methodology is not 
			2 http://www.wagsoft.com/RSTTool/ 
			specified and it does not include texts annotated by 
			several people. 
			Another well-known corpus is the Potsdam 
			Commentary Corpus, for German (Stede, 2004; 
			Reitter and Stede, 2003). This corpus includes 173 
			texts on politics from the on-line newspaper 
			Markische Allgemeine Zeitung. It contains 32,962 
			words and 2,195 sentences. It is annotated with 
			several data: morphology, syntax, rhetorical 
			structure, connectors, correference and informative 
			structure. Nevertheless, only a part of this corpus 
			(10 texts), which the authors name "core corpus", 
			is annotated with all this information. The texts 
			were annotated with the RSTtool. This corpus has 
			several advantages: it is annotated at different 
			levels (the annotation of connectors is especially 
			interesting); all the texts were annotated by two 
			people (with a previous RST training phase); it is 
			free for research purposes, and there is a tool for 
			searching over the corpus (although it is not 
			available on-line). The disadvantages are: the 
			genre and domain of all the texts are the same, the 
			methodology of annotation was quite intuitive 
			(without a manual or specific criteria) and the 
			inter-annotator agreement is not given. 
			For Portuguese, there are 2 corpora, built in 
			order to develop a rhetorical parser (Pardo et al., 
			2008). The first one, the CorpusTCC (Pardo et al., 
			2008), was used as learning corpus for detection of 
			linguistic patterns indicating rhetorical relations. It 
			contains 100 introduction sections of computer 
			science theses (53,000 words and 1,350 sentences). 
			To annotate the corpus a list of 32 rhetorical 
			relations was used. The annotation manual by 
			Carlson and Marcu (2001) was adapted to 
			Portuguese. The annotation tool was the ISI RST 
			Annotation Tool3 , an extension of the RSTtool. 
			The advantages of this corpus are: it is free, it 
			contains an acceptable number of texts and words 
			and it follows a specific annotation methodology. 
			The disadvantage is: it only includes texts of one 
			genre and domain, only annotated by one person. 
			The second one, Rhetalho (Pardo and Seno, 
			2005), was used as reference corpus for the parser 
			evaluation. It contains 50 texts: 20 introduction 
			sections and 10 conclusion sections from computer 
			science scientific articles, and 20 texts from the on- 
			line newspaper Folha de Sao Paulo (7 from the 
			Daily section, 7 from the World section and 6 from 
			3 http://www.isi.edu/~marcu/discourse/ 
			2 
			the Science section). It includes approximately 
			5,000 words. The relations and the annotation tool 
			are the same as those used in the CorpusTCC. The 
			advantages of this corpus are that it is free, it was 
			annotated by 2 people (they both were RST experts 
			and followed an annotation manual) and it contains 
			texts of several genres and domains. The main 
			disadvantage is the scarce amount of texts. 
			The Penn Discourse Treebank (Rashmi et al., 
			2008)f for English includes texts annotated with 
			information related to discourse structure and 
			semantics (without a specific theoretical approach). 
			Its advantages are: its big size (it contains 40,600 
			annotated discourse relations) allows to apply 
			machine learning, and the discourse annotations 
			are aligned with the syntactic constituency 
			annotations of the Penn Treebank. Its limitations 
			are: dependencies across relations are not marked, 
			it only includes texts of the journalistic domain, 
			and it is not free. Although there are several 
			corpora annotated with discourse relations, there is 
			not a corpus of this type for Spanish. 
			3 The RST Spanish Treebank 
			As Sierra (2008) states, a corpus consists of a 
			compilation of a set of written and/or spoken texts 
			sharing some characteristics, created for certain 
			investigation purposes. According to Hovy (2010), 
			we use 7 core questions in corpus design, detailed 
			in the next subsections. 
			3.1 Selecting a Corpus 
			For the RST Spanish Treebank, we wanted to 
			include short texts (finally, the average is 197 
			words by text; the longest containing 1,051 words 
			and the shortest, 25) in order to get a best on-line 
			visualization of the RST trees. Moreover, in the 
			first stage of the project, we preferred to select 
			specialized texts of very different areas, although 
			in the future we plan to include also non- 
			specialized texts (ex. blogs, news, websites) in 
			order to guarantee the representativity of the 
			corpus. We did not find a pre-existing Spanish 
			corpus with these characteristics, so we decided to 
			build our own corpus. Following Cabre (1999), we 
			consider that a text is specialized if it is written by 
			a professional in a given domain. According to this 
			work, specialized texts can be divided in three 
			levels: high (both the author and the potential 
			reader of the text are specialists), average (the 
			author of the text is a specialist, and the potential 
			reader of that text is a student or someone 
			interested in or possessing some prior knowledge 
			about the subject) and low (the author of the text is 
			a specialist, and the potential reader is the general 
			public). The RST Spanish Treebank includes 
			specialized texts of the three mentioned levels: 
			high (scientific articles, conference proceedings, 
			doctoral theses, etc.), average (textbooks) and low 
			(articles and reports from popular magazines, 
			associations' websites, etc.). The texts have been 
			divided in 9 domains (some of them including 
			subdivisions): Astrophysics, Earthquake 
			Engineering, Economy, Law, Linguistics (Applied 
			Linguistics, Language Acquisition, PLN, 
			Terminology), Mathematics (Primary Education, 
			Secondary Education, Scientific Articles), 
			Medicine (Administration of Health Services, 
			Oncology, Orthopedy), Psychology and Sexuality 
			(Clinical Perspective, Psychological Perspective). 
			The size of a corpus is also a polemic question. 
			If the corpus is developed for machine learning, its 
			size will be enough when the application we want 
			to develop obtains acceptable percentages of 
			precision and recall (in the context of that 
			application). Nevertheless, if the corpus is built 
			with descriptive purposes, it is difficult to 
			determine the corpus size. In the case of a corpus 
			annotated with rhetorical relations, it is even more 
			difficult, because there are various factors 
			involved: EDUs, SPANs (that is, a group of related 
			EDUs), nuclearity and relations. In addition, 
			relations are multiple (we use 28). As Hovy (2010: 
			13) mentions, one of the most difficult phenomena 
			to annotate is the discourse structure. Our corpus 
			contains 52,746 words and 267 texts. Table 1 
			includes RST Spanish Treebank statistics in terms 
			of texts, words, sentences and EDUs. 
			Texts Words Sentences EDUs 
			Learning corpus 183 41,555 1,759 2,655 
			Test corpus 84 11,191 497 694 
			Total corpus 267 52,746 2,256 3,349 
			Table 1: RST Spanish Treebank statistics 
			To increase the linear performance of a 
			statistical method, it is necessary that the training 
			corpus size grows exponentially (Zhao et al., 
			2010). However, the RST Spanish Treebank is not 
			designed only to use statistical methods; we think 
			it will be useful to employ symbolic or hybrid 
			3 
			algorithms (combining symbolic and statistical 
			methods). Moreover, this corpus will be dynamic, 
			so we expect to have a bigger corpus in the future, 
			useful to apply machine learning methods. 
			If we measure the corpus size in terms of words 
			or texts, we can take as a reference the other RST 
			corpora. Nevertheless, as Sierra states (2008), it is 
			"absurd" to try to build an exhaustive corpus 
			covering all the aspects of a language. On the 
			contrary, the linguist looks for the 
			representativeness of the texts, that is, tries to 
			create a sample of the studied language, selecting 
			examples which represent the linguistic reality, in 
			order to analyze them in a pertinent way. In this 
			sense and in the frame of this work, we consider 
			that the size will be adequate if the rhetorical trees 
			of the corpus include a representative number of 
			examples of rhetorical relations, at least 20 
			examples of each one (taking into account that the 
			corpus contains 3115 relations, we consider that 
			this quantity is acceptable; however, we expect to 
			have even more examples when the corpus grows). 
			Table 2 shows the number of examples of each 
			relation currently included into the RST Spanish 
			Treebank (N-S: nucleus-satellite relation; N-N: 
			multinuclear relation). As it can be observed, it 
			contains more than 20 examples of most of the 
			relations. The exceptions are the nucleus-satellite 
			relations of Enablement, Evaluation, Summary, 
			Otherwise and Unless, and the multinuclear 
			relations of Conjunction and Disjunction, because 
			it is not so usual to find these rhetorical relations in 
			the language, in comparison with others. Hovy 
			(2010: 128) states that, given the lack of examples 
			in the corpus, there are 2 possible strategies: a) to 
			leave the corpus as it is, with few or no examples 
			of some cases (but the problem will be the lack of 
			training examples for machine learning systems), 
			or b) to add low-frequency examples artificially to 
			"enrich" the corpus (but the problem will be the 
			distortion of the native frequency distribution and 
			perhaps the confusion of machine learning 
			systems). In the current state of our project, we 
			have chosen the first option. We think that, 
			including specialized texts in a second stage, we 
			will get more examples of these less common 
			relations. If we carry out a more granulated 
			segmentation maybe we could obtain more 
			examples; however, we wanted to employ the 
			segmentation criteria used to develop the Spanish 
			RST discourse segmenter (da Cunha et al., 2011). 
			Quantity 
			Relation Type 
			N % 
			Elaboration N-S 765 24.56 
			Preparation N-S 475 15.25 
			Background N-S 204 6.55 
			Result N-S 193 6.20 
			Means N-S 175 5.62 
			List N-N 172 5.52 
			Joint N-N 160 5.14 
			Circumstance N-S 140 4.49 
			Purpose N-S 122 3.92 
			Interpretation N-S 88 2.83 
			Antithesis N-S 80 2.57 
			Cause N-S 77 2.47 
			Sequency N-N 74 2.38 
			Evidence N-S 59 1.89 
			Contrast N-N 58 1.86 
			Condition N-S 53 1.70 
			Concession N-S 50 1.61 
			Justification N-S 39 1.25 
			Solution N-S 32 1.03 
			Motivation N-S 28 0.90 
			Reformulation N-S 22 0.71 
			Otherwise N-S 3 0.10 
			Conjunction N-N 11 0.35 
			Evaluation N-S 11 0.35 
			Disjunction N-N 9 0.29 
			Summary N-S 8 0.26 
			Enablement N-S 5 0.16 
			Unless N-S 2 0.06 
			Table 2: Rhetorical relations in RST Spanish Treebank 
			3.2 Instantiating the Theory 
			Our segmentation and annotation criteria are very 
			similar to the original ones used by Mann and 
			Thompson (1988) for English, and by da Cunha 
			and Iruskieta (2010) for Spanish. We also explore 
			the annotation manual for English by Carlon and 
			Marcu (2001). Though we use some of their 
			postulates, we think that their analysis is too 
			meticulous in some aspects. Because of this, we 
			consider that it is not adjusted to our interest, 
			which is the finding of the simplest and most 
			objective annotation method, orientated to the 
			4 
			future development of a rhetorical parser for 
			Spanish. To sum up, our segmentation criteria are: 
			a) All the sentences of the text are segmented as 
			EDUs (we consider that a sentence is a textual 
			passage between a period and another period, a 
			semicolon, a question mark or an exclamation 
			point; texts' titles are also segmented). Exs.4 
			[Estas son las razones fundamentales que motivaron 
			este trabajo.] 
			[These are the fundamental reasons which motivated this 
			work.] 
			[Estudio de caso unico sobre violencia conyugal] 
			[Study of a case on conjugal violence] 
			b) Intra-sentence EDUs are segmented, using the 
			following criteria: 
			b1) An intra-sentence EDU has to include a finite 
			verb, an infinitive or a gerund. Ex. 
			[Siendo una variante de la eliminacion Gaussiana,] 
			[posee caracteristicas didacticas ventajosas.] 
			[Being a variant of Gaussian elimination,] [it possesses 
			didactic profitable characteristics.] 
			b2) Subject/object subordinate clauses or 
			substantive sentences are not segmented. Ex. 
			[Se muestra que el modelo discreto en diferencias finitas 
			es convergente y que su realizacion se reduce a resolver 
			una sucesion de sistemas lineales tridiagonales.] 
			[It appears that the discreet model in finite differences is 
			convergent and that its accomplishment is to solve a 
			succession of tridiagonal linear systems.] 
			b3) Subordinate relative clauses are not segmented. 
			Ex. 
			[Durante el proceso, que utiliza solo aritmetica entera, 
			se obtiene el determinante de la matriz de coeficientes 
			del sistema, sin necesidad de calculos adicionales.] 
			[During the process, which only uses entire arithmetic, the 
			determinant of the system coefficient matrix is obtained, 
			without additional calculations.] 
			b4) Elements in parentheses are only segmented if 
			they follow the criterion b1. Ex. 
			[Este ano se cumple el bicentenario del nacimiento de 
			Niels (Nicolas, en nuestro idioma) Henrik Abel.] 
			[This year is the bicentenary of Niels's birth (Nicolas, in 
			our language) Henrik Abel.] 
			b5) Embedded units are segmented by means of 
			the non-relation Same-Unit proposed by Carlon 
			and Marcu (2001). Figure 1 shows this structure. 
			[En decadas precedentes se ha puesto de manifiesto,] [y 
			asi lo han atestiguado muchos investigadores de la 
			4 Spanish examples were extracted from the corpus. English 
			translations are ours. 
			terminologia cientifica serbia,] [una tendencia a 
			importar prestamos del ingles.] 
			[In previous decades it has been shown,] [and it has been 
			testified by many researchers of the scientific Serbian 
			terminology,] [a trend to import loanwords from English.] 
			Figure 1: Example of the non-relation Same-Unit 
			3.3 Designing the Interface 
			The annotation tool used in this work is the 
			RSTtool, since it is free and easy to use. Therefore, 
			we preferred to use it instead of designing a new 
			one. Nevertheless, we have designed an on-line 
			interface to include the corpus and to carry out 
			searches over it (see Section 4). 
			3.4 Selecting and Training the Annotators 
			With regard to the corpus annotators, we have a 
			team of 10 people (last year Bachelor's degree 
			students, Master's degree students and PhDs) 5 . 
			Before the annotation, they took a RST course of 6 
			months (100 hours), where the segmentation and 
			annotation methodology used for the development 
			of the RST Spanish Treebank was explained.6 We 
			called this period "training phase". The course had 
			a theoretical and a practical part. In the theoretical 
			part, some criteria with regard to the 3 phases of 
			rhetorical analysis (segmentation, detection of 
			relations, and rhetorical trees building) were given 
			to annotators. In the practical part, firstly, it was 
			explained how to use the RSTtool. Secondly, 
			annotators extracted several texts from the web, 
			following their personal interests, as for example, 
			music, video games, cookery or art webs. They 
			segmented those texts, using the established 
			segmentation criteria. Once segmented, all the 
			doubts and problematic examples were discussed, 
			and they tried to get an agreement on the most 
			complicated cases. Thirdly, the relations were 
			5 We thank annotators (Adriana Valerio, Brenda Castro, 
			Daniel Rodriguez, Ita Cruz, Jessica Mendez, Josue Careaga, 
			Luis Cabrera, Marina Fomicheva and Paulina De La Vega) 
			and interface developers (Luis Cabrera and Juan Rolland). 
			6 This course was given in the framework of a last-year subject 
			in the Spanish Linguistics Degree at UNAM (Mexico City). 
			5 
			analyzed (using a given relations list) and, once 
			again, annotators discussed the difficult cases. 
			After the discussion, texts were re-annotated to 
			verify if the difficulties were solved. This process 
			was doubly interesting, since it helped to create 
			common criteria for the annotation of the final 
			corpus and to define the annotation criteria more 
			clearly and consensually, in order to include them 
			in the RST Spanish Treebank annotation manual. 
			Once annotators agreed on the most difficult cases, 
			we consider that the training phase finished. 
			3.5 Designing and Managing the Annotation 
			Procedure 
			We start from the following annotation definition: 
			Annotation (`tagging') is the process of adding new 
			information into source material by humans 
			(annotators) or suitably trained machines. [...]. The 
			addition process usually requires some sort of 
			mental decision that depends both on the source 
			material and on some theory or knowledge that the 
			annotator has internalized earlier. (Hovy, 2010: 6) 
			Exactly, after our annotators internalized the 
			theory and annotation criteria during the training 
			phase, the "annotation phase" of the final texts 
			included in the RST Spanish Treebank started. In 
			this phase, the annotation tasks were assigned to 
			annotators (the number of texts assigned to each 
			annotator was different, depending on their 
			availability). They were asked to carry out the 
			annotation individually and without questions 
			among them. We calculated that the average time 
			to carry out the annotation of one text was between 
			15 minutes and 1 hour. This time difference is due 
			to the fact that the corpus includes both short and 
			long texts. The annotation process is the following: 
			once a text is segmented, rhetorical relations 
			between EDUs are annotated. First, EDUs inside 
			the same sentence are annotated in a binary way. 
			Second, sentences inside the same paragraph are 
			linked. Finally, paragraphs are linked. 
			Hovy (2010) states that it is difficult to 
			determine if, for the same money (we add "for the 
			same time"), it is better to double-annotate less, or 
			to single-annotate more. As he explains, Dligach et 
			al. (2010) made an experiment with OntoNotes 
			(Pradhan et al., 2007) verb sense annotation. The 
			result was that, assuming the annotation is stable 
			(that is, inter-annotator agreement is high), it is 
			better to annotate more, even with only one 
			annotator. The problem with RST annotation is 
			that there are so many categories to annotate, that 
			is very difficult to obtain a stable annotation. 
			Therefore, we consider it is necessary to have at 
			least some texts double-annotated (or even triple- 
			annotated), in order to have an adequate discourse 
			corpus. This is the reason why, following the RST 
			Discourse Treebank methodology, we use some 
			texts as learning corpus and some others (from the 
			Mathematics, Psychology and Sexuality domains) 
			as test corpus: 69% (183 texts) and 31% (84 texts), 
			respectively. The texts of the learning corpus were 
			annotated by 1 person, whereas the texts of the test 
			corpus were annotated by 2 people. 
			3.6 Validating Results 
			Da Cunha and Iruskieta (2010) measure inter- 
			annotator agreement by using the RST trees 
			comparison methodology by Marcu (2000). This 
			methodology evaluates the agreement on 4 
			elements (EDUs, SPANs, Nuclearity and 
			Relations), by means of precision and recall 
			measures (an annotation with regard to the other 
			one). Following this methodology, we have 
			measured inter-annotator agreement over the test 
			corpus. We employ an on-line automatic tool for 
			RST trees comparison, RSTeval (Mazeiro and 
			Pardo, 2009), where Marcu's methodology has 
			been implemented (for 4 languages: English, 
			Portuguese, Spanish and Basque). We know that 
			there are some other ways to measure agreement, 
			such as Cohen's kappa (Cohen, 1960) or Fleiss's 
			kappa (Fleiss, 1971), for example. Nevertheless, 
			we consider that Marcu's methodology (2000) is 
			suitable to compare adequately 2 annotations of the 
			same original text, because it has been designed 
			specifically for this task. 
			For each trees pair from the test corpus, 
			precision and recall were measured separately. 
			Afterwards, all those individual results were put 
			together to obtain general results. Table 3 shows 
			global results for the 4 categories. The category 
			with more agreement was EDUs (recall: 91.04% / 
			precision: 87.20%), that is, segmentation. This 
			result was expected, since the segmentation criteria 
			given to the annotators were quite precise and the 
			possibility of mistake was low. The lowest 
			agreement was obtained for the category Relations 
			(recall: 78.48% / precision: 76.81%). This result is 
			lower than the other, but we think it is acceptable. 
			In the RST Discourse Treebank the trend was 
			similar to the one detected in our corpus: the 
			6 
			highest agreement is obtained at the segmentation 
			level and the lowest at the relations level. 
			Category Precision Recall 
			EDUs 87.20% 91.04% 
			SPANs 86% 87.31% 
			Nuclearity 82.46% 84.66% 
			Relations 76.81% 78.48% 
			Table 3: Inter-annotator agreement 
			Precision and recall have not been calculated 
			with respect to a gold standard because it does not 
			exist for Spanish. Our future aim is to reach a 
			consensus on the annotation of the test corpus 
			(using an external "judge"), in order to establish a 
			set of texts considered as a preliminary gold 
			standard for this language. We consider that the 
			annotations have quality at present, because inter- 
			annotator agreement is quite high; however, this 
			consensus could solve the typical annotation 
			mistakes we have detected or some ambiguities. 
			We have analyzed the main discrepancy reasons 
			between annotators. With regard to the 
			segmentation, the main one was human mistake; 
			ex. segmenting EDUs without a verb (one 
			annotator segmented the following passage into 2 
			EDUs because she detected a Means relation, but 
			the second EDU does not include any verb): 
			[Ademas estudiamos el desarrollo de criterios para 
			determinar si un semigrupo dado tiene dicha propiedad ] 
			[mediante el estudio de desigualdades de curvatura- 
			dimension. ] 
			[We also study the development of tests in order to 
			determine if a given semi group has this property] [by means 
			of curvature-dimension inequalities.] 
			The second reason was that in the manual some 
			aspects were not explained in detail. For example, 
			if a substantive sentence or a direct/object clause 
			(which must not be segmented, according to the 
			point b2) includes two coordinated clauses, these 
			must not be segmented either. Thus, we found 
			some erroneous segmentations. For example: 
			[Los hombres adultos tienen miedo de fracasar] [y no 
			cumplir con el rol masculino de ser proveedores del 
			hogar y de proteger a su familia.] 
			[Adult men are scared to fail] [and not to fulfill the 
			masculine role of being the suppliers of the home and to 
			protect their family.] 
			This kind of mistakes allowed us to refine our 
			segmentation manual a posteriori. In the future, we 
			will ask the test corpus annotators to make a new 
			annotation of the texts, using the refined manual, in 
			order to check if the agreement increases, in the 
			same way as the RST Discourse Treebank. 
			With regard to rhetorical annotations, we 
			detected 2 main reasons of inter-annotator 
			disagreement. The first one was the ambiguity of 
			some relations and their corresponding connectors; 
			for example, Justification-Reason, Antithesis- 
			Concession or Circumstance-Means relations, like 
			in the following passage (in Spanish, "al" may 
			indicate time or manner): 
			[Los ninos aprenden matematicas] [al resolver 
			problemas.] 
			[Children learn mathematics] [when solving problems.] 
			The second one is due to differences between 
			annotators when determining nuclearity. For 
			example, in the following passage, one annotator 
			marked Background and the other one Elaboration: 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]S_Background [Norma y Andres quieren 
			colocar en el hueco una pecera. ]N_Background 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]N_Elaboration [Norma y Andres quieren 
			colocar en el hueco una pecera. ]S_Elaboration 
			[A hole of 60 x 1.20 cm remained in the wall.] [Norma and 
			Andres want to place a fish tank in the hole.] 
			It is easier to solve segmentation disagreement 
			than relations disagreement, since in this case 
			annotator subjectivity is more evident; we must 
			consider how to refine our manual in this sense. 
			3.7 Delivering and Maintaining the Product 
			Hovy (2010) mentions some technical issues 
			regarding these points: licensing, distribution, 
			maintenance and updates. With regard to licensing 
			and distribution, the RST Spanish Treebank will be 
			free for research purposes. We have a data 
			manager responsible for maintenance and updates. 
			The description of the annotated corpus is also 
			a very important issue (Ide and Pustejovsky, 2010). 
			It is important to provide a high level description 
			of the corpus, including the theoretical framework, 
			the methodology (annotators, annotation manual 
			and tool, agreement, etc.), the means for resource 
			maintenance, the technical aspects, the project 
			leader, the contact, the team, etc. The RST Spanish 
			Treebank includes all this detailed information. 
			XML (with a DTD) has been used, in order the 
			corpus can be reused for several aplications. In the 
			future, we plan to use the standard XCES. 
			7 
			To know more about resources development, 
			linguistic annotation or inter-annotator agreement, 
			we recommend: Palmer et al. (on-line), Palmer and 
			Xue (2010), and Artstein and Poesio (2008). 
			4 The Search Interface of the RST 
			Spanish Treebank 
			The RST Spanish Treebank interface is freely 
			available on-line7. It allows the visualization and 
			downloading of all the texts in txt format, with 
			their corresponding annotated trees in RSTtool 
			format (rs3), as well as in image format (jpg). Each 
			text includes its title, its reference, its web link (if 
			it is an on-line text) and its number of words. The 
			interface shows texts by areas and allows the user 
			to select a subcorpus (including individual files or 
			folders containing several files). The selected 
			subcorpus can be saved on local disk (generating a 
			xml file) for future analyses. 
			The interface includes a statistical tool which 
			allows obtaining statistics of rhetorical relations in 
			a subcorpus selected by the user. The RSTtool also 
			offers this option but it can be only used for one 
			text. We consider that it is more useful for the user 
			to obtain statistics from various texts, in order to 
			get significant statistical results. As the RSTtool, 
			our tool allows to count the multinuclear relations 
			in two ways: a) one unit for each detected 
			multinuclear relation, and b) one unit for each 
			detected nucleus. If we use b), the statistics of the 
			multinuclear relations of Table 2 are higher: List 
			(864), Joint (537), Sequence (289), Contrast (153), 
			Conjunction (28) and Disjunction (24). 
			We are developing another tool, aimed to 
			extract information from the annotated texts, which 
			we will soon include into the interface. This tool 
			will allow to the user to select a subcorpus and to 
			extract from it the EDUs corresponding to the 
			rhetorical relations selected, like a multidocument 
			specialized summarizer guided by user's interests. 
			The RST Spanish Treebank interface also 
			includes a screen which permits the users to send 
			their own annotated texts. Our aim is for the RST 
			Spanish Treebank to become a dynamic corpus, in 
			constant evolution, being increased with texts 
			annotated by users. This has a double advantage 
			since, on the one hand, the corpus will grow and, 
			on the other hand, users will profit from the 
			7 http://www.corpus.unam.mx/rst/ 
			interface's applications, using their own 
			subcorpora. The only requirement is to use the 
			relations and the segmentation and annotation 
			criteria of our project. Once the texts are sent, the 
			RST Spanish Treebank data manager will verify if 
			the annotation corresponds to these criteria. 
			2 State of the Art 
			The most known RST corpus is the RST Discourse 
			Treebank, for English (Carlson et al., 2002a, 
			2002b). It includes 385 texts of the journalistic 
			domain, extracted from the Penn Treebank 
			(Marcus et al., 1993), such as cultural reviews, 
			editorials, economy articles, etc. 347 texts are used 
			as a learning corpus and 38 texts are used as a test 
			corpus. It contains 176,389 words and 21,789 
			EDUs. 13.8% of the texts (that is, 53) were 
			annotated by two people with a list of 78 relations. 
			For annotation, the annotation tool RSTtool 2 
			(O'Donnell, 2000) was used, with some 
			adaptations. The principal advantages of this 
			corpus stand on the high number of annotated texts 
			(for the moment it is the biggest RST corpus) and 
			the clarity of the annotation method (specified in 
			the annotation manual by Carlson and Marcu, 
			2001). However, some drawbacks remain. The 
			corpus is not free, it is not on-line and it only 
			includes texts of one domain (journalistic). 
			For English there is also the Discourse 
			Relations Reference Corpus (Taboada and 
			Renkema, 2008). This corpus includes 65 texts 
			(each one tagged by one annotator) of several types 
			and from several sources: 21 articles from the Wall 
			Street Journal extracted from the RST Discourse 
			Treebank, 30 movies and books' reviews extracted 
			from the epinions.com website, and 14 diverse 
			texts, including letters, webs, magazine articles, 
			newspaper editorials, etc. The tool used for 
			annotation was also the RSTtool. The advantages 
			of this corpus are that it is free and on-line, and it 
			includes texts of several types and domains. The 
			disadvantages are that the amount of texts is not 
			very high, the annotation methodology is not 
			2 http://www.wagsoft.com/RSTTool/ 
			specified and it does not include texts annotated by 
			several people. 
			Another well-known corpus is the Potsdam 
			Commentary Corpus, for German (Stede, 2004; 
			Reitter and Stede, 2003). This corpus includes 173 
			texts on politics from the on-line newspaper 
			Markische Allgemeine Zeitung. It contains 32,962 
			words and 2,195 sentences. It is annotated with 
			several data: morphology, syntax, rhetorical 
			structure, connectors, correference and informative 
			structure. Nevertheless, only a part of this corpus 
			(10 texts), which the authors name "core corpus", 
			is annotated with all this information. The texts 
			were annotated with the RSTtool. This corpus has 
			several advantages: it is annotated at different 
			levels (the annotation of connectors is especially 
			interesting); all the texts were annotated by two 
			people (with a previous RST training phase); it is 
			free for research purposes, and there is a tool for 
			searching over the corpus (although it is not 
			available on-line). The disadvantages are: the 
			genre and domain of all the texts are the same, the 
			methodology of annotation was quite intuitive 
			(without a manual or specific criteria) and the 
			inter-annotator agreement is not given. 
			For Portuguese, there are 2 corpora, built in 
			order to develop a rhetorical parser (Pardo et al., 
			2008). The first one, the CorpusTCC (Pardo et al., 
			2008), was used as learning corpus for detection of 
			linguistic patterns indicating rhetorical relations. It 
			contains 100 introduction sections of computer 
			science theses (53,000 words and 1,350 sentences). 
			To annotate the corpus a list of 32 rhetorical 
			relations was used. The annotation manual by 
			Carlson and Marcu (2001) was adapted to 
			Portuguese. The annotation tool was the ISI RST 
			Annotation Tool3 , an extension of the RSTtool. 
			The advantages of this corpus are: it is free, it 
			contains an acceptable number of texts and words 
			and it follows a specific annotation methodology. 
			The disadvantage is: it only includes texts of one 
			genre and domain, only annotated by one person. 
			The second one, Rhetalho (Pardo and Seno, 
			2005), was used as reference corpus for the parser 
			evaluation. It contains 50 texts: 20 introduction 
			sections and 10 conclusion sections from computer 
			science scientific articles, and 20 texts from the on- 
			line newspaper Folha de Sao Paulo (7 from the 
			Daily section, 7 from the World section and 6 from 
			3 http://www.isi.edu/~marcu/discourse/ 
			2 
			the Science section). It includes approximately 
			5,000 words. The relations and the annotation tool 
			are the same as those used in the CorpusTCC. The 
			advantages of this corpus are that it is free, it was 
			annotated by 2 people (they both were RST experts 
			and followed an annotation manual) and it contains 
			texts of several genres and domains. The main 
			disadvantage is the scarce amount of texts. 
			The Penn Discourse Treebank (Rashmi et al., 
			2008)f for English includes texts annotated with 
			information related to discourse structure and 
			semantics (without a specific theoretical approach). 
			Its advantages are: its big size (it contains 40,600 
			annotated discourse relations) allows to apply 
			machine learning, and the discourse annotations 
			are aligned with the syntactic constituency 
			annotations of the Penn Treebank. Its limitations 
			are: dependencies across relations are not marked, 
			it only includes texts of the journalistic domain, 
			and it is not free. Although there are several 
			corpora annotated with discourse relations, there is 
			not a corpus of this type for Spanish. 
			3 The RST Spanish Treebank 
			As Sierra (2008) states, a corpus consists of a 
			compilation of a set of written and/or spoken texts 
			sharing some characteristics, created for certain 
			investigation purposes. According to Hovy (2010), 
			we use 7 core questions in corpus design, detailed 
			in the next subsections. 
			3.1 Selecting a Corpus 
			For the RST Spanish Treebank, we wanted to 
			include short texts (finally, the average is 197 
			words by text; the longest containing 1,051 words 
			and the shortest, 25) in order to get a best on-line 
			visualization of the RST trees. Moreover, in the 
			first stage of the project, we preferred to select 
			specialized texts of very different areas, although 
			in the future we plan to include also non- 
			specialized texts (ex. blogs, news, websites) in 
			order to guarantee the representativity of the 
			corpus. We did not find a pre-existing Spanish 
			corpus with these characteristics, so we decided to 
			build our own corpus. Following Cabre (1999), we 
			consider that a text is specialized if it is written by 
			a professional in a given domain. According to this 
			work, specialized texts can be divided in three 
			levels: high (both the author and the potential 
			reader of the text are specialists), average (the 
			author of the text is a specialist, and the potential 
			reader of that text is a student or someone 
			interested in or possessing some prior knowledge 
			about the subject) and low (the author of the text is 
			a specialist, and the potential reader is the general 
			public). The RST Spanish Treebank includes 
			specialized texts of the three mentioned levels: 
			high (scientific articles, conference proceedings, 
			doctoral theses, etc.), average (textbooks) and low 
			(articles and reports from popular magazines, 
			associations' websites, etc.). The texts have been 
			divided in 9 domains (some of them including 
			subdivisions): Astrophysics, Earthquake 
			Engineering, Economy, Law, Linguistics (Applied 
			Linguistics, Language Acquisition, PLN, 
			Terminology), Mathematics (Primary Education, 
			Secondary Education, Scientific Articles), 
			Medicine (Administration of Health Services, 
			Oncology, Orthopedy), Psychology and Sexuality 
			(Clinical Perspective, Psychological Perspective). 
			The size of a corpus is also a polemic question. 
			If the corpus is developed for machine learning, its 
			size will be enough when the application we want 
			to develop obtains acceptable percentages of 
			precision and recall (in the context of that 
			application). Nevertheless, if the corpus is built 
			with descriptive purposes, it is difficult to 
			determine the corpus size. In the case of a corpus 
			annotated with rhetorical relations, it is even more 
			difficult, because there are various factors 
			involved: EDUs, SPANs (that is, a group of related 
			EDUs), nuclearity and relations. In addition, 
			relations are multiple (we use 28). As Hovy (2010: 
			13) mentions, one of the most difficult phenomena 
			to annotate is the discourse structure. Our corpus 
			contains 52,746 words and 267 texts. Table 1 
			includes RST Spanish Treebank statistics in terms 
			of texts, words, sentences and EDUs. 
			Texts Words Sentences EDUs 
			Learning corpus 183 41,555 1,759 2,655 
			Test corpus 84 11,191 497 694 
			Total corpus 267 52,746 2,256 3,349 
			Table 1: RST Spanish Treebank statistics 
			To increase the linear performance of a 
			statistical method, it is necessary that the training 
			corpus size grows exponentially (Zhao et al., 
			2010). However, the RST Spanish Treebank is not 
			designed only to use statistical methods; we think 
			it will be useful to employ symbolic or hybrid 
			3 
			algorithms (combining symbolic and statistical 
			methods). Moreover, this corpus will be dynamic, 
			so we expect to have a bigger corpus in the future, 
			useful to apply machine learning methods. 
			If we measure the corpus size in terms of words 
			or texts, we can take as a reference the other RST 
			corpora. Nevertheless, as Sierra states (2008), it is 
			"absurd" to try to build an exhaustive corpus 
			covering all the aspects of a language. On the 
			contrary, the linguist looks for the 
			representativeness of the texts, that is, tries to 
			create a sample of the studied language, selecting 
			examples which represent the linguistic reality, in 
			order to analyze them in a pertinent way. In this 
			sense and in the frame of this work, we consider 
			that the size will be adequate if the rhetorical trees 
			of the corpus include a representative number of 
			examples of rhetorical relations, at least 20 
			examples of each one (taking into account that the 
			corpus contains 3115 relations, we consider that 
			this quantity is acceptable; however, we expect to 
			have even more examples when the corpus grows). 
			Table 2 shows the number of examples of each 
			relation currently included into the RST Spanish 
			Treebank (N-S: nucleus-satellite relation; N-N: 
			multinuclear relation). As it can be observed, it 
			contains more than 20 examples of most of the 
			relations. The exceptions are the nucleus-satellite 
			relations of Enablement, Evaluation, Summary, 
			Otherwise and Unless, and the multinuclear 
			relations of Conjunction and Disjunction, because 
			it is not so usual to find these rhetorical relations in 
			the language, in comparison with others. Hovy 
			(2010: 128) states that, given the lack of examples 
			in the corpus, there are 2 possible strategies: a) to 
			leave the corpus as it is, with few or no examples 
			of some cases (but the problem will be the lack of 
			training examples for machine learning systems), 
			or b) to add low-frequency examples artificially to 
			"enrich" the corpus (but the problem will be the 
			distortion of the native frequency distribution and 
			perhaps the confusion of machine learning 
			systems). In the current state of our project, we 
			have chosen the first option. We think that, 
			including specialized texts in a second stage, we 
			will get more examples of these less common 
			relations. If we carry out a more granulated 
			segmentation maybe we could obtain more 
			examples; however, we wanted to employ the 
			segmentation criteria used to develop the Spanish 
			RST discourse segmenter (da Cunha et al., 2011). 
			Quantity 
			Relation Type 
			N % 
			Elaboration N-S 765 24.56 
			Preparation N-S 475 15.25 
			Background N-S 204 6.55 
			Result N-S 193 6.20 
			Means N-S 175 5.62 
			List N-N 172 5.52 
			Joint N-N 160 5.14 
			Circumstance N-S 140 4.49 
			Purpose N-S 122 3.92 
			Interpretation N-S 88 2.83 
			Antithesis N-S 80 2.57 
			Cause N-S 77 2.47 
			Sequency N-N 74 2.38 
			Evidence N-S 59 1.89 
			Contrast N-N 58 1.86 
			Condition N-S 53 1.70 
			Concession N-S 50 1.61 
			Justification N-S 39 1.25 
			Solution N-S 32 1.03 
			Motivation N-S 28 0.90 
			Reformulation N-S 22 0.71 
			Otherwise N-S 3 0.10 
			Conjunction N-N 11 0.35 
			Evaluation N-S 11 0.35 
			Disjunction N-N 9 0.29 
			Summary N-S 8 0.26 
			Enablement N-S 5 0.16 
			Unless N-S 2 0.06 
			Table 2: Rhetorical relations in RST Spanish Treebank 
			3.2 Instantiating the Theory 
			Our segmentation and annotation criteria are very 
			similar to the original ones used by Mann and 
			Thompson (1988) for English, and by da Cunha 
			and Iruskieta (2010) for Spanish. We also explore 
			the annotation manual for English by Carlon and 
			Marcu (2001). Though we use some of their 
			postulates, we think that their analysis is too 
			meticulous in some aspects. Because of this, we 
			consider that it is not adjusted to our interest, 
			which is the finding of the simplest and most 
			objective annotation method, orientated to the 
			4 
			future development of a rhetorical parser for 
			Spanish. To sum up, our segmentation criteria are: 
			a) All the sentences of the text are segmented as 
			EDUs (we consider that a sentence is a textual 
			passage between a period and another period, a 
			semicolon, a question mark or an exclamation 
			point; texts' titles are also segmented). Exs.4 
			[Estas son las razones fundamentales que motivaron 
			este trabajo.] 
			[These are the fundamental reasons which motivated this 
			work.] 
			[Estudio de caso unico sobre violencia conyugal] 
			[Study of a case on conjugal violence] 
			b) Intra-sentence EDUs are segmented, using the 
			following criteria: 
			b1) An intra-sentence EDU has to include a finite 
			verb, an infinitive or a gerund. Ex. 
			[Siendo una variante de la eliminacion Gaussiana,] 
			[posee caracteristicas didacticas ventajosas.] 
			[Being a variant of Gaussian elimination,] [it possesses 
			didactic profitable characteristics.] 
			b2) Subject/object subordinate clauses or 
			substantive sentences are not segmented. Ex. 
			[Se muestra que el modelo discreto en diferencias finitas 
			es convergente y que su realizacion se reduce a resolver 
			una sucesion de sistemas lineales tridiagonales.] 
			[It appears that the discreet model in finite differences is 
			convergent and that its accomplishment is to solve a 
			succession of tridiagonal linear systems.] 
			b3) Subordinate relative clauses are not segmented. 
			Ex. 
			[Durante el proceso, que utiliza solo aritmetica entera, 
			se obtiene el determinante de la matriz de coeficientes 
			del sistema, sin necesidad de calculos adicionales.] 
			[During the process, which only uses entire arithmetic, the 
			determinant of the system coefficient matrix is obtained, 
			without additional calculations.] 
			b4) Elements in parentheses are only segmented if 
			they follow the criterion b1. Ex. 
			[Este ano se cumple el bicentenario del nacimiento de 
			Niels (Nicolas, en nuestro idioma) Henrik Abel.] 
			[This year is the bicentenary of Niels's birth (Nicolas, in 
			our language) Henrik Abel.] 
			b5) Embedded units are segmented by means of 
			the non-relation Same-Unit proposed by Carlon 
			and Marcu (2001). Figure 1 shows this structure. 
			[En decadas precedentes se ha puesto de manifiesto,] [y 
			asi lo han atestiguado muchos investigadores de la 
			4 Spanish examples were extracted from the corpus. English 
			translations are ours. 
			terminologia cientifica serbia,] [una tendencia a 
			importar prestamos del ingles.] 
			[In previous decades it has been shown,] [and it has been 
			testified by many researchers of the scientific Serbian 
			terminology,] [a trend to import loanwords from English.] 
			Figure 1: Example of the non-relation Same-Unit 
			3.3 Designing the Interface 
			The annotation tool used in this work is the 
			RSTtool, since it is free and easy to use. Therefore, 
			we preferred to use it instead of designing a new 
			one. Nevertheless, we have designed an on-line 
			interface to include the corpus and to carry out 
			searches over it (see Section 4). 
			3.4 Selecting and Training the Annotators 
			With regard to the corpus annotators, we have a 
			team of 10 people (last year Bachelor's degree 
			students, Master's degree students and PhDs) 5 . 
			Before the annotation, they took a RST course of 6 
			months (100 hours), where the segmentation and 
			annotation methodology used for the development 
			of the RST Spanish Treebank was explained.6 We 
			called this period "training phase". The course had 
			a theoretical and a practical part. In the theoretical 
			part, some criteria with regard to the 3 phases of 
			rhetorical analysis (segmentation, detection of 
			relations, and rhetorical trees building) were given 
			to annotators. In the practical part, firstly, it was 
			explained how to use the RSTtool. Secondly, 
			annotators extracted several texts from the web, 
			following their personal interests, as for example, 
			music, video games, cookery or art webs. They 
			segmented those texts, using the established 
			segmentation criteria. Once segmented, all the 
			doubts and problematic examples were discussed, 
			and they tried to get an agreement on the most 
			complicated cases. Thirdly, the relations were 
			5 We thank annotators (Adriana Valerio, Brenda Castro, 
			Daniel Rodriguez, Ita Cruz, Jessica Mendez, Josue Careaga, 
			Luis Cabrera, Marina Fomicheva and Paulina De La Vega) 
			and interface developers (Luis Cabrera and Juan Rolland). 
			6 This course was given in the framework of a last-year subject 
			in the Spanish Linguistics Degree at UNAM (Mexico City). 
			5 
			analyzed (using a given relations list) and, once 
			again, annotators discussed the difficult cases. 
			After the discussion, texts were re-annotated to 
			verify if the difficulties were solved. This process 
			was doubly interesting, since it helped to create 
			common criteria for the annotation of the final 
			corpus and to define the annotation criteria more 
			clearly and consensually, in order to include them 
			in the RST Spanish Treebank annotation manual. 
			Once annotators agreed on the most difficult cases, 
			we consider that the training phase finished. 
			3.5 Designing and Managing the Annotation 
			Procedure 
			We start from the following annotation definition: 
			Annotation (`tagging') is the process of adding new 
			information into source material by humans 
			(annotators) or suitably trained machines. [...]. The 
			addition process usually requires some sort of 
			mental decision that depends both on the source 
			material and on some theory or knowledge that the 
			annotator has internalized earlier. (Hovy, 2010: 6) 
			Exactly, after our annotators internalized the 
			theory and annotation criteria during the training 
			phase, the "annotation phase" of the final texts 
			included in the RST Spanish Treebank started. In 
			this phase, the annotation tasks were assigned to 
			annotators (the number of texts assigned to each 
			annotator was different, depending on their 
			availability). They were asked to carry out the 
			annotation individually and without questions 
			among them. We calculated that the average time 
			to carry out the annotation of one text was between 
			15 minutes and 1 hour. This time difference is due 
			to the fact that the corpus includes both short and 
			long texts. The annotation process is the following: 
			once a text is segmented, rhetorical relations 
			between EDUs are annotated. First, EDUs inside 
			the same sentence are annotated in a binary way. 
			Second, sentences inside the same paragraph are 
			linked. Finally, paragraphs are linked. 
			Hovy (2010) states that it is difficult to 
			determine if, for the same money (we add "for the 
			same time"), it is better to double-annotate less, or 
			to single-annotate more. As he explains, Dligach et 
			al. (2010) made an experiment with OntoNotes 
			(Pradhan et al., 2007) verb sense annotation. The 
			result was that, assuming the annotation is stable 
			(that is, inter-annotator agreement is high), it is 
			better to annotate more, even with only one 
			annotator. The problem with RST annotation is 
			that there are so many categories to annotate, that 
			is very difficult to obtain a stable annotation. 
			Therefore, we consider it is necessary to have at 
			least some texts double-annotated (or even triple- 
			annotated), in order to have an adequate discourse 
			corpus. This is the reason why, following the RST 
			Discourse Treebank methodology, we use some 
			texts as learning corpus and some others (from the 
			Mathematics, Psychology and Sexuality domains) 
			as test corpus: 69% (183 texts) and 31% (84 texts), 
			respectively. The texts of the learning corpus were 
			annotated by 1 person, whereas the texts of the test 
			corpus were annotated by 2 people. 
			3.6 Validating Results 
			Da Cunha and Iruskieta (2010) measure inter- 
			annotator agreement by using the RST trees 
			comparison methodology by Marcu (2000). This 
			methodology evaluates the agreement on 4 
			elements (EDUs, SPANs, Nuclearity and 
			Relations), by means of precision and recall 
			measures (an annotation with regard to the other 
			one). Following this methodology, we have 
			measured inter-annotator agreement over the test 
			corpus. We employ an on-line automatic tool for 
			RST trees comparison, RSTeval (Mazeiro and 
			Pardo, 2009), where Marcu's methodology has 
			been implemented (for 4 languages: English, 
			Portuguese, Spanish and Basque). We know that 
			there are some other ways to measure agreement, 
			such as Cohen's kappa (Cohen, 1960) or Fleiss's 
			kappa (Fleiss, 1971), for example. Nevertheless, 
			we consider that Marcu's methodology (2000) is 
			suitable to compare adequately 2 annotations of the 
			same original text, because it has been designed 
			specifically for this task. 
			For each trees pair from the test corpus, 
			precision and recall were measured separately. 
			Afterwards, all those individual results were put 
			together to obtain general results. Table 3 shows 
			global results for the 4 categories. The category 
			with more agreement was EDUs (recall: 91.04% / 
			precision: 87.20%), that is, segmentation. This 
			result was expected, since the segmentation criteria 
			given to the annotators were quite precise and the 
			possibility of mistake was low. The lowest 
			agreement was obtained for the category Relations 
			(recall: 78.48% / precision: 76.81%). This result is 
			lower than the other, but we think it is acceptable. 
			In the RST Discourse Treebank the trend was 
			similar to the one detected in our corpus: the 
			6 
			highest agreement is obtained at the segmentation 
			level and the lowest at the relations level. 
			Category Precision Recall 
			EDUs 87.20% 91.04% 
			SPANs 86% 87.31% 
			Nuclearity 82.46% 84.66% 
			Relations 76.81% 78.48% 
			Table 3: Inter-annotator agreement 
			Precision and recall have not been calculated 
			with respect to a gold standard because it does not 
			exist for Spanish. Our future aim is to reach a 
			consensus on the annotation of the test corpus 
			(using an external "judge"), in order to establish a 
			set of texts considered as a preliminary gold 
			standard for this language. We consider that the 
			annotations have quality at present, because inter- 
			annotator agreement is quite high; however, this 
			consensus could solve the typical annotation 
			mistakes we have detected or some ambiguities. 
			We have analyzed the main discrepancy reasons 
			between annotators. With regard to the 
			segmentation, the main one was human mistake; 
			ex. segmenting EDUs without a verb (one 
			annotator segmented the following passage into 2 
			EDUs because she detected a Means relation, but 
			the second EDU does not include any verb): 
			[Ademas estudiamos el desarrollo de criterios para 
			determinar si un semigrupo dado tiene dicha propiedad ] 
			[mediante el estudio de desigualdades de curvatura- 
			dimension. ] 
			[We also study the development of tests in order to 
			determine if a given semi group has this property] [by means 
			of curvature-dimension inequalities.] 
			The second reason was that in the manual some 
			aspects were not explained in detail. For example, 
			if a substantive sentence or a direct/object clause 
			(which must not be segmented, according to the 
			point b2) includes two coordinated clauses, these 
			must not be segmented either. Thus, we found 
			some erroneous segmentations. For example: 
			[Los hombres adultos tienen miedo de fracasar] [y no 
			cumplir con el rol masculino de ser proveedores del 
			hogar y de proteger a su familia.] 
			[Adult men are scared to fail] [and not to fulfill the 
			masculine role of being the suppliers of the home and to 
			protect their family.] 
			This kind of mistakes allowed us to refine our 
			segmentation manual a posteriori. In the future, we 
			will ask the test corpus annotators to make a new 
			annotation of the texts, using the refined manual, in 
			order to check if the agreement increases, in the 
			same way as the RST Discourse Treebank. 
			With regard to rhetorical annotations, we 
			detected 2 main reasons of inter-annotator 
			disagreement. The first one was the ambiguity of 
			some relations and their corresponding connectors; 
			for example, Justification-Reason, Antithesis- 
			Concession or Circumstance-Means relations, like 
			in the following passage (in Spanish, "al" may 
			indicate time or manner): 
			[Los ninos aprenden matematicas] [al resolver 
			problemas.] 
			[Children learn mathematics] [when solving problems.] 
			The second one is due to differences between 
			annotators when determining nuclearity. For 
			example, in the following passage, one annotator 
			marked Background and the other one Elaboration: 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]S_Background [Norma y Andres quieren 
			colocar en el hueco una pecera. ]N_Background 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]N_Elaboration [Norma y Andres quieren 
			colocar en el hueco una pecera. ]S_Elaboration 
			[A hole of 60 x 1.20 cm remained in the wall.] [Norma and 
			Andres want to place a fish tank in the hole.] 
			It is easier to solve segmentation disagreement 
			than relations disagreement, since in this case 
			annotator subjectivity is more evident; we must 
			consider how to refine our manual in this sense. 
			3.7 Delivering and Maintaining the Product 
			Hovy (2010) mentions some technical issues 
			regarding these points: licensing, distribution, 
			maintenance and updates. With regard to licensing 
			and distribution, the RST Spanish Treebank will be 
			free for research purposes. We have a data 
			manager responsible for maintenance and updates. 
			The description of the annotated corpus is also 
			a very important issue (Ide and Pustejovsky, 2010). 
			It is important to provide a high level description 
			of the corpus, including the theoretical framework, 
			the methodology (annotators, annotation manual 
			and tool, agreement, etc.), the means for resource 
			maintenance, the technical aspects, the project 
			leader, the contact, the team, etc. The RST Spanish 
			Treebank includes all this detailed information. 
			XML (with a DTD) has been used, in order the 
			corpus can be reused for several aplications. In the 
			future, we plan to use the standard XCES. 
			7 
			To know more about resources development, 
			linguistic annotation or inter-annotator agreement, 
			we recommend: Palmer et al. (on-line), Palmer and 
			Xue (2010), and Artstein and Poesio (2008). 
			4 The Search Interface of the RST 
			Spanish Treebank 
			The RST Spanish Treebank interface is freely 
			available on-line7. It allows the visualization and 
			downloading of all the texts in txt format, with 
			their corresponding annotated trees in RSTtool 
			format (rs3), as well as in image format (jpg). Each 
			text includes its title, its reference, its web link (if 
			it is an on-line text) and its number of words. The 
			interface shows texts by areas and allows the user 
			to select a subcorpus (including individual files or 
			folders containing several files). The selected 
			subcorpus can be saved on local disk (generating a 
			xml file) for future analyses. 
			The interface includes a statistical tool which 
			allows obtaining statistics of rhetorical relations in 
			a subcorpus selected by the user. The RSTtool also 
			offers this option but it can be only used for one 
			text. We consider that it is more useful for the user 
			to obtain statistics from various texts, in order to 
			get significant statistical results. As the RSTtool, 
			our tool allows to count the multinuclear relations 
			in two ways: a) one unit for each detected 
			multinuclear relation, and b) one unit for each 
			detected nucleus. If we use b), the statistics of the 
			multinuclear relations of Table 2 are higher: List 
			(864), Joint (537), Sequence (289), Contrast (153), 
			Conjunction (28) and Disjunction (24). 
			We are developing another tool, aimed to 
			extract information from the annotated texts, which 
			we will soon include into the interface. This tool 
			will allow to the user to select a subcorpus and to 
			extract from it the EDUs corresponding to the 
			rhetorical relations selected, like a multidocument 
			specialized summarizer guided by user's interests. 
			The RST Spanish Treebank interface also 
			includes a screen which permits the users to send 
			their own annotated texts. Our aim is for the RST 
			Spanish Treebank to become a dynamic corpus, in 
			constant evolution, being increased with texts 
			annotated by users. This has a double advantage 
			since, on the one hand, the corpus will grow and, 
			on the other hand, users will profit from the 
			7 http://www.corpus.unam.mx/rst/ 
			interface's applications, using their own 
			subcorpora. The only requirement is to use the 
			relations and the segmentation and annotation 
			criteria of our project. Once the texts are sent, the 
			RST Spanish Treebank data manager will verify if 
			the annotation corresponds to these criteria. 


Conclusion :
null			The rate of information growth due to the World Wide Web has called for a need 
			to develop efficient and accurate summarization systems. Although research on 
			summarization started about 50 years ago, there is still a long trail to walk in 
			this field. Over time, attention has drifted from summarizing scientific articles to 
			news articles, electronic mail messages, advertisements, and blogs. Both abstractive 
			and extractive approaches have been attempted, depending on the application at 
			hand. Usually, abstractive summarization requires heavy machinery for language 
			generation and is difficult to replicate or extend to broader domains. In contrast, 
			simple extraction of sentences have produced satisfactory results in large-scale ap- 
			plications, specially in multi-document summarization. The recent popularity of 
			effective newswire summarization systems confirms this claim. 
			This survey emphasizes extractive approaches to summarization using statisti- 
			cal methods. A distinction has been made between single document and multi- 
			document summarization. Since a lot of interesting work is being done far from 
			the mainstream research in this field, we have chosen to include a brief discussion 
			on some methods that we found relevant to future research, even if they focus only 
			on small details related to a general summarization process and not on building an 
			entire summarization system. 
			Finally, some recent trends in automatic evaluation of summarization systems 
			have been surveyed. The low inter-annotator agreement figures observed during 
			manual evaluations suggest that the future of this research area heavily depends on 
			the ability to find efficient ways of automatically evaluating these systems and on 
			the development of measures that are objective enough to be commonly accepted 
			by the research community. 
			The rate of information growth due to the World Wide Web has called for a need 
			to develop efficient and accurate summarization systems. Although research on 
			summarization started about 50 years ago, there is still a long trail to walk in 
			this field. Over time, attention has drifted from summarizing scientific articles to 
			news articles, electronic mail messages, advertisements, and blogs. Both abstractive 
			and extractive approaches have been attempted, depending on the application at 
			hand. Usually, abstractive summarization requires heavy machinery for language 
			generation and is difficult to replicate or extend to broader domains. In contrast, 
			simple extraction of sentences have produced satisfactory results in large-scale ap- 
			plications, specially in multi-document summarization. The recent popularity of 
			effective newswire summarization systems confirms this claim. 
			This survey emphasizes extractive approaches to summarization using statisti- 
			cal methods. A distinction has been made between single document and multi- 
			document summarization. Since a lot of interesting work is being done far from 
			the mainstream research in this field, we have chosen to include a brief discussion 
			on some methods that we found relevant to future research, even if they focus only 
			on small details related to a general summarization process and not on building an 
			entire summarization system. 
			Finally, some recent trends in automatic evaluation of summarization systems 
			have been surveyed. The low inter-annotator agreement figures observed during 
			manual evaluations suggest that the future of this research area heavily depends on 
			the ability to find efficient ways of automatically evaluating these systems and on 
			the development of measures that are objective enough to be commonly accepted 
			by the research community. 
			In this paper we presented WiSeBE, a semi-automatic multi-reference sentence 
			boundary evaluation protocol based on the necessity of having a more reliable 
			way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric 
			which not only evaluates the performance of a system against all references, but 
			also takes into account the agreement between them. According to your point 
			of view, this inclusivity is very important given the difficulties that are present 
			when working with spoken language and the possible disagreements that a task 
			like SBD could provoke. 
			WiSeBE shows to be correlated with standard SBD metrics, however we 
			want to measure its correlation with extrinsic evaluations techniques like auto- 
			matic summarization and machine translation. 
			In this paper we presented WiSeBE, a semi-automatic multi-reference sentence 
			boundary evaluation protocol based on the necessity of having a more reliable 
			way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric 
			which not only evaluates the performance of a system against all references, but 
			also takes into account the agreement between them. According to your point 
			of view, this inclusivity is very important given the difficulties that are present 
			when working with spoken language and the possible disagreements that a task 
			like SBD could provoke. 
			WiSeBE shows to be correlated with standard SBD metrics, however we 
			want to measure its correlation with extrinsic evaluations techniques like auto- 
			matic summarization and machine translation. 
			We think that this work means an important step 
			for the RST research in Spanish, and that the RST 
			Spanish Treebank will be useful to carry out 
			diverse researches about RST in this language, 
			from a descriptive point of view (ex. analysis of 
			texts from different domains or genres) and an 
			applied point of view (development of discourse 
			parsers and NLP applications, like automatic 
			summarization, automatic translation, IE, etc.). 
			For the moment the corpus' size is acceptable 
			and, though the percentage of double-annotated 
			texts is not very high, we think that having 10 
			annotators (using the same annotation manual) 
			avoids the bias of only one annotator. In addition, 
			the corpus includes texts of diverse domains and 
			genres, which provides us with a heterogeneous 
			Spanish corpus. Moreover, the corpus interface 
			that we have designed allows the user to select a 
			subcorpus and to analyze it statistically. In 
			addition, we think that it is essential to release a 
			free corpus, on-line and dynamic, that is, in 
			continuous growth. Nevertheless, we are conscious 
			that our work still has certain limitations, which we 
			will try to solve in the future. In the short term, we 
			have 5 aims: 
			a) To add one more annotator for the test corpus 
			and to measure inter-annotator agreement. 
			b) To use more agreement measures, like kappa. 
			c) To reach a consensus on the annotation of the 
			test corpus, in order to establish a set of texts 
			considered as a preliminary gold standard. 
			d) To finish and to evaluate the IE tool. 
			e) To analyze the corpus to extract linguistic 
			patterns for the automatic relations detection. 
			In the long term, we consider other aims: 
			f) To increase the corpus, by adding non- 
			specialized texts, and new domains and genres. 
			g) To annotate all the texts by 3 people, to get a 
			representative gold-standard for Spanish (this aim 
			will depend on the funding of the project). 
			8 
			We think that this work means an important step 
			for the RST research in Spanish, and that the RST 
			Spanish Treebank will be useful to carry out 
			diverse researches about RST in this language, 
			from a descriptive point of view (ex. analysis of 
			texts from different domains or genres) and an 
			applied point of view (development of discourse 
			parsers and NLP applications, like automatic 
			summarization, automatic translation, IE, etc.). 
			For the moment the corpus' size is acceptable 
			and, though the percentage of double-annotated 
			texts is not very high, we think that having 10 
			annotators (using the same annotation manual) 
			avoids the bias of only one annotator. In addition, 
			the corpus includes texts of diverse domains and 
			genres, which provides us with a heterogeneous 
			Spanish corpus. Moreover, the corpus interface 
			that we have designed allows the user to select a 
			subcorpus and to analyze it statistically. In 
			addition, we think that it is essential to release a 
			free corpus, on-line and dynamic, that is, in 
			continuous growth. Nevertheless, we are conscious 
			that our work still has certain limitations, which we 
			will try to solve in the future. In the short term, we 
			have 5 aims: 
			a) To add one more annotator for the test corpus 
			and to measure inter-annotator agreement. 
			b) To use more agreement measures, like kappa. 
			c) To reach a consensus on the annotation of the 
			test corpus, in order to establish a set of texts 
			considered as a preliminary gold standard. 
			d) To finish and to evaluate the IE tool. 
			e) To analyze the corpus to extract linguistic 
			patterns for the automatic relations detection. 
			In the long term, we consider other aims: 
			f) To increase the corpus, by adding non- 
			specialized texts, and new domains and genres. 
			g) To annotate all the texts by 3 people, to get a 
			representative gold-standard for Spanish (this aim 
			will depend on the funding of the project). 
			8 


Discussion :
null			In this paper we have described SMMR, a scal- 
			able sentence scoring method based on MMR that 
			achieves very promising results. An important as- 
			pect of our sentence scoring method is that it does 
			not requires re-ranking nor linguistic knowledge, 
			which makes it a simple and fast approach to the 
			issue of update summarization. It was pointed out 
			at the DUC 2007 workshop that Question Answer- 
			ing and query-oriented summarization have been 
			converging on a common task. The value added 
			by summarization lies in the linguistic quality. Ap- 
			proaches mixing IR techniques are well suited for 
			query-oriented summarization but they require in- 
			tensive work for making the summary fluent and 
			coherent. Among the others, this is a point that we 
			think is worthy of further investigation. 
			In this paper we have described SMMR, a scal- 
			able sentence scoring method based on MMR that 
			achieves very promising results. An important as- 
			pect of our sentence scoring method is that it does 
			not requires re-ranking nor linguistic knowledge, 
			which makes it a simple and fast approach to the 
			issue of update summarization. It was pointed out 
			at the DUC 2007 workshop that Question Answer- 
			ing and query-oriented summarization have been 
			converging on a common task. The value added 
			by summarization lies in the linguistic quality. Ap- 
			proaches mixing IR techniques are well suited for 
			query-oriented summarization but they require in- 
			tensive work for making the summary fluent and 
			coherent. Among the others, this is a point that we 
			think is worthy of further investigation. 
			5.1 RGA R 
			and Fleiss' Kappa correlation 
			In Sect. 3 we described the WiSeBE score and how it relies on the RGAR 
			value 
			to scale the performance of CT 
			over RW 
			. RGAR 
			can intuitively be consider an 
			agreement value over all elements of R. To test this hypothesis, we computed 
			the Pearson correlation coefficient (PCC) [21] between RGAR 
			and the Fleiss' 
			Kappa [4] of each video in the dataset (R 
			). 
			A linear correlation between RGAR 
			and R 
			can be observed in Table 6. This 
			is confirmed by a PCC value equal to 0.890, which means a very strong positive 
			linear correlation between them. 
			5.2 F 1mean vs. W iSeBE 
			Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean 
			. 
			While it is true that they show a linear correlation, WiSeBE may produce a 
			128 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 5. WiSeBE evaluation 
			Transcript System F1mean 
			F1RW 
			RGAR 
			WiSeBE 
			v1 S1 0.432 0.495 0.691 0.342 
			S2 0.480 0.513 0.354 
			v2 S1 0.578 0.659 0.688 0.453 
			S2 0.549 0.595 0.409 
			v3 S1 0.270 0.303 0.684 0.207 
			S2 0.325 0.400 0.274 
			v4 S1 0.505 0.593 0.578 0.342 
			S2 0.735 0.800 0.462 
			v5 S1 0.592 0.614 0.767 0.471 
			S2 0.499 0.500 0.383 
			v6 S1 0.443 0.550 0.541 0.298 
			S2 0.457 0.535 0.289 
			v7 S1 0.518 0.592 0.617 0.366 
			S2 0.539 0.606 0.374 
			v8 S1 0.429 0.494 0.525 0.259 
			S2 0.487 0.508 0.267 
			v9 S1 0.459 0.569 0.604 0.344 
			S2 0.541 0.667 0.403 
			v10 S1 0.582 0.581 0.619 0.359 
			S2 0.487 0.545 0.338 
			Mean scores S1 0.481 0.545 0.631 0.344 
			S2 0.510 0.567 0.355 
			Table 6. Agreement within dataset 
			Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 
			RGAR 
			0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619 
			R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718 
			different system ranking than F1mean 
			given the integral multi-reference principle 
			it follows. However, what we consider the most profitable about WiSeBE is the 
			twofold inclusion of all available references it performs. First, the construction of 
			RW 
			to provide a more inclusive reference against to whom be evaluated and then, 
			the computation of RGAR 
			, which scales the result depending of the agreement 
			between references. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 129 
			5.1 RGA R 
			and Fleiss' Kappa correlation 
			In Sect. 3 we described the WiSeBE score and how it relies on the RGAR 
			value 
			to scale the performance of CT 
			over RW 
			. RGAR 
			can intuitively be consider an 
			agreement value over all elements of R. To test this hypothesis, we computed 
			the Pearson correlation coefficient (PCC) [21] between RGAR 
			and the Fleiss' 
			Kappa [4] of each video in the dataset (R 
			). 
			A linear correlation between RGAR 
			and R 
			can be observed in Table 6. This 
			is confirmed by a PCC value equal to 0.890, which means a very strong positive 
			linear correlation between them. 
			5.2 F 1mean vs. W iSeBE 
			Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean 
			. 
			While it is true that they show a linear correlation, WiSeBE may produce a 
			128 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 5. WiSeBE evaluation 
			Transcript System F1mean 
			F1RW 
			RGAR 
			WiSeBE 
			v1 S1 0.432 0.495 0.691 0.342 
			S2 0.480 0.513 0.354 
			v2 S1 0.578 0.659 0.688 0.453 
			S2 0.549 0.595 0.409 
			v3 S1 0.270 0.303 0.684 0.207 
			S2 0.325 0.400 0.274 
			v4 S1 0.505 0.593 0.578 0.342 
			S2 0.735 0.800 0.462 
			v5 S1 0.592 0.614 0.767 0.471 
			S2 0.499 0.500 0.383 
			v6 S1 0.443 0.550 0.541 0.298 
			S2 0.457 0.535 0.289 
			v7 S1 0.518 0.592 0.617 0.366 
			S2 0.539 0.606 0.374 
			v8 S1 0.429 0.494 0.525 0.259 
			S2 0.487 0.508 0.267 
			v9 S1 0.459 0.569 0.604 0.344 
			S2 0.541 0.667 0.403 
			v10 S1 0.582 0.581 0.619 0.359 
			S2 0.487 0.545 0.338 
			Mean scores S1 0.481 0.545 0.631 0.344 
			S2 0.510 0.567 0.355 
			Table 6. Agreement within dataset 
			Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 
			RGAR 
			0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619 
			R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718 
			different system ranking than F1mean 
			given the integral multi-reference principle 
			it follows. However, what we consider the most profitable about WiSeBE is the 
			twofold inclusion of all available references it performs. First, the construction of 
			RW 
			to provide a more inclusive reference against to whom be evaluated and then, 
			the computation of RGAR 
			, which scales the result depending of the agreement 
			between references. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 129 


Reference :
null			Boudin, F. and J.M. Torres-Moreno. 2007. A Co- 
			sine Maximization-Minimization approach for User- 
			Oriented Multi-Document Update Summarization. 
			In Recent Advances in Natural Language Processing 
			(RANLP), pages 81-87. 
			Carbonell, J. and J. Goldstein. 1998. The use of MMR, 
			diversity-based reranking for reordering documents 
			and producing summaries. In 21st annual interna- 
			tional ACM SIGIR conference on Research and de- 
			velopment in information retrieval, pages 335-336. 
			ACM Press New York, NY, USA. 
			Hachey, B., G. Murray, and D. Reitter. 2005. The 
			Embra System at DUC 2005: Query-oriented Multi- 
			document Summarization with a Very Large Latent 
			Semantic Space. In Document Understanding Con- 
			ference (DUC). 
			Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC's 
			GISTexter at DUC 2007: Machine Reading for Up- 
			date Summarization. In Document Understanding 
			Conference (DUC). 
			Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and 
			S. Ye. 2007. NUS at DUC 2007: Using Evolu- 
			tionary Models of Text. In Document Understanding 
			Conference (DUC). 
			Lin, C.Y. 2004. Rouge: A Package for Automatic 
			Evaluation of Summaries. In Workshop on Text Sum- 
			marization Branches Out, pages 25-26. 
			Mani, I. and M.T. Maybury. 1999. Advances in Auto- 
			matic Text Summarization. MIT Press. 
			Mani, I. and G. Wilson. 2000. Robust temporal pro- 
			cessing of news. In 38th Annual Meeting on Asso- 
			ciation for Computational Linguistics, pages 69-76. 
			Association for Computational Linguistics Morris- 
			town, NJ, USA. 
			Murray, G., S. Renals, and J. Carletta. 2005. Extractive 
			Summarization of Meeting Recordings. In Ninth Eu- 
			ropean Conference on Speech Communication and 
			Technology. ISCA. 
			Salton, G., A. Wong, and C. S. Yang. 1975. A vector 
			space model for automatic indexing. Communica- 
			tions of the ACM, 18(11):613-620. 
			Swan, R. and J. Allan. 2000. Automatic generation 
			of overview timelines. In 23rd annual international 
			ACM SIGIR conference on Research and develop- 
			ment in information retrieval, pages 49-56. 
			Winkler, W. E. 1999. The state of record linkage and 
			current research problems. In Survey Methods Sec- 
			tion, pages 73-79. 
			Witte, R., R. Krestel, and S. Bergler. 2007. Generat- 
			ing Update Summaries for DUC 2007. In Document 
			Understanding Conference (DUC). 
			Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS 
			at DUC 2005: Understanding documents via con- 
			cept links. In Document Understanding Conference 
			(DUC). 
			26 
			Boudin, F. and J.M. Torres-Moreno. 2007. A Co- 
			sine Maximization-Minimization approach for User- 
			Oriented Multi-Document Update Summarization. 
			In Recent Advances in Natural Language Processing 
			(RANLP), pages 81-87. 
			Carbonell, J. and J. Goldstein. 1998. The use of MMR, 
			diversity-based reranking for reordering documents 
			and producing summaries. In 21st annual interna- 
			tional ACM SIGIR conference on Research and de- 
			velopment in information retrieval, pages 335-336. 
			ACM Press New York, NY, USA. 
			Hachey, B., G. Murray, and D. Reitter. 2005. The 
			Embra System at DUC 2005: Query-oriented Multi- 
			document Summarization with a Very Large Latent 
			Semantic Space. In Document Understanding Con- 
			ference (DUC). 
			Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC's 
			GISTexter at DUC 2007: Machine Reading for Up- 
			date Summarization. In Document Understanding 
			Conference (DUC). 
			Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and 
			S. Ye. 2007. NUS at DUC 2007: Using Evolu- 
			tionary Models of Text. In Document Understanding 
			Conference (DUC). 
			Lin, C.Y. 2004. Rouge: A Package for Automatic 
			Evaluation of Summaries. In Workshop on Text Sum- 
			marization Branches Out, pages 25-26. 
			Mani, I. and M.T. Maybury. 1999. Advances in Auto- 
			matic Text Summarization. MIT Press. 
			Mani, I. and G. Wilson. 2000. Robust temporal pro- 
			cessing of news. In 38th Annual Meeting on Asso- 
			ciation for Computational Linguistics, pages 69-76. 
			Association for Computational Linguistics Morris- 
			town, NJ, USA. 
			Murray, G., S. Renals, and J. Carletta. 2005. Extractive 
			Summarization of Meeting Recordings. In Ninth Eu- 
			ropean Conference on Speech Communication and 
			Technology. ISCA. 
			Salton, G., A. Wong, and C. S. Yang. 1975. A vector 
			space model for automatic indexing. Communica- 
			tions of the ACM, 18(11):613-620. 
			Swan, R. and J. Allan. 2000. Automatic generation 
			of overview timelines. In 23rd annual international 
			ACM SIGIR conference on Research and develop- 
			ment in information retrieval, pages 49-56. 
			Winkler, W. E. 1999. The state of record linkage and 
			current research problems. In Survey Methods Sec- 
			tion, pages 73-79. 
			Witte, R., R. Krestel, and S. Bergler. 2007. Generat- 
			ing Update Summaries for DUC 2007. In Document 
			Understanding Conference (DUC). 
			Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS 
			at DUC 2005: Understanding documents via con- 
			cept links. In Document Understanding Conference 
			(DUC). 
			26 
			Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla- 
			tions of Mathematical Monographs). Oxford University Press. [20] 
			Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable 
			summarizer with knowledge acquired from robust nlp techniques. In Mani, I. 
			and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 
			71-80. MIT Press. [4, 5] 
			Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization. 
			In Proceedings ISTS'97. [8] 
			Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the 
			context of multi-document summarization. In Proceedings of ACL '99. [12, 13, 
			14, 16] 
			Baxendale, P. (1958). Machine-made index for technical literature - an experiment. 
			IBM Journal of Research Development, 2(4):354-361. [2, 3, 5] 
			Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The 
			mathematics of statistical machine translation: parameter estimation. Comput. 
			Linguist., 19(2):263-311. [18] 
			Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and 
			Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05: 
			Proceedings of the 22nd international conference on Machine learning, pages 89- 
			96, New York, NY, USA. ACM. [8] 
			Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking 
			for reordering documents and producing summaries. In Proceedings of SIGIR '98, 
			pages 335-336, New York, NY, USA. [12, 14, 15] 
			Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. 
			PhD thesis, University of Pennsylvania. [13, 20] 
			Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov 
			models. In Proceedings of SIGIR '01, pages 406-407, New York, NY, USA. [6] 
			Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25] 
			Daum 
			e III, H. and Marcu, D. (2002). A noisy-channel model for document com- 
			pression. In Proceedings of the Conference of the Association of Computational 
			Linguistics (ACL 2002). [20] 
			Daum 
			e III, H. and Marcu, D. (2004). A tree-position kernel for document compres- 
			sion. In Proceedings of DUC2004. [20] 
			Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the 
			ACM, 16(2):264-285. [2, 3, 4] 
			28 
			Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu- 
			tions. IEEE Transactions on Information Theory, 49(7):1858-1860. [26] 
			Evans, D. K. (2005). Similarity-based multilingual multi-document summarization. 
			Technical Report CUCS-014-05, Columbia University. [12, 17] 
			Gous, A. (1999). Spherical subfamily models. [20, 21] 
			Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for 
			natural language processing and information retrieval. In Proc. 17th International 
			Conf. on Machine Learning, pages 351-358. Morgan Kaufmann, San Francisco, 
			CA. [20] 
			Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In 
			Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza- 
			tion, pages 81-94. MIT Press. [17] 
			Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen- 
			tence compression. In AAAI/IAAI, pages 703-710. [19] 
			Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer. 
			In Proceedings SIGIR '95, pages 68-73, New York, NY, USA. [4] 
			Lebanon, G. (2006). Sequential document representations and simplicial curves. In 
			Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. [22] 
			Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words 
			framework for document representation. J. Mach. Learn. Res., 8:2405-2441. [21, 
			22] 
			Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of 
			CIKM '99, pages 55-62, New York, NY, USA. [5] 
			Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In 
			Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro- 
			ceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain. [8, 23, 24, 
			25] 
			Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap- 
			proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL 
			'06, pages 463-470, Morristown, NJ, USA. [25, 26] 
			Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of 
			the Fifth conference on Applied natural language processing, pages 283-290, San 
			Francisco, CA, USA. [5] 
			Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In 
			Proceedings of the ACL-02 Workshop on Automatic Summarization, pages 45-51, 
			Morristown, NJ, USA. [23] 
			29 
			Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of 
			Research Development, 2(2):159-165. [2, 3, 6, 8] 
			Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search 
			and matching. In AAAI/IAAI, pages 622-628. [15, 16] 
			Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In 
			Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages 
			206-215, Montreal, Canada. [9, 10, 20] 
			Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of 
			natural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst. 
			[10] 
			McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E. 
			(1999). Towards multidocument summarization by reformulation: Progress and 
			prospects. In AAAI/IAAI, pages 453-460. [11, 12, 13, 14, 16] 
			McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news 
			articles. In Proceedings of SIGIR '95, pages 74-82, Seattle, Washington. [8, 11, 
			12] 
			Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM, 
			38(11):39-41. [4, 9] 
			Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned 
			from the document understanding conference. In Proceedings of AAAI 2005, 
			Pittsburgh, USA. [7] 
			Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical 
			structure extraction. In Proceedings of Coling '94, pages 344-348, Morristown, 
			NJ, USA. [9] 
			Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings 
			of the ACL'02 Workshop on Automatic Summarization, pages 1-8, Morristown, 
			NJ, USA. [7] 
			Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for 
			automatic evaluation of machine translation. In Proceedings of ACL '02, pages 
			311-318, Morristown, NJ, USA. [24] 
			Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue 
			on summarization. Computational Linguistics., 28(4):399-408. [1, 2] 
			Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization 
			of multiple documents: sentence extraction, utility-based evaluation, and user 
			studies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages 
			21-30, Morristown, NJ, USA. [12, 16, 17] 
			30 
			Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza- 
			tion of multiple documents. Information Processing and Management 40 (2004), 
			40:919-938. [16, 17] 
			Radev, D. R. and McKeown, K. (1998). Generating natural language summaries 
			from multiple on-line sources. Computational Linguistics, 24(3):469-500. [12] 
			Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in 
			automatic information. In Proceedings of SIGIR '88, pages 147-160, New York, 
			NY, USA. [15] 
			Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic 
			indexing. Communications of the ACM, 18:229-237. [20] 
			Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving 
			hard satisfiability problems. In AAAI, pages 440-446. [11] 
			Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document 
			summarization by combining RankNet and third-party sources. In Proceedings of 
			the EMNLP-CoNLL, pages 448-457. [7, 8] 
			Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract): 
			a statistical approach to generating highly condensed non-extractive summaries. 
			In Proceedings of SIGIR '99, pages 315-316, New York, NY, USA. [18] 
			31 
			Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla- 
			tions of Mathematical Monographs). Oxford University Press. [20] 
			Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable 
			summarizer with knowledge acquired from robust nlp techniques. In Mani, I. 
			and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 
			71-80. MIT Press. [4, 5] 
			Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization. 
			In Proceedings ISTS'97. [8] 
			Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the 
			context of multi-document summarization. In Proceedings of ACL '99. [12, 13, 
			14, 16] 
			Baxendale, P. (1958). Machine-made index for technical literature - an experiment. 
			IBM Journal of Research Development, 2(4):354-361. [2, 3, 5] 
			Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The 
			mathematics of statistical machine translation: parameter estimation. Comput. 
			Linguist., 19(2):263-311. [18] 
			Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and 
			Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05: 
			Proceedings of the 22nd international conference on Machine learning, pages 89- 
			96, New York, NY, USA. ACM. [8] 
			Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking 
			for reordering documents and producing summaries. In Proceedings of SIGIR '98, 
			pages 335-336, New York, NY, USA. [12, 14, 15] 
			Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. 
			PhD thesis, University of Pennsylvania. [13, 20] 
			Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov 
			models. In Proceedings of SIGIR '01, pages 406-407, New York, NY, USA. [6] 
			Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25] 
			Daum 
			e III, H. and Marcu, D. (2002). A noisy-channel model for document com- 
			pression. In Proceedings of the Conference of the Association of Computational 
			Linguistics (ACL 2002). [20] 
			Daum 
			e III, H. and Marcu, D. (2004). A tree-position kernel for document compres- 
			sion. In Proceedings of DUC2004. [20] 
			Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the 
			ACM, 16(2):264-285. [2, 3, 4] 
			28 
			Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu- 
			tions. IEEE Transactions on Information Theory, 49(7):1858-1860. [26] 
			Evans, D. K. (2005). Similarity-based multilingual multi-document summarization. 
			Technical Report CUCS-014-05, Columbia University. [12, 17] 
			Gous, A. (1999). Spherical subfamily models. [20, 21] 
			Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for 
			natural language processing and information retrieval. In Proc. 17th International 
			Conf. on Machine Learning, pages 351-358. Morgan Kaufmann, San Francisco, 
			CA. [20] 
			Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In 
			Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza- 
			tion, pages 81-94. MIT Press. [17] 
			Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen- 
			tence compression. In AAAI/IAAI, pages 703-710. [19] 
			Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer. 
			In Proceedings SIGIR '95, pages 68-73, New York, NY, USA. [4] 
			Lebanon, G. (2006). Sequential document representations and simplicial curves. In 
			Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. [22] 
			Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words 
			framework for document representation. J. Mach. Learn. Res., 8:2405-2441. [21, 
			22] 
			Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of 
			CIKM '99, pages 55-62, New York, NY, USA. [5] 
			Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In 
			Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro- 
			ceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain. [8, 23, 24, 
			25] 
			Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap- 
			proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL 
			'06, pages 463-470, Morristown, NJ, USA. [25, 26] 
			Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of 
			the Fifth conference on Applied natural language processing, pages 283-290, San 
			Francisco, CA, USA. [5] 
			Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In 
			Proceedings of the ACL-02 Workshop on Automatic Summarization, pages 45-51, 
			Morristown, NJ, USA. [23] 
			29 
			Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of 
			Research Development, 2(2):159-165. [2, 3, 6, 8] 
			Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search 
			and matching. In AAAI/IAAI, pages 622-628. [15, 16] 
			Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In 
			Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages 
			206-215, Montreal, Canada. [9, 10, 20] 
			Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of 
			natural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst. 
			[10] 
			McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E. 
			(1999). Towards multidocument summarization by reformulation: Progress and 
			prospects. In AAAI/IAAI, pages 453-460. [11, 12, 13, 14, 16] 
			McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news 
			articles. In Proceedings of SIGIR '95, pages 74-82, Seattle, Washington. [8, 11, 
			12] 
			Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM, 
			38(11):39-41. [4, 9] 
			Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned 
			from the document understanding conference. In Proceedings of AAAI 2005, 
			Pittsburgh, USA. [7] 
			Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical 
			structure extraction. In Proceedings of Coling '94, pages 344-348, Morristown, 
			NJ, USA. [9] 
			Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings 
			of the ACL'02 Workshop on Automatic Summarization, pages 1-8, Morristown, 
			NJ, USA. [7] 
			Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for 
			automatic evaluation of machine translation. In Proceedings of ACL '02, pages 
			311-318, Morristown, NJ, USA. [24] 
			Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue 
			on summarization. Computational Linguistics., 28(4):399-408. [1, 2] 
			Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization 
			of multiple documents: sentence extraction, utility-based evaluation, and user 
			studies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages 
			21-30, Morristown, NJ, USA. [12, 16, 17] 
			30 
			Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza- 
			tion of multiple documents. Information Processing and Management 40 (2004), 
			40:919-938. [16, 17] 
			Radev, D. R. and McKeown, K. (1998). Generating natural language summaries 
			from multiple on-line sources. Computational Linguistics, 24(3):469-500. [12] 
			Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in 
			automatic information. In Proceedings of SIGIR '88, pages 147-160, New York, 
			NY, USA. [15] 
			Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic 
			indexing. Communications of the ACM, 18:229-237. [20] 
			Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving 
			hard satisfiability problems. In AAAI, pages 440-446. [11] 
			Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document 
			summarization by combining RankNet and third-party sources. In Proceedings of 
			the EMNLP-CoNLL, pages 448-457. [7, 8] 
			Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract): 
			a statistical approach to generating highly condensed non-extractive summaries. 
			In Proceedings of SIGIR '99, pages 315-316, New York, NY, USA. [18] 
			31 
			1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- 
			nized speech for web presentation of large audio archive. In: 2012 35th International 
			Conference on Telecommunications and Signal Processing (TSP), pp. 441-445. 
			IEEE (2012) 
			2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over 
			a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, 
			A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134-138. Springer, Cham 
			(2016). https://doi.org/10.1007/978-3-319-41552-9 14 
			3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented 
			transcript based on word vector. In: LREC (2016) 
			4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull. 
			76(5), 378 (1971) 
			5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- 
			works. In: IEEE International Conference on Information Systems and Economic 
			Intelligence (2017) 
			6. Gonz 
			alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- 
			scripts informativeness study: an approach based on automatic summarization. In: 
			Conf 
			erence en Recherche d'Information et Applications (CORIA), Rennes, France, 
			May (2018) 
			7. Gonz 
			alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for 
			French with subword-level information vectors and convolutional neural networks. 
			arXiv preprint arXiv:1802.04559 (2018) 
			8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts. 
			In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium 
			ISCA Tutorial and Research Workshop (ITRW) (2000) 
			130 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni- 
			tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6), 
			82-97 (2012) 
			10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech 
			recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308-318 
			(2015) 
			11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com- 
			put. Linguist. 32(4), 485-525 (2006) 
			12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts 
			using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology 
			Workshop (SLT), pp. 433-440. IEEE (2016) 
			13. Kol 
			a 
			r, J., Lamel, L.: Development and evaluation of automatic punctuation for 
			French and english speech-to-text. In: Thirteenth Annual Conference of the Inter- 
			national Speech Communication Association (2012) 
			14. Kol 
			a 
			r, J.,  
			Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broad- 
			cast news speech. In: SPECOM 2004 (2004) 
			15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine 
			learning from imbalanced data for sentence boundary detection in speech. Comput. 
			Speech Lang. 20(4), 468-494 (2006) 
			16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran- 
			dom fields. In: Proceedings of the 2010 Conference on Empirical Methods in Natu- 
			ral Language Processing. pp. 177-186. Association for Computational Linguistics 
			(2010) 
			17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In: 
			Conference on Empirical Methods in Natural Language Processing (1996) 
			18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg- 
			mentation of speech for automatic summarization. In: 2006 IEEE International 
			Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I. 
			IEEE (2006) 
			19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro- 
			ceedings of the Fourth Conference on Applied Natural Language Processing, pp. 
			78-83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA, 
			USA (1994) 
			20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua- 
			tion. Comput. Linguist. 23(2), 241-267 (1997) 
			21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc. 
			R. Soc. Lond. 58, 240-242 (1895) 
			22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical 
			phrase-based translation. In: Proceedings of the International Workshop on Spoken 
			Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014) 
			23. Rott, M.,  
			Cerva, P.: Speech-to-text summarization using automatic phrase extrac- 
			tion from recognized text. In: Sojka, P., Hor 
			ak, A., Kope 
			cek, I., Pala, K. (eds.) TSD 
			2016. LNCS (LNAI), vol. 9924, pp. 101-108. Springer, Cham (2016). https://doi. 
			org/10.1007/978-3-319-45510-5 12 
			24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based 
			study. In: Proceedings of the Fourth International Conference on Spoken Language, 
			1996. ICSLP 1996, vol. 3, pp. 1868-1871. IEEE (1996) 
			25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In: 
			Proceedings of the sixth conference on Applied natural language processing, pp. 
			84-89. Association for Computational Linguistics (2000) 
			WiSeBE: Window-Based Sentence Boundary Evaluation 131 
			26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational 
			speech. In: Proceedings of the Fourth International Conference on Spoken Lan- 
			guage, 1996. ICSLP 1996, vol. 2, pp. 1005-1008. IEEE (1996) 
			27. Strassel, S.: Simple metadata annotation specification v5. 0, linguistic data consor- 
			tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE 
			V5.0.pdf 
			28. Tilk, O., Alum 
			ae, T.: Bidirectional recurrent neural network with attention mech- 
			anism for punctuation restoration. In: Interspeech 2016 (2016) 
			29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen- 
			tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704 
			(2017) 
			30. Ueffing, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation 
			prediction for spoken and written text. In: Interspeech, pp. 3097-3101 (2013) 
			31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disfluency removal for 
			improving spoken language translation. In: 2010 IEEE International Conference on 
			Acoustics Speech and Signal Processing (ICASSP), pp. 5214-5217. IEEE (2010) 
			32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network 
			approach for sentence boundary detection in broadcast news. In: Fifteenth Annual 
			Conference of the International Speech Communication Association (2014) 
			33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https:// 
			doi.org/10.1007/978-1-4471-5779-3 
			1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- 
			nized speech for web presentation of large audio archive. In: 2012 35th International 
			Conference on Telecommunications and Signal Processing (TSP), pp. 441-445. 
			IEEE (2012) 
			2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over 
			a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, 
			A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134-138. Springer, Cham 
			(2016). https://doi.org/10.1007/978-3-319-41552-9 14 
			3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented 
			transcript based on word vector. In: LREC (2016) 
			4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull. 
			76(5), 378 (1971) 
			5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- 
			works. In: IEEE International Conference on Information Systems and Economic 
			Intelligence (2017) 
			6. Gonz 
			alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- 
			scripts informativeness study: an approach based on automatic summarization. In: 
			Conf 
			erence en Recherche d'Information et Applications (CORIA), Rennes, France, 
			May (2018) 
			7. Gonz 
			alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for 
			French with subword-level information vectors and convolutional neural networks. 
			arXiv preprint arXiv:1802.04559 (2018) 
			8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts. 
			In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium 
			ISCA Tutorial and Research Workshop (ITRW) (2000) 
			130 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni- 
			tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6), 
			82-97 (2012) 
			10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech 
			recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308-318 
			(2015) 
			11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com- 
			put. Linguist. 32(4), 485-525 (2006) 
			12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts 
			using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology 
			Workshop (SLT), pp. 433-440. IEEE (2016) 
			13. Kol 
			a 
			r, J., Lamel, L.: Development and evaluation of automatic punctuation for 
			French and english speech-to-text. In: Thirteenth Annual Conference of the Inter- 
			national Speech Communication Association (2012) 
			14. Kol 
			a 
			r, J.,  
			Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broad- 
			cast news speech. In: SPECOM 2004 (2004) 
			15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine 
			learning from imbalanced data for sentence boundary detection in speech. Comput. 
			Speech Lang. 20(4), 468-494 (2006) 
			16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran- 
			dom fields. In: Proceedings of the 2010 Conference on Empirical Methods in Natu- 
			ral Language Processing. pp. 177-186. Association for Computational Linguistics 
			(2010) 
			17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In: 
			Conference on Empirical Methods in Natural Language Processing (1996) 
			18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg- 
			mentation of speech for automatic summarization. In: 2006 IEEE International 
			Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I. 
			IEEE (2006) 
			19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro- 
			ceedings of the Fourth Conference on Applied Natural Language Processing, pp. 
			78-83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA, 
			USA (1994) 
			20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua- 
			tion. Comput. Linguist. 23(2), 241-267 (1997) 
			21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc. 
			R. Soc. Lond. 58, 240-242 (1895) 
			22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical 
			phrase-based translation. In: Proceedings of the International Workshop on Spoken 
			Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014) 
			23. Rott, M.,  
			Cerva, P.: Speech-to-text summarization using automatic phrase extrac- 
			tion from recognized text. In: Sojka, P., Hor 
			ak, A., Kope 
			cek, I., Pala, K. (eds.) TSD 
			2016. LNCS (LNAI), vol. 9924, pp. 101-108. Springer, Cham (2016). https://doi. 
			org/10.1007/978-3-319-45510-5 12 
			24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based 
			study. In: Proceedings of the Fourth International Conference on Spoken Language, 
			1996. ICSLP 1996, vol. 3, pp. 1868-1871. IEEE (1996) 
			25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In: 
			Proceedings of the sixth conference on Applied natural language processing, pp. 
			84-89. Association for Computational Linguistics (2000) 
			WiSeBE: Window-Based Sentence Boundary Evaluation 131 
			26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational 
			speech. In: Proceedings of the Fourth International Conference on Spoken Lan- 
			guage, 1996. ICSLP 1996, vol. 2, pp. 1005-1008. IEEE (1996) 
			27. Strassel, S.: Simple metadata annotation specification v5. 0, linguistic data consor- 
			tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE 
			V5.0.pdf 
			28. Tilk, O., Alum 
			ae, T.: Bidirectional recurrent neural network with attention mech- 
			anism for punctuation restoration. In: Interspeech 2016 (2016) 
			29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen- 
			tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704 
			(2017) 
			30. Ueffing, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation 
			prediction for spoken and written text. In: Interspeech, pp. 3097-3101 (2013) 
			31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disfluency removal for 
			improving spoken language translation. In: 2010 IEEE International Conference on 
			Acoustics Speech and Signal Processing (ICASSP), pp. 5214-5217. IEEE (2010) 
			32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network 
			approach for sentence boundary detection in broadcast news. In: Fifteenth Annual 
			Conference of the International Speech Communication Association (2014) 
			33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https:// 
			doi.org/10.1007/978-1-4471-5779-3 
			Ron Artstein, and Massimo Poesio. 2008. Survey 
			Article: Inter-Coder Agreement for Computational 
			Linguistics. Computational Linguistics, 34(4):555- 
			596. 
			Nadjet Bouayad-Agha, Leo Wanner, and Daniel 
			Nicklass. 2006. Discourse structuring of dynamic 
			content. Procesamiento del lenguaje natural, 37:207- 
			213. 
			M. Teresa Cabre (1999). La terminologia: 
			representacion y comunicacion. Barcelona: IULA- 
			UPF. 
			Lynn Carlson and Daniel Marcu. 2001. Discourse 
			Tagging Reference Manual. ISI Technical Report 
			ISITR-545. Los Angeles: University of Southern 
			California. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002a. RST Discourse Treebank. 
			Pennsylvania: Linguistic Data Consortium. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002b. Building a Discourse-Tagged 
			Corpus in the Framework of Rhetorical Structure 
			Theory. In Proceedings of the 2nd SIGDIAL 
			Workshop on Discourse and Dialogue, Eurospeech 
			2001. 
			Jacob Cohen. 1960. A coefficient of agreement for 
			nominal scales. Educational and Psychological 
			Measurement, 20(1):37-46 
			Iria da Cunha, Eric SanJuan, Juan-Manuel Torres- 
			Moreno, Marina Lloberes, and Irene Castellon. 2010. 
			Discourse Segmentation for Spanish based on 
			Shallow Parsing. Lecture Notes in Computer 
			Science, 6437:13-23. 
			Iria da Cunha, and Mikel Iruskieta. 2010. Comparing 
			rhetorical structures of different languages: The 
			influence of translation strategies. Discourse Studies, 
			12(5):563-598. 
			Iria da Cunha, Leo Wanner, and M. Teresa Cabre. 2007. 
			Summarization of specialized discourse: The case of 
			medical articles in Spanish. Terminology, 13(2):249- 
			286. 
			Dmitriy Dligach, Rodney D. Nielsen, and Martha 
			Palmer. 2010. To Annotate More Accurately or to 
			Annotate More. In Proceedings of the 4th Linguistic 
			Annotation Workshop (LAW-IV). 48th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Joseph L. Fleis. 1971. Measuring nominal scale 
			agreement among many raters. Psychological 
			Bulletin, 76(5):378-382. 
			Eduard Hovy. 2010. Annotation. A Tutorial. Presented 
			at the 48th Annual Meeting of the Association for 
			Computational Linguistics. 
			Nancy Ide and Pustejovsky, J. (2010). What Does 
			Interoperability Mean, anyway? Toward an 
			Operational Definition of Interoperability. In 
			Proceedings of the Second International Conference 
			on Global Interoperability for Language Resources 
			(ICGL 2010). 
			William C. Mann, and Sandra A. Thompson. 1988. 
			Rhetorical structure theory: Toward a functional 
			theory of text organization. Text, 8(3):243-281. 
			Daniel Marcu. 2000. The Theory and Practice of 
			Discourse Parsing Summarization. Massachusetts: 
			Institute of Technology. 
			Mitchell P. Marcus, Beatrice Santorini, Mary A. 
			Marcinkiewicz. 1993. Building a large annotated 
			corpus of English: the Penn Treenbank. 
			Computational Linguistics, 19(2):313-330. 
			Michael O'Donnell. 2000. RSTTOOL 2.4 - A markup 
			tool for rhetorical structure theory. In Proceedings of 
			the International Natural Language Generation 
			Conference. 253-256. 
			Martha Palmer, and Nianwen Xue. 2010. Linguistic 
			Annotation. Handbook of Computational Linguistics 
			and Natural Language Processing. 
			Martha Palmer, Randee Tangi, Stephanie Strassel, 
			Christiane Fellbaum, and Eduard Hovy (on-line). 
			Historical Development and Future Directions in 
			Data Resource Development. MINDS report. 
			http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf 
			Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
			Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
			OntoNotes: A Unified Relational Semantic 
			Representation. In Proceedings of the First IEEE 
			International Conference on Semantic Computing 
			(ICSC-07). 
			Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
			Miltsakaki, Livio Robaldo, Aravind Joshi, and 
			Bonnie Webber. 2008. The Penn Discourse Treebank 
			2.0. In Proceedings of the 6th International 
			Conference on Language Resources and Evaluation 
			(LREC 2008). 
			David Reitter, and Mandred Stede. 2003. Step by step: 
			underspecified markup in incremental rhetorical 
			analysis. In Proceedings of the 4th International 
			9 
			Workshop on Linguistically Interpreted Corpora 
			(LINC-03). 
			Magdalena Romera. 2004. Discourse Functional Units: 
			The Expression of Coherence Relations in Spoken 
			Spanish. Munich: LINCOM. 
			Thiago Alexandre Salgueiro Pardo, and Lucia Helena 
			Machado Rino. 2001. A summary planner based on a 
			three-level discourse model. In Proceedings of 
			Natural Language Processing Pacific Rim 
			Symposium. 533-538. 
			Thiago Alexandre Salgueiro Pardo, Maria das Gracas 
			Volpe Nunes, and Lucia Helena Machado Rino. 
			2008. DiZer: An Automatic Discourse Analyzer for 
			Brazilian Portuguese. Lecture Notes in Artificial 
			Intelligence, 3171:224-234. 
			Thiago Alexandre Salgueiro Pardo, and Eloize Rossi 
			Marques Seno. 2005. Rhetalho: um corpus de 
			referencia anotado retoricamente. In Anais do V 
			Encontro de Corpora. Sao Carlos-SP, Brasil. 
			Gerardo Sierra. 2008. Diseno de corpus textuales para 
			fines linguisticos. In Proceedings of the IX Encuentro 
			Internacional de Linguistica en el Noroeste 2. 445- 
			462. 
			Manfred Stede. 2004. The Potsdam commentary corpus. 
			In Proceedings of the Workshop on Discourse 
			Annotation, 42nd Meeting of the Association for 
			Computational Linguistics. 
			Maite Taboada. 2004. Building Coherence and 
			Cohesion: Task-Oriented Dialogue in English and 
			Spanish. Amsterdam/Philadelphia: John Benjamins. 
			Maite Taboada, and Jan Renkema. 2008. Discourse 
			Relations Reference Corpus [Corpus]. Simon Fraser 
			University and Tilburg University. 
			http://www.sfu.ca/rst/06tools/discourse_relations_cor 
			pus.html. 
			Maite Taboada, and William C. Mann. 2006a. 
			Rhetorical Structure Theory: Looking Back and 
			Moving Ahead. Discourse Studies, 8(3):423-459. 
			Maite Taboada, and William C. Mann. 2006b. 
			Applications of Rhetorical Structure Theory. 
			Discourse Studies, 8(4):567-588. 
			Milan Tofiloski, Julian Brooke, and Maite Taboada. 
			2009. A Syntactic and Lexical-Based Discourse 
			Segmenter. In Proceedings of the 47th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large 
			a Corpus Do We Need: Statistical Method Versus 
			Rule-based Method. In Proceedings of the Seventh 
			conference on International Language Resources and 
			Evaluation (LREC'10). 
			10 
			Ron Artstein, and Massimo Poesio. 2008. Survey 
			Article: Inter-Coder Agreement for Computational 
			Linguistics. Computational Linguistics, 34(4):555- 
			596. 
			Nadjet Bouayad-Agha, Leo Wanner, and Daniel 
			Nicklass. 2006. Discourse structuring of dynamic 
			content. Procesamiento del lenguaje natural, 37:207- 
			213. 
			M. Teresa Cabre (1999). La terminologia: 
			representacion y comunicacion. Barcelona: IULA- 
			UPF. 
			Lynn Carlson and Daniel Marcu. 2001. Discourse 
			Tagging Reference Manual. ISI Technical Report 
			ISITR-545. Los Angeles: University of Southern 
			California. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002a. RST Discourse Treebank. 
			Pennsylvania: Linguistic Data Consortium. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002b. Building a Discourse-Tagged 
			Corpus in the Framework of Rhetorical Structure 
			Theory. In Proceedings of the 2nd SIGDIAL 
			Workshop on Discourse and Dialogue, Eurospeech 
			2001. 
			Jacob Cohen. 1960. A coefficient of agreement for 
			nominal scales. Educational and Psychological 
			Measurement, 20(1):37-46 
			Iria da Cunha, Eric SanJuan, Juan-Manuel Torres- 
			Moreno, Marina Lloberes, and Irene Castellon. 2010. 
			Discourse Segmentation for Spanish based on 
			Shallow Parsing. Lecture Notes in Computer 
			Science, 6437:13-23. 
			Iria da Cunha, and Mikel Iruskieta. 2010. Comparing 
			rhetorical structures of different languages: The 
			influence of translation strategies. Discourse Studies, 
			12(5):563-598. 
			Iria da Cunha, Leo Wanner, and M. Teresa Cabre. 2007. 
			Summarization of specialized discourse: The case of 
			medical articles in Spanish. Terminology, 13(2):249- 
			286. 
			Dmitriy Dligach, Rodney D. Nielsen, and Martha 
			Palmer. 2010. To Annotate More Accurately or to 
			Annotate More. In Proceedings of the 4th Linguistic 
			Annotation Workshop (LAW-IV). 48th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Joseph L. Fleis. 1971. Measuring nominal scale 
			agreement among many raters. Psychological 
			Bulletin, 76(5):378-382. 
			Eduard Hovy. 2010. Annotation. A Tutorial. Presented 
			at the 48th Annual Meeting of the Association for 
			Computational Linguistics. 
			Nancy Ide and Pustejovsky, J. (2010). What Does 
			Interoperability Mean, anyway? Toward an 
			Operational Definition of Interoperability. In 
			Proceedings of the Second International Conference 
			on Global Interoperability for Language Resources 
			(ICGL 2010). 
			William C. Mann, and Sandra A. Thompson. 1988. 
			Rhetorical structure theory: Toward a functional 
			theory of text organization. Text, 8(3):243-281. 
			Daniel Marcu. 2000. The Theory and Practice of 
			Discourse Parsing Summarization. Massachusetts: 
			Institute of Technology. 
			Mitchell P. Marcus, Beatrice Santorini, Mary A. 
			Marcinkiewicz. 1993. Building a large annotated 
			corpus of English: the Penn Treenbank. 
			Computational Linguistics, 19(2):313-330. 
			Michael O'Donnell. 2000. RSTTOOL 2.4 - A markup 
			tool for rhetorical structure theory. In Proceedings of 
			the International Natural Language Generation 
			Conference. 253-256. 
			Martha Palmer, and Nianwen Xue. 2010. Linguistic 
			Annotation. Handbook of Computational Linguistics 
			and Natural Language Processing. 
			Martha Palmer, Randee Tangi, Stephanie Strassel, 
			Christiane Fellbaum, and Eduard Hovy (on-line). 
			Historical Development and Future Directions in 
			Data Resource Development. MINDS report. 
			http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf 
			Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
			Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
			OntoNotes: A Unified Relational Semantic 
			Representation. In Proceedings of the First IEEE 
			International Conference on Semantic Computing 
			(ICSC-07). 
			Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
			Miltsakaki, Livio Robaldo, Aravind Joshi, and 
			Bonnie Webber. 2008. The Penn Discourse Treebank 
			2.0. In Proceedings of the 6th International 
			Conference on Language Resources and Evaluation 
			(LREC 2008). 
			David Reitter, and Mandred Stede. 2003. Step by step: 
			underspecified markup in incremental rhetorical 
			analysis. In Proceedings of the 4th International 
			9 
			Workshop on Linguistically Interpreted Corpora 
			(LINC-03). 
			Magdalena Romera. 2004. Discourse Functional Units: 
			The Expression of Coherence Relations in Spoken 
			Spanish. Munich: LINCOM. 
			Thiago Alexandre Salgueiro Pardo, and Lucia Helena 
			Machado Rino. 2001. A summary planner based on a 
			three-level discourse model. In Proceedings of 
			Natural Language Processing Pacific Rim 
			Symposium. 533-538. 
			Thiago Alexandre Salgueiro Pardo, Maria das Gracas 
			Volpe Nunes, and Lucia Helena Machado Rino. 
			2008. DiZer: An Automatic Discourse Analyzer for 
			Brazilian Portuguese. Lecture Notes in Artificial 
			Intelligence, 3171:224-234. 
			Thiago Alexandre Salgueiro Pardo, and Eloize Rossi 
			Marques Seno. 2005. Rhetalho: um corpus de 
			referencia anotado retoricamente. In Anais do V 
			Encontro de Corpora. Sao Carlos-SP, Brasil. 
			Gerardo Sierra. 2008. Diseno de corpus textuales para 
			fines linguisticos. In Proceedings of the IX Encuentro 
			Internacional de Linguistica en el Noroeste 2. 445- 
			462. 
			Manfred Stede. 2004. The Potsdam commentary corpus. 
			In Proceedings of the Workshop on Discourse 
			Annotation, 42nd Meeting of the Association for 
			Computational Linguistics. 
			Maite Taboada. 2004. Building Coherence and 
			Cohesion: Task-Oriented Dialogue in English and 
			Spanish. Amsterdam/Philadelphia: John Benjamins. 
			Maite Taboada, and Jan Renkema. 2008. Discourse 
			Relations Reference Corpus [Corpus]. Simon Fraser 
			University and Tilburg University. 
			http://www.sfu.ca/rst/06tools/discourse_relations_cor 
			pus.html. 
			Maite Taboada, and William C. Mann. 2006a. 
			Rhetorical Structure Theory: Looking Back and 
			Moving Ahead. Discourse Studies, 8(3):423-459. 
			Maite Taboada, and William C. Mann. 2006b. 
			Applications of Rhetorical Structure Theory. 
			Discourse Studies, 8(4):567-588. 
			Milan Tofiloski, Julian Brooke, and Maite Taboada. 
			2009. A Syntactic and Lexical-Based Discourse 
			Segmenter. In Proceedings of the 47th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large 
			a Corpus Do We Need: Statistical Method Versus 
			Rule-based Method. In Proceedings of the Seventh 
			conference on International Language Resources and 
			Evaluation (LREC'10). 
			10 
