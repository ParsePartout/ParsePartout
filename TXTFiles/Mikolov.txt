Nom du fichier :
			Mikolov.pdf
Titre :
			Efficient Estimation of Word Representations in Vector Space
Nombre d'auteur :
 			4
Auteurs :
			Tomas Mikolov
			Greg Corrado
			Kai Chen
			Jeffrey Dean
Abstract :
			We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.

Intro :
			Many current NLP systems and techniques treat words as atomic units - there is no notion of similar-ity between words, as these are represented as indices in a vocabulary. This choice has several goodreasons - simplicity, robustness and the observation that simple models trained on huge amounts ofdata outperform complex systems trained on less data. An example is the popular N-gram modelused for statistical language modeling - today, it is possible to train N-grams on virtually all availabledata (trillions of words [3]).However, the simple techniques are at their limits in many tasks. For example, the amount ofrelevant in-domain data for automatic speech recognition is limited - the performance is usuallydominated by the size of high quality transcribed speech data (often just millions of words). Inmachine translation, the existing corpora for many languages contain only a few billions of wordsor less. Thus, there are situations where simple scaling up of the basic techniques will not result inany significant progress, and we have to focus on more advanced techniques.With progress of machine learning techniques in recent years, it has become possible to train morecomplex models on much larger data set, and they typically outperform the simple models. Probablythe most successful concept is to use distributed representations of words [10]. For example, neuralnetwork based language models significantly outperform N-gram models [1, 27, 17].

Corps :			1.1 Goals of the PaperThe main goal of this paper is to introduce techniques that can be used for learning high-quality wordvectors from huge data sets with billions of words, and with millions of words in the vocabulary. Asfar as we know, none of the previously proposed architectures has been successfully trained on more1arXiv:1301.3781v3 [cs.CL] 7 Sep 2013than a few hundred of millions of words, with a modest dimensionality of the word vectors between50 - 100.We use recently proposed techniques for measuring the quality of the resulting vector representa-tions, with the expectation that not only will similar words tend to be close to each other, but thatwords can have multiple degrees of similarity [20]. This has been observed earlier in the contextof inflectional languages - for example, nouns can have multiple word endings, and if we search forsimilar words in a subspace of the original vector space, it is possible to find words that have similarendings [13, 14].Somewhat surprisingly, it was found that similarity of word representations goes beyond simplesyntactic regularities. Using a word offset technique where simple algebraic operations are per-formed on the word vectors, it was shown for example that vector("King") - vector("Man") + vec-tor("Woman") results in a vector that is closest to the vector representation of the word Queen [20].In this paper, we try to maximize accuracy of these vector operations by developing new modelarchitectures that preserve the linear regularities among words. We design a new comprehensive testset for measuring both syntactic and semantic regularities1, and show that many such regularitiescan be learned with high accuracy. Moreover, we discuss how training time and accuracy dependson the dimensionality of the word vectors and on the amount of the training data.1.2 Previous WorkRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular modelarchitecture for estimating neural network language model (NNLM) was proposed in [1], where afeedforward neural network with a linear projection layer and a non-linear hidden layer was used tolearn jointly the word vector representation and a statistical language model. This work has beenfollowed by many others.Another interesting architecture of NNLM was presented in [13, 14], where the word vectors arefirst learned using neural network with a single hidden layer. The word vectors are then used to trainthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In thiswork, we directly extend this architecture, and focus just on the first step where the word vectors arelearned using a simple model.It was later shown that the word vectors can be used to significantly improve and simplify manyNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using differentmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting wordvectors were made available for future research and comparison2. However, as far as we know, thesearchitectures were significantly more computationally expensive for training than the one proposedin [13], with the exception of certain version of log-bilinear model where diagonal weight matricesare used [23].2 Model ArchitecturesMany different types of models were proposed for estimating continuous representations of words,including the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).In this paper, we focus on distributed representations of words learned by neural networks, as it waspreviously shown that they perform significantly better than LSA for preserving linear regularitiesamong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.Similar to [18], to compare different model architectures we define first the computational complex-ity of a model as the number of parameters that need to be accessed to fully train the model. Next,we will try to maximize the accuracy, while minimizing the computational complexity.1The test set is available at www.fit.vutbr.cz/imikolov/rnnlm/word-test.v1.txt2http://ronan.collobert.com/senna/http://metaoptimize.com/projects/wordreprs/http://www.fit.vutbr.cz/imikolov/rnnlm/http://ai.stanford.edu/ehhuang/2For all the following models, the training complexity is proportional toO = E x T x Q, (1)where E is number of the training epochs, T is the number of the words in the training set and Q isdefined further for each model architecture. Common choice is E = 3 - 50 and T up to one billion.All models are trained using stochastic gradient descent and backpropagation [26].2.1 Feedforward Neural Net Language Model (NNLM)The probabilistic feedforward neural network language model has been proposed in [1]. It consistsof input, projection, hidden and output layers. At the input layer, N previous words are encodedusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to aprojection layer P that has dimensionality N x D, using a shared projection matrix. As only Ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.The NNLM architecture becomes complex for computation between the projection and the hiddenlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of theprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000units. Moreover, the hidden layer is used to compute probability distribution over all the words in thevocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexityper each training example isQ = N x D + N x D x H + H x V, (2)where the dominating term is H x V . However, several practical solutions were proposed foravoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalizedmodels completely by using models that are not normalized during training [4, 9]. With binary treerepresentations of the vocabulary, the number of output units that need to be evaluated can go downto around log2(V ). Thus, most of the complexity is caused by the term N x D x H.In our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binarytree. This follows previous observations that the frequency of words works well for obtaining classesin neural net language models [16]. Huffman trees assign short binary codes to frequent words, andthis further reduces the number of output units that need to be evaluated: while balanced binary treewould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requiresonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one millionwords, this results in about two times speedup in evaluation. While this is not crucial speedup forneural network LMs as the computational bottleneck is in the N xDxH term, we will later proposearchitectures that do not have hidden layers and thus depend heavily on the efficiency of the softmaxnormalization.2.2 Recurrent Neural Net Language Model (RNNLM)Recurrent neural network based language model has been proposed to overcome certain limitationsof the feedforward NNLM, such as the need to specify the context length (the order of the model N),and because theoretically RNNs can efficiently represent more complex patterns than the shallowneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden andoutput layer. What is special for this type of model is the recurrent matrix that connects hiddenlayer to itself, using time-delayed connections. This allows the recurrent model to form some kindof short term memory, as information from the past can be represented by the hidden layer state thatgets updated based on the current input and the state of the hidden layer in the previous time step.The complexity per training example of the RNN model isQ = H x H + H x V, (3)where the word representations D have the same dimensionality as the hidden layer H. Again, theterm H x V can be efficiently reduced to H x log2(V ) by using hierarchical softmax. Most of thecomplexity then comes from H x H.32.3 Parallel Training of Neural NetworksTo train models on huge data sets, we have implemented several models on top of a large-scaledistributed framework called DistBelief [6], including the feedforward NNLM and the new modelsproposed in this paper. The framework allows us to run multiple replicas of the same model inparallel, and each replica synchronizes its gradient updates through a centralized server that keepsall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent withan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to useone hundred or more model replicas, each using many CPU cores at different machines in a datacenter.3 New Log-linear ModelsIn this section, we propose two new model architectures for learning distributed representationsof words that try to minimize computational complexity. The main observation from the previoussection was that most of the complexity is caused by the non-linear hidden layer in the model. Whilethis is what makes neural networks so attractive, we decided to explore simpler models that mightnot be able to represent the data as precisely as neural networks, but can possibly be trained on muchmore data efficiently.The new architectures directly follow those proposed in our earlier work [13, 14], where it wasfound that neural network language model can be successfully trained in two steps: first, continuousword vectors are learned using simple model, and then the N-gram NNLM is trained on top of thesedistributed representations of words. While there has been later substantial amount of work thatfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.Note that related models have been proposed also much earlier [26, 8].3.1 Continuous Bag-of-Words ModelThe first proposed architecture is similar to the feedforward NNLM, where the non-linear hiddenlayer is removed and the projection layer is shared for all words (not just the projection matrix);thus, all words get projected into the same position (their vectors are averaged). We call this archi-tecture a bag-of-words model as the order of words in the history does not influence the projection.Furthermore, we also use words from the future; we have obtained the best performance on the taskintroduced in the next section by building a log-linear classifier with four future and four historywords at the input, where the training criterion is to correctly classify the current (middle) word.Training complexity is thenQ = N x D + D x log2(V ). (4)We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuousdistributed representation of the context. The model architecture is shown at Figure 1. Note that theweight matrix between the input and the projection layer is shared for all word positions in the sameway as in the NNLM.3.2 Continuous Skip-gram ModelThe second architecture is similar to CBOW, but instead of predicting the current word based on thecontext, it tries to maximize classification of a word based on another word in the same sentence.More precisely, we use each current word as an input to a log-linear classifier with continuousprojection layer, and predict words within a certain range before and after the current word. Wefound that increasing the range improves quality of the resulting word vectors, but it also increasesthe computational complexity. Since the more distant words are usually less related to the currentword than those close to it, we give less weight to the distant words by sampling less from thosewords in our training examples.The training complexity of this architecture is proportional toQ = C x (D + D x log2(V )), (5)where C is the maximum distance of the words. Thus, if we choose C = 5, for each training wordwe will select randomly a number R in range < 1; C >, and then use R words from history and4w(t-2)w(t+1)w(t-1)w(t+2)w(t)SUMINPUT PROJECTION OUTPUTw(t)INPUT PROJECTION OUTPUTw(t-2)w(t-1)w(t+1)w(t+2)CBOW Skip-gramFigure 1: New model architectures. The CBOW architecture predicts the current word based on thecontext, and the Skip-gram predicts surrounding words given the current word.R words from the future of the current word as correct labels. This will require us to do R x 2word classifications, with the current word as input, and each of the R + R words as output. In thefollowing experiments, we use C = 10.4 ResultsTo compare the quality of different versions of word vectors, previous papers typically use a tableshowing example words and their most similar words, and understand them intuitively. Althoughit is easy to show that word France is similar to Italy and perhaps some other countries, it is muchmore challenging when subjecting those vectors in a more complex similarity task, as follows. Wefollow previous observation that there can be many different types of similarities between words, forexample, word big is similar to bigger in the same sense that small is similar to smaller. Exampleof another type of relationship can be word pairs big - biggest and small - smallest [20]. We furtherdenote two pairs of words with the same relationship as a question, as we can ask: "What is theword that is similar to small in the same sense as biggest is similar to big?"Somewhat surprisingly, these questions can be answered by performing simple algebraic operationswith the vector representation of words. To find a word that is similar to small in the same sense asbiggest is similar to big, we can simply compute vector X = vector("biggest")-vector("big")+vector("small"). Then, we search in the vector space for the word closest to X measured by cosinedistance, and use it as the answer to the question (we discard the input question words during thissearch). When the word vectors are well trained, it is possible to find the correct answer (wordsmallest) using this method.Finally, we found that when we train high dimensional word vectors on a large amount of data, theresulting vectors can be used to answer very subtle semantic relationships between words, such asa city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectorswith such semantic relationships could be used to improve many existing NLP applications, suchas machine translation, information retrieval and question answering systems, and may enable otherfuture applications yet to be invented.5Table 1: Examples of five types of semantic and nine types of syntactic questions in the Semantic-Syntactic Word Relationship test set.Type of relationship Word Pair 1 Word Pair 2Common capital city Athens Greece Oslo NorwayAll capital cities Astana Kazakhstan Harare ZimbabweCurrency Angola kwanza Iran rialCity-in-state Chicago Illinois Stockton CaliforniaMan-Woman brother sister grandson granddaughterAdjective to adverb apparent apparently rapid rapidlyOpposite possibly impossibly ethical unethicalComparative great greater tough tougherSuperlative easy easiest lucky luckiestPresent Participle think thinking read readingNationality adjective Switzerland Swiss Cambodia CambodianPast tense walking walked swimming swamPlural nouns mouse mice dollar dollarsPlural verbs work works speak speaks4.1 Task DescriptionTo measure quality of the word vectors, we define a comprehensive test set that contains five typesof semantic questions, and nine types of syntactic questions. Two examples from each category areshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questionsin each category were created in two steps: first, a list of similar word pairs was created manually.Then, a large list of questions is formed by connecting two word pairs. For example, we made alist of 68 large American cities and the states they belong to, and formed about 2.5K questions bypicking two word pairs at random. We have included in our test set only single token words, thusmulti-word entities are not present (such as New York).We evaluate the overall accuracy for all question types, and for each question type separately (se-mantic, syntactic). Question is assumed to be correctly answered only if the closest word to thevector computed using the above method is exactly the same as the correct word in the question;synonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likelyto be impossible, as the current models do not have any input information about word morphology.However, we believe that usefulness of the word vectors for certain applications should be positivelycorrelated with this accuracy metric. Further progress can be achieved by incorporating informationabout structure of words, especially for the syntactic questions.4.2 Maximization of AccuracyWe have used a Google News corpus for training the word vectors. This corpus contains about6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, weare facing time constrained optimization problem, as it can be expected that both using more dataand higher dimensional word vectors will improve the accuracy. To estimate the best choice ofmodel architecture for obtaining as good as possible results quickly, we have first evaluated modelstrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.The results using the CBOW architecture with different choice of word vector dimensionality andincreasing amount of the training data are shown in Table 2.It can be seen that after some point, adding more dimensions or adding more training data providesdiminishing improvements. So, we have to increase both vector dimensionality and the amountof the training data together. While this observation might seem trivial, it must be noted that it iscurrently popular to train word vectors on relatively large amounts of data, but with insufficient size6Table 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set, using wordvectors from the CBOW architecture with limited vocabulary. Only questions containing words fromthe most frequent 30k words are used.Dimensionality / Training words 24M 49M 98M 196M 391M 783M50 13.4 15.7 18.6 19.1 22.5 23.2100 19.4 23.1 27.8 28.7 33.4 32.2300 23.2 29.2 35.3 38.6 43.7 45.9600 24.0 30.1 36.5 40.8 46.6 50.4Table 3: Comparison of architectures using models trained on the same data, with 640-dimensionalword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,and on the syntactic relationship test set of [20]Model Semantic-Syntactic Word Relationship test set MSR Word RelatednessArchitecture Semantic Accuracy [%] Syntactic Accuracy [%] Test Set [20]RNNLM 9 36 35NNLM 23 53 47CBOW 24 64 61Skip-gram 55 59 56(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about thesame increase of computational complexity as increasing vector size twice.For the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-ent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, sothat it approaches zero at the end of the last training epoch.4.3 Comparison of Model ArchitecturesFirst we compare different model architectures for deriving the word vectors using the same trainingdata and using the same dimensionality of 640 of the word vectors. In the further experiments, weuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted tothe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntacticsimilarity between words3.The training data consists of several LDC corpora and is described in detail in [18] (320M words,82K vocabulary). We used these data to provide a comparison to a previously trained recurrentneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-forward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],using a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as theprojection layer has size 640 x 8).In Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostlyon the syntactic questions. The NNLM vectors perform significantly better than the RNN - this isnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hiddenlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about thesame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactictask than the CBOW model (but still better than the NNLM), and much better on the semantic partof the test than all the other models.Next, we evaluated our models trained using one CPU only and compared the results against publiclyavailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset3We thank Geoff Zweig for providing us the test set.7Table 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-ship test set, and word vectors from our models. Full vocabularies are used.Model Vector Training Accuracy [%]Dimensionality wordsSemantic Syntactic TotalCollobert-Weston NNLM 50 660M 9.3 12.3 11.0Turian NNLM 50 37M 1.4 2.6 2.1Turian NNLM 200 37M 1.4 2.2 1.8Mnih NNLM 50 37M 1.8 9.1 5.8Mnih NNLM 100 37M 3.3 13.2 8.8Mikolov RNNLM 80 320M 4.9 18.4 12.7Mikolov RNNLM 640 320M 8.6 36.5 24.6Huang NNLM 50 990M 13.3 11.6 12.3Our NNLM 20 6B 12.9 26.4 20.3Our NNLM 50 6B 27.9 55.8 43.2Our NNLM 100 6B 34.2 64.5 50.8CBOW 300 783M 15.5 53.1 36.1Skip-gram 300 783M 50.0 55.9 53.3Table 5: Comparison of models trained for three epochs on the same data and models trained forone epoch. Accuracy is reported on the full Semantic-Syntactic data set.Model Vector Training Accuracy [%] Training timeDimensionality words [days]Semantic Syntactic Total3 epoch CBOW 300 783M 15.5 53.1 36.1 13 epoch Skip-gram 300 783M 50.0 55.9 53.3 31 epoch CBOW 300 783M 13.8 49.9 33.6 0.31 epoch CBOW 300 1.6B 16.1 52.6 36.1 0.61 epoch CBOW 600 783M 15.4 53.3 36.2 0.71 epoch Skip-gram 300 783M 45.6 52.2 49.2 11 epoch Skip-gram 300 1.6B 52.2 55.1 53.8 21 epoch Skip-gram 600 783M 56.7 54.5 55.5 2.5of the Google News data in about a day, while training time for the Skip-gram model was about threedays.For experiments reported further, we used just one training epoch (again, we decrease the learningrate linearly so that it approaches zero at the end of training). Training a model on twice as muchdata using one epoch gives comparable or better results than iterating over the same data for threeepochs, as is shown in Table 5, and provides additional small speedup.4.4 Large Scale Parallel Training of ModelsAs mentioned earlier, we have implemented various models in a distributed framework called Dis-tBelief. Below we report the results of several models trained on the Google News 6B data set,with mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-grad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an8Table 6: Comparison of models trained using the DistBelief distributed framework. Note thattraining of NNLM with 1000-dimensional vectors would take too long to complete.Model Vector Training Accuracy [%] Training timeDimensionality words [days x CPU cores]Semantic Syntactic TotalNNLM 100 6B 34.2 64.5 50.8 14 x 180CBOW 1000 6B 57.3 68.9 63.7 2 x 140Skip-gram 1000 6B 66.1 65.1 65.6 2.5 x 125Table 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.Architecture Accuracy [%]4-gram [32] 39Average LSA similarity [32] 49Log-bilinear model [24] 54.8RNNLMs [19] 55.4Skip-gram 48.0Skip-gram + RNNLMs 58.9estimate since the data center machines are shared with other production tasks, and the usage canfluctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage ofthe CBOW model and the Skip-gram model are much closer to each other than their single-machineimplementations. The result are reported in Table 6.4.5 Microsoft Research Sentence Completion ChallengeThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancinglanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where oneword is missing in each sentence and the goal is to select word that is the most coherent with therest of the sentence, given a list of five reasonable choices. Performance of several techniques hasbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinearmodel [24] and a combination of recurrent neural networks that currently holds the state of the artperformance of 55.4% accuracy on this benchmark [19].We have explored the performance of Skip-gram architecture on this task. First, we train the 640-dimensional model on 50M words provided in [32]. Then, we compute score of each sentence inthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.The final sentence score is then the sum of these individual predictions. Using the sentence scores,we choose the most likely sentence.A short summary of some previous results together with the new results is presented in Table 7.While the Skip-gram model itself does not perform on this task better than LSA similarity, the scoresfrom this model are complementary to scores obtained with RNNLMs, and a weighted combinationleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and58.7% on the test part of the set).5 Examples of the Learned RelationshipsTable 8 shows words that follow various relationships. We follow the approach described above: therelationship is defined by subtracting two word vectors, and the result is added to another word. Thusfor example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, althoughthere is clearly a lot of room for further improvements (note that using our accuracy metric that9Table 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-gram model trained on 783M words with 300 dimensionality).Relationship Example 1 Example 2 Example 3France - Paris Italy: Rome Japan: Tokyo Florida: Tallahasseebig - bigger small: larger cold: colder quick: quickerMiami - Florida Baltimore: Maryland Dallas: Texas Kona: HawaiiEinstein - scientist Messi: midfielder Mozart: violinist Picasso: painterSarkozy - France Berlusconi: Italy Merkel: Germany Koizumi: Japancopper - Cu zinc: Zn gold: Au uranium: plutoniumBerlusconi - Silvio Sarkozy: Nicolas Putin: Medvedev Obama: BarackMicrosoft - Windows Google: Android IBM: Linux Apple: iPhoneMicrosoft - Ballmer Google: Yahoo IBM: McNealy Apple: JobsJapan - sushi Germany: bratwurst France: tapas USA: pizzaassumes exact match, the results in Table 8 would score only about 60%). We believe that wordvectors trained on even larger data sets with larger dimensionality will perform significantly better,and will enable the development of new innovative applications. Another way to improve accuracy isto provide more than one example of the relationship. By using ten examples instead of one to formthe relationship vector (we average the individual vectors together), we have observed improvementof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.It is also possible to apply the vector operations to solve different tasks. For example, we haveobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list ofwords, and finding the most distant word vector. This is a popular type of problems in certain humanintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.

Conclusion :
In this paper we studied the quality of vector representations of words derived by various models ona collection of syntactic and semantic language tasks. We observed that it is possible to train highquality word vectors using very simple model architectures, compared to the popular neural networkmodels (both feedforward and recurrent). Because of the much lower computational complexity, itis possible to compute very accurate high dimensional word vectors from a much larger data set.Using the DistBelief distributed framework, it should be possible to train the CBOW and Skip-grammodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. Thatis several orders of magnitude larger than the best previously published results for similar models.An interesting task where the word vectors have recently been shown to significantly outperform theprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors wereused together with other techniques to achieve over 50% increase in Spearman's rank correlationover the previous best result [31]. The neural network based word vectors were previously appliedto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It canbe expected that these applications can benefit from the model architectures described in this paper.Our ongoing work shows that the word vectors can be successfully applied to automatic extensionof facts in Knowledge Bases, and also for verification of correctness of existing facts. Resultsfrom machine translation experiments also look very promising. In the future, it would be alsointeresting to compare our techniques to Latent Relational Analysis [30] and others. We believe thatour comprehensive test set will help the research community to improve the existing techniques forestimating the word vectors. We also expect that high quality word vectors will become an importantbuilding block for future NLP applications.10

Discussion :


Reference :
[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-chine Learning Research, 3:1137-1155, 2003.[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-chines, MIT Press, 2007.[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machinetranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural LanguageProcessing and Computational Language Learning, 2007.[4] R. Collobert and J. Weston. A Unified Architecture for Natural Language Processing: DeepNeural Networks with Multitask Learning. In International Conference on Machine Learning,ICML, 2008.[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-guage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-2537, 2011.[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.Senior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning andstochastic optimization. Journal of Machine Learning Research, 2011.[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representationsvia Global Context and Multiple Word Prototypes. In: Proc. Association for ComputationalLinguistics, 2012.[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-tributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,MIT Press, 1986.[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuringdegrees of relational similarity. In: Proceedings of the 6th International Workshop on SemanticEvaluation (SemEval 2012), 2012.[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors forsentiment analysis. In Proceedings of ACL, 2011.[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-versity of Technology, 2007.[14] T. Mikolov, J. Kopecky, L. Burget, O. Glembek and J. Cernocky. Neural network based lan-guage models for higly inflective languages, In: Proc. ICASSP 2009.[15] T. Mikolov, M. Karafiat, L. Burget, J. Cernocky, S. Khudanpur. Recurrent neural networkbased language model, In: Proceedings of Interspeech, 2010.[16] T. Mikolov, S. Kombrink, L. Burget, J. Cernocky, S. Khudanpur. Extensions of recurrent neuralnetwork language model, In: Proceedings of ICASSP 2011.[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. Cernocky. Empirical Evaluation and Com-bination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.4The code is available at https://code.google.com/p/word2vec/11[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. Cernocky. Strategies for Training Large ScaleNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-ing, 2011.[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-sity of Technology, 2012.[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-tations. NAACL HLT 2013.[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations ofWords and Phrases and their Compositionality. Accepted to NIPS 2013.[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,2007.[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in NeuralInformation Processing Systems 21, MIT Press, 2009.[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic languagemodels. ICML, 2012.[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,2005.[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-propagating errors. Nature, 323:533.536, 1986.[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,2007.[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling andUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method forSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-tional Joint Conference on Artificial Intelligence, 2005.[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models forMeasuring Relational Similarity. NAACL HLT 2013.[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, MicrosoftResearch Technical Report MSR-TR-2011-129, 2011.12