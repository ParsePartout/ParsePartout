Nom du fichier :
			mikheev J02-3002.pdf
Titre :
			Periods, Capitalized Words, etc.
Nombre d'auteur :
 			1
Auteur :
			Andrei Mikheev
Abstract :
			In this article we present an approach for tackling three important aspects of text normalization: sentence boundary disambiguation, disambiguation of capitalized words in positions where capitalization is expected, and identification of abbreviations. As opposed to the two dominant techniques of computing statistics or writing specialized grammars, our document-centered approach works by considering suggestive local contexts and repetitions of individual words within a document. This approach proved to be robust to domain shifts and new lexica and produced performance on the level with the highest reported results. When incorporated into a part-of-speech tagger, it helped reduce the error rate significantly on capitalized words and sentence boundaries. We also investigated the portability to other languages and obtained encouraging results.

Intro :
			null			Extensive experiments on query-oriented multi- 
			document summarization have been carried out 
			over the past few years. Most of the strategies 
			to produce summaries are based on an extrac- 
			tion method, which identifies salient textual seg- 
			ments, most often sentences, in documents. Sen- 
			tences containing the most salient concepts are se- 
			lected, ordered and assembled according to their 
			relevance to produce summaries (also called ex- 
			tracts) (Mani and Maybury, 1999). 
			Recently emerged from the Document Under- 
			standing Conference (DUC) 20071, update sum- 
			marization attempts to enhance summarization 
			when more information about knowledge acquired 
			by the user is available. It asks the following ques- 
			tion: has the user already read documents on the 
			topic? In the case of a positive answer, producing 
			an extract focusing on only new facts is of inter- 
			est. In this way, an important issue is introduced: 
			c 2008. Licensed under the Creative Commons 
			Attribution-Noncommercial-Share Alike 3.0 Unported li- 
			cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). 
			Some rights reserved. 
			1Document Understanding Conferences are conducted 
			since 2000 by the National Institute of Standards and Tech- 
			nology (NIST), http://www-nlpir.nist.gov 
			redundancy with previously read documents (his- 
			tory) has to be removed from the extract. 
			A natural way to go about update summarization 
			would be extracting temporal tags (dates, elapsed 
			times, temporal expressions...) (Mani and Wilson, 
			2000) or to automatically construct the timeline 
			from documents (Swan and Allan, 2000). These 
			temporal marks could be used to focus extracts on 
			the most recently written facts. However, most re- 
			cently written facts are not necessarily new facts. 
			Machine Reading (MR) was used by (Hickl et 
			al., 2007) to construct knowledge representations 
			from clusters of documents. Sentences contain- 
			ing "new" information (i.e. that could not be in- 
			ferred by any previously considered document) 
			are selected to generate summary. However, this 
			highly efficient approach (best system in DUC 
			2007 update) requires large linguistic resources. 
			(Witte et al., 2007) propose a rule-based system 
			based on fuzzy coreference cluster graphs. Again, 
			this approach requires to manually write the sen- 
			tence ranking scheme. Several strategies remain- 
			ing on post-processing redundancy removal tech- 
			niques have been suggested. Extracts constructed 
			from history were used by (Boudin and Torres- 
			Moreno, 2007) to minimize history's redundancy. 
			(Lin et al., 2007) have proposed a modified Max- 
			imal Marginal Relevance (MMR) (Carbonell and 
			Goldstein, 1998) re-ranker during sentence selec- 
			tion, constructing the summary by incrementally 
			re-ranking sentences. 
			In this paper, we propose a scalable sentence 
			scoring method for update summarization derived 
			from MMR. Motivated by the need for relevant 
			novelty, candidate sentences are selected accord- 
			ing to a combined criterion of query relevance and 
			dissimilarity with previously read sentences. The 
			rest of the paper is organized as follows. Section 2 
			23 
			introduces our proposed sentence scoring method 
			and Section 3 presents experiments and evaluates 
			Extensive experiments on query-oriented multi- 
			document summarization have been carried out 
			over the past few years. Most of the strategies 
			to produce summaries are based on an extrac- 
			tion method, which identifies salient textual seg- 
			ments, most often sentences, in documents. Sen- 
			tences containing the most salient concepts are se- 
			lected, ordered and assembled according to their 
			relevance to produce summaries (also called ex- 
			tracts) (Mani and Maybury, 1999). 
			Recently emerged from the Document Under- 
			standing Conference (DUC) 20071, update sum- 
			marization attempts to enhance summarization 
			when more information about knowledge acquired 
			by the user is available. It asks the following ques- 
			tion: has the user already read documents on the 
			topic? In the case of a positive answer, producing 
			an extract focusing on only new facts is of inter- 
			est. In this way, an important issue is introduced: 
			c 2008. Licensed under the Creative Commons 
			Attribution-Noncommercial-Share Alike 3.0 Unported li- 
			cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). 
			Some rights reserved. 
			1Document Understanding Conferences are conducted 
			since 2000 by the National Institute of Standards and Tech- 
			nology (NIST), http://www-nlpir.nist.gov 
			redundancy with previously read documents (his- 
			tory) has to be removed from the extract. 
			A natural way to go about update summarization 
			would be extracting temporal tags (dates, elapsed 
			times, temporal expressions...) (Mani and Wilson, 
			2000) or to automatically construct the timeline 
			from documents (Swan and Allan, 2000). These 
			temporal marks could be used to focus extracts on 
			the most recently written facts. However, most re- 
			cently written facts are not necessarily new facts. 
			Machine Reading (MR) was used by (Hickl et 
			al., 2007) to construct knowledge representations 
			from clusters of documents. Sentences contain- 
			ing "new" information (i.e. that could not be in- 
			ferred by any previously considered document) 
			are selected to generate summary. However, this 
			highly efficient approach (best system in DUC 
			2007 update) requires large linguistic resources. 
			(Witte et al., 2007) propose a rule-based system 
			based on fuzzy coreference cluster graphs. Again, 
			this approach requires to manually write the sen- 
			tence ranking scheme. Several strategies remain- 
			ing on post-processing redundancy removal tech- 
			niques have been suggested. Extracts constructed 
			from history were used by (Boudin and Torres- 
			Moreno, 2007) to minimize history's redundancy. 
			(Lin et al., 2007) have proposed a modified Max- 
			imal Marginal Relevance (MMR) (Carbonell and 
			Goldstein, 1998) re-ranker during sentence selec- 
			tion, constructing the summary by incrementally 
			re-ranking sentences. 
			In this paper, we propose a scalable sentence 
			scoring method for update summarization derived 
			from MMR. Motivated by the need for relevant 
			novelty, candidate sentences are selected accord- 
			ing to a combined criterion of query relevance and 
			dissimilarity with previously read sentences. The 
			rest of the paper is organized as follows. Section 2 
			23 
			introduces our proposed sentence scoring method 
			and Section 3 presents experiments and evaluates 
			The subfield of summarization has been investigated by the NLP community for 
			nearly the last half century. Radev et al. (2002) define a summary as "a text that 
			is produced from one or more texts, that conveys important information in the 
			original text(s), and that is no longer than half of the original text(s) and usually 
			significantly less than that". This simple definition captures three important aspects 
			that characterize research on automatic summarization: 
			* Summaries may be produced from a single document or multiple documents, 
			* Summaries should preserve important information, 
			* Summaries should be short. 
			Even if we agree unanimously on these points, it seems from the literature that 
			any attempt to provide a more elaborate definition for the task would result in 
			disagreement within the community. In fact, many approaches differ on the manner 
			of their problem formulations. We start by introducing some common terms in the 
			1 
			summarization dialect: extraction is the procedure of identifying important sections 
			of the text and producing them verbatim; abstraction aims to produce important 
			material in a new way; fusion combines extracted parts coherently; and compression 
			aims to throw out unimportant sections of the text (Radev et al., 2002). 
			Earliest instances of research on summarizing scientific documents proposed 
			paradigms for extracting salient sentences from text using features like word and 
			phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key 
			phrases (Edmundson, 1969). Various work published since then has concentrated on 
			other domains, mostly on newswire data. Many approaches addressed the problem 
			by building systems depending of the type of the required summary. While extractive 
			summarization is mainly concerned with what the summary content should be, usu- 
			ally relying solely on extraction of sentences, abstractive summarization puts strong 
			emphasis on the form, aiming to produce a grammatical summary, which usually 
			requires advanced language generation techniques. In a paradigm more tuned to 
			information retrieval (IR), one can also consider topic-driven summarization, that 
			assumes that the summary content depends on the preference of the user and can 
			be assessed via a query, making the final summary focused on a particular topic. 
			A crucial issue that will certainly drive future research on summarization is 
			evaluation. During the last fifteen years, many system evaluation competitions like 
			TREC,1 DUC2 and MUC3 have created sets of training material and have estab- 
			lished baselines for performance levels. However, a universal strategy to evaluate 
			summarization systems is still absent. 
			In this survey, we primarily aim to investigate how empirical methods have been 
			used to build summarization systems. The rest of the paper is organized as fol- 
			lows: Section 2 describes single-document summarization, focusing on extractive 
			techniques. Section 3 progresses to discuss the area of multi-document summariza- 
			tion, where a few abstractive approaches that pioneered the field are also considered. 
			Section 4 briefly discusses some unconventional approaches that we believe can be 
			useful in the future of summarization research. Section 5 elaborates a few eval- 
			uation techniques and describes some of the standards for evaluating summaries 
			The subfield of summarization has been investigated by the NLP community for 
			nearly the last half century. Radev et al. (2002) define a summary as "a text that 
			is produced from one or more texts, that conveys important information in the 
			original text(s), and that is no longer than half of the original text(s) and usually 
			significantly less than that". This simple definition captures three important aspects 
			that characterize research on automatic summarization: 
			* Summaries may be produced from a single document or multiple documents, 
			* Summaries should preserve important information, 
			* Summaries should be short. 
			Even if we agree unanimously on these points, it seems from the literature that 
			any attempt to provide a more elaborate definition for the task would result in 
			disagreement within the community. In fact, many approaches differ on the manner 
			of their problem formulations. We start by introducing some common terms in the 
			1 
			summarization dialect: extraction is the procedure of identifying important sections 
			of the text and producing them verbatim; abstraction aims to produce important 
			material in a new way; fusion combines extracted parts coherently; and compression 
			aims to throw out unimportant sections of the text (Radev et al., 2002). 
			Earliest instances of research on summarizing scientific documents proposed 
			paradigms for extracting salient sentences from text using features like word and 
			phrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and key 
			phrases (Edmundson, 1969). Various work published since then has concentrated on 
			other domains, mostly on newswire data. Many approaches addressed the problem 
			by building systems depending of the type of the required summary. While extractive 
			summarization is mainly concerned with what the summary content should be, usu- 
			ally relying solely on extraction of sentences, abstractive summarization puts strong 
			emphasis on the form, aiming to produce a grammatical summary, which usually 
			requires advanced language generation techniques. In a paradigm more tuned to 
			information retrieval (IR), one can also consider topic-driven summarization, that 
			assumes that the summary content depends on the preference of the user and can 
			be assessed via a query, making the final summary focused on a particular topic. 
			A crucial issue that will certainly drive future research on summarization is 
			evaluation. During the last fifteen years, many system evaluation competitions like 
			TREC,1 DUC2 and MUC3 have created sets of training material and have estab- 
			lished baselines for performance levels. However, a universal strategy to evaluate 
			summarization systems is still absent. 
			In this survey, we primarily aim to investigate how empirical methods have been 
			used to build summarization systems. The rest of the paper is organized as fol- 
			lows: Section 2 describes single-document summarization, focusing on extractive 
			techniques. Section 3 progresses to discuss the area of multi-document summariza- 
			tion, where a few abstractive approaches that pioneered the field are also considered. 
			Section 4 briefly discusses some unconventional approaches that we believe can be 
			useful in the future of summarization research. Section 5 elaborates a few eval- 
			uation techniques and describes some of the standards for evaluating summaries 
			The goal of Automatic Speech Recognition (ASR) is to transform spoken data 
			into a written representation, thus enabling natural human-machine interaction 
			[33] with further Natural Language Processing (NLP) tasks. Machine transla- 
			tion, question answering, semantic parsing, POS tagging, sentiment analysis and 
			automatic text summarization; originally developed to work with formal writ- 
			ten texts, can be applied over the transcripts made by ASR systems [2,25,31]. 
			However, before applying any of these NLP tasks a segmentation process called 
			Sentence Boundary Detection (SBD) should be performed over ASR transcripts 
			to reach a minimal syntactic information in the text. 
			To measure the performance of a SBD system, the automatically segmented 
			transcript is evaluated against a single reference normally done by a human. But 
			c Springer Nature Switzerland AG 2018 
			I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119-131, 2018. 
			https://doi.org/10.1007/978-3-030-04497-8_10 
			120 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			given a transcript, does it exist a unique reference? Or, is it possible that the 
			same transcript could be segmented in five different ways by five different people 
			in the same conditions? If so, which one is correct; and more important, how 
			to fairly evaluate the automatically segmented transcript? These questions are 
			the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a 
			new semi-supervised metric for evaluating SBD systems based on multi-reference 
			(dis)agreement. 
			The rest of this article is organized as follows. In Sect. 2 we set the frame of 
			SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3, 
			followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE 
			and discussion over the method and alternative multi-reference evaluation is 
			The goal of Automatic Speech Recognition (ASR) is to transform spoken data 
			into a written representation, thus enabling natural human-machine interaction 
			[33] with further Natural Language Processing (NLP) tasks. Machine transla- 
			tion, question answering, semantic parsing, POS tagging, sentiment analysis and 
			automatic text summarization; originally developed to work with formal writ- 
			ten texts, can be applied over the transcripts made by ASR systems [2,25,31]. 
			However, before applying any of these NLP tasks a segmentation process called 
			Sentence Boundary Detection (SBD) should be performed over ASR transcripts 
			to reach a minimal syntactic information in the text. 
			To measure the performance of a SBD system, the automatically segmented 
			transcript is evaluated against a single reference normally done by a human. But 
			c Springer Nature Switzerland AG 2018 
			I. Batyrshin et al. (Eds.): MICAI 2018, LNAI 11289, pp. 119-131, 2018. 
			https://doi.org/10.1007/978-3-030-04497-8_10 
			120 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			given a transcript, does it exist a unique reference? Or, is it possible that the 
			same transcript could be segmented in five different ways by five different people 
			in the same conditions? If so, which one is correct; and more important, how 
			to fairly evaluate the automatically segmented transcript? These questions are 
			the foundations of Window-based Sentence Boundary Evaluation (WiSeBE), a 
			new semi-supervised metric for evaluating SBD systems based on multi-reference 
			(dis)agreement. 
			The rest of this article is organized as follows. In Sect. 2 we set the frame of 
			SBD and how it is normally evaluated. WiSeBE is formally described in Sect. 3, 
			followed by a multi-reference evaluation in Sect. 4. Further analysis of WiSeBE 
			and discussion over the method and alternative multi-reference evaluation is 
			The Rhetorical Structure Theory (RST) (Mann and 
			Thompson, 1988) is a language independent theory 
			based on the idea that a text can be segmented into 
			Elementary Discourse Units (EDUs) linked by 
			means of nucleus-satellite or multinuclear 
			rhetorical relations. In the first case, the satellite 
			gives additional information about the other one, 
			the nucleus, on which it depends (ex. Result, 
			Condition, Elaboration or Concession). In the 
			second case, several elements, all nuclei, are 
			connected at the same level, that is, there are no 
			elements dependent on others and they all have the 
			same importance with regard to the intentions of 
			the author of the text (ex. Contrast, List, Joint or 
			Sequence). The rhetorical analysis of a text by 
			means of RST includes 3 phases: segmentation, 
			detection of relations and building of hierarchical 
			rhetorical trees. For more information about RST 
			we recommend the original article of Mann and 
			Thompson (1988), the web site of RST1 and the 
			RST review by Taboada and Mann (2006a). 
			RST has been used to develop several 
			applications, like automatic summarization, 
			information extraction (IE), text generation, 
			question-answering, automatic translation, etc. 
			(Taboada and Mann, 2006b). Nevertheless, most of 
			these works have been developed for English, 
			German or Portuguese. This is due to the fact that 
			at present corpora annotated with RST relations are 
			available only for these languages (for English: 
			Carlson et al., 2002, Taboada and Renkema, 2008; 
			for German: Stede, 2004; for Portuguese: Pardo et 
			al., 2008) and there are automatic RST parsers for 
			two of them (for English: Marcu, 2000; for 
			Portuguese: Pardo et al., 2008) or automatic RST 
			segmenters (for English: Tofiloski et al., 2009). 
			Scientific community working on RST applied to 
			Spanish is very small. For example, Bouayad-Agha 
			et al. (2006) apply RST to text generation in 
			several languages, Spanish among them. Da Cunha 
			et al. (2007) develop a summarization system for 
			medical texts in Spanish based on RST. Da Cunha 
			and Iruskieta (2010) perform a contrastive analysis 
			of Spanish and Basque texts. Romera (2004) 
			analyzes coherence relations by means of RST in 
			spoken Spanish. Taboada (2004) applies RST to 
			analyze the resources used by speakers to elaborate 
			conversations in English and Spanish. 
			We consider that it is necessary to build a 
			Spanish corpus annotated by means of RST. This 
			corpus should be useful for the development of a 
			rhetorical parser for this language and several other 
			applications related to computational linguistics, 
			like those developed for other languages 
			1 http://www.sfu.ca/rst/index.html 
			1 
			(automatic translation, automatic summarization, 
			IE, etc.). And that is what we pretend to achieve 
			with our work. We present the development of the 
			RST Spanish Treebank, the first Spanish corpus 
			annotated by means of RST. 
			In Section 2, we present the state of the art 
			about RST annotated corpora. In Section 3, we 
			explain the characteristics of the RST Spanish 
			Treebank. In Section 4, we show the search 
			interface we have developed. In Section 5, we 
			The Rhetorical Structure Theory (RST) (Mann and 
			Thompson, 1988) is a language independent theory 
			based on the idea that a text can be segmented into 
			Elementary Discourse Units (EDUs) linked by 
			means of nucleus-satellite or multinuclear 
			rhetorical relations. In the first case, the satellite 
			gives additional information about the other one, 
			the nucleus, on which it depends (ex. Result, 
			Condition, Elaboration or Concession). In the 
			second case, several elements, all nuclei, are 
			connected at the same level, that is, there are no 
			elements dependent on others and they all have the 
			same importance with regard to the intentions of 
			the author of the text (ex. Contrast, List, Joint or 
			Sequence). The rhetorical analysis of a text by 
			means of RST includes 3 phases: segmentation, 
			detection of relations and building of hierarchical 
			rhetorical trees. For more information about RST 
			we recommend the original article of Mann and 
			Thompson (1988), the web site of RST1 and the 
			RST review by Taboada and Mann (2006a). 
			RST has been used to develop several 
			applications, like automatic summarization, 
			information extraction (IE), text generation, 
			question-answering, automatic translation, etc. 
			(Taboada and Mann, 2006b). Nevertheless, most of 
			these works have been developed for English, 
			German or Portuguese. This is due to the fact that 
			at present corpora annotated with RST relations are 
			available only for these languages (for English: 
			Carlson et al., 2002, Taboada and Renkema, 2008; 
			for German: Stede, 2004; for Portuguese: Pardo et 
			al., 2008) and there are automatic RST parsers for 
			two of them (for English: Marcu, 2000; for 
			Portuguese: Pardo et al., 2008) or automatic RST 
			segmenters (for English: Tofiloski et al., 2009). 
			Scientific community working on RST applied to 
			Spanish is very small. For example, Bouayad-Agha 
			et al. (2006) apply RST to text generation in 
			several languages, Spanish among them. Da Cunha 
			et al. (2007) develop a summarization system for 
			medical texts in Spanish based on RST. Da Cunha 
			and Iruskieta (2010) perform a contrastive analysis 
			of Spanish and Basque texts. Romera (2004) 
			analyzes coherence relations by means of RST in 
			spoken Spanish. Taboada (2004) applies RST to 
			analyze the resources used by speakers to elaborate 
			conversations in English and Spanish. 
			We consider that it is necessary to build a 
			Spanish corpus annotated by means of RST. This 
			corpus should be useful for the development of a 
			rhetorical parser for this language and several other 
			applications related to computational linguistics, 
			like those developed for other languages 
			1 http://www.sfu.ca/rst/index.html 
			1 
			(automatic translation, automatic summarization, 
			IE, etc.). And that is what we pretend to achieve 
			with our work. We present the development of the 
			RST Spanish Treebank, the first Spanish corpus 
			annotated by means of RST. 
			In Section 2, we present the state of the art 
			about RST annotated corpora. In Section 3, we 
			explain the characteristics of the RST Spanish 
			Treebank. In Section 4, we show the search 
			interface we have developed. In Section 5, we 
			There is a big gap between the summaries produced 
			by current automatic summarizers and the abstracts 
			written by human professionals. Certainly one fac- 
			tor contributing to this gap is that automatic sys- 
			tems can not always correctly identify the important 
			topics of an article. Another factor, however, which 
			has received little attention, is that automatic sum- 
			marizers have poor text generation techniques. Most 
			automatic summarizers rely on extracting key sen- 
			tences or paragraphs from an article to produce a 
			summary. Since the extracted sentences are discon- 
			nected in the original article, when they are strung 
			together, the resulting summary can be inconcise, 
			incoherent, and sometimes even misleading. 
			We present a cut and paste based text sum- 
			marization technique, aimed at reducing the gap 
			between automatically generated summaries and 
			human-written abstracts. Rather than focusing 
			on how to identify key sentences, as do other re- 
			searchers, we study how to generate the text of a 
			summary once key sentences have been extracted. 
			The main idea of cut and paste summarization 
			is to reuse the text in an article to generate the 
			summary. However, instead of simply extracting 
			sentences as current summarizers do, the cut and 
			paste system will "smooth" the extracted sentences 
			by editing them. Such edits mainly involve cutting 
			phrases and pasting them together in novel ways. 
			The key features of this work are: 
			(1) The identification of cutting and past- 
			ing operations. We identified six operations that 
			can be used alone or together to transform extracted 
			sentences into sentences in human-written abstracts. 
			The operations were identified based on manual and 
			automatic comparison of human-written abstracts 
			and the original articles. Examples include sentence 
			reduction, sentence combination, syntactic transfor- 
			mation, and lexical paraphrasing. 
			(2) Development of an automatic system to 
			perform cut and paste operations. Two opera- 
			tions - sentence reduction and sentence combination 
			- are most effective in transforming extracted sen- 
			tences into summary sentences that are as concise 
			and coherent as in human-written abstracts. We 
			implemented a sentence reduction module that re- 
			moves extraneous phrases from extracted sentences, 
			and a sentence combination module that merges the 
			extracted sentences or the reduced forms resulting 
			from sentence reduction. Our sentence reduction 
			model determines what to cut based on multiple 
			sources of information, including syntactic knowl- 
			edge, context, and statistics learned from corpus 
			analysis. It improves the conciseness of extracted 
			sentences, making them concise and on target. Our 
			sentence combination module implements combina- 
			tion rules that were identified by observing examples 
			written by human professionals. It improves the co- 
			herence of extracted sentences. 
			(3) Decomposing human-wrltten summary 
			sentences. The cut and paste technique we propose 
			here is a new computational model which we based 
			on analysis of human-written abstracts. To do this 
			analysis, we developed an automatic system that can 
			match a phrase in a human-written abstract to the 
			corresponding phrase in the article, identifying its 
			most likely location. This decomposition program 
			allows us to analyze the construction of sentences 
			in a human-written abstract. Its results have been 
			used to train and test the sentence reduction and 
			sentence combination module. 
			In Section 2, we discuss the cut and paste tech- 
			nique in general, from both a professional and com- 
			putational perspective. We also describe the six cut 
			and paste operations. In Section 3, we describe the 
			178 
			system architecture. The major components of the 
			system, including sentence reduction, sentence com- 
			bination, decomposition, and sentence selection, are 
			described in Section 4. The evaluation results are 
			shown in Section 5. Related work is discussed in 
			Section 6. Finally, we conclude and discuss future 
			work. 
			Document sentence: When it arrives some- 
			time next year in new TV sets, the V-chip will 
			give parents a new and potentially revolution- 
			ary device to block out programs they don't 
			want their children to see. 
			Summary sentence: The V-chip will give par- 
			ents a device to block out programs they don't 
			There is a big gap between the summaries produced 
			by current automatic summarizers and the abstracts 
			written by human professionals. Certainly one fac- 
			tor contributing to this gap is that automatic sys- 
			tems can not always correctly identify the important 
			topics of an article. Another factor, however, which 
			has received little attention, is that automatic sum- 
			marizers have poor text generation techniques. Most 
			automatic summarizers rely on extracting key sen- 
			tences or paragraphs from an article to produce a 
			summary. Since the extracted sentences are discon- 
			nected in the original article, when they are strung 
			together, the resulting summary can be inconcise, 
			incoherent, and sometimes even misleading. 
			We present a cut and paste based text sum- 
			marization technique, aimed at reducing the gap 
			between automatically generated summaries and 
			human-written abstracts. Rather than focusing 
			on how to identify key sentences, as do other re- 
			searchers, we study how to generate the text of a 
			summary once key sentences have been extracted. 
			The main idea of cut and paste summarization 
			is to reuse the text in an article to generate the 
			summary. However, instead of simply extracting 
			sentences as current summarizers do, the cut and 
			paste system will "smooth" the extracted sentences 
			by editing them. Such edits mainly involve cutting 
			phrases and pasting them together in novel ways. 
			The key features of this work are: 
			(1) The identification of cutting and past- 
			ing operations. We identified six operations that 
			can be used alone or together to transform extracted 
			sentences into sentences in human-written abstracts. 
			The operations were identified based on manual and 
			automatic comparison of human-written abstracts 
			and the original articles. Examples include sentence 
			reduction, sentence combination, syntactic transfor- 
			mation, and lexical paraphrasing. 
			(2) Development of an automatic system to 
			perform cut and paste operations. Two opera- 
			tions - sentence reduction and sentence combination 
			- are most effective in transforming extracted sen- 
			tences into summary sentences that are as concise 
			and coherent as in human-written abstracts. We 
			implemented a sentence reduction module that re- 
			moves extraneous phrases from extracted sentences, 
			and a sentence combination module that merges the 
			extracted sentences or the reduced forms resulting 
			from sentence reduction. Our sentence reduction 
			model determines what to cut based on multiple 
			sources of information, including syntactic knowl- 
			edge, context, and statistics learned from corpus 
			analysis. It improves the conciseness of extracted 
			sentences, making them concise and on target. Our 
			sentence combination module implements combina- 
			tion rules that were identified by observing examples 
			written by human professionals. It improves the co- 
			herence of extracted sentences. 
			(3) Decomposing human-wrltten summary 
			sentences. The cut and paste technique we propose 
			here is a new computational model which we based 
			on analysis of human-written abstracts. To do this 
			analysis, we developed an automatic system that can 
			match a phrase in a human-written abstract to the 
			corresponding phrase in the article, identifying its 
			most likely location. This decomposition program 
			allows us to analyze the construction of sentences 
			in a human-written abstract. Its results have been 
			used to train and test the sentence reduction and 
			sentence combination module. 
			In Section 2, we discuss the cut and paste tech- 
			nique in general, from both a professional and com- 
			putational perspective. We also describe the six cut 
			and paste operations. In Section 3, we describe the 
			178 
			system architecture. The major components of the 
			system, including sentence reduction, sentence com- 
			bination, decomposition, and sentence selection, are 
			described in Section 4. The evaluation results are 
			shown in Section 5. Related work is discussed in 
			Section 6. Finally, we conclude and discuss future 
			work. 
			Document sentence: When it arrives some- 
			time next year in new TV sets, the V-chip will 
			give parents a new and potentially revolution- 
			ary device to block out programs they don't 
			want their children to see. 
			Summary sentence: The V-chip will give par- 
			ents a device to block out programs they don't 
			The current era is increasingly influenced by the prominence 
			of smart data and mobile applications. The work presented 
			in this paper has been carried out in one industrial project 
			(VOCAGEN) aiming at automating the production of struc- 
			tured data from human machine dialogues. Specifically, the 
			targeted application drives dialogues with people working in 
			a construction area for populating a database reporting key 
			data extracted from those dialogues. This application requires 
			complex processing for both transcripting speeches but also for 
			driving dialogues. The first process is required for good speech 
			recognition in a noisy environment. The second processing is 
			required because the database needs to be populated with both 
			right and complete data; indeed, people tend to apply a broad 
			(colloquial) vocabulary and the transcripted words need to be 
			used for filling in the corresponding data. Additionally, if some 
			data populate the database, additional data may be required 
			for completeness, thus the dialogue should enable to get those 
			additional data (e.g. if the word "room" is recognised and used 
			to populate the database, the location of the room must also 
			be got; this can be done by driving the dialogue). 
			The application provides people with "hand-free" device, 
			enabling a complete, quick and standardized reporting. First 
			usages of this application will be oriented to reporting failures 
			and problems in constructions. 
			The two processing steps mentioned above require on the 
			one side a "language model" (for transcripting the sentences) 
			and on the other side a "knowledge model" for driving the 
			dialogue and correctly understanding the meaning of the word. 
			The knowledge model is mainly an ontology of the domain (in 
			this case, the construction domain) providing the standardized 
			concepts and their relationships. As well-known, building such 
			knowledge models needs time and is costly; one of the earlier 
			questions raised by our industrial partners has been about 
			"how to build, as automaticaly as possible, such a knowledge 
			model". This question is closely related to the interest of 
			quickly adapting the application to other domains (than the 
			construction one) for reaching new markets. We developed a 
			complete methodology and system for partially answering the 
			question, focusing on how to extract a relevant terminology 
			from a collection of technical specifications. 
			The rest of the paper is organized as follow. Section II 
			present context of the project. Related work are reviewed in 
			Section III. Section IV presents collected resources and some 
			statistics about them. Section V describes the methodology de- 
			veloped for extracting relevant terms from collected resources. 
			The details about the evaluation are presented in Section VI-A 
			The current era is increasingly influenced by the prominence 
			of smart data and mobile applications. The work presented 
			in this paper has been carried out in one industrial project 
			(VOCAGEN) aiming at automating the production of struc- 
			tured data from human machine dialogues. Specifically, the 
			targeted application drives dialogues with people working in 
			a construction area for populating a database reporting key 
			data extracted from those dialogues. This application requires 
			complex processing for both transcripting speeches but also for 
			driving dialogues. The first process is required for good speech 
			recognition in a noisy environment. The second processing is 
			required because the database needs to be populated with both 
			right and complete data; indeed, people tend to apply a broad 
			(colloquial) vocabulary and the transcripted words need to be 
			used for filling in the corresponding data. Additionally, if some 
			data populate the database, additional data may be required 
			for completeness, thus the dialogue should enable to get those 
			additional data (e.g. if the word "room" is recognised and used 
			to populate the database, the location of the room must also 
			be got; this can be done by driving the dialogue). 
			The application provides people with "hand-free" device, 
			enabling a complete, quick and standardized reporting. First 
			usages of this application will be oriented to reporting failures 
			and problems in constructions. 
			The two processing steps mentioned above require on the 
			one side a "language model" (for transcripting the sentences) 
			and on the other side a "knowledge model" for driving the 
			dialogue and correctly understanding the meaning of the word. 
			The knowledge model is mainly an ontology of the domain (in 
			this case, the construction domain) providing the standardized 
			concepts and their relationships. As well-known, building such 
			knowledge models needs time and is costly; one of the earlier 
			questions raised by our industrial partners has been about 
			"how to build, as automaticaly as possible, such a knowledge 
			model". This question is closely related to the interest of 
			quickly adapting the application to other domains (than the 
			construction one) for reaching new markets. We developed a 
			complete methodology and system for partially answering the 
			question, focusing on how to extract a relevant terminology 
			from a collection of technical specifications. 
			The rest of the paper is organized as follow. Section II 
			present context of the project. Related work are reviewed in 
			Section III. Section IV presents collected resources and some 
			statistics about them. Section V describes the methodology de- 
			veloped for extracting relevant terms from collected resources. 
			The details about the evaluation are presented in Section VI-A 
			Since the nineties, social suffering has been a theme that has 
			received much attention from public and associative action. 
			Among the consequences, there is an explosion of listening 
			places or socio-technical devices of communication whose 
			objectives consist in moderating the various forms of suffering 
			by the liberation of the speech for a therapeutic purpose [1] 
			[2]. As part of the METICS project, a suicide prevention 
			association developed an application of web chat to meet 
			this need. The web chat is an area that allows anyone to 
			express and share with a volunteer listener their concerns and 
			anguishes. The main specificity of this device is its anonymous 
			nature. Protected by a pseudonym, the writers are invited 
			to discuss with a volunteer the problematic aspects of their 
			existence. Several thousand anonymous conversations have 
			been gathered and form a corpus of unpublished stories about 
			human distress. The purpose of the METICS project is to make 
			visible the ordinary forms of suffering usually removed from 
			common spaces and to grasp both its modes of enunciation and 
			digital support. In this study, we want to automatically identify 
			the reason for coming on the web chat for each participant. 
			Indeed, even if the association provided us with the theme 
			of all the conversations (work, loneliness, violence, racism, 
			addictions, family, etc.), the original reason has not been 
			preserved. In what follows, we first review some of the related 
			work in Section II. Section III presents the resources used and 
			gives some statistics about the collection. An overview of the 
			system and the strategy for identify the reason for coming 
			on the web chat is given in Section IV. Section V presents 
			the experimental protocol, an evaluation of our system and an 
			interpretation of the final results on the collection of human 
			Since the nineties, social suffering has been a theme that has 
			received much attention from public and associative action. 
			Among the consequences, there is an explosion of listening 
			places or socio-technical devices of communication whose 
			objectives consist in moderating the various forms of suffering 
			by the liberation of the speech for a therapeutic purpose [1] 
			[2]. As part of the METICS project, a suicide prevention 
			association developed an application of web chat to meet 
			this need. The web chat is an area that allows anyone to 
			express and share with a volunteer listener their concerns and 
			anguishes. The main specificity of this device is its anonymous 
			nature. Protected by a pseudonym, the writers are invited 
			to discuss with a volunteer the problematic aspects of their 
			existence. Several thousand anonymous conversations have 
			been gathered and form a corpus of unpublished stories about 
			human distress. The purpose of the METICS project is to make 
			visible the ordinary forms of suffering usually removed from 
			common spaces and to grasp both its modes of enunciation and 
			digital support. In this study, we want to automatically identify 
			the reason for coming on the web chat for each participant. 
			Indeed, even if the association provided us with the theme 
			of all the conversations (work, loneliness, violence, racism, 
			addictions, family, etc.), the original reason has not been 
			preserved. In what follows, we first review some of the related 
			work in Section II. Section III presents the resources used and 
			gives some statistics about the collection. An overview of the 
			system and the strategy for identify the reason for coming 
			on the web chat is given in Section IV. Section V presents 
			the experimental protocol, an evaluation of our system and an 
			interpretation of the final results on the collection of human 
			Disambiguation of sentence boundaries and normalization of capitalized words, as 
			well as identification of abbreviations, however small in comparison to other tasks 
			of text processing, are of primary importance in the developing of practical text- 
			processing applications. These tasks are usually performed before actual "intelligent" 
			text processing starts, and errors made at this stage are very likely to cause more errors 
			at later stages and are therefore very dangerous. 
			Disambiguation of capitalized words in mixed-case texts has received little atten- 
			tion in the natural language processing and information retrieval communities, but in 
			fact it plays an important role in many tasks. In mixed-case texts capitalized words 
			usually denote proper names (names of organizations, locations, people, artifacts, etc.), 
			but there are special positions in the text where capitalization is expected. Such manda- 
			tory positions include the first word in a sentence, words in titles with all significant 
			words capitalized or table entries, a capitalized word after a colon or open quote, and 
			the first word in a list entry, among others. Capitalized words in these and some other 
			positions present a case of ambiguity: they can stand for proper names, as in White 
			later said . . . , or they can be just capitalized common words, as in White elephants are 
			. . . . The disambiguation of capitalized words in ambiguous positions leads to the 
			identification of proper names (or their derivatives), and in this article we will use 
			these two terms and the term case normalization interchangeably. 
			Church (1995, p. 294) studied, among other simple text normalization techniques, 
			the effect of case normalization for different words and showed that "sometimes case 
			variants refer to the same thing (hurricane and Hurricane), sometimes they refer to 
			different things (continental and Continental) and sometimes they don't refer to much 
			of anything (e.g., anytime and Anytime)." Obviously these differences arise because 
			some capitalized words stand for proper names (such as Continental, the name of an 
			airline) and some do not. 
			 Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place, 
			Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk 
			290 
			Computational Linguistics Volume 28, Number 3 
			Proper names are the main concern of the named-entity recognition subtask (Chin- 
			chor 1998) of information extraction. The main objective of this subtask is the identi- 
			fication of proper names and also their classification into semantic categories (person, 
			organization, location, etc.).1 There the disambiguation of the first word in a sentence 
			(and in other ambiguous positions) is one of the central problems: about 20% of named 
			entities occur in ambiguous positions. For instance, the word Black in the sentence- 
			initial position can stand for a person's surname but can also refer to the color. Even 
			in multiword capitalized phrases, the first word can belong to the rest of the phrase 
			or can be just an external modifier. In the sentence Daily, Mason and Partners lost their 
			court case, it is clear that Daily, Mason and Partners is the name of a company. In the 
			sentence Unfortunately, Mason and Partners lost their court case, the name of the company 
			does not include the word Unfortunately, but the word Daily is just as common a word 
			as Unfortunately. 
			Identification of proper names is also important in machine translation, because 
			usually proper names are transliterated (i.e., phonetically translated) rather than prop- 
			erly (semantically) translated. In confidential texts, such as medical records, proper 
			names must be identified and removed before making such texts available to people 
			unauthorized to have access to personally identifiable information. And in general, 
			most tasks that involve text analysis will benefit from the robust disambiguation of 
			capitalized words into proper names and common words. 
			Another important task of text normalization is sentence boundary disambigua- 
			tion (SBD) or sentence splitting. Segmenting text into sentences is an important aspect 
			in developing many applications: syntactic parsing, information extraction, machine 
			translation, question answering, text alignment, document summarization, etc. Sen- 
			tence splitting in most cases is a simple matter: a period, an exclamation mark, or a 
			question mark usually signals a sentence boundary. In certain cases, however, a period 
			denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily 
			signal a sentence boundary. Furthermore, an abbreviation itself can be the last token 
			in a sentence in which case its period acts at the same time as part of this abbreviation 
			and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD 
			problem can be found in Palmer and Hearst (1997). 
			The disambiguation of capitalized words and sentence boundaries presents a 
			chicken-and-egg problem. If we know that a capitalized word that follows a period is 
			a common word, we can safely assign such period as sentence terminal. On the other 
			hand, if we know that a period is not sentence terminal, then we can conclude that 
			the following capitalized word is a proper name. 
			Another frequent source of ambiguity in end-of-sentence marking is introduced by 
			abbreviations: if we know that the word that precedes a period is not an abbreviation, 
			then almost certainly this period denotes a sentence boundary. If, however, this word 
			is an abbreviation, then it is not that easy to make a clear decision. This problem is 
			exacerbated by the fact that abbreviations do not form a closed set; that is, one can- 
			not list all possible abbreviations. Moreover, abbreviations can coincide with regular 
			words; for example, "in" can denote an abbreviation for "inches," "no" can denote an 
			abbreviation for "number," and "bus" can denote an abbreviation for "business." 
			In this article we present a method that tackles sentence boundaries, capitalized 
			words, and abbreviations in a uniform way through a document-centered approach. 
			As opposed to the two dominant techniques of computing statistics about the words 
			that surround potential sentence boundaries or writing specialized grammars, our ap- 
			1 In this article we are concerned only with the identification of proper names. 
			291 
			Mikheev Periods, Capitalized Words, etc. 
			proach disambiguates capitalized words and abbreviations by considering suggestive 
			local contexts and repetitions of individual words within a document. It then applies 
			Disambiguation of sentence boundaries and normalization of capitalized words, as 
			well as identification of abbreviations, however small in comparison to other tasks 
			of text processing, are of primary importance in the developing of practical text- 
			processing applications. These tasks are usually performed before actual "intelligent" 
			text processing starts, and errors made at this stage are very likely to cause more errors 
			at later stages and are therefore very dangerous. 
			Disambiguation of capitalized words in mixed-case texts has received little atten- 
			tion in the natural language processing and information retrieval communities, but in 
			fact it plays an important role in many tasks. In mixed-case texts capitalized words 
			usually denote proper names (names of organizations, locations, people, artifacts, etc.), 
			but there are special positions in the text where capitalization is expected. Such manda- 
			tory positions include the first word in a sentence, words in titles with all significant 
			words capitalized or table entries, a capitalized word after a colon or open quote, and 
			the first word in a list entry, among others. Capitalized words in these and some other 
			positions present a case of ambiguity: they can stand for proper names, as in White 
			later said . . . , or they can be just capitalized common words, as in White elephants are 
			. . . . The disambiguation of capitalized words in ambiguous positions leads to the 
			identification of proper names (or their derivatives), and in this article we will use 
			these two terms and the term case normalization interchangeably. 
			Church (1995, p. 294) studied, among other simple text normalization techniques, 
			the effect of case normalization for different words and showed that "sometimes case 
			variants refer to the same thing (hurricane and Hurricane), sometimes they refer to 
			different things (continental and Continental) and sometimes they don't refer to much 
			of anything (e.g., anytime and Anytime)." Obviously these differences arise because 
			some capitalized words stand for proper names (such as Continental, the name of an 
			airline) and some do not. 
			 Institute for Communicating and Collaborative Systems, Division of Informatics, 2 Buccleuch Place, 
			Edinburgh EH8 9LW, UK. E-mail: mikheev@cogsci.ed.ac.uk 
			290 
			Computational Linguistics Volume 28, Number 3 
			Proper names are the main concern of the named-entity recognition subtask (Chin- 
			chor 1998) of information extraction. The main objective of this subtask is the identi- 
			fication of proper names and also their classification into semantic categories (person, 
			organization, location, etc.).1 There the disambiguation of the first word in a sentence 
			(and in other ambiguous positions) is one of the central problems: about 20% of named 
			entities occur in ambiguous positions. For instance, the word Black in the sentence- 
			initial position can stand for a person's surname but can also refer to the color. Even 
			in multiword capitalized phrases, the first word can belong to the rest of the phrase 
			or can be just an external modifier. In the sentence Daily, Mason and Partners lost their 
			court case, it is clear that Daily, Mason and Partners is the name of a company. In the 
			sentence Unfortunately, Mason and Partners lost their court case, the name of the company 
			does not include the word Unfortunately, but the word Daily is just as common a word 
			as Unfortunately. 
			Identification of proper names is also important in machine translation, because 
			usually proper names are transliterated (i.e., phonetically translated) rather than prop- 
			erly (semantically) translated. In confidential texts, such as medical records, proper 
			names must be identified and removed before making such texts available to people 
			unauthorized to have access to personally identifiable information. And in general, 
			most tasks that involve text analysis will benefit from the robust disambiguation of 
			capitalized words into proper names and common words. 
			Another important task of text normalization is sentence boundary disambigua- 
			tion (SBD) or sentence splitting. Segmenting text into sentences is an important aspect 
			in developing many applications: syntactic parsing, information extraction, machine 
			translation, question answering, text alignment, document summarization, etc. Sen- 
			tence splitting in most cases is a simple matter: a period, an exclamation mark, or a 
			question mark usually signals a sentence boundary. In certain cases, however, a period 
			denotes a decimal point or is a part of an abbreviation, and thus it does not necessarily 
			signal a sentence boundary. Furthermore, an abbreviation itself can be the last token 
			in a sentence in which case its period acts at the same time as part of this abbreviation 
			and as the end-of-sentence indicator (fullstop). A detailed introduction to the SBD 
			problem can be found in Palmer and Hearst (1997). 
			The disambiguation of capitalized words and sentence boundaries presents a 
			chicken-and-egg problem. If we know that a capitalized word that follows a period is 
			a common word, we can safely assign such period as sentence terminal. On the other 
			hand, if we know that a period is not sentence terminal, then we can conclude that 
			the following capitalized word is a proper name. 
			Another frequent source of ambiguity in end-of-sentence marking is introduced by 
			abbreviations: if we know that the word that precedes a period is not an abbreviation, 
			then almost certainly this period denotes a sentence boundary. If, however, this word 
			is an abbreviation, then it is not that easy to make a clear decision. This problem is 
			exacerbated by the fact that abbreviations do not form a closed set; that is, one can- 
			not list all possible abbreviations. Moreover, abbreviations can coincide with regular 
			words; for example, "in" can denote an abbreviation for "inches," "no" can denote an 
			abbreviation for "number," and "bus" can denote an abbreviation for "business." 
			In this article we present a method that tackles sentence boundaries, capitalized 
			words, and abbreviations in a uniform way through a document-centered approach. 
			As opposed to the two dominant techniques of computing statistics about the words 
			that surround potential sentence boundaries or writing specialized grammars, our ap- 
			1 In this article we are concerned only with the identification of proper names. 
			291 
			Mikheev Periods, Capitalized Words, etc. 
			proach disambiguates capitalized words and abbreviations by considering suggestive 
			local contexts and repetitions of individual words within a document. It then applies 


Corps :			null			2 Method 
			The underlying idea of our method is that as the 
			number of sentences in the history increases, the 
			likelihood to have redundant information within 
			candidate sentences also increases. We propose 
			a scalable sentence scoring method derived from 
			MMR that, as the size of the history increases, 
			gives more importance to non-redundancy that to 
			query relevance. We define H to represent the pre- 
			viously read documents (history), Q to represent 
			the query and s the candidate sentence. The fol- 
			lowing subsections formally define the similarity 
			measures and the scalable MMR scoring method. 
			2.1 A query-oriented multi-document 
			summarizer 
			We have first started by implementing a simple 
			summarizer for which the task is to produce query- 
			focused summaries from clusters of documents. 
			Each document is pre-processed: documents are 
			segmented into sentences, sentences are filtered 
			(words which do not carry meaning are removed 
			such as functional words or common words) and 
			normalized using a lemmas database (i.e. inflected 
			forms "go", "goes", "went", "gone"... are replaced 
			by "go"). An N-dimensional term-space , where 
			N is the number of different terms found in the 
			cluster, is constructed. Sentences are represented 
			in  by vectors in which each component is the 
			term frequency within the sentence. Sentence scor- 
			ing can be seen as a passage retrieval task in Infor- 
			mation Retrieval (IR). Each sentence s is scored by 
			computing a combination of two similarity mea- 
			sures between the sentence and the query. The first 
			measure is the well known cosine angle (Salton et 
			al., 1975) between the sentence and the query vec- 
			torial representations in  (denoted respectively s 
			and Q). The second similarity measure is based 
			on the Jaro-Winkler distance (Winkler, 1999). The 
			original Jaro-Winkler measure, denoted JW, uses 
			the number of matching characters and transposi- 
			tions to compute a similarity score between two 
			terms, giving more favourable ratings to terms that 
			match from the beginning. We have extended this 
			measure to calculate the similarity between the 
			sentence s and the query Q: 
			JWe(s, Q) = 
			1 
			|Q| 
			* 
			qQ 
			max 
			mS 
			JW(q, m) (1) 
			where S is the term set of s in which the terms 
			m that already have maximized JW(q, m) are re- 
			moved. The use of JWe smooths normalization and 
			misspelling errors. Each sentence s is scored using 
			the linear combination: 
			Sim1(s, Q) =  * cosine(s, Q) 
			+ (1 - ) * JWe(s, Q) (2) 
			where  = 0.7, optimally tuned on the past DUCs 
			data (2005 and 2006). The system produces a list 
			of ranked sentences from which the summary is 
			constructed by arranging the high scored sentences 
			until the desired size is reached. 
			2.2 A scalable MMR approach 
			MMR re-ranking algorithm has been successfully 
			used in query-oriented summarization (Ye et al., 
			2005). It strives to reduce redundancy while main- 
			taining query relevance in selected sentences. The 
			summary is constructed incrementally from a list 
			of ranked sentences, at each iteration the sentence 
			which maximizes MMR is chosen: 
			MMR = arg max 
			sS 
			[  * Sim1(s, Q) 
			- (1 - ) * max 
			sjE 
			Sim2(s, sj) ] (3) 
			where S is the set of candidates sentences and E 
			is the set of selected sentences.  represents an 
			interpolation coefficient between sentence's rele- 
			vance and non-redundancy. Sim2(s, sj) is a nor- 
			malized Longest Common Substring (LCS) mea- 
			sure between sentences s and sj. Detecting sen- 
			tence rehearsals, LCS is well adapted for redun- 
			dancy removal. 
			We propose an interpretation of MMR to tackle 
			the update summarization issue. Since Sim1 and 
			Sim2 are ranged in [0, 1], they can be seen as prob- 
			abilities even though they are not. Just as rewriting 
			(3) as (NR stands for Novelty Relevance): 
			NR = arg max 
			sS 
			[  * Sim1(s, Q) 
			+ (1 - ) * (1 - max 
			shH 
			Sim2(s, sh)) ] (4) 
			We can understand that (4) equates to an OR com- 
			bination. But as we are looking for a more intu- 
			itive AND and since the similarities are indepen- 
			dent, we have to use the product combination. The 
			24 
			scoring method defined in (2) is modified into a 
			double maximization criterion in which the best 
			ranked sentence will be the most relevant to the 
			query AND the most different to the sentences in 
			H. 
			SMMR(s) = Sim1(s, Q) 
			* 1 - max 
			shH 
			Sim2(s, sh) 
			f(H) 
			(5) 
			Decreasing  in (3) with the length of the sum- 
			mary was suggested by (Murray et al., 2005) and 
			successfully used in the DUC 2005 by (Hachey 
			et al., 2005), thereby emphasizing the relevance 
			at the outset but increasingly prioritizing redun- 
			dancy removal as the process continues. Sim- 
			ilarly, we propose to follow this assumption in 
			SMMR using a function denoted f that as the 
			amount of data in history increases, prioritize non- 
			redundancy (f(H)  0). 
			3 Experiments 
			The method described in the previous section has 
			been implemented and evaluated by using the 
			DUC 2007 update corpus2. The following subsec- 
			tions present details of the different experiments 
			we have conducted. 
			3.1 The DUC 2007 update corpus 
			We used for our experiments the DUC 2007 up- 
			date competition data set. The corpus is composed 
			of 10 topics, with 25 documents per topic. The up- 
			date task goal was to produce short (100 words) 
			multi-document update summaries of newswire ar- 
			ticles under the assumption that the user has al- 
			ready read a set of earlier articles. The purpose 
			of each update summary will be to inform the 
			reader of new information about a particular topic. 
			Given a DUC topic and its 3 document clusters: A 
			(10 documents), B (8 documents) and C (7 doc- 
			uments), the task is to create from the documents 
			three brief, fluent summaries that contribute to sat- 
			isfying the information need expressed in the topic 
			statement. 
			1. A summary of documents in cluster A. 
			2. An update summary of documents in B, un- 
			der the assumption that the reader has already 
			read documents in A. 
			2More information about the DUC 2007 corpus is avail- 
			able at http://duc.nist.gov/. 
			3. An update summary of documents in C, un- 
			der the assumption that the reader has already 
			read documents in A and B. 
			Within a topic, the document clusters must be pro- 
			cessed in chronological order. Our system gener- 
			ates a summary for each cluster by arranging the 
			high ranked sentences until the limit of 100 words 
			is reached. 
			3.2 Evaluation 
			Most existing automated evaluation methods work 
			by comparing the generated summaries to one or 
			more reference summaries (ideally, produced by 
			humans). To evaluate the quality of our generated 
			summaries, we choose to use the ROUGE3 (Lin, 
			2004) evaluation toolkit, that has been found to be 
			highly correlated with human judgments. ROUGE- 
			N is a n-gram recall measure calculated between 
			a candidate summary and a set of reference sum- 
			maries. In our experiments ROUGE-1, ROUGE-2 
			and ROUGE-SU4 will be computed. 
			3.3 Results 
			Table 1 reports the results obtained on the DUC 
			2007 update data set for different sentence scor- 
			ing methods. cosine + JWe stands for the scor- 
			ing method defined in (2) and NR improves it 
			with sentence re-ranking defined in equation (4). 
			SMMR is the combined adaptation we have pro- 
			posed in (5). The function f(H) used in SMMR is 
			the simple rational function 1 
			H 
			, where H increases 
			with the number of previous clusters (f(H) = 1 
			for cluster A, 1 
			2 
			for cluster B and 1 
			3 
			for cluster C). 
			This function allows to simply test the assumption 
			that non-redundancy have to be favoured as the 
			size of history grows. Baseline results are obtained 
			on summaries generated by taking the leading sen- 
			tences of the most recent documents of the cluster, 
			up to 100 words (official baseline of DUC). The 
			table also lists the three top performing systems at 
			DUC 2007 and the lowest scored human reference. 
			As we can see from these results, SMMR out- 
			performs the other sentence scoring methods. By 
			ways of comparison our system would have been 
			ranked second at the DUC 2007 update competi- 
			tion. Moreover, no post-processing was applied to 
			the selected sentences leaving an important margin 
			of progress. Another interesting result is the high 
			performance of the non-update specific method 
			(cosine + JWe) that could be due to the small size 
			3ROUGE is available at http://haydn.isi.edu/ROUGE/. 
			25 
			of the corpus (little redundancy between clusters). 
			ROUGE-1 ROUGE-2 ROUGE-SU4 
			Baseline 0.26232 0.04543 0.08247 
			3rd system 0.35715 0.09622 0.13245 
			2nd system 0.36965 0.09851 0.13509 
			cosine + JWe 
			0.35905 0.10161 0.13701 
			NR 0.36207 0.10042 0.13781 
			SMMR 0.36323 0.10223 0.13886 
			1st system 0.37032 0.11189 0.14306 
			Worst human 0.40497 0.10511 0.14779 
			Table 1: ROUGE average recall scores computed 
			on the DUC 2007 update corpus. 
			2 Method 
			The underlying idea of our method is that as the 
			number of sentences in the history increases, the 
			likelihood to have redundant information within 
			candidate sentences also increases. We propose 
			a scalable sentence scoring method derived from 
			MMR that, as the size of the history increases, 
			gives more importance to non-redundancy that to 
			query relevance. We define H to represent the pre- 
			viously read documents (history), Q to represent 
			the query and s the candidate sentence. The fol- 
			lowing subsections formally define the similarity 
			measures and the scalable MMR scoring method. 
			2.1 A query-oriented multi-document 
			summarizer 
			We have first started by implementing a simple 
			summarizer for which the task is to produce query- 
			focused summaries from clusters of documents. 
			Each document is pre-processed: documents are 
			segmented into sentences, sentences are filtered 
			(words which do not carry meaning are removed 
			such as functional words or common words) and 
			normalized using a lemmas database (i.e. inflected 
			forms "go", "goes", "went", "gone"... are replaced 
			by "go"). An N-dimensional term-space , where 
			N is the number of different terms found in the 
			cluster, is constructed. Sentences are represented 
			in  by vectors in which each component is the 
			term frequency within the sentence. Sentence scor- 
			ing can be seen as a passage retrieval task in Infor- 
			mation Retrieval (IR). Each sentence s is scored by 
			computing a combination of two similarity mea- 
			sures between the sentence and the query. The first 
			measure is the well known cosine angle (Salton et 
			al., 1975) between the sentence and the query vec- 
			torial representations in  (denoted respectively s 
			and Q). The second similarity measure is based 
			on the Jaro-Winkler distance (Winkler, 1999). The 
			original Jaro-Winkler measure, denoted JW, uses 
			the number of matching characters and transposi- 
			tions to compute a similarity score between two 
			terms, giving more favourable ratings to terms that 
			match from the beginning. We have extended this 
			measure to calculate the similarity between the 
			sentence s and the query Q: 
			JWe(s, Q) = 
			1 
			|Q| 
			* 
			qQ 
			max 
			mS 
			JW(q, m) (1) 
			where S is the term set of s in which the terms 
			m that already have maximized JW(q, m) are re- 
			moved. The use of JWe smooths normalization and 
			misspelling errors. Each sentence s is scored using 
			the linear combination: 
			Sim1(s, Q) =  * cosine(s, Q) 
			+ (1 - ) * JWe(s, Q) (2) 
			where  = 0.7, optimally tuned on the past DUCs 
			data (2005 and 2006). The system produces a list 
			of ranked sentences from which the summary is 
			constructed by arranging the high scored sentences 
			until the desired size is reached. 
			2.2 A scalable MMR approach 
			MMR re-ranking algorithm has been successfully 
			used in query-oriented summarization (Ye et al., 
			2005). It strives to reduce redundancy while main- 
			taining query relevance in selected sentences. The 
			summary is constructed incrementally from a list 
			of ranked sentences, at each iteration the sentence 
			which maximizes MMR is chosen: 
			MMR = arg max 
			sS 
			[  * Sim1(s, Q) 
			- (1 - ) * max 
			sjE 
			Sim2(s, sj) ] (3) 
			where S is the set of candidates sentences and E 
			is the set of selected sentences.  represents an 
			interpolation coefficient between sentence's rele- 
			vance and non-redundancy. Sim2(s, sj) is a nor- 
			malized Longest Common Substring (LCS) mea- 
			sure between sentences s and sj. Detecting sen- 
			tence rehearsals, LCS is well adapted for redun- 
			dancy removal. 
			We propose an interpretation of MMR to tackle 
			the update summarization issue. Since Sim1 and 
			Sim2 are ranged in [0, 1], they can be seen as prob- 
			abilities even though they are not. Just as rewriting 
			(3) as (NR stands for Novelty Relevance): 
			NR = arg max 
			sS 
			[  * Sim1(s, Q) 
			+ (1 - ) * (1 - max 
			shH 
			Sim2(s, sh)) ] (4) 
			We can understand that (4) equates to an OR com- 
			bination. But as we are looking for a more intu- 
			itive AND and since the similarities are indepen- 
			dent, we have to use the product combination. The 
			24 
			scoring method defined in (2) is modified into a 
			double maximization criterion in which the best 
			ranked sentence will be the most relevant to the 
			query AND the most different to the sentences in 
			H. 
			SMMR(s) = Sim1(s, Q) 
			* 1 - max 
			shH 
			Sim2(s, sh) 
			f(H) 
			(5) 
			Decreasing  in (3) with the length of the sum- 
			mary was suggested by (Murray et al., 2005) and 
			successfully used in the DUC 2005 by (Hachey 
			et al., 2005), thereby emphasizing the relevance 
			at the outset but increasingly prioritizing redun- 
			dancy removal as the process continues. Sim- 
			ilarly, we propose to follow this assumption in 
			SMMR using a function denoted f that as the 
			amount of data in history increases, prioritize non- 
			redundancy (f(H)  0). 
			3 Experiments 
			The method described in the previous section has 
			been implemented and evaluated by using the 
			DUC 2007 update corpus2. The following subsec- 
			tions present details of the different experiments 
			we have conducted. 
			3.1 The DUC 2007 update corpus 
			We used for our experiments the DUC 2007 up- 
			date competition data set. The corpus is composed 
			of 10 topics, with 25 documents per topic. The up- 
			date task goal was to produce short (100 words) 
			multi-document update summaries of newswire ar- 
			ticles under the assumption that the user has al- 
			ready read a set of earlier articles. The purpose 
			of each update summary will be to inform the 
			reader of new information about a particular topic. 
			Given a DUC topic and its 3 document clusters: A 
			(10 documents), B (8 documents) and C (7 doc- 
			uments), the task is to create from the documents 
			three brief, fluent summaries that contribute to sat- 
			isfying the information need expressed in the topic 
			statement. 
			1. A summary of documents in cluster A. 
			2. An update summary of documents in B, un- 
			der the assumption that the reader has already 
			read documents in A. 
			2More information about the DUC 2007 corpus is avail- 
			able at http://duc.nist.gov/. 
			3. An update summary of documents in C, un- 
			der the assumption that the reader has already 
			read documents in A and B. 
			Within a topic, the document clusters must be pro- 
			cessed in chronological order. Our system gener- 
			ates a summary for each cluster by arranging the 
			high ranked sentences until the limit of 100 words 
			is reached. 
			3.2 Evaluation 
			Most existing automated evaluation methods work 
			by comparing the generated summaries to one or 
			more reference summaries (ideally, produced by 
			humans). To evaluate the quality of our generated 
			summaries, we choose to use the ROUGE3 (Lin, 
			2004) evaluation toolkit, that has been found to be 
			highly correlated with human judgments. ROUGE- 
			N is a n-gram recall measure calculated between 
			a candidate summary and a set of reference sum- 
			maries. In our experiments ROUGE-1, ROUGE-2 
			and ROUGE-SU4 will be computed. 
			3.3 Results 
			Table 1 reports the results obtained on the DUC 
			2007 update data set for different sentence scor- 
			ing methods. cosine + JWe stands for the scor- 
			ing method defined in (2) and NR improves it 
			with sentence re-ranking defined in equation (4). 
			SMMR is the combined adaptation we have pro- 
			posed in (5). The function f(H) used in SMMR is 
			the simple rational function 1 
			H 
			, where H increases 
			with the number of previous clusters (f(H) = 1 
			for cluster A, 1 
			2 
			for cluster B and 1 
			3 
			for cluster C). 
			This function allows to simply test the assumption 
			that non-redundancy have to be favoured as the 
			size of history grows. Baseline results are obtained 
			on summaries generated by taking the leading sen- 
			tences of the most recent documents of the cluster, 
			up to 100 words (official baseline of DUC). The 
			table also lists the three top performing systems at 
			DUC 2007 and the lowest scored human reference. 
			As we can see from these results, SMMR out- 
			performs the other sentence scoring methods. By 
			ways of comparison our system would have been 
			ranked second at the DUC 2007 update competi- 
			tion. Moreover, no post-processing was applied to 
			the selected sentences leaving an important margin 
			of progress. Another interesting result is the high 
			performance of the non-update specific method 
			(cosine + JWe) that could be due to the small size 
			3ROUGE is available at http://haydn.isi.edu/ROUGE/. 
			25 
			of the corpus (little redundancy between clusters). 
			ROUGE-1 ROUGE-2 ROUGE-SU4 
			Baseline 0.26232 0.04543 0.08247 
			3rd system 0.35715 0.09622 0.13245 
			2nd system 0.36965 0.09851 0.13509 
			cosine + JWe 
			0.35905 0.10161 0.13701 
			NR 0.36207 0.10042 0.13781 
			SMMR 0.36323 0.10223 0.13886 
			1st system 0.37032 0.11189 0.14306 
			Worst human 0.40497 0.10511 0.14779 
			Table 1: ROUGE average recall scores computed 
			on the DUC 2007 update corpus. 
			2 Single-Document Summarization 
			Usually, the flow of information in a given document is not uniform, which means 
			that some parts are more important than others. The major challenge in summa- 
			rization lies in distinguishing the more informative parts of a document from the 
			less ones. Though there have been instances of research describing the automatic 
			creation of abstracts, most work presented in the literature relies on verbatim ex- 
			traction of sentences to address the problem of single-document summarization. In 
			1See http://trec.nist.gov/. 
			2See http://duc.nist.gov/. 
			3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/. 
			muc 7 toc.html 
			2 
			this section, we describe some eminent extractive techniques. First, we look at early 
			work from the 1950s and 60s that kicked off research on summarization. Second, 
			we concentrate on approaches involving machine learning techniques published in 
			the 1990s to today. Finally, we briefly describe some techniques that use a more 
			complex natural language analysis to tackle the problem. 
			2.1 Early Work 
			Most early work on single-document summarization focused on technical documents. 
			Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de- 
			scribes research done at IBM in the 1950s. In his work, Luhn proposed that the 
			frequency of a particular word in an article provides an useful measure of its sig- 
			nificance. There are several key ideas put forward in this paper that have assumed 
			importance in later work on summarization. As a first step, words were stemmed to 
			their root forms, and stop words were deleted. Luhn then compiled a list of content 
			words sorted by decreasing frequency, the index providing a significance measure of 
			the word. On a sentence level, a significance factor was derived that reflects the 
			number of occurrences of significant words within a sentence, and the linear distance 
			between them due to the intervention of non-significant words. All sentences are 
			ranked in order of their significance factor, and the top ranking sentences are finally 
			selected to form the auto-abstract. 
			Related work (Baxendale, 1958), also done at IBM and published in the same 
			journal, provides early insight on a particular feature helpful in finding salient parts 
			of documents: the sentence position. Towards this goal, the author examined 200 
			paragraphs to find that in 85% of the paragraphs the topic sentence came as the first 
			one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate 
			way to select a topic sentence would be to choose one of these two. This positional 
			feature has since been used in many complex machine learning based systems. 
			Edmundson (1969) describes a system that produces document extracts. His 
			primary contribution was the development of a typical structure for an extractive 
			summarization experiment. At first, the author developed a protocol for creating 
			manual extracts, that was applied in a set of 400 technical documents. The two 
			features of word frequency and positional importance were incorporated from the 
			previous two works. Two other features were used: the presence of cue words 
			(presence of words like significant, or hardly), and the skeleton of the document 
			(whether the sentence is a title or heading). Weights were attached to each of these 
			features manually to score each sentence. During evaluation, it was found that about 
			44% of the auto-extracts matched the manual extracts. 
			2.2 Machine Learning Methods 
			In the 1990s, with the advent of machine learning techniques in NLP, a series of semi- 
			nal publications appeared that employed statistical techniques to produce document 
			extracts. While initially most systems assumed feature independence and relied on 
			naive-Bayes methods, others have focused on the choice of appropriate features and 
			3 
			on learning algorithms that make no independence assumptions. Other significant 
			approaches involved hidden Markov models and log-linear models to improve ex- 
			tractive summarization. A very recent paper, in contrast, used neural networks and 
			third party features (like common words in search engine queries) to improve purely 
			extractive single document summarization. We next describe all these approaches 
			in more detail. 
			2.2.1 Naive-Bayes Methods 
			Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able 
			to learn from data. The classification function categorizes each sentence as worthy 
			of extraction or not, using a naive-Bayes classifier. Let s be a particular sentence, 
			S the set of sentences that make up the summary, and F1, . . . , Fk the features. 
			Assuming independence of the features: 
			P(s  S | F1, F2, ..Fk) = 
			k 
			i=1 
			P(Fi | s  S) * P(s  S) 
			k 
			i=1 
			P(Fi) 
			(1) 
			The features were compliant to (Edmundson, 1969), but additionally included the 
			sentence length and the presence of uppercase words. Each sentence was given a 
			score according to (1), and only the n top sentences were extracted. To evaluate 
			the system, a corpus of technical documents with manual abstracts was used in 
			the following way: for each sentence in the manual abstract, the authors manually 
			analyzed its match with the actual document sentences and created a mapping 
			(e.g. exact match with a sentence, matching a join of two sentences, not matchable, 
			etc.). The auto-extracts were then evaluated against this mapping. Feature analysis 
			revealed that a system using only the position and the cue features, along with the 
			sentence length sentence feature, performed best. 
			Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richer 
			features. They describe a system called DimSum that made use of features like 
			term frequency (tf ) and inverse document frequency (idf) to derive signature words.4 
			The idf was computed from a large corpus of the same domain as the concerned 
			documents. Statistically derived two-noun word collocations were used as units for 
			counting, along with single words. A named-entity tagger was used and each entity 
			was considered as a single token. They also employed some shallow discourse analysis 
			like reference to same entities in the text, maintaining cohesion. The references 
			were resolved at a very shallow level by linking name aliases within a document 
			like "U.S." to "United States", or "IBM" for "International Business Machines". 
			Synonyms and morphological variants were also merged while considering lexical 
			terms, the former being identified by using Wordnet (Miller, 1995). The corpora 
			used in the experiments were from newswire, some of which belonged to the TREC 
			evaluations. 
			4Words that indicate key concepts in a document. 
			4 
			2.2.2 Rich Features and Decision Trees 
			Lin and Hovy (1997) studied the importance of a single feature, sentence position. 
			Just weighing a sentence by its position in text, which the authors term as the 
			"position method", arises from the idea that texts generally follow a predictable 
			discourse structure, and that the sentences of greater topic centrality tend to occur in 
			certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse 
			structure significantly varies over domains, the position method cannot be defined 
			as naively as in (Baxendale, 1958). The paper makes an important contribution by 
			investigating techniques of tailoring the position method towards optimality over a 
			genre and how it can be evaluated for effectiveness. A newswire corpus was used, the 
			collection of Ziff-Davis texts produced from the TIPSTER5 program; it consists of 
			text about computer and related hardware, accompanied by a set of key topic words 
			and a small abstract of six sentences. For each document in the corpus, the authors 
			measured the yield of each sentence position against the topic keywords. They then 
			ranked the sentence positions by their average yield to produce the Optimal Position 
			Policy (OPP) for topic positions for the genre. 
			Two kinds of evaluation were performed. Previously unseen text was used for 
			testing whether the same procedure would work in a different domain. The first 
			evaluation showed contours exactly like the training documents. In the second eval- 
			uation, word overlap of manual abstracts with the extracted sentences was measured. 
			Windows in abstracts were compared with windows on the selected sentences and 
			corresponding precision and recall values were measured. A high degree of coverage 
			indicated the effectiveness of the position method. 
			In later work, Lin (1999) broke away from the assumption that features are 
			independent of each other and tried to model the problem of sentence extraction 
			using decision trees, instead of a naive-Bayes classifier. He examined a lot of fea- 
			tures and their effect on sentence extraction. The data used in this work is a 
			publicly available collection of texts, classified into various topics, provided by the 
			TIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems. 
			The dataset contains essential text fragments (phrases, clauses, and sentences) which 
			must be included in summaries to answer some TREC topics. These fragments were 
			each evaluated by a human judge. The experiments described in the paper are with 
			the SUMMARIST system developed at the University of Southern California. The 
			system extracted sentences from the documents and those were matched against 
			human extracts, like most early work on extractive summarization. 
			Some novel features were the query signature (normalized score given to sen- 
			tences depending on number of query words that they contain), IR signature (the 
			m most salient words in the corpus, similar to the signature words of (Aone et al., 
			1999)), numerical data (boolean value 1 given to sentences that contained a num- 
			ber in them), proper name (boolean value 1 given to sentences that contained a 
			proper name in them), pronoun or adjective (boolean value 1 given to sentences 
			5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. 
			6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html. 
			5 
			that contained a pronoun or adjective in them), weekday or month (similar as pre- 
			vious feature) and quotation (similar as previous feature). It is worth noting that 
			some features like the query signature are question-oriented because of the setting 
			of the evaluation, unlike a generalized summarization framework. 
			The author experimented with various baselines, like using only the positional 
			feature, or using a simple combination of all features by adding their values. When 
			evaluated by matching machine extracted and human extracted sentences, the deci- 
			sion tree classifier was clearly the winner for the whole dataset, but for three topics, 
			a naive combination of features beat it. Lin conjectured that this happened because 
			some of the features were independent of each other. Feature analysis suggested 
			that the IR signature was a valuable feature, corroborating the early findings of 
			Luhn (1958). 
			2.2.3 Hidden Markov Models 
			In contrast with previous approaches, that were mostly feature-based and non- 
			sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentence 
			from a document using a hidden Markov model (HMM). The basic motivation for 
			using a sequential model is to account for local dependencies between sentences. 
			Only three features were used: position of the sentence in the document (built into 
			the state structure of the HMM), number of terms in the sentence, and likeliness of 
			the sentence terms given the document terms. 
			no 3 
			2 
			1 no 
			no 
			no 
			Figure 1: Markov model to extract to three summary sentences from a document 
			(Conroy and O'leary, 2001). 
			The HMM was structured as follows: it contained 2s + 1 states, alternating be- 
			tween s summary states and s+1 nonsummary states. The authors allowed "hesita- 
			tion" only in nonsummary states and "skipping next state" only in summary states. 
			Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the 
			TREC dataset as training corpus, the authors obtained the maximum-likelihood 
			estimate for each transition probability, forming the transition matrix estimate  
			M, 
			whose element (i, j) is the empirical probability of transitioning from state i to j. 
			Associated with each state i was an output function, bi(O) = Pr(O | state i) where 
			O is an observed vector of features. They made a simplifying assumption that the 
			features are multivariate normal. The output function for each state was thus esti- 
			mated by using the training data to compute the maximum likelihood estimate of 
			its mean and covariance matrix. They estimated 2s+1 means, but assumed that all 
			of the output functions shared a common covariance matrix. Evaluation was done 
			6 
			by comparing with human generated extracts. 
			2.2.4 Log-Linear Models 
			Osborne (2002) claims that existing approaches to summarization have always as- 
			sumed feature independence. The author used log-linear models to obviate this 
			assumption and showed empirically that the system produced better extracts than 
			a naive-Bayes model, with a prior appended to both models. Let c be a label, s 
			the item we are interested in labeling, fi the i-th feature, and i the corresponding 
			feature weight. The conditional log-linear model used by Osborne (2002) can be 
			stated as follows: 
			P(c | s) = 
			1 
			Z(s) 
			exp 
			i 
			ifi(c, s) , (2) 
			where Z(s) = c 
			exp ( i 
			ifi(c, s)). In this domain, there are only two possible 
			labels: either the sentence is to be extracted or it is not. The weights were trained 
			by conjugate gradient descent. The authors added a non-uniform prior to the model, 
			claiming that a log-linear model tends to reject too many sentences for inclusion in 
			a summary. The same prior was also added to a naive-Bayes model for comparison. 
			The classification took place as follows: 
			label(s) = arg max 
			cC 
			P(c) * P(s, c) = arg max 
			cC 
			log P(c) + 
			i 
			ifi(c, s) . (3) 
			The authors optimized the prior using the f2 score of the classifier as an objective 
			function on a part of the dataset (in the technical domain). The summaries were 
			evaluated using the standard f2 score where f2 = 2pr 
			p+r 
			, where the precision and recall 
			measures were measured against human generated extracts. The features included 
			word pairs (pairs of words with all words truncated to ten characters), sentence 
			length, sentence position, and naive discourse features like inside introduction or 
			inside conclusion. With respect to f2 score, the log-linear model outperformed the 
			naive-Bayes classifier with the prior, exhibiting the former's effectiveness. 
			2.2.5 Neural Networks and Third Party Features 
			In 2001-02, DUC issued a task of creating a 100-word summary of a single news 
			article. However, the best performing systems in the evaluations could not outper- 
			form the baseline with statistical significance. This extremely strong baseline has 
			been analyzed by Nenkova (2005) and corresponds to the selection of the first n 
			sentences of a newswire article. This surprising result has been attributed to the 
			journalistic convention of putting the most important part of an article in the initial 
			paragraphs. After 2002, the task of single-document summarization for newswire 
			was dropped from DUC. Svore et al. (2007) propose an algorithm based on neu- 
			ral nets and the use of third party datasets to tackle the problem of extractive 
			summarization, outperforming the baseline with statistical significance. 
			7 
			The authors used a dataset containing 1365 documents gathered from CNN.com, 
			each consisting of the title, timestamp, three or four human generated story high- 
			lights and the article text. They considered the task of creating three machine 
			highlights. The human generated highlights were not verbatim extractions from the 
			article itself. The authors evaluated their system using two metrics: the first one 
			concatenated the three highlights produced by the system, concatenated the three 
			human generated highlights, and compared these two blocks; the second metric con- 
			sidered the ordering and compared the sentences on an individual level. 
			Svore et al. (2007) trained a model from the labels and the features for each 
			sentence of an article, that could infer the proper ranking of sentences in a test 
			document. The ranking was accomplished using RankNet (Burges et al., 2005), a 
			pair-based neural network algorithm designed to rank a set of inputs that uses the 
			gradient descent method for training. For the training set, they used ROUGE-1 
			(Lin, 2004) to score the similarity of a human written highlight and a sentence 
			in the document. These similarity scores were used as soft labels during training, 
			contrasting with other approaches where sentences are "hard-labeled", as selected 
			or not. 
			Some of the used features based on position or n-grams frequencies have been 
			observed in previous work. However, the novelty of the framework lay in the use 
			of features that derived information from query logs from Microsoft's news search 
			engine7 and Wikipedia8 entries. The authors conjecture that if a document sentence 
			contained keywords used in the news search engine, or entities found in Wikipedia 
			articles, then there is a greater chance of having that sentence in the highlight. The 
			extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically 
			significant improvements over the baseline of selecting the first three sentences in a 
			document. 
			2.3 Deep Natural Language Analysis Methods 
			In this subsection, we describe a set of papers that detail approaches towards single- 
			document summarization involving complex natural language analysis techniques. 
			None of these papers solve the problem using machine learning, but rather use a set 
			of heuristics to create document extracts. Most of these techniques try to model the 
			text's discourse structure. 
			Barzilay and Elhadad (1997) describe a work that used considerable amount of 
			linguistic analysis for performing the task of summarization. For a better under- 
			standing of their method, we need to define a lexical chain: it is a sequence of related 
			words in a text, spanning short (adjacent words or sentences) or long distances (en- 
			tire text). The authors' method progressed with the following steps: segmentation 
			of the text, identification of lexical chains, and using strong lexical chains to identify 
			the sentences worthy of extraction. They tried to reach a middle ground between 
			(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep 
			7See http://search.live.com/news. 
			8See http://en.wikipedia.org. 
			8 
			semantic structure of the text, while the latter relied on word statistics of the doc- 
			uments. The authors describe the notion of cohesion in text as a means of sticking 
			together different parts of the text. Lexical cohesion is a notable example where 
			semantically related words are used. For example, let us take a look at the following 
			sentence.9 
			John bought a Jag. He loves the car. (4) 
			Here, the word car refers to the word Jag in the previous sentence, and exemplifies 
			lexical cohesion. The phenomenon of cohesion occurs not only at the word level, 
			but at word sequences too, resulting in lexical chains, which the authors used as 
			a source representation for summarization. Semantically related words and word 
			sequences were identified in the document, and several chains were extracted, that 
			form a representation of the document. To find out lexical chains, the authors used 
			Wordnet (Miller, 1995), applying three generic steps: 
			1. Selecting a set of candidate words. 
			2. For each candidate word, finding an appropriate chain relying on a relatedness 
			criterion among members of the chains, 
			3. If it is found, inserting the word in the chain and updating it accordingly. 
			The relatedness was measured in terms of Wordnet distance. Simple nouns and 
			noun compounds were used as starting point to find the set of candidates. In the 
			final steps, strong lexical chains were used to create the summaries. The chains were 
			scored by their length and homogeneity. Then the authors used a few heuristics to 
			select the significant sentences. 
			In another paper, Ono et al. (1994) put forward a computational model of dis- 
			course for Japanese expository writings, where they elaborate a practical procedure 
			for extracting the discourse rhetorical structure, a binary tree representing relations 
			between chunks of sentences (rhetorical structure trees are used more intensively in 
			(Marcu, 1998a), as we will see below). This structure was extracted using a series 
			of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can- 
			didate generation and preference judgement. Evaluation was based on the relative 
			importance of rhetorical relations. In the following step, the nodes of the rhetori- 
			cal structure tree were pruned to reduce the sentence, keeping its important parts. 
			Same was done for paragraphs to finally produce the summary. Evaluation was done 
			with respect to sentence coverage and 30 editorial articles of a Japanese newspaper 
			were used as the dataset. The articles had corresponding sets of key sentences and 
			most important key sentences judged by human subjects. The key sentence coverage 
			was about 51% and the most important key sentence coverage was 74%, indicating 
			encouraging results. 
			Marcu (1998a) describes a unique approach towards summarization that, unlike 
			most other previous work, does not assume that the sentences in a document form 
			a flat sequence. This paper used discourse based heuristics with the traditional 
			9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html. 
			9 
			features that have been used in the summarization literature. The discourse theory 
			used in this paper is the Rhetorical Structure Theory (RST) that holds between 
			two non-overlapping pieces of text spans: the nucleus and the satellite. The author 
			mentions that the distinction between nuclei and satellites comes from the empir- 
			ical observation that the nucleus expresses what is more essential to the writer's 
			purpose than the satellite; and that the nucleus of a rhetorical relation is compre- 
			hensible independent of the satellite, but not vice versa. Marcu (1998b) describes 
			the details of a rhetorical parser producing a discourse tree. Figure 2 shows an 
			example discourse tree for a text example detailed in the paper. Once such a dis- 
			Antithesis 
			2 
			Elaboration 
			Elaboration 
			2 
			2 
			Elaboration 
			3 
			Justification 
			8 
			Exemplification 
			1 2 3 4 5 7 8 
			4 5 
			8 10 
			9 10 
			5 6 
			Contrast 
			Evidence 
			Concession 
			Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the 
			nodes denote sentence numbers from the text example. The text below the number 
			in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and 
			the normals ones are the NUCLEI. 
			course structure is created, a partial ordering of important units can be developed 
			from the tree. Each equivalence class in the partial ordering is derived from the 
			new sentences at a particular level of the discourse tree. In Figure 2, we observe 
			that sentence 2 is at the root, followed by sentence 8 in the second level. In the 
			third level, sentence 3 and 10 are observed, and so forth. The equivalence classes 
			are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6. 
			If it is specified that the summary should contain the top k% of the text, the first 
			k% of the units in the partial ordering can be selected to produce the summary. The 
			author talks about a summarization system based just on this method in (Marcu, 
			1998b) and in one of his earlier papers. In this paper, he merged the discourse 
			based heuristics with traditional heuristics. The metrics used were clustering based 
			10 
			metric (each node in the discourse tree was assigned a cluster score; for leaves the 
			score was 0, for the internal nodes it was given by the similarity of the immediate 
			children; discourse tree A was chosen to be better than B if its clustering score 
			was higher), marker based metric (a discourse structure A was chosen to be better 
			than a discourse structure B if A used more rhetorical relations than B), rhetorical 
			clustering based technique (measured the similarity between salient units of two text 
			spans), shape based metric (preferred a discourse tree A over B if A was more skewed 
			towards the right than B), title based metric, position based metric, connectedness 
			based metric (cosine similarity of an unit to all other text units, a discourse structure 
			A was chosen to be better than B if its connectedness measure was more than B). 
			A weighted linear combination of all these scores gave the score of a discourse 
			structure. To find the best combination of heuristics, the author computed the 
			weights that maximized the F-score on the training dataset, which was constituted 
			by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al., 
			1992) that performed a greedy search in a seven dimensional space of the metrics. 
			For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved 
			for the 10% summaries which was 3.5% higher than a baseline lead based algorithm, 
			which was very encouraging. 
			3 Multi-Document Summarization 
			Extraction of a single summary from multiple documents has gained interest since 
			mid 1990s, most applications being in the domain of news articles. Several Web- 
			based news clustering systems were inspired by research on multi-document summa- 
			rization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12 
			This departs from single-document summarization since the problem involves mul- 
			tiple sources of information that overlap and supplement each other, being contra- 
			dictory at occasions. So the key tasks are not only identifying and coping with 
			redundancy across documents, but also recognizing novelty and ensuring that the 
			final summary is both coherent and complete. 
			The field seems to have been pioneered by the NLP group at Columbia University 
			(McKeown and Radev, 1995), where a summarization system called SUMMONS13 
			was developed by extending already existing technology for template-driven message 
			understanding systems. Although in that early stage multi-document summariza- 
			tion was mainly seen as a task requiring substantial capabilities of both language 
			interpretation and generation, it later gained autonomy, as people coming from dif- 
			ferent communities added new perspectives to the problem. Extractive techniques 
			have been applied, making use of similarity measures between pairs of sentences. 
			Approaches vary on how these similarities are used: some identify common themes 
			through clustering and then select one sentence to represent each cluster (McKeown 
			10See http://news.google.com. 
			11See http://newsblaster.cs.columbia.edu. 
			12See http://NewsInEssence.com. 
			13SUMMarizing Online NewS articles. 
			11 
			et al., 1999; Radev et al., 2000), others generate a composite sentence from each 
			cluster (Barzilay et al., 1999), while some approaches work dynamically by includ- 
			ing each candidate passage only if it is considered novel with respect to the previous 
			included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998). 
			Some recent work extends multi-document summarization to multilingual environ- 
			ments (Evans, 2005). 
			The way the problem is posed has also varied over time. While in some pub- 
			lications it is claimed that extractive techniques would not be effective for multi- 
			document summarization (McKeown and Radev, 1995; McKeown et al., 1999), some 
			years later that claim was overturned, as extractive systems like MEAD14 (Radev 
			et al., 2000) achieved good performance in large scale summarization of news arti- 
			cles. This can be explained by the fact that summarization systems often distinguish 
			among themselves about what their goal actually is. While some systems, like SUM- 
			MONS, are designed to work in strict domains, aiming to build a sort of briefing 
			that highlights differences and updates accross different news reports, putting much 
			emphasis on how information is presented to the user, others, like MEAD, are large 
			scale systems that intend to work in general domains, being more concerned with 
			information content rather than form. Consequently, systems of the former kind re- 
			quire a strong effort on language generation to produce a grammatical and coherent 
			summary, while latter systems are probably more close to the information retrieval 
			paradigm. Abstractive systems like SUMMONS are difficult to replicate, as they 
			heavily rely on the adaptation of internal tools to perform information extraction 
			and language generation. On the other hand, extractive systems are generally easy 
			to implement from scratch, and this makes them appealing when sophisticated NLP 
			tools are not available. 
			3.1 Abstraction and Information Fusion 
			As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown, 
			1998) is the first historical example of a multi-document summarization system. It 
			tackles single events about a narrow domain (news articles about terrorism) and 
			produces a briefing merging relevant information about each event and how reports 
			by different news agencies have evolved over time. The whole thread of reports is 
			then presented, as illustrated in the following example of a "good" summary: 
			"In the afternoon of February 26, 1993, Reuters reported that a suspect 
			bomb killed at least five people in the World Trade Center. However, 
			Associated Press announced that exactly five people were killed in the 
			blast. Finally, Associated Press announced that Arab terrorists were 
			possibly responsible for the terrorist act." 
			Rather than working with raw text, SUMMONS reads a database previously 
			built by a template-based message understanding system. A full multi-document 
			14Available for download at http://www.summarization.com/mead/. 
			12 
			summarizer is built by concatenating the two systems, first processing full text as 
			input and filling template slots, and then synthesizing a summary from the extracted 
			information. The architecture of SUMMONS consists of two major components: a 
			content planner that selects the information to include in the summary through 
			combination of the input templates, and a linguistic generator that selects the right 
			words to express the information in grammatical and coherent text. The latter 
			component was devised by adapting existing language generation tools, namely the 
			FUF/SURGE system15. Content planning, on the other hand, is made through 
			summary operators, a set of heuristic rules that perform operations like "change of 
			perspective", "contradiction", "refinement", etc. Some of these operations require 
			resolving conflicts, i.e., contradictory information among different sources or time 
			instants; others complete pieces of information that are included in some articles 
			and not in others, combining them into a single template. At the end, the linguis- 
			tic generator gathers all the combined information and uses connective phrases to 
			synthesize a summary. 
			While this framework seems promising when the domain is narrow enough so that 
			the templates can be designed by hand, a generalization for broader domains would 
			be problematic. This was improved later by McKeown et al. (1999) and Barzilay 
			et al. (1999), where the input is now a set of related documents in raw text, like 
			those retrieved by a standard search engine in response to a query. The system starts 
			by identifying themes, i.e., sets of similar text units (usually paragraphs). This is 
			formulated as a clustering problem. To compute a similarity measure between text 
			units, these are mapped to vectors of features, that include single words weighted 
			by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet 
			database and a database of semantic classes of verbs. For each pair of paragraphs, a 
			vector is computed that represents matches on the different features. Decision rules 
			that were learned from data are then used to classify each pair of text units either 
			as similar or dissimilar; this in turn feeds a subsequent algorithm that places the 
			most related paragraphs in the same theme. 
			Once themes are identified, the system enters its second stage: information fu- 
			sion. The goal is to decide which sentences of a theme should be included in the 
			summary. Rather than just picking a sentence that is a group representative, the 
			authors propose an algorithm which compares and intersects predicate argument 
			structures of the phrases within each theme to determine which are repeated often 
			enough to be included in the summary. This is done as follows: first, sentences are 
			parsed through Collins' statistical parser (Collins, 1999) and converted into depen- 
			dency trees, which allows capturing the predicate-argument structure and identify 
			functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence 
			representation. 
			The comparison algorithm then traverses these dependency trees recursively, 
			adding identical nodes to the output tree. Once full phrases (a verb with at least 
			two constituents) are found, they are marked to be included in the summary. If two 
			15FUF, SURGE, and other tools developed by the Columbia NLP group are available at 
			http://www1.cs.columbia.edu/nlp/tools.cgi. 
			13 
			and Kan 1998]. We match two verbs that share the same 
			semantic class in this classifi cation. 
			In addition to the above primitive features that all com- 
			pare single items from each text unit, we use composite fea- 
			tures that combine pairs of primitive features. Our compos- 
			ite features impose particular constraints on the order of the 
			two elements in the pair, on the maximum distance between 
			the two elements, and on the syntactic classes that the two 
			elements come from. They can vary from a simple com- 
			bination (e.g., "two text units must share two words to be 
			similar") to complex cases with many conditions (e.g., "two 
			text units must have matching noun phrases that appear in 
			the same order and with relative difference in position no 
			more than fi ve"). In this manner, we capture information 
			on how similarly related elements are spaced out in the two 
			text units, as well as syntactic information on word combi- 
			nations. Matches on composite features indicate combined 
			evidence for the similarity of the two units. 
			To determine whether the units match overall, we employ 
			a machine learning algorithm [Cohen 1996] that induces de- 
			cision rules using the features that really make a difference. 
			A set of pairs of units already marked as similar or not by a 
			human is used for training the classifi er. We have manually 
			marked a set of 8,225 paragraph comparisons from the TDT 
			corpus for training and evaluating our similarity classifi er. 
			For comparison, we also use an implementation of the 
			TF*IDF method which is standard for matching texts in in- 
			formation retrieval. We compute the total frequency (TF) of 
			words in each text unit and the number of units in our train- 
			ing set each word appears in (DF, or document frequency). 
			Then each text unit is represented as a vector of TF*IDF 
			scores, calculated as 
			TF(wordi 
			) * log Total number of units 
			DF(wordi 
			) 
			Similarity between text units is measured by the cosine of 
			the angle between the corresponding two vectors (i.e., the 
			normalized inner product of the two vectors), and the opti- 
			mal value of a threshold for judging two units as similar is 
			computed from the training set. 
			After all pairwise similarities between text units have 
			been calculated, we utilize a clustering algorithm to iden- 
			tify themes. As a paragraph may belong to multiple themes, 
			most standard clustering algorithms, which partition their 
			input set, are not suitable for our task. We use a greedy, 
			one-pass algorithm that fi rst constructs groups from the most 
			similar paragraphs, seeding the groups with the fully con- 
			nected subcomponents of the graph that the similarity rela- 
			tionship induces over the set of paragraphs, and then places 
			additional paragraphs within a group if the fraction of the 
			members of the group they are similar to exceeds a preset 
			threshold. 
			Language Generation 
			Given a group of similar paragraphs--a theme--the prob- 
			lem is to create a concise and fluent fusion of information in 
			this theme, reflecting facts common to all paragraphs. A 
			straightforward method would be to pick a representative 
			subject 
			class: noun 
			27 
			class: cardinal 
			bombing 
			class: noun 
			McVeigh with 
			class: preposition 
			definite: yes 
			charge 
			class: verb voice :passive 
			polarity: + 
			tense: past 
			Figure 4: Dependency grammar representation of the sen- 
			tence "McVeigh, 27, was charged with the bombing". 
			sentence that meets some criteria (e.g., a threshold number 
			of common content words). In practice, however, any repre- 
			sentative sentence will usually include embedded phrase(s) 
			containing information that is not common to all sentences 
			in the theme. Furthermore, other sentences in the theme of- 
			ten contain additional information not presented in the rep- 
			resentative sentence. Our approach, therefore, uses inter- 
			section among theme sentences to identify phrases common 
			to most paragraphs and then generates a new sentence from 
			identifi ed phrases. 
			Intersection among Theme Sentences 
			Intersection is carried out in the content planner, which uses 
			a parser for interpreting the input sentences, with our new 
			work focusing on the comparison of phrases. Theme sen- 
			tences are fi rst run through a statistical parser[Collins 1996] 
			and then, in order to identify functional roles (e.g., subject, 
			object), are converted to a dependency grammar representa- 
			tion [Kittredge and Mel' 
			cuk 1983], which makes predicate- 
			argument structure explicit. 
			We developed a rule-based component to produce func- 
			tional roles, which transforms the phrase-structure output of 
			Collins' parser to dependency grammar; function words (de- 
			terminers and auxiliaries) are eliminated from the tree and 
			corresponding syntactic features are updated. An example 
			of a theme sentence and its dependency grammar represen- 
			tation are shown in Figure 4. Each non-auxiliary word in the 
			sentence has a node in the representation, and this node is 
			connected to its direct dependents. 
			The comparison algorithm starts with all subtrees rooted 
			at verbs from the input dependency structure, and traverses 
			them recursively: if two nodes are identical, they are added 
			to the output tree, and their children are compared. Once 
			a full phrase (verb with at least two constituents) has been 
			found, it is confi rmed for inclusion in the summary. 
			Diffi culties arise when two nodes are not identical, but are 
			similar. Such phrases may be paraphrases of each other and 
			still convey essentially the same information. Since theme 
			sentences are a priori close semantically, this signifi cantly 
			Figure 3: Dependency tree representing the sentence "McVeigh, 27, was charged 
			with the bombing" (extracted from (McKeown et al., 1999)). 
			phrases, rooted at some node, are not identical but yet similar, the hypothesis that 
			they are paraphrases of each other is considered; to take this into account, corpus- 
			driven paraphrasing rules are written to allow paraphrase intersection.16 Once the 
			summary content (represented as predicate-argument structures) is decided, a gram- 
			matical text is generated by translating those structures into the arguments expected 
			by the FUF/SURGE language generation system. 
			3.2 Topic-driven Summarization and MMR 
			Carbonell and Goldstein (1998) made a major contribution to topic-driven sum- 
			marization by introducing the maximal marginal relevance (MMR) measure. The 
			idea is to combine query relevance with information novelty; it may be applicable 
			in several tasks ranging from text retrieval to topic-driven summarization. MMR 
			simultaneously rewards relevant sentences and penalizes redundant ones by consid- 
			ering a linear combination of two similarity measures. 
			Let Q be a query or user profile and R a ranked list of documents retrieved by 
			a search engine. Consider an incremental procedure that selects documents, one at 
			a time, and adds them to a set S. So let S be the set of already selected documents 
			in a particular step, and R \ S the set of yet unselected documents in R. For each 
			candidate document Di  R \ S, its marginal relevance MR(Di) is computed as: 
			MR(Di) := Sim1(Di, Q) - (1 - ) max 
			DjS 
			Sim2(Di, Dj) (5) 
			where  is a parameter lying in [0, 1] that controls the relative importance given 
			to relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the 
			16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al., 
			1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization 
			in different syntactic categories (e.g. classifier vs. apposition), change in grammatical features 
			(active/passive, time, number, etc.), head omission, transformation from one POS to another, 
			using semantically related words (e.g. synonyms), etc. 
			14 
			experiments both were set to the standard cosine similarity traditionally used in the 
			vector space model, Sim1(x, y) = Sim2(x, y) = x,y 
			x * y 
			. The document achieving the 
			highest marginal relevance, DMMR = arg maxDiR\S 
			MR(Di), is then selected, i.e., 
			added to S, and the procedure continues until a maximum number of documents 
			are selected or a minimum relevance threshold is attained. Carbonell and Goldstein 
			(1998) found experimentally that choosing dynamically the value of  turns out to be 
			more effective than keeping it fixed, namely starting with small values (  0.3) to 
			give more emphasis to novelty, and then increasing it (  0.7) to focus on the most 
			relevant documents. To perform summarization, documents can be first segmented 
			into sentences or paragraphs, and after a query is submitted, the MMR algorithm 
			can be applied followed by a selection of the top ranking passages, reordering them as 
			they appeared in the original documents, and presenting the result as the summary. 
			One of the attractive points in using MMR for summarization is its topic-oriented 
			feature, through its dependency on the query Q, which makes it particularly ap- 
			pealing to generate summaries according to a user profile: as the authors claim, "a 
			different user with different information needs may require a totally different sum- 
			mary of the same document." This assertion was not being taken into account by 
			previous multi-document summarization systems. 
			3.3 Graph Spreading Activation 
			Mani and Bloedorn (1997) describe an information extraction framework for sum- 
			marization, a graph-based method to find similarities and dissimilarities in pairs 
			of documents. Albeit no textual summary is generated, the summary content is 
			represented via entities (concepts) and relations that are displayed respectively as 
			nodes and edges of a graph. Rather than extracting sentences, they detect salient 
			regions of the graph via a spreading activation technique.17 
			This approach shares with the method described in Section 3.2 the property 
			of being topic-driven; there is an additional input that stands for the topic with 
			respect to which the summary is to be generated. The topic is represented through 
			a set of entry nodes in the graph. A document is represented as a graph as follows: 
			each node represents the occurrence of a single word (i.e., one word together with 
			its position in the text). Each node can have several kinds of links: adjacency 
			links (ADJ) to adjacent words in the text, SAME links to other occurrences of the 
			same word, and ALPHA links encoding semantic relationships captured through 
			Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of 
			adjacent nodes which belong to the same phrase, and NAME and COREF links 
			stand for co-referential name occurrences; Fig. 4 shows some of these links. 
			Once the graph is built, topic nodes are identified by stem comparison and be- 
			come the entry nodes. A search for semantically related text is then propagated from 
			these to the other nodes of the graph, in a process called spreading activation. Salient 
			17The name "spreading activation" is borrowed from a method used in information retrieval 
			(Salton and Buckley, 1988) to expand the search vocabulary. 
			18See http://www.netowl.com. 
			15 
			1.39: Aoki, the Japanese ambassador, said in telephone calls to 
			Fujimori. 
			Japanesebroadcaster NHK that the rebels wanted to talk directly to 
			1.43:According to some estimates, only a couple hundred armed 
			followers remain. 
			2.19 They are freeing u 
			not doing us any harm," 
			... 
			2.27:Although the MRTA 
			early days in the mid-198 
			give to the poor, it lost pu 
			turning increasingly to ki 
			billion in damage to the c 
			since 1980. 
			and drug activities. 2.28: 
			Peru have cost at least 3 
			... 
			close ties with Japan. 
			1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and 
			the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea, 
			... 
			... 
			... 
			2.26:The MRTA called T 
			"Breaking The Silence." 
			1.32: President Alberto Fujimori, who is of Japanese ancestry, has had 
			Germany, Austria and Venezuela. Hood-style movement tha 
			negotiations with the gov 
			dawn on Wednesday. 
			... 
			... 
			2.22:The attack was a ma 
			Fujimori's government, w 
			virtual victory in a 16-yea 
			rebels belonging to the M 
			and better-known Maoist 
			... 
			1.28:Many leaders of the Tupac Amaru which is smaller than Peru's 
			was captured in June 1992 and is serving a life sentence, as is his 
			us: `Don't lift your heads up or you will be shot." 
			1.19: 
			hostages," a rebel who did not give his name told a local radio station in 
			a telephone call from inside the compound. 
			"The guerillas stalked around the residence grounds threatening 
			lieutenant, Peter Cardenas. 
			1.25: "We are clear: the liberation of all our comrades, or we die with all the 
			1.30:Other top commanders conceded defeat July 1993. 
			and surrendered in 
			COREF 
			Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay, 
			ADJ 
			1.38:Fujimori whose sister was among the 
			an emergency cabinet meeting today. 
			hostages released, called 
			ALPHA 
			ADJ 
			, the rebels threatened to kill the remaining 
			captives. 
			1.24:Early Wednesday 
			Figure 5: Texts of two related articles. The top 5 salient sentences containing common 
			words in bold face; likewise, the top 5 salient sentences containing unique words have th 
			Figure 4: Examples of nodes and links in the graph for a particular sentence (detail 
			extracted from from a figure in (Mani and Bloedorn, 1997)). 
			words and phrases are initialized according to their TF-IDF score. The weight of 
			neighboring nodes depends on the node link traveled and is an exponentially decay- 
			ing function of the distance of the traversed path. Traveling within a sentence is 
			made cheaper than across sentence boundaries, which in turn is cheaper than across 
			paragraph boundaries. Given a pair of document graphs, common nodes are identi- 
			fied either by sharing the same stem or by being synonyms. Analogously, difference 
			nodes are those that are not common. For each sentence in both documents, two 
			scores are computed: one score that reflects the presence of common nodes, which 
			is computed as the average weight of these nodes; and another score that computes 
			instead the average weights of difference nodes. Both scores are computed after 
			spreading activation. In the end, the sentences that have higher common and dif- 
			ferent scores are highlighted, the user being able to specify the maximal number of 
			common and different sentences to control the output. In the future, the authors 
			expect to use these structure to actually compose abstractive summaries, rather 
			than just highlighting pieces of text. 
			3.4 Centroid-based Summarization 
			Although clustering techniques were already being employed by McKeown et al. 
			(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000) 
			pioneered the use of cluster centroids to play a central role in summarization. A full 
			description of the centroid-based approach that underlies the MEAD system can 
			be found in (Radev et al., 2004); here we sketch briefly the main points. Perhaps 
			the most appealing feature is the fact that it does not make use of any language 
			generation module, unlike most previous systems. All documents are modeled as 
			bags-of-words. The system is also easily scalable and domain-independent. 
			The first stage consists of topic detection, whose goal is to group together news 
			articles that describe the same event. To accomplish this task, an agglomerative 
			clustering algorithm is used that operates over the TF-IDF vector representations 
			of the documents, successively adding documents to clusters and recomputing the 
			16 
			centroids according to 
			cj = dCj 
			 
			d 
			|Cj| 
			(6) 
			where cj is the centroid of the j-th cluster, Cj is the set of documents that belong 
			to that cluster, its cardinality being |Cj|, and  
			d is a "truncated version" of d that 
			vanishes on those words whose TF-IDF scores are below a threshold. Centroids 
			can thus be regarded as pseudo-documents that include those words whose TF- 
			IDF scores are above a threshold in the documents that constitute the cluster. Each 
			event cluster is a collection of (typically 2 to 10) news articles from multiple sources, 
			chronologically ordered, describing an event as it develops over time. 
			The second stage uses the centroids to identify sentences in each cluster that 
			are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics 
			are defined that resemble the two summands in the MMR (see Section 3.2): cluster- 
			based relative utility (CBRU) and cross-sentence informational subsumption (CSIS). 
			The first accounts for how relevant a particular sentence is to the general topic of 
			the entire cluster; the second is a measure of redundancy among sentences. Unlike 
			MMR, these metrics are not query-dependent. Given one cluster C of documents 
			segmented into n sentences, and a compression rate R, a sequence of nR sentences 
			are extracted in the same order as they appear in the original documents, which in 
			turn are ordered chronologically. The selection of the sentences is made by approx- 
			imating their CBRU and CSIS.19 For each sentence si, three different features are 
			used: 
			* Its centroid value (Ci), defined as the sum of the centroid values of all the 
			words in the sentence, 
			* A positional value (Pi), that is used to make leading sentences more important. 
			Let Cmax be the centroid value of the highest ranked sentence in the document. 
			Then Pi = n-i+1 
			n 
			Cmax. 
			* The first-sentence overlap (Fi), defined as the inner product between the word 
			occurrence vector of sentence i and that of the first sentence of the document. 
			The final score of each sentence is a combination of the three scores above minus a 
			redundancy penalty (Rs) for each sentence that overlaps highly ranked sentences. 
			3.5 Multilingual Multi-document Summarization 
			Evans (2005) addresses the task of summarizing documents written in multiple 
			languages; this had already been sketched by Hovy and Lin (1999). Multilingual 
			summarization is still at an early stage, but this framework looks quite useful for 
			newswire applications that need to combine information from foreign news agen- 
			cies. Evans (2005) considered the scenario where there is a preferred language in 
			which the summary is to be written, and multiple documents in the preferred and 
			19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details). 
			17 
			in foreign languages are available. In their experiments, the preferred language was 
			English and the documents are news articles in English and Arabic. The rationale is 
			to summarize the English articles without discarding the information contained in 
			the Arabic documents. The IBM's statistical machine translation system is first ap- 
			plied to translate the Arabic documents to English. Then a search is made, for each 
			translated text unit, to see whether there is a similar sentence or not in the English 
			documents. If so, and if the sentence is found relevant enough to be included in the 
			summary, the similar English sentence is included instead of the Arabic-to-English 
			translation. This way, the final summary is more likely to be grammatical, since 
			machine translation is known to be far from perfect. On the other hand, the result 
			is also expected to have higher coverage than using just the English documents, 
			since the information contained in the Arabic documents can help to decide about 
			the relevance of each sentence. In order to measure similarity between sentences, a 
			tool named SimFinder20 was employed: this is a tool for clustering text based on 
			similarity over a variety of lexical and syntactic features using a log-linear regression 
			model. 
			4 Other Approaches to Summarization 
			This section describes briefly some unconventional approaches that, rather than 
			aiming to build full summarization systems, investigate some details that underlie 
			the summarization process, and that we conjecture to have a role to play in future 
			research on this field. 
			4.1 Short Summaries 
			Witbrock and Mittal (1999) claim that extractive summarization is not very pow- 
			erful in that the extracts are not concise enough when very short summaries are 
			required. They present a system that generated headline style summaries. The cor- 
			pus used in this work was newswire articles from Reuters and the Associated Press, 
			publicly available at the LDC21. The system learned statistical models of the rela- 
			tionship between source text units and headline units. It attempted to model both 
			the order and the likelihood of the appearance of tokens in the target documents. 
			Both the models, one for content selection and the other for surface realization were 
			used to co-constrain each other during the search in the summary generation task. 
			For content selection, the model learned a translation model between a docu- 
			ment and its summary (Brown et al., 1993). This model in the simplest case can be 
			thought as a mapping between a word in the document and the likelihood of some 
			word appearing in the summary. To simplify the model, the authors assumed that 
			the probability of a word appearing in a summary is independent of its structure. 
			This mapping boils down to the fact that the probability of a particular summary 
			20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder. 
			21See http://ldc.upenn.edu. 
			18 
			candidate is the product of the probabilities of the summary content and that con- 
			tent being expressed using a particular structure. 
			The surface realization model used was a bigram model. Viterbi beam search 
			was used to efficiently find a near-optimal summary. The Markov assumption was 
			violated by using backtracking at every state to strongly discourage paths that 
			repeated terms, since bigrams that start repeating often seem to pathologically 
			overwhelm the search otherwise. To evaluate the system, the authors compared 
			its output against the actual headlines for a set of input newswire stories. Since 
			phrasing could not be compared, they compared the generated headlines against 
			the actual headlines, as well as the top ranked summary sentence of the story. Since 
			the system did not have a mechanism to determine the optimal length of a headline, 
			six headlines for each story were generated, ranging in length from 4 to 10 words 
			and they measured the term-overlap between each of the generated headlines and 
			the test. For headline length 4, there was 0.89 overlap in the headline and there was 
			0.91 overlap amongst the top scored sentence, indicating useful results. 
			4.2 Sentence Compression 
			Knight and Marcu (2000) introduced a statistical approach to sentence compression. 
			The authors believe that understanding the simpler task of compressing a sentence 
			may be a fruitful first step to later tackle the problems of single and multi-document 
			summarization. 
			Sentence compression is defined as follows: given a sequence of words W = 
			w1w2 . . . wn that constitute a sentence, find a subsequence wi1 
			wi2 
			. . . wik 
			, with 
			1  i1 < i2 < . . . ik  n, that is a compressed version of W. Note that there 
			are 2n possibilities of output. Knight and Marcu (2000) considered two different 
			approaches: one that is inspired by the noisy-channel model, and another one based 
			on decision trees. Due to its simplicity and elegance, we describe the first approach 
			here. 
			The noisy-channel model considers that one starts with a short summary s, 
			drawn according to the source model P(s), which is then subject to channel noise to 
			become the full sentence t, in a process guided by the channel model P(t|s). When 
			the string t is observed, one wants to recover the original summary according to: 
			 
			s = arg max 
			s 
			P(s|t) = arg max 
			s 
			P(s)P(t|s). (7) 
			This model has the advantage of decoupling the goals of producing a short text that 
			looks grammatical (incorporated in the source model) and of preserving important 
			information (which is done through the channel model). In (Knight and Marcu, 
			2000), the source and channel models are simple models inspired by probabilistic 
			context-free grammars (PCFGs). The following probability mass functions are de- 
			fined over parse trees rather than strings: Ptree(s), the probability of a parse tree 
			that generates s, and Pexpand tree(t|s), the probability that a small parse tree that 
			generates s is expanded to a longer one that generates t. 
			19 
			The sentence t is first parsed by using Collins' parser (Collins, 1999). Then, 
			rather than computing Ptree(s) over all the 2n hypotheses for s, which would be 
			exponential in the sentence length, a shaded-forest structure is used: the parse 
			tree of t is traversed and the grammar (learned from the Penn Treebank22) is used 
			to check recursively which nodes may be removed from each production in order 
			to achieve another valid production. This algorithm allows to compute efficiently 
			Ptree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually, 
			the noisy channel model works the other way around: summaries are the original 
			strings that are expanded via expansion templates. Expansion operations have the 
			effect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and 
			Pexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram 
			distribution over the leaves of the tree (i.e. the words). In the end, the log probability 
			is (heuristically) divided by the length of the sentence s in order not to penalize 
			excessively longer sentences (this is done commonly in speech recognition). 
			More recently, Daum 
			e III and Marcu (2002) extended this approach to document 
			compression by using rhetorical structure theory as in Marcu (1998a), where the 
			entire document is represented as a tree, hence allowing not only to compress relevant 
			sentences, but also to drop irrelevant ones. In this framework, Daum 
			e III and Marcu 
			(2004) employed kernel methods to decide for each node in the tree whether or not 
			it should be kept. 
			4.3 Sequential document representation 
			We conclude this section by mentioning some recent work that concerns document 
			representation, with applications in summarization. In the bag-of-words representa- 
			tion (Salton et al., 1975) each document is represented as a sparse vector in a very 
			large Euclidean space, indexed by words in the vocabulary V . A well-known tech- 
			nique in information retrieval to capture word correlation is latent semantic indexing 
			(LSI), that aims to find a linear subspace of dimension k  |V | where documents 
			may be approximately represented by their projections. 
			These classical approaches assume by convenience that Euclidean geometry is 
			a proper model for text documents. As an alternative, Gous (1999) and Hall and 
			Hofmann (2000) used the framework of information geometry (Amari and Nagaoka, 
			2001) to generalize LSI to the multinomial manifold, which can be identified with 
			the probability simplex 
			Pn-1 = x  Rn | 
			n 
			i=1 
			xi = 1, xi  0 for i = 1, . . . , n . (8) 
			Instead of finding a linear subspace, as in the Euclidean case, they learn a subman- 
			ifold of Pn-1. To illustrate this idea, Gous (1999) split a book (Machiavelli's The 
			Prince) into several text blocks (its numbered pages), considered each page as a 
			point in P|V |-1, and projected data into a 2-dimensional submanifold. The result is 
			22See http://www.cis.upenn.edu/~treebank/. 
			20 
			the representation of the book as a sequential path in R2, tracking the evolution of 
			the subject matter of the book over the course of its pages (see Fig. 5). Inspired by 
			Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex- 
			tracted from (Gous, 1999)). The inflection around page 85 reflects a real change in 
			the subject matter, where the book shifts from political theory to a more biograph- 
			ical discourse. 
			this framework, Lebanon et al. (2007) suggested representing a document as a sim- 
			plicial curve (i.e. a curve in the probability simplex), yielding the locally weighted 
			bag-of-words (lowbow) model. According to this representation, a length-normalized 
			document is a function x : [0, 1] x V  R+ such that 
			wjV 
			x(t, wj) = 1, for any t  [0, 1]. (9) 
			We can regard the document as a continuous signal, and x(t, wj) as expressing 
			the relevance of word wj at instant t. This generalizes both the pure sequential 
			representation and the (global) bag-of-words model. Let y = (y1, . . . , yn)  V n be 
			a n-length document. The pure sequential representation of y arises by defining 
			x = xseq with: 
			xseq(t, wj) = 
			1, if wj = y tn 
			0, if wj = y tn 
			, 
			(10) 
			where a denotes the smallest integer greater than a. The global bag-of-words 
			representation of x corresponds to defining x = xbow, where 
			xbow(, wj) = 
			1 
			0 
			xseq(t, wj)dt,   [0, 1], j = 1, . . . , |V |. (11) 
			In this case, the curve degenerates into a single point in the simplex, which is 
			the maximum likelihood estimate of the multinomial parameters. An intermediate 
			21 
			representation arises by smoothing (10) via a function f, : [0, 1]  R++, where 
			  [0, 1] and   R++ are respectively a location and a scale parameter. An 
			example of such a smoothing function is the truncated Gaussian defined in [0, 1] 
			and normalized. This allows defining the lowbow representation at  of the n-lenght 
			document (y1, . . . , yn)  V n as the function x : [0, 1] x V  R+ such that: 
			x(, wj) = 
			1 
			0 
			xseq(t, wj)f,(t)dt. (12) 
			The scale of the smoothing function controls the amount of locality/globality in 
			the document representation (see Fig. 6): when    we recover the global bow 
			representation (11); when   0, we approach the pure sequential representation 
			(10). 
			Figure 6: The lowbow representation of a document with |V | = 3, for several values 
			of the scale parameter  (extracted from (Lebanon, 2006)). 
			Representing a document as a simplicial curve allows us to characterize geomet- 
			rically several properties of the document. For example, the tangent vector field 
			along the curve describes sequential "topic trends" and their change; the curvature 
			measures the amount of wigglyness or deviation from a geodesic path. This prop- 
			erties can be useful for tasks like text segmentation or summarization; for example 
			plotting the velocity of the curve ||  
			x()|| along time offers a visualization of the doc- 
			ument where local maxima tend to correspond to topic boundaries (see (Lebanon 
			et al., 2007) for more information). 
			22 
			5 Evaluation 
			Evaluating a summary is a difficult task because there does not exist an ideal sum- 
			mary for a given document or set of documents. From papers surveyed in the previ- 
			ous sections and elsewhere in literature, it has been found that agreement between 
			human summarizers is quite low, both for evaluating and generating summaries. 
			More than the form of the summary, it is difficult to evaluate the summary con- 
			tent. Another important problem in summary evaluation is the widespread use of 
			disparate metrics. The absence of a standard human or automatic evaluation met- 
			ric makes it very hard to compare different systems and establish a baseline. This 
			problem is not present in other NLP problems, like parsing. Besides this, manual 
			evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation 
			of summaries as in the DUC conferences would require over 3000 hours of human ef- 
			forts. Hence, an evaluation metric having high correlation with human scores would 
			obviate the process of manual evaluation. In this section, we would look at some im- 
			portant recent papers that have been able to create standards in the summarization 
			community. 
			5.1 Human and Automatic Evaluation 
			Lin and Hovy (2002) describe and compare various human and automatic metrics to 
			evaluate summaries. They focus on the evaluation procedure used in the Document 
			Understanding Conference 2001 (DUC-2001), where the Summary Evaluation En- 
			vironment (SEE) interface was used to support the human evaluation part. NIST 
			assessors in DUC-2001 compared manually written ideal summaries with summaries 
			generated automatically by summarization systems and baseline summaries. Each 
			text was decomposed into a list of units (sentences) and displayed in separate win- 
			dows in SEE. To measure the content of summaries, assessors stepped through each 
			model unit (MU) from the ideal summaries and marked all system units (SU) shar- 
			ing content with the current model unit, rating them with scores in the range 1 - 4 
			to specify that the marked system units express all (4), most (3), some (2) or hardly 
			any (1) of the content of the current model unit. Grammaticality, cohesion, and co- 
			herence were also rated similarly by the assessors. The weighted recall at threshold 
			t (where t range from 1 to 4) is then defined as 
			Recallt = 
			Number of MUs marked at or above t 
			Number of MUs in the model summary 
			. (13) 
			An interesting study is presented that shows how unstable the human markings 
			for overlapping units are. For multiple systems, the coverage scores assigned to the 
			same units were different by human assessors 18% of the time for the single document 
			task and 7.6% of the time for multi-document task. The authors also observe that 
			inter-human agreement is quite low in creating extracts from documents ( 40% for 
			single-documents and  29% for multi-documents). To overcome the instability of 
			human evaluations, they proposed using automatic metrics for summary evaluation. 
			23 
			Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001), 
			they outline an accumulative n-gram matching score (which they call NAMS), 
			NAMS = a1 * NAM1 + a2 * NAM2 + a3 * NAM3 + a4 * NAM4, (14) 
			where the NAMn n-gram hit ratio is defined as: 
			# of matched n-grams between MU and S 
			total # of n-grams in MU 
			(15) 
			with S denoting here the whole system summary, and where only content words 
			were used in forming the n-grams. Different configurations of ai were tried; the 
			best correlation with human judgement (using Spearman's rank order correlation 
			coefficient) was achieved using a configuration giving 2/3 weight to bigram matches 
			and 1/3 to unigrams matches with stemming done by the Porter stemmer. 
			5.2 ROUGE 
			Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist- 
			ing Evaluation (ROUGE)23 that have become standards of automatic evaluation of 
			summaries. 
			In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be 
			a summary generated automatically by some system. Let n(d) be a binary vector 
			representing the n-grams contained in a document d; the i-th component i 
			n 
			(d) is 1 
			if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an 
			n-gram recall based statistic that can be computed as follows: 
			ROUGE-N(s) = rR 
			n(r), n(s) 
			rR 
			n(r), n(r) 
			, (16) 
			where ., . denotes the usual inner product of vectors. This measure is closely related 
			to BLEU which is a precision related measure. Unlike other measures previously 
			considered, ROUGE-N can be used for multiple reference summaries, which is quite 
			useful in practical situations. An alternative is taking the most similar summary in 
			the reference set, 
			ROUGE-Nmulti(s) = max 
			rR 
			n(r), n(s) 
			n(r), n(r) 
			. (17) 
			Another metric in (Lin, 2004) applies the concept of longest common subse- 
			quences24 (LCS). The rationale is: the longer the LCS between two summary sen- 
			tences, the more similar they are. Let r1, . . . , ru be the reference sentences of the 
			documents in R, and s a candidate summary (considered as a concatenation of 
			sentences). The ROUGE-L is defined as an LCS based F-measure: 
			ROUGE-L(s) = 
			(1 + 2)RLCSPLCS 
			RLCS + 2PLCS 
			(18) 
			23See http://openrouge.com/default.aspx. 
			24A subsequence of a string s = s1 
			. . . sn 
			is a string of the form si1 
			. . . sin 
			where 1  i1 
			< . . . in 
			 n. 
			24 
			where RLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			Pu 
			i=1 
			|ri| 
			, PLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			|s| 
			, |x| denotes the length of 
			sentence x, LCS(x, y) denotes the length of the LCS between sentences x and y, 
			and  is a (usually large) parameter to balance precision and recall. Notice that 
			the LCS function may be computed by a simple dynamic programming approach. 
			The metric (18) is further refined by including weights that penalize subsequence 
			matches that are not consecutive, yielding a new measure denoted ROUGE-W. 
			Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen 
			as a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let 2(d) 
			be a binary vector indexed by ordered pairs of words; the i-th component i 
			2 
			(d) is 
			1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is 
			computed as follows: 
			ROUGE-S(s) = 
			(1 + 2)RSPS 
			RS + 2PS 
			(19) 
			where RS(s) = Pu 
			i=1 
			2(ri),2(s) 
			Pu 
			i=1 
			2(ri),2(ri) 
			and PS(s) = Pu 
			i=1 
			2(ri),2(s) 
			2(s),2(s) 
			. 
			The various versions of ROUGE were evaluated by computing the correlation 
			coefficient between ROUGE scores and human judgement scores. ROUGE-2 per- 
			formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and 
			ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How- 
			ever, correlation achieved with human judgement for multi-document summarization 
			was not as high as single-document ones; improvement on this side of the paradigm 
			is an open research topic. 
			5.3 Information-theoretic Evaluation of Summaries 
			A very recent approach (Lin et al., 2006) proposes to use an information-theoretic 
			method to automatic evaluation of summaries. The central idea is to use a diver- 
			gence measure between a pair of probability distributions, in this case the Jensen- 
			Shannon divergence, where the first distribution is derived from an automatic sum- 
			mary and the second from a set of reference summaries. This approach has the 
			advantage of suiting both the single-document and the multi-document summariza- 
			tion scenarios. 
			Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton 
			set in the case of single-document summarization). Assume that a distribution 
			parameterized by R generates reference summaries of the documents in D. The 
			task of summarization can be seen as that of estimating R. Analogously, assume 
			that every summarization system is governed by some distribution parameterized 
			by A. Then, we may define a good summarizer as one for which A is close to R. 
			One information-theoretic measure between distributions that is adequate for this 
			is the KL divergence (Cover and Thomas, 1991), 
			KL(pA ||pR ) = 
			m 
			i=1 
			pA 
			i 
			log 
			pA 
			i 
			pR 
			i 
			. (20) 
			However, the KL divergence is unbounded and goes to infinity whenever pA 
			i 
			vanishes 
			25 
			and pR 
			i 
			does not, which requires using some kind of smoothing when estimating the 
			distributions. Lin et al. (2006) claims that the measure used here should also be 
			symmetric,25 another thing that the KL divergence is not. Hence, they propose to 
			use the Jensen-Shannon divergence which is bounded and symmetric:26 
			JS(pA ||pR ) = 
			1 
			2 
			KL(pA ||r) + 
			1 
			2 
			KL(pR ||r) = 
			= H(r) - 
			1 
			2 
			H(pA ) - 
			1 
			2 
			H(pA ), (21) 
			where r = 1 
			2 
			pA + 1 
			2 
			pR is the average distribution. 
			To evaluate a summary SA given a reference summary SR, the authors propose 
			to use the negative JS divergence between the estimates of pA and pR given the 
			summaries, 
			score(SA|SR) = -JS(p 
			A ||p 
			R ) (22) 
			The parameters are estimated via a posteriori maximization assuming a multi- 
			nomial generation model for each summary (which means that they are modeled as 
			bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial 
			family). So: 
			 
			A = arg max 
			A 
			p(SA|A)p(A), (23) 
			where (m being the number of distinct words, a1, . . . , am being the word counts in 
			the summary, a0 = m 
			i=1 
			ai) 
			p(SA|A) = 
			(a0 + 1) 
			m 
			i=1 
			(ai + 1) 
			m 
			i=1 
			A,i 
			ai (24) 
			and 
			p(A) = 
			(0) 
			m 
			i=1 
			(i) 
			m 
			i=1 
			A,i 
			i-1 (25) 
			where i are hyper-parameters and 0 = m 
			i=1 
			i. After some algebra, we get 
			 
			A,i = 
			ai + i - 1 
			a0 + 0 - m 
			(26) 
			which is similar to MLE with smoothing.27  
			R is estimated analogously using the 
			reference summary SR. Not surprisingly, if we have more than one reference sum- 
			mary, the MAP estimation given all summaries equals MAP estimation given their 
			concatenation into a single summary. 
			25However, the authors do not give much support for this claim. In our view, there is no reason 
			to require symmetry. 
			26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfies 
			the axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora 
			of properties that are presented elsewhere, but this is out of scope of this survey. 
			27In particular if i 
			= 1 it is just maximum likelihood estimation (MLE). 
			26 
			The authors experimented three automatic evaluation schemes (JS with smooth- 
			ing, JS without smoothing, and KL divergence) against manual evaluation; the best 
			performance was achieved by JS without smoothing. This is not surprising since, as 
			seen above, the JS divergence is bounded, unlike the KL divergence, and so it does 
			not require smoothing. Smoothing has the effect of pulling the two distributions 
			more close to the uniform distribution. 
			2 Single-Document Summarization 
			Usually, the flow of information in a given document is not uniform, which means 
			that some parts are more important than others. The major challenge in summa- 
			rization lies in distinguishing the more informative parts of a document from the 
			less ones. Though there have been instances of research describing the automatic 
			creation of abstracts, most work presented in the literature relies on verbatim ex- 
			traction of sentences to address the problem of single-document summarization. In 
			1See http://trec.nist.gov/. 
			2See http://duc.nist.gov/. 
			3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/. 
			muc 7 toc.html 
			2 
			this section, we describe some eminent extractive techniques. First, we look at early 
			work from the 1950s and 60s that kicked off research on summarization. Second, 
			we concentrate on approaches involving machine learning techniques published in 
			the 1990s to today. Finally, we briefly describe some techniques that use a more 
			complex natural language analysis to tackle the problem. 
			2.1 Early Work 
			Most early work on single-document summarization focused on technical documents. 
			Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de- 
			scribes research done at IBM in the 1950s. In his work, Luhn proposed that the 
			frequency of a particular word in an article provides an useful measure of its sig- 
			nificance. There are several key ideas put forward in this paper that have assumed 
			importance in later work on summarization. As a first step, words were stemmed to 
			their root forms, and stop words were deleted. Luhn then compiled a list of content 
			words sorted by decreasing frequency, the index providing a significance measure of 
			the word. On a sentence level, a significance factor was derived that reflects the 
			number of occurrences of significant words within a sentence, and the linear distance 
			between them due to the intervention of non-significant words. All sentences are 
			ranked in order of their significance factor, and the top ranking sentences are finally 
			selected to form the auto-abstract. 
			Related work (Baxendale, 1958), also done at IBM and published in the same 
			journal, provides early insight on a particular feature helpful in finding salient parts 
			of documents: the sentence position. Towards this goal, the author examined 200 
			paragraphs to find that in 85% of the paragraphs the topic sentence came as the first 
			one and in 7% of the time it was the last sentence. Thus, a naive but fairly accurate 
			way to select a topic sentence would be to choose one of these two. This positional 
			feature has since been used in many complex machine learning based systems. 
			Edmundson (1969) describes a system that produces document extracts. His 
			primary contribution was the development of a typical structure for an extractive 
			summarization experiment. At first, the author developed a protocol for creating 
			manual extracts, that was applied in a set of 400 technical documents. The two 
			features of word frequency and positional importance were incorporated from the 
			previous two works. Two other features were used: the presence of cue words 
			(presence of words like significant, or hardly), and the skeleton of the document 
			(whether the sentence is a title or heading). Weights were attached to each of these 
			features manually to score each sentence. During evaluation, it was found that about 
			44% of the auto-extracts matched the manual extracts. 
			2.2 Machine Learning Methods 
			In the 1990s, with the advent of machine learning techniques in NLP, a series of semi- 
			nal publications appeared that employed statistical techniques to produce document 
			extracts. While initially most systems assumed feature independence and relied on 
			naive-Bayes methods, others have focused on the choice of appropriate features and 
			3 
			on learning algorithms that make no independence assumptions. Other significant 
			approaches involved hidden Markov models and log-linear models to improve ex- 
			tractive summarization. A very recent paper, in contrast, used neural networks and 
			third party features (like common words in search engine queries) to improve purely 
			extractive single document summarization. We next describe all these approaches 
			in more detail. 
			2.2.1 Naive-Bayes Methods 
			Kupiec et al. (1995) describe a method derived from Edmundson (1969) that is able 
			to learn from data. The classification function categorizes each sentence as worthy 
			of extraction or not, using a naive-Bayes classifier. Let s be a particular sentence, 
			S the set of sentences that make up the summary, and F1, . . . , Fk the features. 
			Assuming independence of the features: 
			P(s  S | F1, F2, ..Fk) = 
			k 
			i=1 
			P(Fi | s  S) * P(s  S) 
			k 
			i=1 
			P(Fi) 
			(1) 
			The features were compliant to (Edmundson, 1969), but additionally included the 
			sentence length and the presence of uppercase words. Each sentence was given a 
			score according to (1), and only the n top sentences were extracted. To evaluate 
			the system, a corpus of technical documents with manual abstracts was used in 
			the following way: for each sentence in the manual abstract, the authors manually 
			analyzed its match with the actual document sentences and created a mapping 
			(e.g. exact match with a sentence, matching a join of two sentences, not matchable, 
			etc.). The auto-extracts were then evaluated against this mapping. Feature analysis 
			revealed that a system using only the position and the cue features, along with the 
			sentence length sentence feature, performed best. 
			Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richer 
			features. They describe a system called DimSum that made use of features like 
			term frequency (tf ) and inverse document frequency (idf) to derive signature words.4 
			The idf was computed from a large corpus of the same domain as the concerned 
			documents. Statistically derived two-noun word collocations were used as units for 
			counting, along with single words. A named-entity tagger was used and each entity 
			was considered as a single token. They also employed some shallow discourse analysis 
			like reference to same entities in the text, maintaining cohesion. The references 
			were resolved at a very shallow level by linking name aliases within a document 
			like "U.S." to "United States", or "IBM" for "International Business Machines". 
			Synonyms and morphological variants were also merged while considering lexical 
			terms, the former being identified by using Wordnet (Miller, 1995). The corpora 
			used in the experiments were from newswire, some of which belonged to the TREC 
			evaluations. 
			4Words that indicate key concepts in a document. 
			4 
			2.2.2 Rich Features and Decision Trees 
			Lin and Hovy (1997) studied the importance of a single feature, sentence position. 
			Just weighing a sentence by its position in text, which the authors term as the 
			"position method", arises from the idea that texts generally follow a predictable 
			discourse structure, and that the sentences of greater topic centrality tend to occur in 
			certain specifiable locations (e.g. title, abstracts, etc). However, since the discourse 
			structure significantly varies over domains, the position method cannot be defined 
			as naively as in (Baxendale, 1958). The paper makes an important contribution by 
			investigating techniques of tailoring the position method towards optimality over a 
			genre and how it can be evaluated for effectiveness. A newswire corpus was used, the 
			collection of Ziff-Davis texts produced from the TIPSTER5 program; it consists of 
			text about computer and related hardware, accompanied by a set of key topic words 
			and a small abstract of six sentences. For each document in the corpus, the authors 
			measured the yield of each sentence position against the topic keywords. They then 
			ranked the sentence positions by their average yield to produce the Optimal Position 
			Policy (OPP) for topic positions for the genre. 
			Two kinds of evaluation were performed. Previously unseen text was used for 
			testing whether the same procedure would work in a different domain. The first 
			evaluation showed contours exactly like the training documents. In the second eval- 
			uation, word overlap of manual abstracts with the extracted sentences was measured. 
			Windows in abstracts were compared with windows on the selected sentences and 
			corresponding precision and recall values were measured. A high degree of coverage 
			indicated the effectiveness of the position method. 
			In later work, Lin (1999) broke away from the assumption that features are 
			independent of each other and tried to model the problem of sentence extraction 
			using decision trees, instead of a naive-Bayes classifier. He examined a lot of fea- 
			tures and their effect on sentence extraction. The data used in this work is a 
			publicly available collection of texts, classified into various topics, provided by the 
			TIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems. 
			The dataset contains essential text fragments (phrases, clauses, and sentences) which 
			must be included in summaries to answer some TREC topics. These fragments were 
			each evaluated by a human judge. The experiments described in the paper are with 
			the SUMMARIST system developed at the University of Southern California. The 
			system extracted sentences from the documents and those were matched against 
			human extracts, like most early work on extractive summarization. 
			Some novel features were the query signature (normalized score given to sen- 
			tences depending on number of query words that they contain), IR signature (the 
			m most salient words in the corpus, similar to the signature words of (Aone et al., 
			1999)), numerical data (boolean value 1 given to sentences that contained a num- 
			ber in them), proper name (boolean value 1 given to sentences that contained a 
			proper name in them), pronoun or adjective (boolean value 1 given to sentences 
			5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/. 
			6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html. 
			5 
			that contained a pronoun or adjective in them), weekday or month (similar as pre- 
			vious feature) and quotation (similar as previous feature). It is worth noting that 
			some features like the query signature are question-oriented because of the setting 
			of the evaluation, unlike a generalized summarization framework. 
			The author experimented with various baselines, like using only the positional 
			feature, or using a simple combination of all features by adding their values. When 
			evaluated by matching machine extracted and human extracted sentences, the deci- 
			sion tree classifier was clearly the winner for the whole dataset, but for three topics, 
			a naive combination of features beat it. Lin conjectured that this happened because 
			some of the features were independent of each other. Feature analysis suggested 
			that the IR signature was a valuable feature, corroborating the early findings of 
			Luhn (1958). 
			2.2.3 Hidden Markov Models 
			In contrast with previous approaches, that were mostly feature-based and non- 
			sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentence 
			from a document using a hidden Markov model (HMM). The basic motivation for 
			using a sequential model is to account for local dependencies between sentences. 
			Only three features were used: position of the sentence in the document (built into 
			the state structure of the HMM), number of terms in the sentence, and likeliness of 
			the sentence terms given the document terms. 
			no 3 
			2 
			1 no 
			no 
			no 
			Figure 1: Markov model to extract to three summary sentences from a document 
			(Conroy and O'leary, 2001). 
			The HMM was structured as follows: it contained 2s + 1 states, alternating be- 
			tween s summary states and s+1 nonsummary states. The authors allowed "hesita- 
			tion" only in nonsummary states and "skipping next state" only in summary states. 
			Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using the 
			TREC dataset as training corpus, the authors obtained the maximum-likelihood 
			estimate for each transition probability, forming the transition matrix estimate  
			M, 
			whose element (i, j) is the empirical probability of transitioning from state i to j. 
			Associated with each state i was an output function, bi(O) = Pr(O | state i) where 
			O is an observed vector of features. They made a simplifying assumption that the 
			features are multivariate normal. The output function for each state was thus esti- 
			mated by using the training data to compute the maximum likelihood estimate of 
			its mean and covariance matrix. They estimated 2s+1 means, but assumed that all 
			of the output functions shared a common covariance matrix. Evaluation was done 
			6 
			by comparing with human generated extracts. 
			2.2.4 Log-Linear Models 
			Osborne (2002) claims that existing approaches to summarization have always as- 
			sumed feature independence. The author used log-linear models to obviate this 
			assumption and showed empirically that the system produced better extracts than 
			a naive-Bayes model, with a prior appended to both models. Let c be a label, s 
			the item we are interested in labeling, fi the i-th feature, and i the corresponding 
			feature weight. The conditional log-linear model used by Osborne (2002) can be 
			stated as follows: 
			P(c | s) = 
			1 
			Z(s) 
			exp 
			i 
			ifi(c, s) , (2) 
			where Z(s) = c 
			exp ( i 
			ifi(c, s)). In this domain, there are only two possible 
			labels: either the sentence is to be extracted or it is not. The weights were trained 
			by conjugate gradient descent. The authors added a non-uniform prior to the model, 
			claiming that a log-linear model tends to reject too many sentences for inclusion in 
			a summary. The same prior was also added to a naive-Bayes model for comparison. 
			The classification took place as follows: 
			label(s) = arg max 
			cC 
			P(c) * P(s, c) = arg max 
			cC 
			log P(c) + 
			i 
			ifi(c, s) . (3) 
			The authors optimized the prior using the f2 score of the classifier as an objective 
			function on a part of the dataset (in the technical domain). The summaries were 
			evaluated using the standard f2 score where f2 = 2pr 
			p+r 
			, where the precision and recall 
			measures were measured against human generated extracts. The features included 
			word pairs (pairs of words with all words truncated to ten characters), sentence 
			length, sentence position, and naive discourse features like inside introduction or 
			inside conclusion. With respect to f2 score, the log-linear model outperformed the 
			naive-Bayes classifier with the prior, exhibiting the former's effectiveness. 
			2.2.5 Neural Networks and Third Party Features 
			In 2001-02, DUC issued a task of creating a 100-word summary of a single news 
			article. However, the best performing systems in the evaluations could not outper- 
			form the baseline with statistical significance. This extremely strong baseline has 
			been analyzed by Nenkova (2005) and corresponds to the selection of the first n 
			sentences of a newswire article. This surprising result has been attributed to the 
			journalistic convention of putting the most important part of an article in the initial 
			paragraphs. After 2002, the task of single-document summarization for newswire 
			was dropped from DUC. Svore et al. (2007) propose an algorithm based on neu- 
			ral nets and the use of third party datasets to tackle the problem of extractive 
			summarization, outperforming the baseline with statistical significance. 
			7 
			The authors used a dataset containing 1365 documents gathered from CNN.com, 
			each consisting of the title, timestamp, three or four human generated story high- 
			lights and the article text. They considered the task of creating three machine 
			highlights. The human generated highlights were not verbatim extractions from the 
			article itself. The authors evaluated their system using two metrics: the first one 
			concatenated the three highlights produced by the system, concatenated the three 
			human generated highlights, and compared these two blocks; the second metric con- 
			sidered the ordering and compared the sentences on an individual level. 
			Svore et al. (2007) trained a model from the labels and the features for each 
			sentence of an article, that could infer the proper ranking of sentences in a test 
			document. The ranking was accomplished using RankNet (Burges et al., 2005), a 
			pair-based neural network algorithm designed to rank a set of inputs that uses the 
			gradient descent method for training. For the training set, they used ROUGE-1 
			(Lin, 2004) to score the similarity of a human written highlight and a sentence 
			in the document. These similarity scores were used as soft labels during training, 
			contrasting with other approaches where sentences are "hard-labeled", as selected 
			or not. 
			Some of the used features based on position or n-grams frequencies have been 
			observed in previous work. However, the novelty of the framework lay in the use 
			of features that derived information from query logs from Microsoft's news search 
			engine7 and Wikipedia8 entries. The authors conjecture that if a document sentence 
			contained keywords used in the news search engine, or entities found in Wikipedia 
			articles, then there is a greater chance of having that sentence in the highlight. The 
			extracts were evaluated using ROUGE-1 and ROUGE-2, and showed statistically 
			significant improvements over the baseline of selecting the first three sentences in a 
			document. 
			2.3 Deep Natural Language Analysis Methods 
			In this subsection, we describe a set of papers that detail approaches towards single- 
			document summarization involving complex natural language analysis techniques. 
			None of these papers solve the problem using machine learning, but rather use a set 
			of heuristics to create document extracts. Most of these techniques try to model the 
			text's discourse structure. 
			Barzilay and Elhadad (1997) describe a work that used considerable amount of 
			linguistic analysis for performing the task of summarization. For a better under- 
			standing of their method, we need to define a lexical chain: it is a sequence of related 
			words in a text, spanning short (adjacent words or sentences) or long distances (en- 
			tire text). The authors' method progressed with the following steps: segmentation 
			of the text, identification of lexical chains, and using strong lexical chains to identify 
			the sentences worthy of extraction. They tried to reach a middle ground between 
			(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep 
			7See http://search.live.com/news. 
			8See http://en.wikipedia.org. 
			8 
			semantic structure of the text, while the latter relied on word statistics of the doc- 
			uments. The authors describe the notion of cohesion in text as a means of sticking 
			together different parts of the text. Lexical cohesion is a notable example where 
			semantically related words are used. For example, let us take a look at the following 
			sentence.9 
			John bought a Jag. He loves the car. (4) 
			Here, the word car refers to the word Jag in the previous sentence, and exemplifies 
			lexical cohesion. The phenomenon of cohesion occurs not only at the word level, 
			but at word sequences too, resulting in lexical chains, which the authors used as 
			a source representation for summarization. Semantically related words and word 
			sequences were identified in the document, and several chains were extracted, that 
			form a representation of the document. To find out lexical chains, the authors used 
			Wordnet (Miller, 1995), applying three generic steps: 
			1. Selecting a set of candidate words. 
			2. For each candidate word, finding an appropriate chain relying on a relatedness 
			criterion among members of the chains, 
			3. If it is found, inserting the word in the chain and updating it accordingly. 
			The relatedness was measured in terms of Wordnet distance. Simple nouns and 
			noun compounds were used as starting point to find the set of candidates. In the 
			final steps, strong lexical chains were used to create the summaries. The chains were 
			scored by their length and homogeneity. Then the authors used a few heuristics to 
			select the significant sentences. 
			In another paper, Ono et al. (1994) put forward a computational model of dis- 
			course for Japanese expository writings, where they elaborate a practical procedure 
			for extracting the discourse rhetorical structure, a binary tree representing relations 
			between chunks of sentences (rhetorical structure trees are used more intensively in 
			(Marcu, 1998a), as we will see below). This structure was extracted using a series 
			of NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can- 
			didate generation and preference judgement. Evaluation was based on the relative 
			importance of rhetorical relations. In the following step, the nodes of the rhetori- 
			cal structure tree were pruned to reduce the sentence, keeping its important parts. 
			Same was done for paragraphs to finally produce the summary. Evaluation was done 
			with respect to sentence coverage and 30 editorial articles of a Japanese newspaper 
			were used as the dataset. The articles had corresponding sets of key sentences and 
			most important key sentences judged by human subjects. The key sentence coverage 
			was about 51% and the most important key sentence coverage was 74%, indicating 
			encouraging results. 
			Marcu (1998a) describes a unique approach towards summarization that, unlike 
			most other previous work, does not assume that the sentences in a document form 
			a flat sequence. This paper used discourse based heuristics with the traditional 
			9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html. 
			9 
			features that have been used in the summarization literature. The discourse theory 
			used in this paper is the Rhetorical Structure Theory (RST) that holds between 
			two non-overlapping pieces of text spans: the nucleus and the satellite. The author 
			mentions that the distinction between nuclei and satellites comes from the empir- 
			ical observation that the nucleus expresses what is more essential to the writer's 
			purpose than the satellite; and that the nucleus of a rhetorical relation is compre- 
			hensible independent of the satellite, but not vice versa. Marcu (1998b) describes 
			the details of a rhetorical parser producing a discourse tree. Figure 2 shows an 
			example discourse tree for a text example detailed in the paper. Once such a dis- 
			Antithesis 
			2 
			Elaboration 
			Elaboration 
			2 
			2 
			Elaboration 
			3 
			Justification 
			8 
			Exemplification 
			1 2 3 4 5 7 8 
			4 5 
			8 10 
			9 10 
			5 6 
			Contrast 
			Evidence 
			Concession 
			Figure 2: Example of a discourse tree from Marcu (1998a). The numbers in the 
			nodes denote sentence numbers from the text example. The text below the number 
			in selected nodes are rhetorical relations. The dotted nodes are SATELLITES and 
			the normals ones are the NUCLEI. 
			course structure is created, a partial ordering of important units can be developed 
			from the tree. Each equivalence class in the partial ordering is derived from the 
			new sentences at a particular level of the discourse tree. In Figure 2, we observe 
			that sentence 2 is at the root, followed by sentence 8 in the second level. In the 
			third level, sentence 3 and 10 are observed, and so forth. The equivalence classes 
			are 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6. 
			If it is specified that the summary should contain the top k% of the text, the first 
			k% of the units in the partial ordering can be selected to produce the summary. The 
			author talks about a summarization system based just on this method in (Marcu, 
			1998b) and in one of his earlier papers. In this paper, he merged the discourse 
			based heuristics with traditional heuristics. The metrics used were clustering based 
			10 
			metric (each node in the discourse tree was assigned a cluster score; for leaves the 
			score was 0, for the internal nodes it was given by the similarity of the immediate 
			children; discourse tree A was chosen to be better than B if its clustering score 
			was higher), marker based metric (a discourse structure A was chosen to be better 
			than a discourse structure B if A used more rhetorical relations than B), rhetorical 
			clustering based technique (measured the similarity between salient units of two text 
			spans), shape based metric (preferred a discourse tree A over B if A was more skewed 
			towards the right than B), title based metric, position based metric, connectedness 
			based metric (cosine similarity of an unit to all other text units, a discourse structure 
			A was chosen to be better than B if its connectedness measure was more than B). 
			A weighted linear combination of all these scores gave the score of a discourse 
			structure. To find the best combination of heuristics, the author computed the 
			weights that maximized the F-score on the training dataset, which was constituted 
			by newswire articles. To do this, he used a GSAT-like algorithm (Selman et al., 
			1992) that performed a greedy search in a seven dimensional space of the metrics. 
			For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achieved 
			for the 10% summaries which was 3.5% higher than a baseline lead based algorithm, 
			which was very encouraging. 
			3 Multi-Document Summarization 
			Extraction of a single summary from multiple documents has gained interest since 
			mid 1990s, most applications being in the domain of news articles. Several Web- 
			based news clustering systems were inspired by research on multi-document summa- 
			rization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12 
			This departs from single-document summarization since the problem involves mul- 
			tiple sources of information that overlap and supplement each other, being contra- 
			dictory at occasions. So the key tasks are not only identifying and coping with 
			redundancy across documents, but also recognizing novelty and ensuring that the 
			final summary is both coherent and complete. 
			The field seems to have been pioneered by the NLP group at Columbia University 
			(McKeown and Radev, 1995), where a summarization system called SUMMONS13 
			was developed by extending already existing technology for template-driven message 
			understanding systems. Although in that early stage multi-document summariza- 
			tion was mainly seen as a task requiring substantial capabilities of both language 
			interpretation and generation, it later gained autonomy, as people coming from dif- 
			ferent communities added new perspectives to the problem. Extractive techniques 
			have been applied, making use of similarity measures between pairs of sentences. 
			Approaches vary on how these similarities are used: some identify common themes 
			through clustering and then select one sentence to represent each cluster (McKeown 
			10See http://news.google.com. 
			11See http://newsblaster.cs.columbia.edu. 
			12See http://NewsInEssence.com. 
			13SUMMarizing Online NewS articles. 
			11 
			et al., 1999; Radev et al., 2000), others generate a composite sentence from each 
			cluster (Barzilay et al., 1999), while some approaches work dynamically by includ- 
			ing each candidate passage only if it is considered novel with respect to the previous 
			included passages, via maximal marginal relevance (Carbonell and Goldstein, 1998). 
			Some recent work extends multi-document summarization to multilingual environ- 
			ments (Evans, 2005). 
			The way the problem is posed has also varied over time. While in some pub- 
			lications it is claimed that extractive techniques would not be effective for multi- 
			document summarization (McKeown and Radev, 1995; McKeown et al., 1999), some 
			years later that claim was overturned, as extractive systems like MEAD14 (Radev 
			et al., 2000) achieved good performance in large scale summarization of news arti- 
			cles. This can be explained by the fact that summarization systems often distinguish 
			among themselves about what their goal actually is. While some systems, like SUM- 
			MONS, are designed to work in strict domains, aiming to build a sort of briefing 
			that highlights differences and updates accross different news reports, putting much 
			emphasis on how information is presented to the user, others, like MEAD, are large 
			scale systems that intend to work in general domains, being more concerned with 
			information content rather than form. Consequently, systems of the former kind re- 
			quire a strong effort on language generation to produce a grammatical and coherent 
			summary, while latter systems are probably more close to the information retrieval 
			paradigm. Abstractive systems like SUMMONS are difficult to replicate, as they 
			heavily rely on the adaptation of internal tools to perform information extraction 
			and language generation. On the other hand, extractive systems are generally easy 
			to implement from scratch, and this makes them appealing when sophisticated NLP 
			tools are not available. 
			3.1 Abstraction and Information Fusion 
			As far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown, 
			1998) is the first historical example of a multi-document summarization system. It 
			tackles single events about a narrow domain (news articles about terrorism) and 
			produces a briefing merging relevant information about each event and how reports 
			by different news agencies have evolved over time. The whole thread of reports is 
			then presented, as illustrated in the following example of a "good" summary: 
			"In the afternoon of February 26, 1993, Reuters reported that a suspect 
			bomb killed at least five people in the World Trade Center. However, 
			Associated Press announced that exactly five people were killed in the 
			blast. Finally, Associated Press announced that Arab terrorists were 
			possibly responsible for the terrorist act." 
			Rather than working with raw text, SUMMONS reads a database previously 
			built by a template-based message understanding system. A full multi-document 
			14Available for download at http://www.summarization.com/mead/. 
			12 
			summarizer is built by concatenating the two systems, first processing full text as 
			input and filling template slots, and then synthesizing a summary from the extracted 
			information. The architecture of SUMMONS consists of two major components: a 
			content planner that selects the information to include in the summary through 
			combination of the input templates, and a linguistic generator that selects the right 
			words to express the information in grammatical and coherent text. The latter 
			component was devised by adapting existing language generation tools, namely the 
			FUF/SURGE system15. Content planning, on the other hand, is made through 
			summary operators, a set of heuristic rules that perform operations like "change of 
			perspective", "contradiction", "refinement", etc. Some of these operations require 
			resolving conflicts, i.e., contradictory information among different sources or time 
			instants; others complete pieces of information that are included in some articles 
			and not in others, combining them into a single template. At the end, the linguis- 
			tic generator gathers all the combined information and uses connective phrases to 
			synthesize a summary. 
			While this framework seems promising when the domain is narrow enough so that 
			the templates can be designed by hand, a generalization for broader domains would 
			be problematic. This was improved later by McKeown et al. (1999) and Barzilay 
			et al. (1999), where the input is now a set of related documents in raw text, like 
			those retrieved by a standard search engine in response to a query. The system starts 
			by identifying themes, i.e., sets of similar text units (usually paragraphs). This is 
			formulated as a clustering problem. To compute a similarity measure between text 
			units, these are mapped to vectors of features, that include single words weighted 
			by their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnet 
			database and a database of semantic classes of verbs. For each pair of paragraphs, a 
			vector is computed that represents matches on the different features. Decision rules 
			that were learned from data are then used to classify each pair of text units either 
			as similar or dissimilar; this in turn feeds a subsequent algorithm that places the 
			most related paragraphs in the same theme. 
			Once themes are identified, the system enters its second stage: information fu- 
			sion. The goal is to decide which sentences of a theme should be included in the 
			summary. Rather than just picking a sentence that is a group representative, the 
			authors propose an algorithm which compares and intersects predicate argument 
			structures of the phrases within each theme to determine which are repeated often 
			enough to be included in the summary. This is done as follows: first, sentences are 
			parsed through Collins' statistical parser (Collins, 1999) and converted into depen- 
			dency trees, which allows capturing the predicate-argument structure and identify 
			functional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentence 
			representation. 
			The comparison algorithm then traverses these dependency trees recursively, 
			adding identical nodes to the output tree. Once full phrases (a verb with at least 
			two constituents) are found, they are marked to be included in the summary. If two 
			15FUF, SURGE, and other tools developed by the Columbia NLP group are available at 
			http://www1.cs.columbia.edu/nlp/tools.cgi. 
			13 
			and Kan 1998]. We match two verbs that share the same 
			semantic class in this classifi cation. 
			In addition to the above primitive features that all com- 
			pare single items from each text unit, we use composite fea- 
			tures that combine pairs of primitive features. Our compos- 
			ite features impose particular constraints on the order of the 
			two elements in the pair, on the maximum distance between 
			the two elements, and on the syntactic classes that the two 
			elements come from. They can vary from a simple com- 
			bination (e.g., "two text units must share two words to be 
			similar") to complex cases with many conditions (e.g., "two 
			text units must have matching noun phrases that appear in 
			the same order and with relative difference in position no 
			more than fi ve"). In this manner, we capture information 
			on how similarly related elements are spaced out in the two 
			text units, as well as syntactic information on word combi- 
			nations. Matches on composite features indicate combined 
			evidence for the similarity of the two units. 
			To determine whether the units match overall, we employ 
			a machine learning algorithm [Cohen 1996] that induces de- 
			cision rules using the features that really make a difference. 
			A set of pairs of units already marked as similar or not by a 
			human is used for training the classifi er. We have manually 
			marked a set of 8,225 paragraph comparisons from the TDT 
			corpus for training and evaluating our similarity classifi er. 
			For comparison, we also use an implementation of the 
			TF*IDF method which is standard for matching texts in in- 
			formation retrieval. We compute the total frequency (TF) of 
			words in each text unit and the number of units in our train- 
			ing set each word appears in (DF, or document frequency). 
			Then each text unit is represented as a vector of TF*IDF 
			scores, calculated as 
			TF(wordi 
			) * log Total number of units 
			DF(wordi 
			) 
			Similarity between text units is measured by the cosine of 
			the angle between the corresponding two vectors (i.e., the 
			normalized inner product of the two vectors), and the opti- 
			mal value of a threshold for judging two units as similar is 
			computed from the training set. 
			After all pairwise similarities between text units have 
			been calculated, we utilize a clustering algorithm to iden- 
			tify themes. As a paragraph may belong to multiple themes, 
			most standard clustering algorithms, which partition their 
			input set, are not suitable for our task. We use a greedy, 
			one-pass algorithm that fi rst constructs groups from the most 
			similar paragraphs, seeding the groups with the fully con- 
			nected subcomponents of the graph that the similarity rela- 
			tionship induces over the set of paragraphs, and then places 
			additional paragraphs within a group if the fraction of the 
			members of the group they are similar to exceeds a preset 
			threshold. 
			Language Generation 
			Given a group of similar paragraphs--a theme--the prob- 
			lem is to create a concise and fluent fusion of information in 
			this theme, reflecting facts common to all paragraphs. A 
			straightforward method would be to pick a representative 
			subject 
			class: noun 
			27 
			class: cardinal 
			bombing 
			class: noun 
			McVeigh with 
			class: preposition 
			definite: yes 
			charge 
			class: verb voice :passive 
			polarity: + 
			tense: past 
			Figure 4: Dependency grammar representation of the sen- 
			tence "McVeigh, 27, was charged with the bombing". 
			sentence that meets some criteria (e.g., a threshold number 
			of common content words). In practice, however, any repre- 
			sentative sentence will usually include embedded phrase(s) 
			containing information that is not common to all sentences 
			in the theme. Furthermore, other sentences in the theme of- 
			ten contain additional information not presented in the rep- 
			resentative sentence. Our approach, therefore, uses inter- 
			section among theme sentences to identify phrases common 
			to most paragraphs and then generates a new sentence from 
			identifi ed phrases. 
			Intersection among Theme Sentences 
			Intersection is carried out in the content planner, which uses 
			a parser for interpreting the input sentences, with our new 
			work focusing on the comparison of phrases. Theme sen- 
			tences are fi rst run through a statistical parser[Collins 1996] 
			and then, in order to identify functional roles (e.g., subject, 
			object), are converted to a dependency grammar representa- 
			tion [Kittredge and Mel' 
			cuk 1983], which makes predicate- 
			argument structure explicit. 
			We developed a rule-based component to produce func- 
			tional roles, which transforms the phrase-structure output of 
			Collins' parser to dependency grammar; function words (de- 
			terminers and auxiliaries) are eliminated from the tree and 
			corresponding syntactic features are updated. An example 
			of a theme sentence and its dependency grammar represen- 
			tation are shown in Figure 4. Each non-auxiliary word in the 
			sentence has a node in the representation, and this node is 
			connected to its direct dependents. 
			The comparison algorithm starts with all subtrees rooted 
			at verbs from the input dependency structure, and traverses 
			them recursively: if two nodes are identical, they are added 
			to the output tree, and their children are compared. Once 
			a full phrase (verb with at least two constituents) has been 
			found, it is confi rmed for inclusion in the summary. 
			Diffi culties arise when two nodes are not identical, but are 
			similar. Such phrases may be paraphrases of each other and 
			still convey essentially the same information. Since theme 
			sentences are a priori close semantically, this signifi cantly 
			Figure 3: Dependency tree representing the sentence "McVeigh, 27, was charged 
			with the bombing" (extracted from (McKeown et al., 1999)). 
			phrases, rooted at some node, are not identical but yet similar, the hypothesis that 
			they are paraphrases of each other is considered; to take this into account, corpus- 
			driven paraphrasing rules are written to allow paraphrase intersection.16 Once the 
			summary content (represented as predicate-argument structures) is decided, a gram- 
			matical text is generated by translating those structures into the arguments expected 
			by the FUF/SURGE language generation system. 
			3.2 Topic-driven Summarization and MMR 
			Carbonell and Goldstein (1998) made a major contribution to topic-driven sum- 
			marization by introducing the maximal marginal relevance (MMR) measure. The 
			idea is to combine query relevance with information novelty; it may be applicable 
			in several tasks ranging from text retrieval to topic-driven summarization. MMR 
			simultaneously rewards relevant sentences and penalizes redundant ones by consid- 
			ering a linear combination of two similarity measures. 
			Let Q be a query or user profile and R a ranked list of documents retrieved by 
			a search engine. Consider an incremental procedure that selects documents, one at 
			a time, and adds them to a set S. So let S be the set of already selected documents 
			in a particular step, and R \ S the set of yet unselected documents in R. For each 
			candidate document Di  R \ S, its marginal relevance MR(Di) is computed as: 
			MR(Di) := Sim1(Di, Q) - (1 - ) max 
			DjS 
			Sim2(Di, Dj) (5) 
			where  is a parameter lying in [0, 1] that controls the relative importance given 
			to relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the 
			16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al., 
			1999). Examples are: ordering of sentence components, main clause vs. relative clause, realization 
			in different syntactic categories (e.g. classifier vs. apposition), change in grammatical features 
			(active/passive, time, number, etc.), head omission, transformation from one POS to another, 
			using semantically related words (e.g. synonyms), etc. 
			14 
			experiments both were set to the standard cosine similarity traditionally used in the 
			vector space model, Sim1(x, y) = Sim2(x, y) = x,y 
			x * y 
			. The document achieving the 
			highest marginal relevance, DMMR = arg maxDiR\S 
			MR(Di), is then selected, i.e., 
			added to S, and the procedure continues until a maximum number of documents 
			are selected or a minimum relevance threshold is attained. Carbonell and Goldstein 
			(1998) found experimentally that choosing dynamically the value of  turns out to be 
			more effective than keeping it fixed, namely starting with small values (  0.3) to 
			give more emphasis to novelty, and then increasing it (  0.7) to focus on the most 
			relevant documents. To perform summarization, documents can be first segmented 
			into sentences or paragraphs, and after a query is submitted, the MMR algorithm 
			can be applied followed by a selection of the top ranking passages, reordering them as 
			they appeared in the original documents, and presenting the result as the summary. 
			One of the attractive points in using MMR for summarization is its topic-oriented 
			feature, through its dependency on the query Q, which makes it particularly ap- 
			pealing to generate summaries according to a user profile: as the authors claim, "a 
			different user with different information needs may require a totally different sum- 
			mary of the same document." This assertion was not being taken into account by 
			previous multi-document summarization systems. 
			3.3 Graph Spreading Activation 
			Mani and Bloedorn (1997) describe an information extraction framework for sum- 
			marization, a graph-based method to find similarities and dissimilarities in pairs 
			of documents. Albeit no textual summary is generated, the summary content is 
			represented via entities (concepts) and relations that are displayed respectively as 
			nodes and edges of a graph. Rather than extracting sentences, they detect salient 
			regions of the graph via a spreading activation technique.17 
			This approach shares with the method described in Section 3.2 the property 
			of being topic-driven; there is an additional input that stands for the topic with 
			respect to which the summary is to be generated. The topic is represented through 
			a set of entry nodes in the graph. A document is represented as a graph as follows: 
			each node represents the occurrence of a single word (i.e., one word together with 
			its position in the text). Each node can have several kinds of links: adjacency 
			links (ADJ) to adjacent words in the text, SAME links to other occurrences of the 
			same word, and ALPHA links encoding semantic relationships captured through 
			Wordnet and NetOwl18. Besides these, PHRASE links tie together sequences of 
			adjacent nodes which belong to the same phrase, and NAME and COREF links 
			stand for co-referential name occurrences; Fig. 4 shows some of these links. 
			Once the graph is built, topic nodes are identified by stem comparison and be- 
			come the entry nodes. A search for semantically related text is then propagated from 
			these to the other nodes of the graph, in a process called spreading activation. Salient 
			17The name "spreading activation" is borrowed from a method used in information retrieval 
			(Salton and Buckley, 1988) to expand the search vocabulary. 
			18See http://www.netowl.com. 
			15 
			1.39: Aoki, the Japanese ambassador, said in telephone calls to 
			Fujimori. 
			Japanesebroadcaster NHK that the rebels wanted to talk directly to 
			1.43:According to some estimates, only a couple hundred armed 
			followers remain. 
			2.19 They are freeing u 
			not doing us any harm," 
			... 
			2.27:Although the MRTA 
			early days in the mid-198 
			give to the poor, it lost pu 
			turning increasingly to ki 
			billion in damage to the c 
			since 1980. 
			and drug activities. 2.28: 
			Peru have cost at least 3 
			... 
			close ties with Japan. 
			1.33: Among the hostages were Japanese Ambassador Morihisa Aoki and 
			the ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea, 
			... 
			... 
			... 
			2.26:The MRTA called T 
			"Breaking The Silence." 
			1.32: President Alberto Fujimori, who is of Japanese ancestry, has had 
			Germany, Austria and Venezuela. Hood-style movement tha 
			negotiations with the gov 
			dawn on Wednesday. 
			... 
			... 
			2.22:The attack was a ma 
			Fujimori's government, w 
			virtual victory in a 16-yea 
			rebels belonging to the M 
			and better-known Maoist 
			... 
			1.28:Many leaders of the Tupac Amaru which is smaller than Peru's 
			was captured in June 1992 and is serving a life sentence, as is his 
			us: `Don't lift your heads up or you will be shot." 
			1.19: 
			hostages," a rebel who did not give his name told a local radio station in 
			a telephone call from inside the compound. 
			"The guerillas stalked around the residence grounds threatening 
			lieutenant, Peter Cardenas. 
			1.25: "We are clear: the liberation of all our comrades, or we die with all the 
			1.30:Other top commanders conceded defeat July 1993. 
			and surrendered in 
			COREF 
			Maoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay, 
			ADJ 
			1.38:Fujimori whose sister was among the 
			an emergency cabinet meeting today. 
			hostages released, called 
			ALPHA 
			ADJ 
			, the rebels threatened to kill the remaining 
			captives. 
			1.24:Early Wednesday 
			Figure 5: Texts of two related articles. The top 5 salient sentences containing common 
			words in bold face; likewise, the top 5 salient sentences containing unique words have th 
			Figure 4: Examples of nodes and links in the graph for a particular sentence (detail 
			extracted from from a figure in (Mani and Bloedorn, 1997)). 
			words and phrases are initialized according to their TF-IDF score. The weight of 
			neighboring nodes depends on the node link traveled and is an exponentially decay- 
			ing function of the distance of the traversed path. Traveling within a sentence is 
			made cheaper than across sentence boundaries, which in turn is cheaper than across 
			paragraph boundaries. Given a pair of document graphs, common nodes are identi- 
			fied either by sharing the same stem or by being synonyms. Analogously, difference 
			nodes are those that are not common. For each sentence in both documents, two 
			scores are computed: one score that reflects the presence of common nodes, which 
			is computed as the average weight of these nodes; and another score that computes 
			instead the average weights of difference nodes. Both scores are computed after 
			spreading activation. In the end, the sentences that have higher common and dif- 
			ferent scores are highlighted, the user being able to specify the maximal number of 
			common and different sentences to control the output. In the future, the authors 
			expect to use these structure to actually compose abstractive summaries, rather 
			than just highlighting pieces of text. 
			3.4 Centroid-based Summarization 
			Although clustering techniques were already being employed by McKeown et al. 
			(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000) 
			pioneered the use of cluster centroids to play a central role in summarization. A full 
			description of the centroid-based approach that underlies the MEAD system can 
			be found in (Radev et al., 2004); here we sketch briefly the main points. Perhaps 
			the most appealing feature is the fact that it does not make use of any language 
			generation module, unlike most previous systems. All documents are modeled as 
			bags-of-words. The system is also easily scalable and domain-independent. 
			The first stage consists of topic detection, whose goal is to group together news 
			articles that describe the same event. To accomplish this task, an agglomerative 
			clustering algorithm is used that operates over the TF-IDF vector representations 
			of the documents, successively adding documents to clusters and recomputing the 
			16 
			centroids according to 
			cj = dCj 
			 
			d 
			|Cj| 
			(6) 
			where cj is the centroid of the j-th cluster, Cj is the set of documents that belong 
			to that cluster, its cardinality being |Cj|, and  
			d is a "truncated version" of d that 
			vanishes on those words whose TF-IDF scores are below a threshold. Centroids 
			can thus be regarded as pseudo-documents that include those words whose TF- 
			IDF scores are above a threshold in the documents that constitute the cluster. Each 
			event cluster is a collection of (typically 2 to 10) news articles from multiple sources, 
			chronologically ordered, describing an event as it develops over time. 
			The second stage uses the centroids to identify sentences in each cluster that 
			are central to the topic of the entire cluster. In (Radev et al., 2000), two metrics 
			are defined that resemble the two summands in the MMR (see Section 3.2): cluster- 
			based relative utility (CBRU) and cross-sentence informational subsumption (CSIS). 
			The first accounts for how relevant a particular sentence is to the general topic of 
			the entire cluster; the second is a measure of redundancy among sentences. Unlike 
			MMR, these metrics are not query-dependent. Given one cluster C of documents 
			segmented into n sentences, and a compression rate R, a sequence of nR sentences 
			are extracted in the same order as they appear in the original documents, which in 
			turn are ordered chronologically. The selection of the sentences is made by approx- 
			imating their CBRU and CSIS.19 For each sentence si, three different features are 
			used: 
			* Its centroid value (Ci), defined as the sum of the centroid values of all the 
			words in the sentence, 
			* A positional value (Pi), that is used to make leading sentences more important. 
			Let Cmax be the centroid value of the highest ranked sentence in the document. 
			Then Pi = n-i+1 
			n 
			Cmax. 
			* The first-sentence overlap (Fi), defined as the inner product between the word 
			occurrence vector of sentence i and that of the first sentence of the document. 
			The final score of each sentence is a combination of the three scores above minus a 
			redundancy penalty (Rs) for each sentence that overlaps highly ranked sentences. 
			3.5 Multilingual Multi-document Summarization 
			Evans (2005) addresses the task of summarizing documents written in multiple 
			languages; this had already been sketched by Hovy and Lin (1999). Multilingual 
			summarization is still at an early stage, but this framework looks quite useful for 
			newswire applications that need to combine information from foreign news agen- 
			cies. Evans (2005) considered the scenario where there is a preferred language in 
			which the summary is to be written, and multiple documents in the preferred and 
			19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details). 
			17 
			in foreign languages are available. In their experiments, the preferred language was 
			English and the documents are news articles in English and Arabic. The rationale is 
			to summarize the English articles without discarding the information contained in 
			the Arabic documents. The IBM's statistical machine translation system is first ap- 
			plied to translate the Arabic documents to English. Then a search is made, for each 
			translated text unit, to see whether there is a similar sentence or not in the English 
			documents. If so, and if the sentence is found relevant enough to be included in the 
			summary, the similar English sentence is included instead of the Arabic-to-English 
			translation. This way, the final summary is more likely to be grammatical, since 
			machine translation is known to be far from perfect. On the other hand, the result 
			is also expected to have higher coverage than using just the English documents, 
			since the information contained in the Arabic documents can help to decide about 
			the relevance of each sentence. In order to measure similarity between sentences, a 
			tool named SimFinder20 was employed: this is a tool for clustering text based on 
			similarity over a variety of lexical and syntactic features using a log-linear regression 
			model. 
			4 Other Approaches to Summarization 
			This section describes briefly some unconventional approaches that, rather than 
			aiming to build full summarization systems, investigate some details that underlie 
			the summarization process, and that we conjecture to have a role to play in future 
			research on this field. 
			4.1 Short Summaries 
			Witbrock and Mittal (1999) claim that extractive summarization is not very pow- 
			erful in that the extracts are not concise enough when very short summaries are 
			required. They present a system that generated headline style summaries. The cor- 
			pus used in this work was newswire articles from Reuters and the Associated Press, 
			publicly available at the LDC21. The system learned statistical models of the rela- 
			tionship between source text units and headline units. It attempted to model both 
			the order and the likelihood of the appearance of tokens in the target documents. 
			Both the models, one for content selection and the other for surface realization were 
			used to co-constrain each other during the search in the summary generation task. 
			For content selection, the model learned a translation model between a docu- 
			ment and its summary (Brown et al., 1993). This model in the simplest case can be 
			thought as a mapping between a word in the document and the likelihood of some 
			word appearing in the summary. To simplify the model, the authors assumed that 
			the probability of a word appearing in a summary is independent of its structure. 
			This mapping boils down to the fact that the probability of a particular summary 
			20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder. 
			21See http://ldc.upenn.edu. 
			18 
			candidate is the product of the probabilities of the summary content and that con- 
			tent being expressed using a particular structure. 
			The surface realization model used was a bigram model. Viterbi beam search 
			was used to efficiently find a near-optimal summary. The Markov assumption was 
			violated by using backtracking at every state to strongly discourage paths that 
			repeated terms, since bigrams that start repeating often seem to pathologically 
			overwhelm the search otherwise. To evaluate the system, the authors compared 
			its output against the actual headlines for a set of input newswire stories. Since 
			phrasing could not be compared, they compared the generated headlines against 
			the actual headlines, as well as the top ranked summary sentence of the story. Since 
			the system did not have a mechanism to determine the optimal length of a headline, 
			six headlines for each story were generated, ranging in length from 4 to 10 words 
			and they measured the term-overlap between each of the generated headlines and 
			the test. For headline length 4, there was 0.89 overlap in the headline and there was 
			0.91 overlap amongst the top scored sentence, indicating useful results. 
			4.2 Sentence Compression 
			Knight and Marcu (2000) introduced a statistical approach to sentence compression. 
			The authors believe that understanding the simpler task of compressing a sentence 
			may be a fruitful first step to later tackle the problems of single and multi-document 
			summarization. 
			Sentence compression is defined as follows: given a sequence of words W = 
			w1w2 . . . wn that constitute a sentence, find a subsequence wi1 
			wi2 
			. . . wik 
			, with 
			1  i1 < i2 < . . . ik  n, that is a compressed version of W. Note that there 
			are 2n possibilities of output. Knight and Marcu (2000) considered two different 
			approaches: one that is inspired by the noisy-channel model, and another one based 
			on decision trees. Due to its simplicity and elegance, we describe the first approach 
			here. 
			The noisy-channel model considers that one starts with a short summary s, 
			drawn according to the source model P(s), which is then subject to channel noise to 
			become the full sentence t, in a process guided by the channel model P(t|s). When 
			the string t is observed, one wants to recover the original summary according to: 
			 
			s = arg max 
			s 
			P(s|t) = arg max 
			s 
			P(s)P(t|s). (7) 
			This model has the advantage of decoupling the goals of producing a short text that 
			looks grammatical (incorporated in the source model) and of preserving important 
			information (which is done through the channel model). In (Knight and Marcu, 
			2000), the source and channel models are simple models inspired by probabilistic 
			context-free grammars (PCFGs). The following probability mass functions are de- 
			fined over parse trees rather than strings: Ptree(s), the probability of a parse tree 
			that generates s, and Pexpand tree(t|s), the probability that a small parse tree that 
			generates s is expanded to a longer one that generates t. 
			19 
			The sentence t is first parsed by using Collins' parser (Collins, 1999). Then, 
			rather than computing Ptree(s) over all the 2n hypotheses for s, which would be 
			exponential in the sentence length, a shaded-forest structure is used: the parse 
			tree of t is traversed and the grammar (learned from the Penn Treebank22) is used 
			to check recursively which nodes may be removed from each production in order 
			to achieve another valid production. This algorithm allows to compute efficiently 
			Ptree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually, 
			the noisy channel model works the other way around: summaries are the original 
			strings that are expanded via expansion templates. Expansion operations have the 
			effect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) and 
			Pexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigram 
			distribution over the leaves of the tree (i.e. the words). In the end, the log probability 
			is (heuristically) divided by the length of the sentence s in order not to penalize 
			excessively longer sentences (this is done commonly in speech recognition). 
			More recently, Daum 
			e III and Marcu (2002) extended this approach to document 
			compression by using rhetorical structure theory as in Marcu (1998a), where the 
			entire document is represented as a tree, hence allowing not only to compress relevant 
			sentences, but also to drop irrelevant ones. In this framework, Daum 
			e III and Marcu 
			(2004) employed kernel methods to decide for each node in the tree whether or not 
			it should be kept. 
			4.3 Sequential document representation 
			We conclude this section by mentioning some recent work that concerns document 
			representation, with applications in summarization. In the bag-of-words representa- 
			tion (Salton et al., 1975) each document is represented as a sparse vector in a very 
			large Euclidean space, indexed by words in the vocabulary V . A well-known tech- 
			nique in information retrieval to capture word correlation is latent semantic indexing 
			(LSI), that aims to find a linear subspace of dimension k  |V | where documents 
			may be approximately represented by their projections. 
			These classical approaches assume by convenience that Euclidean geometry is 
			a proper model for text documents. As an alternative, Gous (1999) and Hall and 
			Hofmann (2000) used the framework of information geometry (Amari and Nagaoka, 
			2001) to generalize LSI to the multinomial manifold, which can be identified with 
			the probability simplex 
			Pn-1 = x  Rn | 
			n 
			i=1 
			xi = 1, xi  0 for i = 1, . . . , n . (8) 
			Instead of finding a linear subspace, as in the Euclidean case, they learn a subman- 
			ifold of Pn-1. To illustrate this idea, Gous (1999) split a book (Machiavelli's The 
			Prince) into several text blocks (its numbered pages), considered each page as a 
			point in P|V |-1, and projected data into a 2-dimensional submanifold. The result is 
			22See http://www.cis.upenn.edu/~treebank/. 
			20 
			the representation of the book as a sequential path in R2, tracking the evolution of 
			the subject matter of the book over the course of its pages (see Fig. 5). Inspired by 
			Figure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex- 
			tracted from (Gous, 1999)). The inflection around page 85 reflects a real change in 
			the subject matter, where the book shifts from political theory to a more biograph- 
			ical discourse. 
			this framework, Lebanon et al. (2007) suggested representing a document as a sim- 
			plicial curve (i.e. a curve in the probability simplex), yielding the locally weighted 
			bag-of-words (lowbow) model. According to this representation, a length-normalized 
			document is a function x : [0, 1] x V  R+ such that 
			wjV 
			x(t, wj) = 1, for any t  [0, 1]. (9) 
			We can regard the document as a continuous signal, and x(t, wj) as expressing 
			the relevance of word wj at instant t. This generalizes both the pure sequential 
			representation and the (global) bag-of-words model. Let y = (y1, . . . , yn)  V n be 
			a n-length document. The pure sequential representation of y arises by defining 
			x = xseq with: 
			xseq(t, wj) = 
			1, if wj = y tn 
			0, if wj = y tn 
			, 
			(10) 
			where a denotes the smallest integer greater than a. The global bag-of-words 
			representation of x corresponds to defining x = xbow, where 
			xbow(, wj) = 
			1 
			0 
			xseq(t, wj)dt,   [0, 1], j = 1, . . . , |V |. (11) 
			In this case, the curve degenerates into a single point in the simplex, which is 
			the maximum likelihood estimate of the multinomial parameters. An intermediate 
			21 
			representation arises by smoothing (10) via a function f, : [0, 1]  R++, where 
			  [0, 1] and   R++ are respectively a location and a scale parameter. An 
			example of such a smoothing function is the truncated Gaussian defined in [0, 1] 
			and normalized. This allows defining the lowbow representation at  of the n-lenght 
			document (y1, . . . , yn)  V n as the function x : [0, 1] x V  R+ such that: 
			x(, wj) = 
			1 
			0 
			xseq(t, wj)f,(t)dt. (12) 
			The scale of the smoothing function controls the amount of locality/globality in 
			the document representation (see Fig. 6): when    we recover the global bow 
			representation (11); when   0, we approach the pure sequential representation 
			(10). 
			Figure 6: The lowbow representation of a document with |V | = 3, for several values 
			of the scale parameter  (extracted from (Lebanon, 2006)). 
			Representing a document as a simplicial curve allows us to characterize geomet- 
			rically several properties of the document. For example, the tangent vector field 
			along the curve describes sequential "topic trends" and their change; the curvature 
			measures the amount of wigglyness or deviation from a geodesic path. This prop- 
			erties can be useful for tasks like text segmentation or summarization; for example 
			plotting the velocity of the curve ||  
			x()|| along time offers a visualization of the doc- 
			ument where local maxima tend to correspond to topic boundaries (see (Lebanon 
			et al., 2007) for more information). 
			22 
			5 Evaluation 
			Evaluating a summary is a difficult task because there does not exist an ideal sum- 
			mary for a given document or set of documents. From papers surveyed in the previ- 
			ous sections and elsewhere in literature, it has been found that agreement between 
			human summarizers is quite low, both for evaluating and generating summaries. 
			More than the form of the summary, it is difficult to evaluate the summary con- 
			tent. Another important problem in summary evaluation is the widespread use of 
			disparate metrics. The absence of a standard human or automatic evaluation met- 
			ric makes it very hard to compare different systems and establish a baseline. This 
			problem is not present in other NLP problems, like parsing. Besides this, manual 
			evaluation is too expensive: as stated by Lin (2004), large scale manual evaluation 
			of summaries as in the DUC conferences would require over 3000 hours of human ef- 
			forts. Hence, an evaluation metric having high correlation with human scores would 
			obviate the process of manual evaluation. In this section, we would look at some im- 
			portant recent papers that have been able to create standards in the summarization 
			community. 
			5.1 Human and Automatic Evaluation 
			Lin and Hovy (2002) describe and compare various human and automatic metrics to 
			evaluate summaries. They focus on the evaluation procedure used in the Document 
			Understanding Conference 2001 (DUC-2001), where the Summary Evaluation En- 
			vironment (SEE) interface was used to support the human evaluation part. NIST 
			assessors in DUC-2001 compared manually written ideal summaries with summaries 
			generated automatically by summarization systems and baseline summaries. Each 
			text was decomposed into a list of units (sentences) and displayed in separate win- 
			dows in SEE. To measure the content of summaries, assessors stepped through each 
			model unit (MU) from the ideal summaries and marked all system units (SU) shar- 
			ing content with the current model unit, rating them with scores in the range 1 - 4 
			to specify that the marked system units express all (4), most (3), some (2) or hardly 
			any (1) of the content of the current model unit. Grammaticality, cohesion, and co- 
			herence were also rated similarly by the assessors. The weighted recall at threshold 
			t (where t range from 1 to 4) is then defined as 
			Recallt = 
			Number of MUs marked at or above t 
			Number of MUs in the model summary 
			. (13) 
			An interesting study is presented that shows how unstable the human markings 
			for overlapping units are. For multiple systems, the coverage scores assigned to the 
			same units were different by human assessors 18% of the time for the single document 
			task and 7.6% of the time for multi-document task. The authors also observe that 
			inter-human agreement is quite low in creating extracts from documents ( 40% for 
			single-documents and  29% for multi-documents). To overcome the instability of 
			human evaluations, they proposed using automatic metrics for summary evaluation. 
			23 
			Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001), 
			they outline an accumulative n-gram matching score (which they call NAMS), 
			NAMS = a1 * NAM1 + a2 * NAM2 + a3 * NAM3 + a4 * NAM4, (14) 
			where the NAMn n-gram hit ratio is defined as: 
			# of matched n-grams between MU and S 
			total # of n-grams in MU 
			(15) 
			with S denoting here the whole system summary, and where only content words 
			were used in forming the n-grams. Different configurations of ai were tried; the 
			best correlation with human judgement (using Spearman's rank order correlation 
			coefficient) was achieved using a configuration giving 2/3 weight to bigram matches 
			and 1/3 to unigrams matches with stemming done by the Porter stemmer. 
			5.2 ROUGE 
			Lin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist- 
			ing Evaluation (ROUGE)23 that have become standards of automatic evaluation of 
			summaries. 
			In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s be 
			a summary generated automatically by some system. Let n(d) be a binary vector 
			representing the n-grams contained in a document d; the i-th component i 
			n 
			(d) is 1 
			if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is an 
			n-gram recall based statistic that can be computed as follows: 
			ROUGE-N(s) = rR 
			n(r), n(s) 
			rR 
			n(r), n(r) 
			, (16) 
			where ., . denotes the usual inner product of vectors. This measure is closely related 
			to BLEU which is a precision related measure. Unlike other measures previously 
			considered, ROUGE-N can be used for multiple reference summaries, which is quite 
			useful in practical situations. An alternative is taking the most similar summary in 
			the reference set, 
			ROUGE-Nmulti(s) = max 
			rR 
			n(r), n(s) 
			n(r), n(r) 
			. (17) 
			Another metric in (Lin, 2004) applies the concept of longest common subse- 
			quences24 (LCS). The rationale is: the longer the LCS between two summary sen- 
			tences, the more similar they are. Let r1, . . . , ru be the reference sentences of the 
			documents in R, and s a candidate summary (considered as a concatenation of 
			sentences). The ROUGE-L is defined as an LCS based F-measure: 
			ROUGE-L(s) = 
			(1 + 2)RLCSPLCS 
			RLCS + 2PLCS 
			(18) 
			23See http://openrouge.com/default.aspx. 
			24A subsequence of a string s = s1 
			. . . sn 
			is a string of the form si1 
			. . . sin 
			where 1  i1 
			< . . . in 
			 n. 
			24 
			where RLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			Pu 
			i=1 
			|ri| 
			, PLCS(s) = Pu 
			i=1 
			LCS(ri,s) 
			|s| 
			, |x| denotes the length of 
			sentence x, LCS(x, y) denotes the length of the LCS between sentences x and y, 
			and  is a (usually large) parameter to balance precision and recall. Notice that 
			the LCS function may be computed by a simple dynamic programming approach. 
			The metric (18) is further refined by including weights that penalize subsequence 
			matches that are not consecutive, yielding a new measure denoted ROUGE-W. 
			Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seen 
			as a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let 2(d) 
			be a binary vector indexed by ordered pairs of words; the i-th component i 
			2 
			(d) is 
			1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S is 
			computed as follows: 
			ROUGE-S(s) = 
			(1 + 2)RSPS 
			RS + 2PS 
			(19) 
			where RS(s) = Pu 
			i=1 
			2(ri),2(s) 
			Pu 
			i=1 
			2(ri),2(ri) 
			and PS(s) = Pu 
			i=1 
			2(ri),2(s) 
			2(s),2(s) 
			. 
			The various versions of ROUGE were evaluated by computing the correlation 
			coefficient between ROUGE scores and human judgement scores. ROUGE-2 per- 
			formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, and 
			ROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How- 
			ever, correlation achieved with human judgement for multi-document summarization 
			was not as high as single-document ones; improvement on this side of the paradigm 
			is an open research topic. 
			5.3 Information-theoretic Evaluation of Summaries 
			A very recent approach (Lin et al., 2006) proposes to use an information-theoretic 
			method to automatic evaluation of summaries. The central idea is to use a diver- 
			gence measure between a pair of probability distributions, in this case the Jensen- 
			Shannon divergence, where the first distribution is derived from an automatic sum- 
			mary and the second from a set of reference summaries. This approach has the 
			advantage of suiting both the single-document and the multi-document summariza- 
			tion scenarios. 
			Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singleton 
			set in the case of single-document summarization). Assume that a distribution 
			parameterized by R generates reference summaries of the documents in D. The 
			task of summarization can be seen as that of estimating R. Analogously, assume 
			that every summarization system is governed by some distribution parameterized 
			by A. Then, we may define a good summarizer as one for which A is close to R. 
			One information-theoretic measure between distributions that is adequate for this 
			is the KL divergence (Cover and Thomas, 1991), 
			KL(pA ||pR ) = 
			m 
			i=1 
			pA 
			i 
			log 
			pA 
			i 
			pR 
			i 
			. (20) 
			However, the KL divergence is unbounded and goes to infinity whenever pA 
			i 
			vanishes 
			25 
			and pR 
			i 
			does not, which requires using some kind of smoothing when estimating the 
			distributions. Lin et al. (2006) claims that the measure used here should also be 
			symmetric,25 another thing that the KL divergence is not. Hence, they propose to 
			use the Jensen-Shannon divergence which is bounded and symmetric:26 
			JS(pA ||pR ) = 
			1 
			2 
			KL(pA ||r) + 
			1 
			2 
			KL(pR ||r) = 
			= H(r) - 
			1 
			2 
			H(pA ) - 
			1 
			2 
			H(pA ), (21) 
			where r = 1 
			2 
			pA + 1 
			2 
			pR is the average distribution. 
			To evaluate a summary SA given a reference summary SR, the authors propose 
			to use the negative JS divergence between the estimates of pA and pR given the 
			summaries, 
			score(SA|SR) = -JS(p 
			A ||p 
			R ) (22) 
			The parameters are estimated via a posteriori maximization assuming a multi- 
			nomial generation model for each summary (which means that they are modeled as 
			bags-of-words) and using Dirichlet priors (the conjugate priors of the multinomial 
			family). So: 
			 
			A = arg max 
			A 
			p(SA|A)p(A), (23) 
			where (m being the number of distinct words, a1, . . . , am being the word counts in 
			the summary, a0 = m 
			i=1 
			ai) 
			p(SA|A) = 
			(a0 + 1) 
			m 
			i=1 
			(ai + 1) 
			m 
			i=1 
			A,i 
			ai (24) 
			and 
			p(A) = 
			(0) 
			m 
			i=1 
			(i) 
			m 
			i=1 
			A,i 
			i-1 (25) 
			where i are hyper-parameters and 0 = m 
			i=1 
			i. After some algebra, we get 
			 
			A,i = 
			ai + i - 1 
			a0 + 0 - m 
			(26) 
			which is similar to MLE with smoothing.27  
			R is estimated analogously using the 
			reference summary SR. Not surprisingly, if we have more than one reference sum- 
			mary, the MAP estimation given all summaries equals MAP estimation given their 
			concatenation into a single summary. 
			25However, the authors do not give much support for this claim. In our view, there is no reason 
			to require symmetry. 
			26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfies 
			the axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethora 
			of properties that are presented elsewhere, but this is out of scope of this survey. 
			27In particular if i 
			= 1 it is just maximum likelihood estimation (MLE). 
			26 
			The authors experimented three automatic evaluation schemes (JS with smooth- 
			ing, JS without smoothing, and KL divergence) against manual evaluation; the best 
			performance was achieved by JS without smoothing. This is not surprising since, as 
			seen above, the JS divergence is bounded, unlike the KL divergence, and so it does 
			not require smoothing. Smoothing has the effect of pulling the two distributions 
			more close to the uniform distribution. 
			2 Sentence Boundary Detection 
			Sentence Boundary Detection (SBD) has been a major research topic science 
			ASR moved to more general domains as conversational speech [17,24,26]. Per- 
			formance of ASR systems has improved over the years with the inclusion and 
			combination of new Deep Neural Networks methods [5,9,33]. As a general rule, 
			the output of ASR systems lacks of any syntactic information such as capital- 
			ization and sentence boundaries, showing the interest of ASR systems to obtain 
			the correct sequence of words with almost no concern of the overall structure of 
			the document [8]. 
			Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence 
			Boundary Disambiguation. This task aims to segment a formal written text into 
			well formed sentences based on the existent punctuation marks [11,19,20,29]. In 
			this context a sentence is defined (for English) by the Cambridge Dictionary1 
			as: 
			"a group of words, usually containing a verb, that expresses a thought in 
			the form of a statement, question, instruction, or exclamation and starts 
			with a capital letter when written". 
			PMD carries certain complications, some given the ambiguity of punctuation 
			marks within a sentence. A period can denote an acronym, an abbreviation, the 
			end of the sentence or a combination of them as in the following example: 
			The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. 
			director Christopher A. Wray next Thursday at 8 p.m. 
			However its difficulties, DPM profits of morphological and lexical information 
			to achieve a correct sentence segmentation. By contrast, segmenting an ASR 
			transcript should be done without any (or almost any) lexical information and 
			a flurry definition of sentence. 
			1 https://dictionary.cambridge.org/. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 121 
			The obvious division in spoken language may be considered speaker utter- 
			ances. However, in a normal conversation or even in a monologue, the way ideas 
			are organized differs largely from written text. This differences, added to disflu- 
			encies like revisions, repetitions, restarts, interruptions and hesitations make the 
			definition of a sentence unclear thus complicating the segmentation task [27]. 
			Table 1 exemplifies some of the difficulties that are present when working with 
			spoken language. 
			Table 1. Sentence Boundary Detection example 
			Speech transcript SBD applied to transcript 
			two two women can look out after 
			a kid so bad as a man and a 
			woman can so you can have a you 
			can have a mother and a father 
			that that still don't do right with 
			the kid and you can have to men 
			that can so as long as the love 
			each other as long as they love 
			each other it doesn't matter 
			two // two women can look out 
			after a kid so bad as a man and a 
			woman can // so you can have a 
			// you can have a mother and a 
			father that // that still don't do 
			right with the kid and you can 
			have to men that can // so as 
			long as the love each other // as 
			long as they love each other it 
			doesn't matter// 
			Stolcke and Shriberg [26] considered a set of linguistic structures as segments 
			including the following list: 
			- Complete sentences 
			- Stand-alone sentences 
			- Disfluent sentences aborted in mid-utterance 
			- Interjections 
			- Back-channel responses. 
			In [17], Meteer and Iyer divided speaker utterances into segments, consisting 
			each of a single independent clause. A segment was considered to begin either 
			at the beginning of an utterance, or after the end of the preceding segment. Any 
			dysfluency between the end of the previous segments and the begging of current 
			one was considered part of the current segments. 
			Rott and  
			Cerva [23] aimed to summarize news delivered orally segmenting the 
			transcripts into "something that is similar to sentences". They used a syntactic 
			analyzer to identify the phrases within the text. 
			A wide study focused in unbalanced data for the SBD task was performed 
			by Liu et al. [15]. During this study they followed the segmentation scheme pro- 
			posed by the Linguistic Data Consortium2 on the Simple Metadata Annotation 
			Specification V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in 
			Semantic Units. 
			2 https://www.ldc.upenn.edu/. 
			122 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			A Semantic Unit (SU) is considered to be an atomic element of the transcript 
			that manages to express a complete thought or idea on the part of the speaker 
			[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text, 
			but other times (the most part of them) a SU corresponds to a phrase or a single 
			word. 
			SUs seem to be an inclusive conception of a segment, they embrace different 
			previous segment definitions and are flexible enough to deal with the majority 
			of spoken language troubles. For these reasons we will adopt SUs as our segment 
			definition. 
			2.1 Sentence Boundary Evaluation 
			SBD research has been focused on two different aspects; features and methods. 
			Regarding the features, some work focused on acoustic elements like pauses 
			duration, fundamental frequencies, energy, rate of speech, volume change and 
			speaker turn [10,12,14]. 
			The other kind of features used in SBD are textual or lexical features. They 
			rely on the transcript content to extract features like bag-of-word, POS tags or 
			word embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features 
			have also been explored [1,13,14,32], which is advantageous when both audio 
			signal and transcript are available. 
			With respect to the methods used for SBD, they mostly rely on statisti- 
			cal/neural machine translation [12,22], language models [8,15,18,26], conditional 
			random fields [16,30] and deep neural networks [3,7,29]. 
			Despite their differences in features and/or methodology, almost all previous 
			cited research share a common element; the evaluation methodology. Metrics as 
			Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) 
			are used to evaluate the proposed system against one reference. As discussed 
			in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial 
			to have a good segmentation. But comparing the output of a system against a 
			unique reference will provide a reliable score to decide if the system is good or 
			bad? 
			Bohac et al. [1] compared the human ability to punctuate recognized spon- 
			taneous speech. They asked 10 people (correctors) to punctuate about 30 min of 
			ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks 
			placed by correctors varied between 557 and 801; this means a difference of 244 
			segments for the same transcript. Over all correctors, the absolute consensus for 
			period (.) was only 4.6% caused by the replacement of other punctuation marks 
			as semicolons (;) and exclamation marks (!). These results are understandable if 
			we consider the difficulties presented previously in this section. 
			To our knowledge, the amount of studies that have tried to target the sentence 
			boundary evaluation with a multi-reference approach is very small. In [1], Bohac 
			et al. evaluated the overall punctuation accuracy for Czech in a straightforward 
			multi-reference framework. They considered a period (.) valid if at least five of 
			their 10 correctors agreed on its position. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 123 
			Kol 
			a 
			r and Lamel [13] considered two independent references to evaluate their 
			system and proposed two approaches. The fist one was to calculate the SER for 
			each of one the two available references and then compute their mean. They 
			found this approach to be very strict because for those boundaries where no 
			agreement between references existed, the system was going to be partially wrong 
			even the fact that it has correctly predicted the boundary. Their second app- 
			roach tried to moderate the number of unjust penalizations. For this case, a 
			classification was considered incorrect only if it didn't match either of the two 
			references. 
			These two examples exemplify the real need and some straightforward solu- 
			tions for multi-reference evaluation metrics. However, we think that it is possible 
			to consider in a more inclusive approach the similarities and differences that mul- 
			tiple references could provide into a sentence boundary evaluation protocol. 
			3 Window-Based Sentence Boundary Evaluation 
			Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic 
			multi-reference sentence boundary evaluation protocol which considers the per- 
			formance of a candidate segmentation over a set of segmentation references and 
			the agreement between those references. 
			Let R = {R1 
			, R2 
			, ..., Rm 
			} be the set of all available references given a tran- 
			script T = {t1 
			, t2 
			, ..., tn 
			}, where tj 
			is the jth word in the transcript; a reference 
			Ri 
			is defined as a binary vector in terms of the existent SU boundaries in T. 
			Ri 
			= {b1 
			, b2 
			, ..., bn 
			} (1) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			Given a transcript T, the candidate segmentation CT 
			is defined similar to Ri 
			. 
			CT 
			= {b1 
			, b2 
			, ..., bn 
			} (2) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			3.1 General Reference and Agreement Ratio 
			A General Reference (RG 
			) is then constructed to calculate the agreement ratio 
			between all references in. It is defined by the boundary frequencies of each ref- 
			erence Ri 
			 R. 
			RG 
			= {d1 
			, d2 
			, ..., dn 
			} (3) 
			where 
			124 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			dj 
			= 
			m 
			i=1 
			tij 
			tj 
			 T, dj 
			= [0, m] (4) 
			The Agreement Ratio (RGAR 
			) is needed to get a numerical value of the dis- 
			tribution of SU boundaries over R. A value of RGAR 
			close to 0 means a low 
			agreement between references in R, while RGAR 
			= 1 means a perfect agreement 
			(Ri 
			 R, Ri 
			= Ri+1 
			|i = 1, ..., m - 1) in R. 
			RGAR 
			= 
			RGP B 
			RGHA 
			(5) 
			In the equation above, RGP B 
			corresponds to the ponderated common boundaries 
			of RG 
			and RGHA 
			to its hypothetical maximum agreement. 
			RGP B 
			= 
			n 
			j=1 
			dj 
			[dj 
			 2] (6) 
			RGHA 
			= m x 
			dj 
			RG 
			1 [dj 
			= 0] (7) 
			3.2 Window-Boundaries Reference 
			In Sect. 2 we discussed about how disfluencies complicate SU segmentation. In a 
			multi-reference environment this causes disagreement between references around 
			a same SU boundary. The way WiSeBE handle disagreements produced by dis- 
			fluencies is with a Window-boundaries Reference (RW 
			) defined as: 
			RW 
			= {w1 
			, w2 
			, ..., wp 
			} (8) 
			where each window wk 
			considers one or more boundaries dj 
			from RG 
			with a 
			window separation limit equal to RWl 
			. 
			wk 
			= {dj 
			, dj+1 
			, dj+2 
			, ...} (9) 
			3.3 W iSeBE 
			WiSeBE is a normalized score dependent of (1) the performance of CT 
			over RW 
			and (2) the agreement between all references in R. It is defined as: 
			WiSeBE = F1RW 
			x RGAR 
			WiSeBE = [0, 1] (10) 
			where F1RW 
			corresponds to the harmonic mean of precision and recall of CT 
			with respect to RW 
			(Eq. 11), while RGAR 
			is the agreement ratio defined in (5). 
			RGAR 
			can be interpreted as a scaling factor; a low value will penalize the overall 
			WiSeBE score given the low agreement between references. By contrast, for a 
			high agreement in R (RGAR 
			 1), WiSeBE  F1RW 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 125 
			F1RW 
			= 2 x 
			precisionRW 
			x recallRW 
			precisionRW 
			+ recallRW 
			(11) 
			precisionRW 
			= bj 
			CT 
			1 [bj 
			= 1, bj 
			 w w  RW 
			] 
			bj 
			CT 
			1 [bj 
			= 1] 
			(12) 
			recallRW 
			= wk 
			RW 
			1 [wk 
			b b  CT 
			] 
			p 
			(13) 
			Equations 12 and 13 describe precision and recall of CT 
			with respect to RW 
			. 
			Precision is the number of boundaries bj 
			inside any window wk 
			from RW 
			divided 
			by the total number of boundaries bj 
			in CT 
			. Recall corresponds to the number 
			of windows w with at least one boundary b divided by the number of windows 
			w in RW 
			. 
			4 Evaluating with W iSeBE 
			To exemplify the WiSeBE score we evaluated and compared the performance 
			of two different SBD systems over a set of YouTube videos in a multi-reference 
			environment. The first system (S1) employs a Convolutional Neural Network to 
			determine if the middle word of a sliding window corresponds to a SU bound- 
			ary or not [6]. The second approach (S2) by contrast, introduces a bidirectional 
			Recurrent Neural Network model with attention mechanism for boundary detec- 
			tion [28]. 
			In a first glance we performed the evaluation of the systems against each 
			one of the references independently. Then, we implemented a multi-reference 
			evaluation with WiSeBE. 
			4.1 Dataset 
			We focused evaluation over a small but diversified dataset composed by 10 
			YouTube videos in the English language in the news context. The selected videos 
			cover different topics like technology, human rights, terrorism and politics with 
			a length variation between 2 and 10 min. To encourage the diversity of content 
			format we included newscasts, interviews, reports and round tables. 
			During the transcription phase we opted for a manual transcription process 
			because we observed that using transcripts from an ASR system will difficult 
			in a large degree the manual segmentation process. The number of words per 
			transcript oscilate between 271 and 1,602 with a total number of 8,080. 
			We gave clear instructions to three evaluators (ref1 
			, ref2 
			, ref3 
			) of how seg- 
			mentation was needed to be perform, including the SU concept and how punctu- 
			ation marks were going to be taken into account. Periods (.), question marks (?), 
			exclamation marks (!) and semicolons (;) were considered SU delimiters (bound- 
			aries) while colons (:) and commas (,) were considered as internal SU marks. 
			The number of segments per transcript and reference can be seen in Table 2. An 
			interesting remark is that ref3 
			assigns about 43% less boundaries than the mean 
			of the other two references. 
			126 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 2. Manual dataset segmentation 
			Reference v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			ref1 38 42 17 11 55 87 109 72 55 16 502 
			ref2 33 42 16 14 54 98 92 65 51 20 485 
			ref3 23 20 10 6 39 39 76 30 29 9 281 
			4.2 Evaluation 
			We ran both systems (S1 & S2) over the manually transcribed videos obtaining 
			the number of boundaries shown in Table 3. In general, it can be seen that S1 
			predicts 27% more segments than S2. This difference can affect the performance 
			of S1, increasing its probabilities of false positives. 
			Table 3. Automatic dataset segmentation 
			System v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			S1 53 38 15 13 54 108 106 70 71 11 539 
			S2 38 37 12 11 36 92 86 46 53 13 424 
			Table 4 condenses the performance of both systems evaluated against each 
			one of the references independently. If we focus on F1 scores, performance of both 
			systems varies depending of the reference. For ref1 
			, S1 was better in 5 occasions 
			with respect of S2; S1 was better in 2 occasions only for ref2 
			; S1 overperformed 
			S2 in 3 occasions concerning ref3 
			and in 4 occasions for mean (bold). 
			Also from Table 4 we can observe that ref1 
			has a bigger similarity to S1 in 
			5 occasions compared to other two references, while ref2 
			is more similar to S2 
			in 7 transcripts (underline). 
			After computing the mean F1 scores over the transcripts, it can be concluded 
			that in average S2 had a better performance segmenting the dataset compared 
			to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of 
			the dataset? Regardless all references have been considered, nor agreement or 
			disagreement between them has been taken into account. 
			All values related to the WiSeBE score are displayed in Table 5. The Agree- 
			ment Ratio (RGAR 
			) between references oscillates between 0.525 for v8 
			and 0.767 
			for v5 
			. The lower the RGAR 
			, the bigger the penalization WiSeBE will give to 
			the final score. A good example is S2 for transcript v4 
			where F1RW 
			reaches a 
			value of 0.800, but after considering RGAR 
			the WiSeBE score falls to 0.462. 
			It is feasible to think that if all references are taken into account at the same 
			time during evaluation (F1RW 
			), the score will be bigger compared to an average 
			of independent evaluations (F1mean 
			); however this is not always true. That is 
			the case of S1 in v10, which present a slight decrease for F1RW 
			compared to 
			F1mean 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 127 
			Table 4. Independent multi-reference evaluation 
			Transcript System ref1 ref2 ref3 Mean 
			P R F1 P R F1 P R F1 P R F1 
			v1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432 
			S2 0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480 
			v2 S1 0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578 
			S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549 
			v3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270 
			S2 0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325 
			v4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505 
			S2 0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735 
			v5 S1 0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592 
			S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499 
			v6 S1 0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443 
			S2 0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457 
			v7 S1 0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518 
			S2 0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539 
			v8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429 
			S2 0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487 
			v9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459 
			S2 0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541 
			v10 S1 0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582 
			S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487 
			Mean scores S1 -- 0.520 -- 0.510 -- 0.404 -- 0.481 
			S2 -- 0.543 -- 0.554 -- 0.433 -- 0.510 
			An important remark is the behavior of S1 and S2 concerning v6 
			. If evalu- 
			ated without considering any (dis)agreement between references (F1mean 
			), S2 
			overperforms S1; this is inverted once the systems are evaluated with WiSeBE. 
			2 Sentence Boundary Detection 
			Sentence Boundary Detection (SBD) has been a major research topic science 
			ASR moved to more general domains as conversational speech [17,24,26]. Per- 
			formance of ASR systems has improved over the years with the inclusion and 
			combination of new Deep Neural Networks methods [5,9,33]. As a general rule, 
			the output of ASR systems lacks of any syntactic information such as capital- 
			ization and sentence boundaries, showing the interest of ASR systems to obtain 
			the correct sequence of words with almost no concern of the overall structure of 
			the document [8]. 
			Similar to SBD is the Punctuation Marks Disambiguation (PMD) or Sentence 
			Boundary Disambiguation. This task aims to segment a formal written text into 
			well formed sentences based on the existent punctuation marks [11,19,20,29]. In 
			this context a sentence is defined (for English) by the Cambridge Dictionary1 
			as: 
			"a group of words, usually containing a verb, that expresses a thought in 
			the form of a statement, question, instruction, or exclamation and starts 
			with a capital letter when written". 
			PMD carries certain complications, some given the ambiguity of punctuation 
			marks within a sentence. A period can denote an acronym, an abbreviation, the 
			end of the sentence or a combination of them as in the following example: 
			The U.S. president, Mr. Donald Trump, is meeting with the F.B.I. 
			director Christopher A. Wray next Thursday at 8 p.m. 
			However its difficulties, DPM profits of morphological and lexical information 
			to achieve a correct sentence segmentation. By contrast, segmenting an ASR 
			transcript should be done without any (or almost any) lexical information and 
			a flurry definition of sentence. 
			1 https://dictionary.cambridge.org/. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 121 
			The obvious division in spoken language may be considered speaker utter- 
			ances. However, in a normal conversation or even in a monologue, the way ideas 
			are organized differs largely from written text. This differences, added to disflu- 
			encies like revisions, repetitions, restarts, interruptions and hesitations make the 
			definition of a sentence unclear thus complicating the segmentation task [27]. 
			Table 1 exemplifies some of the difficulties that are present when working with 
			spoken language. 
			Table 1. Sentence Boundary Detection example 
			Speech transcript SBD applied to transcript 
			two two women can look out after 
			a kid so bad as a man and a 
			woman can so you can have a you 
			can have a mother and a father 
			that that still don't do right with 
			the kid and you can have to men 
			that can so as long as the love 
			each other as long as they love 
			each other it doesn't matter 
			two // two women can look out 
			after a kid so bad as a man and a 
			woman can // so you can have a 
			// you can have a mother and a 
			father that // that still don't do 
			right with the kid and you can 
			have to men that can // so as 
			long as the love each other // as 
			long as they love each other it 
			doesn't matter// 
			Stolcke and Shriberg [26] considered a set of linguistic structures as segments 
			including the following list: 
			- Complete sentences 
			- Stand-alone sentences 
			- Disfluent sentences aborted in mid-utterance 
			- Interjections 
			- Back-channel responses. 
			In [17], Meteer and Iyer divided speaker utterances into segments, consisting 
			each of a single independent clause. A segment was considered to begin either 
			at the beginning of an utterance, or after the end of the preceding segment. Any 
			dysfluency between the end of the previous segments and the begging of current 
			one was considered part of the current segments. 
			Rott and  
			Cerva [23] aimed to summarize news delivered orally segmenting the 
			transcripts into "something that is similar to sentences". They used a syntactic 
			analyzer to identify the phrases within the text. 
			A wide study focused in unbalanced data for the SBD task was performed 
			by Liu et al. [15]. During this study they followed the segmentation scheme pro- 
			posed by the Linguistic Data Consortium2 on the Simple Metadata Annotation 
			Specification V5.0 guideline (SimpleMDE V5.0) [27], dividing the transcripts in 
			Semantic Units. 
			2 https://www.ldc.upenn.edu/. 
			122 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			A Semantic Unit (SU) is considered to be an atomic element of the transcript 
			that manages to express a complete thought or idea on the part of the speaker 
			[27]. Sometimes a SU corresponds to the equivalent of a sentence in written text, 
			but other times (the most part of them) a SU corresponds to a phrase or a single 
			word. 
			SUs seem to be an inclusive conception of a segment, they embrace different 
			previous segment definitions and are flexible enough to deal with the majority 
			of spoken language troubles. For these reasons we will adopt SUs as our segment 
			definition. 
			2.1 Sentence Boundary Evaluation 
			SBD research has been focused on two different aspects; features and methods. 
			Regarding the features, some work focused on acoustic elements like pauses 
			duration, fundamental frequencies, energy, rate of speech, volume change and 
			speaker turn [10,12,14]. 
			The other kind of features used in SBD are textual or lexical features. They 
			rely on the transcript content to extract features like bag-of-word, POS tags or 
			word embeddings [7,12,16,18,23,26,30]. Mixture of acoustic and lexical features 
			have also been explored [1,13,14,32], which is advantageous when both audio 
			signal and transcript are available. 
			With respect to the methods used for SBD, they mostly rely on statisti- 
			cal/neural machine translation [12,22], language models [8,15,18,26], conditional 
			random fields [16,30] and deep neural networks [3,7,29]. 
			Despite their differences in features and/or methodology, almost all previous 
			cited research share a common element; the evaluation methodology. Metrics as 
			Precision, Recall, F1-score, Classification Error Rate and Slot Error Rate (SER) 
			are used to evaluate the proposed system against one reference. As discussed 
			in Sect. 1, further NLP tasks rely on the result of SBD, meaning that is crucial 
			to have a good segmentation. But comparing the output of a system against a 
			unique reference will provide a reliable score to decide if the system is good or 
			bad? 
			Bohac et al. [1] compared the human ability to punctuate recognized spon- 
			taneous speech. They asked 10 people (correctors) to punctuate about 30 min of 
			ASR transcripts in Czech. For an average of 3,962 words, the punctuation marks 
			placed by correctors varied between 557 and 801; this means a difference of 244 
			segments for the same transcript. Over all correctors, the absolute consensus for 
			period (.) was only 4.6% caused by the replacement of other punctuation marks 
			as semicolons (;) and exclamation marks (!). These results are understandable if 
			we consider the difficulties presented previously in this section. 
			To our knowledge, the amount of studies that have tried to target the sentence 
			boundary evaluation with a multi-reference approach is very small. In [1], Bohac 
			et al. evaluated the overall punctuation accuracy for Czech in a straightforward 
			multi-reference framework. They considered a period (.) valid if at least five of 
			their 10 correctors agreed on its position. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 123 
			Kol 
			a 
			r and Lamel [13] considered two independent references to evaluate their 
			system and proposed two approaches. The fist one was to calculate the SER for 
			each of one the two available references and then compute their mean. They 
			found this approach to be very strict because for those boundaries where no 
			agreement between references existed, the system was going to be partially wrong 
			even the fact that it has correctly predicted the boundary. Their second app- 
			roach tried to moderate the number of unjust penalizations. For this case, a 
			classification was considered incorrect only if it didn't match either of the two 
			references. 
			These two examples exemplify the real need and some straightforward solu- 
			tions for multi-reference evaluation metrics. However, we think that it is possible 
			to consider in a more inclusive approach the similarities and differences that mul- 
			tiple references could provide into a sentence boundary evaluation protocol. 
			3 Window-Based Sentence Boundary Evaluation 
			Window-Based Sentence Boundary Evaluation (WiSeBE) is a semi-automatic 
			multi-reference sentence boundary evaluation protocol which considers the per- 
			formance of a candidate segmentation over a set of segmentation references and 
			the agreement between those references. 
			Let R = {R1 
			, R2 
			, ..., Rm 
			} be the set of all available references given a tran- 
			script T = {t1 
			, t2 
			, ..., tn 
			}, where tj 
			is the jth word in the transcript; a reference 
			Ri 
			is defined as a binary vector in terms of the existent SU boundaries in T. 
			Ri 
			= {b1 
			, b2 
			, ..., bn 
			} (1) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			Given a transcript T, the candidate segmentation CT 
			is defined similar to Ri 
			. 
			CT 
			= {b1 
			, b2 
			, ..., bn 
			} (2) 
			where 
			bj 
			= 
			1 if tj 
			is a boundary 
			0 otherwise 
			3.1 General Reference and Agreement Ratio 
			A General Reference (RG 
			) is then constructed to calculate the agreement ratio 
			between all references in. It is defined by the boundary frequencies of each ref- 
			erence Ri 
			 R. 
			RG 
			= {d1 
			, d2 
			, ..., dn 
			} (3) 
			where 
			124 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			dj 
			= 
			m 
			i=1 
			tij 
			tj 
			 T, dj 
			= [0, m] (4) 
			The Agreement Ratio (RGAR 
			) is needed to get a numerical value of the dis- 
			tribution of SU boundaries over R. A value of RGAR 
			close to 0 means a low 
			agreement between references in R, while RGAR 
			= 1 means a perfect agreement 
			(Ri 
			 R, Ri 
			= Ri+1 
			|i = 1, ..., m - 1) in R. 
			RGAR 
			= 
			RGP B 
			RGHA 
			(5) 
			In the equation above, RGP B 
			corresponds to the ponderated common boundaries 
			of RG 
			and RGHA 
			to its hypothetical maximum agreement. 
			RGP B 
			= 
			n 
			j=1 
			dj 
			[dj 
			 2] (6) 
			RGHA 
			= m x 
			dj 
			RG 
			1 [dj 
			= 0] (7) 
			3.2 Window-Boundaries Reference 
			In Sect. 2 we discussed about how disfluencies complicate SU segmentation. In a 
			multi-reference environment this causes disagreement between references around 
			a same SU boundary. The way WiSeBE handle disagreements produced by dis- 
			fluencies is with a Window-boundaries Reference (RW 
			) defined as: 
			RW 
			= {w1 
			, w2 
			, ..., wp 
			} (8) 
			where each window wk 
			considers one or more boundaries dj 
			from RG 
			with a 
			window separation limit equal to RWl 
			. 
			wk 
			= {dj 
			, dj+1 
			, dj+2 
			, ...} (9) 
			3.3 W iSeBE 
			WiSeBE is a normalized score dependent of (1) the performance of CT 
			over RW 
			and (2) the agreement between all references in R. It is defined as: 
			WiSeBE = F1RW 
			x RGAR 
			WiSeBE = [0, 1] (10) 
			where F1RW 
			corresponds to the harmonic mean of precision and recall of CT 
			with respect to RW 
			(Eq. 11), while RGAR 
			is the agreement ratio defined in (5). 
			RGAR 
			can be interpreted as a scaling factor; a low value will penalize the overall 
			WiSeBE score given the low agreement between references. By contrast, for a 
			high agreement in R (RGAR 
			 1), WiSeBE  F1RW 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 125 
			F1RW 
			= 2 x 
			precisionRW 
			x recallRW 
			precisionRW 
			+ recallRW 
			(11) 
			precisionRW 
			= bj 
			CT 
			1 [bj 
			= 1, bj 
			 w w  RW 
			] 
			bj 
			CT 
			1 [bj 
			= 1] 
			(12) 
			recallRW 
			= wk 
			RW 
			1 [wk 
			b b  CT 
			] 
			p 
			(13) 
			Equations 12 and 13 describe precision and recall of CT 
			with respect to RW 
			. 
			Precision is the number of boundaries bj 
			inside any window wk 
			from RW 
			divided 
			by the total number of boundaries bj 
			in CT 
			. Recall corresponds to the number 
			of windows w with at least one boundary b divided by the number of windows 
			w in RW 
			. 
			4 Evaluating with W iSeBE 
			To exemplify the WiSeBE score we evaluated and compared the performance 
			of two different SBD systems over a set of YouTube videos in a multi-reference 
			environment. The first system (S1) employs a Convolutional Neural Network to 
			determine if the middle word of a sliding window corresponds to a SU bound- 
			ary or not [6]. The second approach (S2) by contrast, introduces a bidirectional 
			Recurrent Neural Network model with attention mechanism for boundary detec- 
			tion [28]. 
			In a first glance we performed the evaluation of the systems against each 
			one of the references independently. Then, we implemented a multi-reference 
			evaluation with WiSeBE. 
			4.1 Dataset 
			We focused evaluation over a small but diversified dataset composed by 10 
			YouTube videos in the English language in the news context. The selected videos 
			cover different topics like technology, human rights, terrorism and politics with 
			a length variation between 2 and 10 min. To encourage the diversity of content 
			format we included newscasts, interviews, reports and round tables. 
			During the transcription phase we opted for a manual transcription process 
			because we observed that using transcripts from an ASR system will difficult 
			in a large degree the manual segmentation process. The number of words per 
			transcript oscilate between 271 and 1,602 with a total number of 8,080. 
			We gave clear instructions to three evaluators (ref1 
			, ref2 
			, ref3 
			) of how seg- 
			mentation was needed to be perform, including the SU concept and how punctu- 
			ation marks were going to be taken into account. Periods (.), question marks (?), 
			exclamation marks (!) and semicolons (;) were considered SU delimiters (bound- 
			aries) while colons (:) and commas (,) were considered as internal SU marks. 
			The number of segments per transcript and reference can be seen in Table 2. An 
			interesting remark is that ref3 
			assigns about 43% less boundaries than the mean 
			of the other two references. 
			126 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 2. Manual dataset segmentation 
			Reference v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			ref1 38 42 17 11 55 87 109 72 55 16 502 
			ref2 33 42 16 14 54 98 92 65 51 20 485 
			ref3 23 20 10 6 39 39 76 30 29 9 281 
			4.2 Evaluation 
			We ran both systems (S1 & S2) over the manually transcribed videos obtaining 
			the number of boundaries shown in Table 3. In general, it can be seen that S1 
			predicts 27% more segments than S2. This difference can affect the performance 
			of S1, increasing its probabilities of false positives. 
			Table 3. Automatic dataset segmentation 
			System v1 
			v2 
			v3 
			v4 
			v5 
			v6 
			v7 
			v8 
			v9 
			v10 Total 
			S1 53 38 15 13 54 108 106 70 71 11 539 
			S2 38 37 12 11 36 92 86 46 53 13 424 
			Table 4 condenses the performance of both systems evaluated against each 
			one of the references independently. If we focus on F1 scores, performance of both 
			systems varies depending of the reference. For ref1 
			, S1 was better in 5 occasions 
			with respect of S2; S1 was better in 2 occasions only for ref2 
			; S1 overperformed 
			S2 in 3 occasions concerning ref3 
			and in 4 occasions for mean (bold). 
			Also from Table 4 we can observe that ref1 
			has a bigger similarity to S1 in 
			5 occasions compared to other two references, while ref2 
			is more similar to S2 
			in 7 transcripts (underline). 
			After computing the mean F1 scores over the transcripts, it can be concluded 
			that in average S2 had a better performance segmenting the dataset compared 
			to S1, obtaining a F1 score equal to 0.510. But... What about the complexity of 
			the dataset? Regardless all references have been considered, nor agreement or 
			disagreement between them has been taken into account. 
			All values related to the WiSeBE score are displayed in Table 5. The Agree- 
			ment Ratio (RGAR 
			) between references oscillates between 0.525 for v8 
			and 0.767 
			for v5 
			. The lower the RGAR 
			, the bigger the penalization WiSeBE will give to 
			the final score. A good example is S2 for transcript v4 
			where F1RW 
			reaches a 
			value of 0.800, but after considering RGAR 
			the WiSeBE score falls to 0.462. 
			It is feasible to think that if all references are taken into account at the same 
			time during evaluation (F1RW 
			), the score will be bigger compared to an average 
			of independent evaluations (F1mean 
			); however this is not always true. That is 
			the case of S1 in v10, which present a slight decrease for F1RW 
			compared to 
			F1mean 
			. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 127 
			Table 4. Independent multi-reference evaluation 
			Transcript System ref1 ref2 ref3 Mean 
			P R F1 P R F1 P R F1 P R F1 
			v1 S1 0.396 0.553 0.462 0.377 0.606 0.465 0.264 0.609 0.368 0.346 0.589 0.432 
			S2 0.474 0.474 0.474 0.474 0.545 0.507 0.368 0.6087 0.459 0.439 0.543 0.480 
			v2 S1 0.605 0.548 0.575 0.711 0.643 0.675 0.368 0.700 0.483 0.561 0.630 0.578 
			S2 0.595 0.524 0.557 0.676 0.595 0.633 0.351 0.650 0.456 0.541 0.590 0.549 
			v3 S1 0.333 0.294 0.313 0.267 0.250 0.258 0.200 0.300 0.240 0.267 0.281 0.270 
			S2 0.417 0.294 0.345 0.417 0.313 0.357 0.250 0.300 0.273 0.361 0.302 0.325 
			v4 S1 0.615 0.571 0.593 0.462 0.545 0.500 0.308 0.667 0.421 0.462 0.595 0.505 
			S2 0.909 0.714 0.800 0.818 0.818 0.818 0.455 0.833 0.588 0.727 0.789 0.735 
			v5 S1 0.630 0.618 0.624 0.593 0.593 0.593 0.481 0.667 0.560 0.568 0.626 0.592 
			S2 0.667 0.436 0.527 0.611 0.407 0.489 0.500 0.462 0.480 0.593 0.435 0.499 
			v6 S1 0.491 0.541 0.515 0.454 0.563 0.503 0.213 0.590 0.313 0.386 0.565 0.443 
			S2 0.500 0.469 0.484 0.522 0.552 0.536 0.250 0.590 0.351 0.4234 0.537 0.457 
			v7 S1 0.594 0.578 0.586 0.462 0.533 0.495 0.406 0.566 0.473 0.487 0.559 0.518 
			S2 0.663 0.523 0.585 0.558 0.522 0.539 0.465 0.526 0.494 0.562 0.524 0.539 
			v8 S1 0.443 0.477 0.459 0.514 0.500 0.507 0.229 0.533 0.320 0.395 0.503 0.429 
			S2 0.609 0.431 0.505 0.652 0.417 0.508 0.370 0.567 0.447 0.543 0.471 0.487 
			v9 S1 0.437 0.564 0.492 0.451 0.627 0.525 0.254 0.621 0.360 0.380 0.603 0.459 
			S2 0.623 0.600 0.611 0.585 0.608 0.596 0.321 0.586 0.414 0.509 0.598 0.541 
			v10 S1 0.818 0.450 0.581 0.818 0.450 0.581 0.455 0.556 0.500 0.697 0.523 0.582 
			S2 0.692 0.450 0.545 0.615 0.500 0.552 0.308 0.444 0.364 0.538 0.4645 0.487 
			Mean scores S1 -- 0.520 -- 0.510 -- 0.404 -- 0.481 
			S2 -- 0.543 -- 0.554 -- 0.433 -- 0.510 
			An important remark is the behavior of S1 and S2 concerning v6 
			. If evalu- 
			ated without considering any (dis)agreement between references (F1mean 
			), S2 
			overperforms S1; this is inverted once the systems are evaluated with WiSeBE. 
			2 State of the Art 
			The most known RST corpus is the RST Discourse 
			Treebank, for English (Carlson et al., 2002a, 
			2002b). It includes 385 texts of the journalistic 
			domain, extracted from the Penn Treebank 
			(Marcus et al., 1993), such as cultural reviews, 
			editorials, economy articles, etc. 347 texts are used 
			as a learning corpus and 38 texts are used as a test 
			corpus. It contains 176,389 words and 21,789 
			EDUs. 13.8% of the texts (that is, 53) were 
			annotated by two people with a list of 78 relations. 
			For annotation, the annotation tool RSTtool 2 
			(O'Donnell, 2000) was used, with some 
			adaptations. The principal advantages of this 
			corpus stand on the high number of annotated texts 
			(for the moment it is the biggest RST corpus) and 
			the clarity of the annotation method (specified in 
			the annotation manual by Carlson and Marcu, 
			2001). However, some drawbacks remain. The 
			corpus is not free, it is not on-line and it only 
			includes texts of one domain (journalistic). 
			For English there is also the Discourse 
			Relations Reference Corpus (Taboada and 
			Renkema, 2008). This corpus includes 65 texts 
			(each one tagged by one annotator) of several types 
			and from several sources: 21 articles from the Wall 
			Street Journal extracted from the RST Discourse 
			Treebank, 30 movies and books' reviews extracted 
			from the epinions.com website, and 14 diverse 
			texts, including letters, webs, magazine articles, 
			newspaper editorials, etc. The tool used for 
			annotation was also the RSTtool. The advantages 
			of this corpus are that it is free and on-line, and it 
			includes texts of several types and domains. The 
			disadvantages are that the amount of texts is not 
			very high, the annotation methodology is not 
			2 http://www.wagsoft.com/RSTTool/ 
			specified and it does not include texts annotated by 
			several people. 
			Another well-known corpus is the Potsdam 
			Commentary Corpus, for German (Stede, 2004; 
			Reitter and Stede, 2003). This corpus includes 173 
			texts on politics from the on-line newspaper 
			Markische Allgemeine Zeitung. It contains 32,962 
			words and 2,195 sentences. It is annotated with 
			several data: morphology, syntax, rhetorical 
			structure, connectors, correference and informative 
			structure. Nevertheless, only a part of this corpus 
			(10 texts), which the authors name "core corpus", 
			is annotated with all this information. The texts 
			were annotated with the RSTtool. This corpus has 
			several advantages: it is annotated at different 
			levels (the annotation of connectors is especially 
			interesting); all the texts were annotated by two 
			people (with a previous RST training phase); it is 
			free for research purposes, and there is a tool for 
			searching over the corpus (although it is not 
			available on-line). The disadvantages are: the 
			genre and domain of all the texts are the same, the 
			methodology of annotation was quite intuitive 
			(without a manual or specific criteria) and the 
			inter-annotator agreement is not given. 
			For Portuguese, there are 2 corpora, built in 
			order to develop a rhetorical parser (Pardo et al., 
			2008). The first one, the CorpusTCC (Pardo et al., 
			2008), was used as learning corpus for detection of 
			linguistic patterns indicating rhetorical relations. It 
			contains 100 introduction sections of computer 
			science theses (53,000 words and 1,350 sentences). 
			To annotate the corpus a list of 32 rhetorical 
			relations was used. The annotation manual by 
			Carlson and Marcu (2001) was adapted to 
			Portuguese. The annotation tool was the ISI RST 
			Annotation Tool3 , an extension of the RSTtool. 
			The advantages of this corpus are: it is free, it 
			contains an acceptable number of texts and words 
			and it follows a specific annotation methodology. 
			The disadvantage is: it only includes texts of one 
			genre and domain, only annotated by one person. 
			The second one, Rhetalho (Pardo and Seno, 
			2005), was used as reference corpus for the parser 
			evaluation. It contains 50 texts: 20 introduction 
			sections and 10 conclusion sections from computer 
			science scientific articles, and 20 texts from the on- 
			line newspaper Folha de Sao Paulo (7 from the 
			Daily section, 7 from the World section and 6 from 
			3 http://www.isi.edu/~marcu/discourse/ 
			2 
			the Science section). It includes approximately 
			5,000 words. The relations and the annotation tool 
			are the same as those used in the CorpusTCC. The 
			advantages of this corpus are that it is free, it was 
			annotated by 2 people (they both were RST experts 
			and followed an annotation manual) and it contains 
			texts of several genres and domains. The main 
			disadvantage is the scarce amount of texts. 
			The Penn Discourse Treebank (Rashmi et al., 
			2008)f for English includes texts annotated with 
			information related to discourse structure and 
			semantics (without a specific theoretical approach). 
			Its advantages are: its big size (it contains 40,600 
			annotated discourse relations) allows to apply 
			machine learning, and the discourse annotations 
			are aligned with the syntactic constituency 
			annotations of the Penn Treebank. Its limitations 
			are: dependencies across relations are not marked, 
			it only includes texts of the journalistic domain, 
			and it is not free. Although there are several 
			corpora annotated with discourse relations, there is 
			not a corpus of this type for Spanish. 
			3 The RST Spanish Treebank 
			As Sierra (2008) states, a corpus consists of a 
			compilation of a set of written and/or spoken texts 
			sharing some characteristics, created for certain 
			investigation purposes. According to Hovy (2010), 
			we use 7 core questions in corpus design, detailed 
			in the next subsections. 
			3.1 Selecting a Corpus 
			For the RST Spanish Treebank, we wanted to 
			include short texts (finally, the average is 197 
			words by text; the longest containing 1,051 words 
			and the shortest, 25) in order to get a best on-line 
			visualization of the RST trees. Moreover, in the 
			first stage of the project, we preferred to select 
			specialized texts of very different areas, although 
			in the future we plan to include also non- 
			specialized texts (ex. blogs, news, websites) in 
			order to guarantee the representativity of the 
			corpus. We did not find a pre-existing Spanish 
			corpus with these characteristics, so we decided to 
			build our own corpus. Following Cabre (1999), we 
			consider that a text is specialized if it is written by 
			a professional in a given domain. According to this 
			work, specialized texts can be divided in three 
			levels: high (both the author and the potential 
			reader of the text are specialists), average (the 
			author of the text is a specialist, and the potential 
			reader of that text is a student or someone 
			interested in or possessing some prior knowledge 
			about the subject) and low (the author of the text is 
			a specialist, and the potential reader is the general 
			public). The RST Spanish Treebank includes 
			specialized texts of the three mentioned levels: 
			high (scientific articles, conference proceedings, 
			doctoral theses, etc.), average (textbooks) and low 
			(articles and reports from popular magazines, 
			associations' websites, etc.). The texts have been 
			divided in 9 domains (some of them including 
			subdivisions): Astrophysics, Earthquake 
			Engineering, Economy, Law, Linguistics (Applied 
			Linguistics, Language Acquisition, PLN, 
			Terminology), Mathematics (Primary Education, 
			Secondary Education, Scientific Articles), 
			Medicine (Administration of Health Services, 
			Oncology, Orthopedy), Psychology and Sexuality 
			(Clinical Perspective, Psychological Perspective). 
			The size of a corpus is also a polemic question. 
			If the corpus is developed for machine learning, its 
			size will be enough when the application we want 
			to develop obtains acceptable percentages of 
			precision and recall (in the context of that 
			application). Nevertheless, if the corpus is built 
			with descriptive purposes, it is difficult to 
			determine the corpus size. In the case of a corpus 
			annotated with rhetorical relations, it is even more 
			difficult, because there are various factors 
			involved: EDUs, SPANs (that is, a group of related 
			EDUs), nuclearity and relations. In addition, 
			relations are multiple (we use 28). As Hovy (2010: 
			13) mentions, one of the most difficult phenomena 
			to annotate is the discourse structure. Our corpus 
			contains 52,746 words and 267 texts. Table 1 
			includes RST Spanish Treebank statistics in terms 
			of texts, words, sentences and EDUs. 
			Texts Words Sentences EDUs 
			Learning corpus 183 41,555 1,759 2,655 
			Test corpus 84 11,191 497 694 
			Total corpus 267 52,746 2,256 3,349 
			Table 1: RST Spanish Treebank statistics 
			To increase the linear performance of a 
			statistical method, it is necessary that the training 
			corpus size grows exponentially (Zhao et al., 
			2010). However, the RST Spanish Treebank is not 
			designed only to use statistical methods; we think 
			it will be useful to employ symbolic or hybrid 
			3 
			algorithms (combining symbolic and statistical 
			methods). Moreover, this corpus will be dynamic, 
			so we expect to have a bigger corpus in the future, 
			useful to apply machine learning methods. 
			If we measure the corpus size in terms of words 
			or texts, we can take as a reference the other RST 
			corpora. Nevertheless, as Sierra states (2008), it is 
			"absurd" to try to build an exhaustive corpus 
			covering all the aspects of a language. On the 
			contrary, the linguist looks for the 
			representativeness of the texts, that is, tries to 
			create a sample of the studied language, selecting 
			examples which represent the linguistic reality, in 
			order to analyze them in a pertinent way. In this 
			sense and in the frame of this work, we consider 
			that the size will be adequate if the rhetorical trees 
			of the corpus include a representative number of 
			examples of rhetorical relations, at least 20 
			examples of each one (taking into account that the 
			corpus contains 3115 relations, we consider that 
			this quantity is acceptable; however, we expect to 
			have even more examples when the corpus grows). 
			Table 2 shows the number of examples of each 
			relation currently included into the RST Spanish 
			Treebank (N-S: nucleus-satellite relation; N-N: 
			multinuclear relation). As it can be observed, it 
			contains more than 20 examples of most of the 
			relations. The exceptions are the nucleus-satellite 
			relations of Enablement, Evaluation, Summary, 
			Otherwise and Unless, and the multinuclear 
			relations of Conjunction and Disjunction, because 
			it is not so usual to find these rhetorical relations in 
			the language, in comparison with others. Hovy 
			(2010: 128) states that, given the lack of examples 
			in the corpus, there are 2 possible strategies: a) to 
			leave the corpus as it is, with few or no examples 
			of some cases (but the problem will be the lack of 
			training examples for machine learning systems), 
			or b) to add low-frequency examples artificially to 
			"enrich" the corpus (but the problem will be the 
			distortion of the native frequency distribution and 
			perhaps the confusion of machine learning 
			systems). In the current state of our project, we 
			have chosen the first option. We think that, 
			including specialized texts in a second stage, we 
			will get more examples of these less common 
			relations. If we carry out a more granulated 
			segmentation maybe we could obtain more 
			examples; however, we wanted to employ the 
			segmentation criteria used to develop the Spanish 
			RST discourse segmenter (da Cunha et al., 2011). 
			Quantity 
			Relation Type 
			N % 
			Elaboration N-S 765 24.56 
			Preparation N-S 475 15.25 
			Background N-S 204 6.55 
			Result N-S 193 6.20 
			Means N-S 175 5.62 
			List N-N 172 5.52 
			Joint N-N 160 5.14 
			Circumstance N-S 140 4.49 
			Purpose N-S 122 3.92 
			Interpretation N-S 88 2.83 
			Antithesis N-S 80 2.57 
			Cause N-S 77 2.47 
			Sequency N-N 74 2.38 
			Evidence N-S 59 1.89 
			Contrast N-N 58 1.86 
			Condition N-S 53 1.70 
			Concession N-S 50 1.61 
			Justification N-S 39 1.25 
			Solution N-S 32 1.03 
			Motivation N-S 28 0.90 
			Reformulation N-S 22 0.71 
			Otherwise N-S 3 0.10 
			Conjunction N-N 11 0.35 
			Evaluation N-S 11 0.35 
			Disjunction N-N 9 0.29 
			Summary N-S 8 0.26 
			Enablement N-S 5 0.16 
			Unless N-S 2 0.06 
			Table 2: Rhetorical relations in RST Spanish Treebank 
			3.2 Instantiating the Theory 
			Our segmentation and annotation criteria are very 
			similar to the original ones used by Mann and 
			Thompson (1988) for English, and by da Cunha 
			and Iruskieta (2010) for Spanish. We also explore 
			the annotation manual for English by Carlon and 
			Marcu (2001). Though we use some of their 
			postulates, we think that their analysis is too 
			meticulous in some aspects. Because of this, we 
			consider that it is not adjusted to our interest, 
			which is the finding of the simplest and most 
			objective annotation method, orientated to the 
			4 
			future development of a rhetorical parser for 
			Spanish. To sum up, our segmentation criteria are: 
			a) All the sentences of the text are segmented as 
			EDUs (we consider that a sentence is a textual 
			passage between a period and another period, a 
			semicolon, a question mark or an exclamation 
			point; texts' titles are also segmented). Exs.4 
			[Estas son las razones fundamentales que motivaron 
			este trabajo.] 
			[These are the fundamental reasons which motivated this 
			work.] 
			[Estudio de caso unico sobre violencia conyugal] 
			[Study of a case on conjugal violence] 
			b) Intra-sentence EDUs are segmented, using the 
			following criteria: 
			b1) An intra-sentence EDU has to include a finite 
			verb, an infinitive or a gerund. Ex. 
			[Siendo una variante de la eliminacion Gaussiana,] 
			[posee caracteristicas didacticas ventajosas.] 
			[Being a variant of Gaussian elimination,] [it possesses 
			didactic profitable characteristics.] 
			b2) Subject/object subordinate clauses or 
			substantive sentences are not segmented. Ex. 
			[Se muestra que el modelo discreto en diferencias finitas 
			es convergente y que su realizacion se reduce a resolver 
			una sucesion de sistemas lineales tridiagonales.] 
			[It appears that the discreet model in finite differences is 
			convergent and that its accomplishment is to solve a 
			succession of tridiagonal linear systems.] 
			b3) Subordinate relative clauses are not segmented. 
			Ex. 
			[Durante el proceso, que utiliza solo aritmetica entera, 
			se obtiene el determinante de la matriz de coeficientes 
			del sistema, sin necesidad de calculos adicionales.] 
			[During the process, which only uses entire arithmetic, the 
			determinant of the system coefficient matrix is obtained, 
			without additional calculations.] 
			b4) Elements in parentheses are only segmented if 
			they follow the criterion b1. Ex. 
			[Este ano se cumple el bicentenario del nacimiento de 
			Niels (Nicolas, en nuestro idioma) Henrik Abel.] 
			[This year is the bicentenary of Niels's birth (Nicolas, in 
			our language) Henrik Abel.] 
			b5) Embedded units are segmented by means of 
			the non-relation Same-Unit proposed by Carlon 
			and Marcu (2001). Figure 1 shows this structure. 
			[En decadas precedentes se ha puesto de manifiesto,] [y 
			asi lo han atestiguado muchos investigadores de la 
			4 Spanish examples were extracted from the corpus. English 
			translations are ours. 
			terminologia cientifica serbia,] [una tendencia a 
			importar prestamos del ingles.] 
			[In previous decades it has been shown,] [and it has been 
			testified by many researchers of the scientific Serbian 
			terminology,] [a trend to import loanwords from English.] 
			Figure 1: Example of the non-relation Same-Unit 
			3.3 Designing the Interface 
			The annotation tool used in this work is the 
			RSTtool, since it is free and easy to use. Therefore, 
			we preferred to use it instead of designing a new 
			one. Nevertheless, we have designed an on-line 
			interface to include the corpus and to carry out 
			searches over it (see Section 4). 
			3.4 Selecting and Training the Annotators 
			With regard to the corpus annotators, we have a 
			team of 10 people (last year Bachelor's degree 
			students, Master's degree students and PhDs) 5 . 
			Before the annotation, they took a RST course of 6 
			months (100 hours), where the segmentation and 
			annotation methodology used for the development 
			of the RST Spanish Treebank was explained.6 We 
			called this period "training phase". The course had 
			a theoretical and a practical part. In the theoretical 
			part, some criteria with regard to the 3 phases of 
			rhetorical analysis (segmentation, detection of 
			relations, and rhetorical trees building) were given 
			to annotators. In the practical part, firstly, it was 
			explained how to use the RSTtool. Secondly, 
			annotators extracted several texts from the web, 
			following their personal interests, as for example, 
			music, video games, cookery or art webs. They 
			segmented those texts, using the established 
			segmentation criteria. Once segmented, all the 
			doubts and problematic examples were discussed, 
			and they tried to get an agreement on the most 
			complicated cases. Thirdly, the relations were 
			5 We thank annotators (Adriana Valerio, Brenda Castro, 
			Daniel Rodriguez, Ita Cruz, Jessica Mendez, Josue Careaga, 
			Luis Cabrera, Marina Fomicheva and Paulina De La Vega) 
			and interface developers (Luis Cabrera and Juan Rolland). 
			6 This course was given in the framework of a last-year subject 
			in the Spanish Linguistics Degree at UNAM (Mexico City). 
			5 
			analyzed (using a given relations list) and, once 
			again, annotators discussed the difficult cases. 
			After the discussion, texts were re-annotated to 
			verify if the difficulties were solved. This process 
			was doubly interesting, since it helped to create 
			common criteria for the annotation of the final 
			corpus and to define the annotation criteria more 
			clearly and consensually, in order to include them 
			in the RST Spanish Treebank annotation manual. 
			Once annotators agreed on the most difficult cases, 
			we consider that the training phase finished. 
			3.5 Designing and Managing the Annotation 
			Procedure 
			We start from the following annotation definition: 
			Annotation (`tagging') is the process of adding new 
			information into source material by humans 
			(annotators) or suitably trained machines. [...]. The 
			addition process usually requires some sort of 
			mental decision that depends both on the source 
			material and on some theory or knowledge that the 
			annotator has internalized earlier. (Hovy, 2010: 6) 
			Exactly, after our annotators internalized the 
			theory and annotation criteria during the training 
			phase, the "annotation phase" of the final texts 
			included in the RST Spanish Treebank started. In 
			this phase, the annotation tasks were assigned to 
			annotators (the number of texts assigned to each 
			annotator was different, depending on their 
			availability). They were asked to carry out the 
			annotation individually and without questions 
			among them. We calculated that the average time 
			to carry out the annotation of one text was between 
			15 minutes and 1 hour. This time difference is due 
			to the fact that the corpus includes both short and 
			long texts. The annotation process is the following: 
			once a text is segmented, rhetorical relations 
			between EDUs are annotated. First, EDUs inside 
			the same sentence are annotated in a binary way. 
			Second, sentences inside the same paragraph are 
			linked. Finally, paragraphs are linked. 
			Hovy (2010) states that it is difficult to 
			determine if, for the same money (we add "for the 
			same time"), it is better to double-annotate less, or 
			to single-annotate more. As he explains, Dligach et 
			al. (2010) made an experiment with OntoNotes 
			(Pradhan et al., 2007) verb sense annotation. The 
			result was that, assuming the annotation is stable 
			(that is, inter-annotator agreement is high), it is 
			better to annotate more, even with only one 
			annotator. The problem with RST annotation is 
			that there are so many categories to annotate, that 
			is very difficult to obtain a stable annotation. 
			Therefore, we consider it is necessary to have at 
			least some texts double-annotated (or even triple- 
			annotated), in order to have an adequate discourse 
			corpus. This is the reason why, following the RST 
			Discourse Treebank methodology, we use some 
			texts as learning corpus and some others (from the 
			Mathematics, Psychology and Sexuality domains) 
			as test corpus: 69% (183 texts) and 31% (84 texts), 
			respectively. The texts of the learning corpus were 
			annotated by 1 person, whereas the texts of the test 
			corpus were annotated by 2 people. 
			3.6 Validating Results 
			Da Cunha and Iruskieta (2010) measure inter- 
			annotator agreement by using the RST trees 
			comparison methodology by Marcu (2000). This 
			methodology evaluates the agreement on 4 
			elements (EDUs, SPANs, Nuclearity and 
			Relations), by means of precision and recall 
			measures (an annotation with regard to the other 
			one). Following this methodology, we have 
			measured inter-annotator agreement over the test 
			corpus. We employ an on-line automatic tool for 
			RST trees comparison, RSTeval (Mazeiro and 
			Pardo, 2009), where Marcu's methodology has 
			been implemented (for 4 languages: English, 
			Portuguese, Spanish and Basque). We know that 
			there are some other ways to measure agreement, 
			such as Cohen's kappa (Cohen, 1960) or Fleiss's 
			kappa (Fleiss, 1971), for example. Nevertheless, 
			we consider that Marcu's methodology (2000) is 
			suitable to compare adequately 2 annotations of the 
			same original text, because it has been designed 
			specifically for this task. 
			For each trees pair from the test corpus, 
			precision and recall were measured separately. 
			Afterwards, all those individual results were put 
			together to obtain general results. Table 3 shows 
			global results for the 4 categories. The category 
			with more agreement was EDUs (recall: 91.04% / 
			precision: 87.20%), that is, segmentation. This 
			result was expected, since the segmentation criteria 
			given to the annotators were quite precise and the 
			possibility of mistake was low. The lowest 
			agreement was obtained for the category Relations 
			(recall: 78.48% / precision: 76.81%). This result is 
			lower than the other, but we think it is acceptable. 
			In the RST Discourse Treebank the trend was 
			similar to the one detected in our corpus: the 
			6 
			highest agreement is obtained at the segmentation 
			level and the lowest at the relations level. 
			Category Precision Recall 
			EDUs 87.20% 91.04% 
			SPANs 86% 87.31% 
			Nuclearity 82.46% 84.66% 
			Relations 76.81% 78.48% 
			Table 3: Inter-annotator agreement 
			Precision and recall have not been calculated 
			with respect to a gold standard because it does not 
			exist for Spanish. Our future aim is to reach a 
			consensus on the annotation of the test corpus 
			(using an external "judge"), in order to establish a 
			set of texts considered as a preliminary gold 
			standard for this language. We consider that the 
			annotations have quality at present, because inter- 
			annotator agreement is quite high; however, this 
			consensus could solve the typical annotation 
			mistakes we have detected or some ambiguities. 
			We have analyzed the main discrepancy reasons 
			between annotators. With regard to the 
			segmentation, the main one was human mistake; 
			ex. segmenting EDUs without a verb (one 
			annotator segmented the following passage into 2 
			EDUs because she detected a Means relation, but 
			the second EDU does not include any verb): 
			[Ademas estudiamos el desarrollo de criterios para 
			determinar si un semigrupo dado tiene dicha propiedad ] 
			[mediante el estudio de desigualdades de curvatura- 
			dimension. ] 
			[We also study the development of tests in order to 
			determine if a given semi group has this property] [by means 
			of curvature-dimension inequalities.] 
			The second reason was that in the manual some 
			aspects were not explained in detail. For example, 
			if a substantive sentence or a direct/object clause 
			(which must not be segmented, according to the 
			point b2) includes two coordinated clauses, these 
			must not be segmented either. Thus, we found 
			some erroneous segmentations. For example: 
			[Los hombres adultos tienen miedo de fracasar] [y no 
			cumplir con el rol masculino de ser proveedores del 
			hogar y de proteger a su familia.] 
			[Adult men are scared to fail] [and not to fulfill the 
			masculine role of being the suppliers of the home and to 
			protect their family.] 
			This kind of mistakes allowed us to refine our 
			segmentation manual a posteriori. In the future, we 
			will ask the test corpus annotators to make a new 
			annotation of the texts, using the refined manual, in 
			order to check if the agreement increases, in the 
			same way as the RST Discourse Treebank. 
			With regard to rhetorical annotations, we 
			detected 2 main reasons of inter-annotator 
			disagreement. The first one was the ambiguity of 
			some relations and their corresponding connectors; 
			for example, Justification-Reason, Antithesis- 
			Concession or Circumstance-Means relations, like 
			in the following passage (in Spanish, "al" may 
			indicate time or manner): 
			[Los ninos aprenden matematicas] [al resolver 
			problemas.] 
			[Children learn mathematics] [when solving problems.] 
			The second one is due to differences between 
			annotators when determining nuclearity. For 
			example, in the following passage, one annotator 
			marked Background and the other one Elaboration: 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]S_Background [Norma y Andres quieren 
			colocar en el hueco una pecera. ]N_Background 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]N_Elaboration [Norma y Andres quieren 
			colocar en el hueco una pecera. ]S_Elaboration 
			[A hole of 60 x 1.20 cm remained in the wall.] [Norma and 
			Andres want to place a fish tank in the hole.] 
			It is easier to solve segmentation disagreement 
			than relations disagreement, since in this case 
			annotator subjectivity is more evident; we must 
			consider how to refine our manual in this sense. 
			3.7 Delivering and Maintaining the Product 
			Hovy (2010) mentions some technical issues 
			regarding these points: licensing, distribution, 
			maintenance and updates. With regard to licensing 
			and distribution, the RST Spanish Treebank will be 
			free for research purposes. We have a data 
			manager responsible for maintenance and updates. 
			The description of the annotated corpus is also 
			a very important issue (Ide and Pustejovsky, 2010). 
			It is important to provide a high level description 
			of the corpus, including the theoretical framework, 
			the methodology (annotators, annotation manual 
			and tool, agreement, etc.), the means for resource 
			maintenance, the technical aspects, the project 
			leader, the contact, the team, etc. The RST Spanish 
			Treebank includes all this detailed information. 
			XML (with a DTD) has been used, in order the 
			corpus can be reused for several aplications. In the 
			future, we plan to use the standard XCES. 
			7 
			To know more about resources development, 
			linguistic annotation or inter-annotator agreement, 
			we recommend: Palmer et al. (on-line), Palmer and 
			Xue (2010), and Artstein and Poesio (2008). 
			4 The Search Interface of the RST 
			Spanish Treebank 
			The RST Spanish Treebank interface is freely 
			available on-line7. It allows the visualization and 
			downloading of all the texts in txt format, with 
			their corresponding annotated trees in RSTtool 
			format (rs3), as well as in image format (jpg). Each 
			text includes its title, its reference, its web link (if 
			it is an on-line text) and its number of words. The 
			interface shows texts by areas and allows the user 
			to select a subcorpus (including individual files or 
			folders containing several files). The selected 
			subcorpus can be saved on local disk (generating a 
			xml file) for future analyses. 
			The interface includes a statistical tool which 
			allows obtaining statistics of rhetorical relations in 
			a subcorpus selected by the user. The RSTtool also 
			offers this option but it can be only used for one 
			text. We consider that it is more useful for the user 
			to obtain statistics from various texts, in order to 
			get significant statistical results. As the RSTtool, 
			our tool allows to count the multinuclear relations 
			in two ways: a) one unit for each detected 
			multinuclear relation, and b) one unit for each 
			detected nucleus. If we use b), the statistics of the 
			multinuclear relations of Table 2 are higher: List 
			(864), Joint (537), Sequence (289), Contrast (153), 
			Conjunction (28) and Disjunction (24). 
			We are developing another tool, aimed to 
			extract information from the annotated texts, which 
			we will soon include into the interface. This tool 
			will allow to the user to select a subcorpus and to 
			extract from it the EDUs corresponding to the 
			rhetorical relations selected, like a multidocument 
			specialized summarizer guided by user's interests. 
			The RST Spanish Treebank interface also 
			includes a screen which permits the users to send 
			their own annotated texts. Our aim is for the RST 
			Spanish Treebank to become a dynamic corpus, in 
			constant evolution, being increased with texts 
			annotated by users. This has a double advantage 
			since, on the one hand, the corpus will grow and, 
			on the other hand, users will profit from the 
			7 http://www.corpus.unam.mx/rst/ 
			interface's applications, using their own 
			subcorpora. The only requirement is to use the 
			relations and the segmentation and annotation 
			criteria of our project. Once the texts are sent, the 
			RST Spanish Treebank data manager will verify if 
			the annotation corresponds to these criteria. 
			2 State of the Art 
			The most known RST corpus is the RST Discourse 
			Treebank, for English (Carlson et al., 2002a, 
			2002b). It includes 385 texts of the journalistic 
			domain, extracted from the Penn Treebank 
			(Marcus et al., 1993), such as cultural reviews, 
			editorials, economy articles, etc. 347 texts are used 
			as a learning corpus and 38 texts are used as a test 
			corpus. It contains 176,389 words and 21,789 
			EDUs. 13.8% of the texts (that is, 53) were 
			annotated by two people with a list of 78 relations. 
			For annotation, the annotation tool RSTtool 2 
			(O'Donnell, 2000) was used, with some 
			adaptations. The principal advantages of this 
			corpus stand on the high number of annotated texts 
			(for the moment it is the biggest RST corpus) and 
			the clarity of the annotation method (specified in 
			the annotation manual by Carlson and Marcu, 
			2001). However, some drawbacks remain. The 
			corpus is not free, it is not on-line and it only 
			includes texts of one domain (journalistic). 
			For English there is also the Discourse 
			Relations Reference Corpus (Taboada and 
			Renkema, 2008). This corpus includes 65 texts 
			(each one tagged by one annotator) of several types 
			and from several sources: 21 articles from the Wall 
			Street Journal extracted from the RST Discourse 
			Treebank, 30 movies and books' reviews extracted 
			from the epinions.com website, and 14 diverse 
			texts, including letters, webs, magazine articles, 
			newspaper editorials, etc. The tool used for 
			annotation was also the RSTtool. The advantages 
			of this corpus are that it is free and on-line, and it 
			includes texts of several types and domains. The 
			disadvantages are that the amount of texts is not 
			very high, the annotation methodology is not 
			2 http://www.wagsoft.com/RSTTool/ 
			specified and it does not include texts annotated by 
			several people. 
			Another well-known corpus is the Potsdam 
			Commentary Corpus, for German (Stede, 2004; 
			Reitter and Stede, 2003). This corpus includes 173 
			texts on politics from the on-line newspaper 
			Markische Allgemeine Zeitung. It contains 32,962 
			words and 2,195 sentences. It is annotated with 
			several data: morphology, syntax, rhetorical 
			structure, connectors, correference and informative 
			structure. Nevertheless, only a part of this corpus 
			(10 texts), which the authors name "core corpus", 
			is annotated with all this information. The texts 
			were annotated with the RSTtool. This corpus has 
			several advantages: it is annotated at different 
			levels (the annotation of connectors is especially 
			interesting); all the texts were annotated by two 
			people (with a previous RST training phase); it is 
			free for research purposes, and there is a tool for 
			searching over the corpus (although it is not 
			available on-line). The disadvantages are: the 
			genre and domain of all the texts are the same, the 
			methodology of annotation was quite intuitive 
			(without a manual or specific criteria) and the 
			inter-annotator agreement is not given. 
			For Portuguese, there are 2 corpora, built in 
			order to develop a rhetorical parser (Pardo et al., 
			2008). The first one, the CorpusTCC (Pardo et al., 
			2008), was used as learning corpus for detection of 
			linguistic patterns indicating rhetorical relations. It 
			contains 100 introduction sections of computer 
			science theses (53,000 words and 1,350 sentences). 
			To annotate the corpus a list of 32 rhetorical 
			relations was used. The annotation manual by 
			Carlson and Marcu (2001) was adapted to 
			Portuguese. The annotation tool was the ISI RST 
			Annotation Tool3 , an extension of the RSTtool. 
			The advantages of this corpus are: it is free, it 
			contains an acceptable number of texts and words 
			and it follows a specific annotation methodology. 
			The disadvantage is: it only includes texts of one 
			genre and domain, only annotated by one person. 
			The second one, Rhetalho (Pardo and Seno, 
			2005), was used as reference corpus for the parser 
			evaluation. It contains 50 texts: 20 introduction 
			sections and 10 conclusion sections from computer 
			science scientific articles, and 20 texts from the on- 
			line newspaper Folha de Sao Paulo (7 from the 
			Daily section, 7 from the World section and 6 from 
			3 http://www.isi.edu/~marcu/discourse/ 
			2 
			the Science section). It includes approximately 
			5,000 words. The relations and the annotation tool 
			are the same as those used in the CorpusTCC. The 
			advantages of this corpus are that it is free, it was 
			annotated by 2 people (they both were RST experts 
			and followed an annotation manual) and it contains 
			texts of several genres and domains. The main 
			disadvantage is the scarce amount of texts. 
			The Penn Discourse Treebank (Rashmi et al., 
			2008)f for English includes texts annotated with 
			information related to discourse structure and 
			semantics (without a specific theoretical approach). 
			Its advantages are: its big size (it contains 40,600 
			annotated discourse relations) allows to apply 
			machine learning, and the discourse annotations 
			are aligned with the syntactic constituency 
			annotations of the Penn Treebank. Its limitations 
			are: dependencies across relations are not marked, 
			it only includes texts of the journalistic domain, 
			and it is not free. Although there are several 
			corpora annotated with discourse relations, there is 
			not a corpus of this type for Spanish. 
			3 The RST Spanish Treebank 
			As Sierra (2008) states, a corpus consists of a 
			compilation of a set of written and/or spoken texts 
			sharing some characteristics, created for certain 
			investigation purposes. According to Hovy (2010), 
			we use 7 core questions in corpus design, detailed 
			in the next subsections. 
			3.1 Selecting a Corpus 
			For the RST Spanish Treebank, we wanted to 
			include short texts (finally, the average is 197 
			words by text; the longest containing 1,051 words 
			and the shortest, 25) in order to get a best on-line 
			visualization of the RST trees. Moreover, in the 
			first stage of the project, we preferred to select 
			specialized texts of very different areas, although 
			in the future we plan to include also non- 
			specialized texts (ex. blogs, news, websites) in 
			order to guarantee the representativity of the 
			corpus. We did not find a pre-existing Spanish 
			corpus with these characteristics, so we decided to 
			build our own corpus. Following Cabre (1999), we 
			consider that a text is specialized if it is written by 
			a professional in a given domain. According to this 
			work, specialized texts can be divided in three 
			levels: high (both the author and the potential 
			reader of the text are specialists), average (the 
			author of the text is a specialist, and the potential 
			reader of that text is a student or someone 
			interested in or possessing some prior knowledge 
			about the subject) and low (the author of the text is 
			a specialist, and the potential reader is the general 
			public). The RST Spanish Treebank includes 
			specialized texts of the three mentioned levels: 
			high (scientific articles, conference proceedings, 
			doctoral theses, etc.), average (textbooks) and low 
			(articles and reports from popular magazines, 
			associations' websites, etc.). The texts have been 
			divided in 9 domains (some of them including 
			subdivisions): Astrophysics, Earthquake 
			Engineering, Economy, Law, Linguistics (Applied 
			Linguistics, Language Acquisition, PLN, 
			Terminology), Mathematics (Primary Education, 
			Secondary Education, Scientific Articles), 
			Medicine (Administration of Health Services, 
			Oncology, Orthopedy), Psychology and Sexuality 
			(Clinical Perspective, Psychological Perspective). 
			The size of a corpus is also a polemic question. 
			If the corpus is developed for machine learning, its 
			size will be enough when the application we want 
			to develop obtains acceptable percentages of 
			precision and recall (in the context of that 
			application). Nevertheless, if the corpus is built 
			with descriptive purposes, it is difficult to 
			determine the corpus size. In the case of a corpus 
			annotated with rhetorical relations, it is even more 
			difficult, because there are various factors 
			involved: EDUs, SPANs (that is, a group of related 
			EDUs), nuclearity and relations. In addition, 
			relations are multiple (we use 28). As Hovy (2010: 
			13) mentions, one of the most difficult phenomena 
			to annotate is the discourse structure. Our corpus 
			contains 52,746 words and 267 texts. Table 1 
			includes RST Spanish Treebank statistics in terms 
			of texts, words, sentences and EDUs. 
			Texts Words Sentences EDUs 
			Learning corpus 183 41,555 1,759 2,655 
			Test corpus 84 11,191 497 694 
			Total corpus 267 52,746 2,256 3,349 
			Table 1: RST Spanish Treebank statistics 
			To increase the linear performance of a 
			statistical method, it is necessary that the training 
			corpus size grows exponentially (Zhao et al., 
			2010). However, the RST Spanish Treebank is not 
			designed only to use statistical methods; we think 
			it will be useful to employ symbolic or hybrid 
			3 
			algorithms (combining symbolic and statistical 
			methods). Moreover, this corpus will be dynamic, 
			so we expect to have a bigger corpus in the future, 
			useful to apply machine learning methods. 
			If we measure the corpus size in terms of words 
			or texts, we can take as a reference the other RST 
			corpora. Nevertheless, as Sierra states (2008), it is 
			"absurd" to try to build an exhaustive corpus 
			covering all the aspects of a language. On the 
			contrary, the linguist looks for the 
			representativeness of the texts, that is, tries to 
			create a sample of the studied language, selecting 
			examples which represent the linguistic reality, in 
			order to analyze them in a pertinent way. In this 
			sense and in the frame of this work, we consider 
			that the size will be adequate if the rhetorical trees 
			of the corpus include a representative number of 
			examples of rhetorical relations, at least 20 
			examples of each one (taking into account that the 
			corpus contains 3115 relations, we consider that 
			this quantity is acceptable; however, we expect to 
			have even more examples when the corpus grows). 
			Table 2 shows the number of examples of each 
			relation currently included into the RST Spanish 
			Treebank (N-S: nucleus-satellite relation; N-N: 
			multinuclear relation). As it can be observed, it 
			contains more than 20 examples of most of the 
			relations. The exceptions are the nucleus-satellite 
			relations of Enablement, Evaluation, Summary, 
			Otherwise and Unless, and the multinuclear 
			relations of Conjunction and Disjunction, because 
			it is not so usual to find these rhetorical relations in 
			the language, in comparison with others. Hovy 
			(2010: 128) states that, given the lack of examples 
			in the corpus, there are 2 possible strategies: a) to 
			leave the corpus as it is, with few or no examples 
			of some cases (but the problem will be the lack of 
			training examples for machine learning systems), 
			or b) to add low-frequency examples artificially to 
			"enrich" the corpus (but the problem will be the 
			distortion of the native frequency distribution and 
			perhaps the confusion of machine learning 
			systems). In the current state of our project, we 
			have chosen the first option. We think that, 
			including specialized texts in a second stage, we 
			will get more examples of these less common 
			relations. If we carry out a more granulated 
			segmentation maybe we could obtain more 
			examples; however, we wanted to employ the 
			segmentation criteria used to develop the Spanish 
			RST discourse segmenter (da Cunha et al., 2011). 
			Quantity 
			Relation Type 
			N % 
			Elaboration N-S 765 24.56 
			Preparation N-S 475 15.25 
			Background N-S 204 6.55 
			Result N-S 193 6.20 
			Means N-S 175 5.62 
			List N-N 172 5.52 
			Joint N-N 160 5.14 
			Circumstance N-S 140 4.49 
			Purpose N-S 122 3.92 
			Interpretation N-S 88 2.83 
			Antithesis N-S 80 2.57 
			Cause N-S 77 2.47 
			Sequency N-N 74 2.38 
			Evidence N-S 59 1.89 
			Contrast N-N 58 1.86 
			Condition N-S 53 1.70 
			Concession N-S 50 1.61 
			Justification N-S 39 1.25 
			Solution N-S 32 1.03 
			Motivation N-S 28 0.90 
			Reformulation N-S 22 0.71 
			Otherwise N-S 3 0.10 
			Conjunction N-N 11 0.35 
			Evaluation N-S 11 0.35 
			Disjunction N-N 9 0.29 
			Summary N-S 8 0.26 
			Enablement N-S 5 0.16 
			Unless N-S 2 0.06 
			Table 2: Rhetorical relations in RST Spanish Treebank 
			3.2 Instantiating the Theory 
			Our segmentation and annotation criteria are very 
			similar to the original ones used by Mann and 
			Thompson (1988) for English, and by da Cunha 
			and Iruskieta (2010) for Spanish. We also explore 
			the annotation manual for English by Carlon and 
			Marcu (2001). Though we use some of their 
			postulates, we think that their analysis is too 
			meticulous in some aspects. Because of this, we 
			consider that it is not adjusted to our interest, 
			which is the finding of the simplest and most 
			objective annotation method, orientated to the 
			4 
			future development of a rhetorical parser for 
			Spanish. To sum up, our segmentation criteria are: 
			a) All the sentences of the text are segmented as 
			EDUs (we consider that a sentence is a textual 
			passage between a period and another period, a 
			semicolon, a question mark or an exclamation 
			point; texts' titles are also segmented). Exs.4 
			[Estas son las razones fundamentales que motivaron 
			este trabajo.] 
			[These are the fundamental reasons which motivated this 
			work.] 
			[Estudio de caso unico sobre violencia conyugal] 
			[Study of a case on conjugal violence] 
			b) Intra-sentence EDUs are segmented, using the 
			following criteria: 
			b1) An intra-sentence EDU has to include a finite 
			verb, an infinitive or a gerund. Ex. 
			[Siendo una variante de la eliminacion Gaussiana,] 
			[posee caracteristicas didacticas ventajosas.] 
			[Being a variant of Gaussian elimination,] [it possesses 
			didactic profitable characteristics.] 
			b2) Subject/object subordinate clauses or 
			substantive sentences are not segmented. Ex. 
			[Se muestra que el modelo discreto en diferencias finitas 
			es convergente y que su realizacion se reduce a resolver 
			una sucesion de sistemas lineales tridiagonales.] 
			[It appears that the discreet model in finite differences is 
			convergent and that its accomplishment is to solve a 
			succession of tridiagonal linear systems.] 
			b3) Subordinate relative clauses are not segmented. 
			Ex. 
			[Durante el proceso, que utiliza solo aritmetica entera, 
			se obtiene el determinante de la matriz de coeficientes 
			del sistema, sin necesidad de calculos adicionales.] 
			[During the process, which only uses entire arithmetic, the 
			determinant of the system coefficient matrix is obtained, 
			without additional calculations.] 
			b4) Elements in parentheses are only segmented if 
			they follow the criterion b1. Ex. 
			[Este ano se cumple el bicentenario del nacimiento de 
			Niels (Nicolas, en nuestro idioma) Henrik Abel.] 
			[This year is the bicentenary of Niels's birth (Nicolas, in 
			our language) Henrik Abel.] 
			b5) Embedded units are segmented by means of 
			the non-relation Same-Unit proposed by Carlon 
			and Marcu (2001). Figure 1 shows this structure. 
			[En decadas precedentes se ha puesto de manifiesto,] [y 
			asi lo han atestiguado muchos investigadores de la 
			4 Spanish examples were extracted from the corpus. English 
			translations are ours. 
			terminologia cientifica serbia,] [una tendencia a 
			importar prestamos del ingles.] 
			[In previous decades it has been shown,] [and it has been 
			testified by many researchers of the scientific Serbian 
			terminology,] [a trend to import loanwords from English.] 
			Figure 1: Example of the non-relation Same-Unit 
			3.3 Designing the Interface 
			The annotation tool used in this work is the 
			RSTtool, since it is free and easy to use. Therefore, 
			we preferred to use it instead of designing a new 
			one. Nevertheless, we have designed an on-line 
			interface to include the corpus and to carry out 
			searches over it (see Section 4). 
			3.4 Selecting and Training the Annotators 
			With regard to the corpus annotators, we have a 
			team of 10 people (last year Bachelor's degree 
			students, Master's degree students and PhDs) 5 . 
			Before the annotation, they took a RST course of 6 
			months (100 hours), where the segmentation and 
			annotation methodology used for the development 
			of the RST Spanish Treebank was explained.6 We 
			called this period "training phase". The course had 
			a theoretical and a practical part. In the theoretical 
			part, some criteria with regard to the 3 phases of 
			rhetorical analysis (segmentation, detection of 
			relations, and rhetorical trees building) were given 
			to annotators. In the practical part, firstly, it was 
			explained how to use the RSTtool. Secondly, 
			annotators extracted several texts from the web, 
			following their personal interests, as for example, 
			music, video games, cookery or art webs. They 
			segmented those texts, using the established 
			segmentation criteria. Once segmented, all the 
			doubts and problematic examples were discussed, 
			and they tried to get an agreement on the most 
			complicated cases. Thirdly, the relations were 
			5 We thank annotators (Adriana Valerio, Brenda Castro, 
			Daniel Rodriguez, Ita Cruz, Jessica Mendez, Josue Careaga, 
			Luis Cabrera, Marina Fomicheva and Paulina De La Vega) 
			and interface developers (Luis Cabrera and Juan Rolland). 
			6 This course was given in the framework of a last-year subject 
			in the Spanish Linguistics Degree at UNAM (Mexico City). 
			5 
			analyzed (using a given relations list) and, once 
			again, annotators discussed the difficult cases. 
			After the discussion, texts were re-annotated to 
			verify if the difficulties were solved. This process 
			was doubly interesting, since it helped to create 
			common criteria for the annotation of the final 
			corpus and to define the annotation criteria more 
			clearly and consensually, in order to include them 
			in the RST Spanish Treebank annotation manual. 
			Once annotators agreed on the most difficult cases, 
			we consider that the training phase finished. 
			3.5 Designing and Managing the Annotation 
			Procedure 
			We start from the following annotation definition: 
			Annotation (`tagging') is the process of adding new 
			information into source material by humans 
			(annotators) or suitably trained machines. [...]. The 
			addition process usually requires some sort of 
			mental decision that depends both on the source 
			material and on some theory or knowledge that the 
			annotator has internalized earlier. (Hovy, 2010: 6) 
			Exactly, after our annotators internalized the 
			theory and annotation criteria during the training 
			phase, the "annotation phase" of the final texts 
			included in the RST Spanish Treebank started. In 
			this phase, the annotation tasks were assigned to 
			annotators (the number of texts assigned to each 
			annotator was different, depending on their 
			availability). They were asked to carry out the 
			annotation individually and without questions 
			among them. We calculated that the average time 
			to carry out the annotation of one text was between 
			15 minutes and 1 hour. This time difference is due 
			to the fact that the corpus includes both short and 
			long texts. The annotation process is the following: 
			once a text is segmented, rhetorical relations 
			between EDUs are annotated. First, EDUs inside 
			the same sentence are annotated in a binary way. 
			Second, sentences inside the same paragraph are 
			linked. Finally, paragraphs are linked. 
			Hovy (2010) states that it is difficult to 
			determine if, for the same money (we add "for the 
			same time"), it is better to double-annotate less, or 
			to single-annotate more. As he explains, Dligach et 
			al. (2010) made an experiment with OntoNotes 
			(Pradhan et al., 2007) verb sense annotation. The 
			result was that, assuming the annotation is stable 
			(that is, inter-annotator agreement is high), it is 
			better to annotate more, even with only one 
			annotator. The problem with RST annotation is 
			that there are so many categories to annotate, that 
			is very difficult to obtain a stable annotation. 
			Therefore, we consider it is necessary to have at 
			least some texts double-annotated (or even triple- 
			annotated), in order to have an adequate discourse 
			corpus. This is the reason why, following the RST 
			Discourse Treebank methodology, we use some 
			texts as learning corpus and some others (from the 
			Mathematics, Psychology and Sexuality domains) 
			as test corpus: 69% (183 texts) and 31% (84 texts), 
			respectively. The texts of the learning corpus were 
			annotated by 1 person, whereas the texts of the test 
			corpus were annotated by 2 people. 
			3.6 Validating Results 
			Da Cunha and Iruskieta (2010) measure inter- 
			annotator agreement by using the RST trees 
			comparison methodology by Marcu (2000). This 
			methodology evaluates the agreement on 4 
			elements (EDUs, SPANs, Nuclearity and 
			Relations), by means of precision and recall 
			measures (an annotation with regard to the other 
			one). Following this methodology, we have 
			measured inter-annotator agreement over the test 
			corpus. We employ an on-line automatic tool for 
			RST trees comparison, RSTeval (Mazeiro and 
			Pardo, 2009), where Marcu's methodology has 
			been implemented (for 4 languages: English, 
			Portuguese, Spanish and Basque). We know that 
			there are some other ways to measure agreement, 
			such as Cohen's kappa (Cohen, 1960) or Fleiss's 
			kappa (Fleiss, 1971), for example. Nevertheless, 
			we consider that Marcu's methodology (2000) is 
			suitable to compare adequately 2 annotations of the 
			same original text, because it has been designed 
			specifically for this task. 
			For each trees pair from the test corpus, 
			precision and recall were measured separately. 
			Afterwards, all those individual results were put 
			together to obtain general results. Table 3 shows 
			global results for the 4 categories. The category 
			with more agreement was EDUs (recall: 91.04% / 
			precision: 87.20%), that is, segmentation. This 
			result was expected, since the segmentation criteria 
			given to the annotators were quite precise and the 
			possibility of mistake was low. The lowest 
			agreement was obtained for the category Relations 
			(recall: 78.48% / precision: 76.81%). This result is 
			lower than the other, but we think it is acceptable. 
			In the RST Discourse Treebank the trend was 
			similar to the one detected in our corpus: the 
			6 
			highest agreement is obtained at the segmentation 
			level and the lowest at the relations level. 
			Category Precision Recall 
			EDUs 87.20% 91.04% 
			SPANs 86% 87.31% 
			Nuclearity 82.46% 84.66% 
			Relations 76.81% 78.48% 
			Table 3: Inter-annotator agreement 
			Precision and recall have not been calculated 
			with respect to a gold standard because it does not 
			exist for Spanish. Our future aim is to reach a 
			consensus on the annotation of the test corpus 
			(using an external "judge"), in order to establish a 
			set of texts considered as a preliminary gold 
			standard for this language. We consider that the 
			annotations have quality at present, because inter- 
			annotator agreement is quite high; however, this 
			consensus could solve the typical annotation 
			mistakes we have detected or some ambiguities. 
			We have analyzed the main discrepancy reasons 
			between annotators. With regard to the 
			segmentation, the main one was human mistake; 
			ex. segmenting EDUs without a verb (one 
			annotator segmented the following passage into 2 
			EDUs because she detected a Means relation, but 
			the second EDU does not include any verb): 
			[Ademas estudiamos el desarrollo de criterios para 
			determinar si un semigrupo dado tiene dicha propiedad ] 
			[mediante el estudio de desigualdades de curvatura- 
			dimension. ] 
			[We also study the development of tests in order to 
			determine if a given semi group has this property] [by means 
			of curvature-dimension inequalities.] 
			The second reason was that in the manual some 
			aspects were not explained in detail. For example, 
			if a substantive sentence or a direct/object clause 
			(which must not be segmented, according to the 
			point b2) includes two coordinated clauses, these 
			must not be segmented either. Thus, we found 
			some erroneous segmentations. For example: 
			[Los hombres adultos tienen miedo de fracasar] [y no 
			cumplir con el rol masculino de ser proveedores del 
			hogar y de proteger a su familia.] 
			[Adult men are scared to fail] [and not to fulfill the 
			masculine role of being the suppliers of the home and to 
			protect their family.] 
			This kind of mistakes allowed us to refine our 
			segmentation manual a posteriori. In the future, we 
			will ask the test corpus annotators to make a new 
			annotation of the texts, using the refined manual, in 
			order to check if the agreement increases, in the 
			same way as the RST Discourse Treebank. 
			With regard to rhetorical annotations, we 
			detected 2 main reasons of inter-annotator 
			disagreement. The first one was the ambiguity of 
			some relations and their corresponding connectors; 
			for example, Justification-Reason, Antithesis- 
			Concession or Circumstance-Means relations, like 
			in the following passage (in Spanish, "al" may 
			indicate time or manner): 
			[Los ninos aprenden matematicas] [al resolver 
			problemas.] 
			[Children learn mathematics] [when solving problems.] 
			The second one is due to differences between 
			annotators when determining nuclearity. For 
			example, in the following passage, one annotator 
			marked Background and the other one Elaboration: 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]S_Background [Norma y Andres quieren 
			colocar en el hueco una pecera. ]N_Background 
			[Quedo un hueco en la pared de 60 x 
			1.20cm.]N_Elaboration [Norma y Andres quieren 
			colocar en el hueco una pecera. ]S_Elaboration 
			[A hole of 60 x 1.20 cm remained in the wall.] [Norma and 
			Andres want to place a fish tank in the hole.] 
			It is easier to solve segmentation disagreement 
			than relations disagreement, since in this case 
			annotator subjectivity is more evident; we must 
			consider how to refine our manual in this sense. 
			3.7 Delivering and Maintaining the Product 
			Hovy (2010) mentions some technical issues 
			regarding these points: licensing, distribution, 
			maintenance and updates. With regard to licensing 
			and distribution, the RST Spanish Treebank will be 
			free for research purposes. We have a data 
			manager responsible for maintenance and updates. 
			The description of the annotated corpus is also 
			a very important issue (Ide and Pustejovsky, 2010). 
			It is important to provide a high level description 
			of the corpus, including the theoretical framework, 
			the methodology (annotators, annotation manual 
			and tool, agreement, etc.), the means for resource 
			maintenance, the technical aspects, the project 
			leader, the contact, the team, etc. The RST Spanish 
			Treebank includes all this detailed information. 
			XML (with a DTD) has been used, in order the 
			corpus can be reused for several aplications. In the 
			future, we plan to use the standard XCES. 
			7 
			To know more about resources development, 
			linguistic annotation or inter-annotator agreement, 
			we recommend: Palmer et al. (on-line), Palmer and 
			Xue (2010), and Artstein and Poesio (2008). 
			4 The Search Interface of the RST 
			Spanish Treebank 
			The RST Spanish Treebank interface is freely 
			available on-line7. It allows the visualization and 
			downloading of all the texts in txt format, with 
			their corresponding annotated trees in RSTtool 
			format (rs3), as well as in image format (jpg). Each 
			text includes its title, its reference, its web link (if 
			it is an on-line text) and its number of words. The 
			interface shows texts by areas and allows the user 
			to select a subcorpus (including individual files or 
			folders containing several files). The selected 
			subcorpus can be saved on local disk (generating a 
			xml file) for future analyses. 
			The interface includes a statistical tool which 
			allows obtaining statistics of rhetorical relations in 
			a subcorpus selected by the user. The RSTtool also 
			offers this option but it can be only used for one 
			text. We consider that it is more useful for the user 
			to obtain statistics from various texts, in order to 
			get significant statistical results. As the RSTtool, 
			our tool allows to count the multinuclear relations 
			in two ways: a) one unit for each detected 
			multinuclear relation, and b) one unit for each 
			detected nucleus. If we use b), the statistics of the 
			multinuclear relations of Table 2 are higher: List 
			(864), Joint (537), Sequence (289), Contrast (153), 
			Conjunction (28) and Disjunction (24). 
			We are developing another tool, aimed to 
			extract information from the annotated texts, which 
			we will soon include into the interface. This tool 
			will allow to the user to select a subcorpus and to 
			extract from it the EDUs corresponding to the 
			rhetorical relations selected, like a multidocument 
			specialized summarizer guided by user's interests. 
			The RST Spanish Treebank interface also 
			includes a screen which permits the users to send 
			their own annotated texts. Our aim is for the RST 
			Spanish Treebank to become a dynamic corpus, in 
			constant evolution, being increased with texts 
			annotated by users. This has a double advantage 
			since, on the one hand, the corpus will grow and, 
			on the other hand, users will profit from the 
			7 http://www.corpus.unam.mx/rst/ 
			interface's applications, using their own 
			subcorpora. The only requirement is to use the 
			relations and the segmentation and annotation 
			criteria of our project. Once the texts are sent, the 
			RST Spanish Treebank data manager will verify if 
			the annotation corresponds to these criteria. 
			2 Cut and paste in summarization 
			2.1 Related work in professional 
			summarizing 
			Professionals take two opposite positions on whether 
			a summary should be produced by cutting and past- 
			ing the original text. One school of scholars is 
			opposed; "(use) your own words... Do not keep 
			too close to the words before you", states an early 
			book on abstracting for American high school stu- 
			dents (Thurber, 1924). Another study, however, 
			shows that professional abstractors actually rely on 
			cutting and pasting to produce summaries: "Their 
			professional role tells abstractors to avoid inventing 
			anything. They follow the author as closely as pos- 
			sible and reintegrate the most important points of 
			a document in a shorter text" (Endres-Niggemeyer 
			et al., 1998). Some studies are somewhere in be- 
			tween: "summary language may or may not follow 
			that of author's" (Fidel, 1986). Other guidelines or 
			books on abstracting (ANSI, 1997; Cremmins, 1982) 
			do not discuss the issue. 
			Our cut and paste based summarization is a com- 
			putational model; we make no claim that humans 
			use the same cut and paste operations. 
			2.2 Cut and paste operations 
			We manually analyzed 30 articles and their corre- 
			sponding human-written summaries; the articles and 
			their summaries come from different domains ( 15 
			general news reports, 5 from the medical domain, 
			10 from the legal domain) and the summaries were 
			written by professionals from different organizations. 
			We found that reusing article text for summarization 
			is almost universal in the corpus we studied. We de- 
			fined six operations that can be used alone, sequen- 
			tially, or simultaneously to transform selected sen- 
			tences from an article into the corresponding sum- 
			mary sentences in its human-written abstract: 
			(1) sentence reduction 
			Remove extraneous phrases from a selected sen- 
			tence, as in the following example 1: 
			1 All the examples in this section were produced by human 
			professionals 
			The deleted material can be at any granularity: a 
			word, a phrase, or a clause. Multiple components 
			can be removed. 
			(2) sentence combination 
			Merge material from several sentences. It can be 
			used together with sentence reduction, as illustrated 
			in the following example, which also uses paraphras- 
			ing: 
			Text Sentence 1: But it also raises serious 
			questions about the privacy of such highly 
			personal information wafting about the digital 
			world. 
			Text Sentence 2: The issue thus fits squarely 
			into the broader debate about privacy and se- 
			curity on the internet, whether it involves pro- 
			tecting credit card number or keeping children 
			from offensive information. 
			Summary sentence: But it also raises the is- 
			sue of privacy of such personal information 
			and this issue hits the head on the nail in the 
			broader debate about privacy and security on 
			the internet. 
			(3) syntactic transformation 
			In both sentence reduction and combination, syn- 
			tactic transformations may be involved. For exam- 
			ple, the position of the subject in a sentence may be 
			moved from the end to the front. 
			(4) lexical paraphrasing 
			Replace phrases with their paraphrases. For in- 
			stance, the summaries substituted point out with 
			note, and fits squarely into with a more picturesque 
			description hits the head on the nail in the previous 
			examples. 
			(5) generalization or specification 
			Replace phrases or clauses with more general or 
			specific descriptions. Examples of generalization 
			and specification include: 
			Generalization: "a proposed new law that 
			would require Web publishers to obtain 
			parental consent before collecting personal in- 
			formation from children" --+ "legislation to 
			protect children's privacy on-line" 
			Specification: "the White House's top drug 
			official" ~ "Gen. Barry R. McCaffrey, the 
			White House's top drug official" 
			179 
			p . . . . . 
			,_e_ yr _', - 
			I , Co-reference ~, 
			I . . . . . . . . . I 
			,]WordNet'l 
			~ned ie~ -~ 
			Input~icle . 
			I ~ 
			I Sentenc i extractinl ) 
			extracteikey sentenc~ 
			Cut and paste based generation 
			[ Sentence reduction ] 
			I Sentence combinatio~ 
			Output summary 
			Figure 1: System architecture 
			(6) reordering 
			Change the order of extracted sentences. For in- 
			stance, place an ending sentence in an article at the 
			beginning of an abstract. 
			In human-written abstracts, there are, of course, 
			sentences that are not based on cut and paste, but 
			completely written from scratch. We used our de- 
			composition program to automatically analyze 300 
			human-written abstracts, and found that 19% of sen- 
			tences in the abstracts were written from scratch. 
			There are also other cut and paste operations not 
			listed here due to their infrequent occurrence. 
			3 System architecture 
			The architecture of our cut and paste based text 
			summarization system is shown in Figure 1. Input 
			to the system is a single document from any domain. 
			In the first stage, extraction, key sentences in the ar- 
			ticle are identified, as in most current summarizers. 
			In the second stage, cut and paste based generation, a 
			sentence reduction module and a sentence combina- 
			tion module implement the operations we observed 
			in human-written abstracts. 
			The cut and paste based component receives as 
			input not only the extracted key sentences, but also 
			the original article. This component can be ported 
			to other single-document summarizers to serve as 
			the generation component, since most current sum- 
			marizers extract key sentences - exactly what the 
			extraction module in our system does. 
			Other resources and tools in the summarization 
			system include a corpus of articles and their human- 
			written abstracts, the automatic decomposition pro- 
			gram, a syntactic parser, a co-reference resolution 
			system, the WordNet lexical database, and a large- 
			scale lexicon we combined from multiple resources. 
			The components in dotted lines are existing tools or 
			resources; all the others were developed by ourselves. 
			4 Major components 
			The main focus of our work is on decomposition of 
			summaries, sentence reduction, and sentence com- 
			bination. We also describe the sentence extraction 
			module, although it is not the main focus of our 
			work. 
			4.1 Decomposition of human-written 
			summary sentences 
			The decomposition program, see (Jing and McKe- 
			own, 1999) for details, is used to analyze the con- 
			struction of sentences in human-written abstracts. 
			The results from decomposition are used to build 
			the training and testing corpora for sentence reduc- 
			tion and sentence combination. 
			The decomposition program answers three ques- 
			tions about a sentence in a human-written abstract: 
			(1) Is the sentence constructed by cutting and past- 
			ing phrases from the input article? (2) If so, what 
			phrases in the sentence come from the original arti- 
			cle? (3) Where in the article do these phrases come 
			from? 
			We used a Hidden Markov Model (Baum, 1972) 
			solution to the decomposition problem. We first 
			mathematically formulated the problem, reducing it 
			to a problem of finding, for each word in a summary 
			180 
			Summary sentence: 
			(F0:S1 arthur b sackler vice president for law and public policy of time warner inc ) 
			(FI:S-1 and) (F2:S0 a member of the direct marketing association told ) (F3:$2 the com- 
			munications subcommittee of the senate commerce committee ) (F4:S-1 that legislation ) 
			(F5:Slto protect ) (F6:$4 children' s ) (F7:$4 privacy ) (F8:$4 online ) (F9:S0 could destroy 
			the spontaneous nature that makes the internet unique ) 
			Source document sentences: 
			Sentence 0: a proposed new law that would require web publishers to obtain parental consent before 
			collecting personal information from children (F9 could destroy the spontaneous nature that 
			makes the internet unique ) (F2 a member of the direct marketing association told) a 
			senate panel thursday 
			Sentence 1:(F0 arthur b sackler vice president for law and public policy of time warner 
			inc ) said the association supported efforts (F5 to protect ) children online but he urged lawmakers 
			to find some middle ground that also allows for interactivity on the internet 
			Sentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such 
			as updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online 
			magazine sackler said in testimony to (F3 the communications subcommittee of the senate 
			commerce committee ) 
			Sentence 4: the subcommittee is considering the (F6 children's ) (F8 online ) (F7 privacy ) 
			protection act which was drafted on the recommendation of the federal trade commission 
			Figure 2: Sample output of the decomposition program 
			sentence, a document position that it most likely 
			comes from. The position of a word in a document 
			is uniquely identified by the position of the sentence 
			where the word appears, and the position of the word 
			within the sentence. Based on the observation of cut 
			and paste practice by humans, we produced a set of 
			general heuristic rules. Sample heuristic rules in- 
			clude: two adjacent words in a summary sentence 
			are most likely to come from two adjacent words in 
			the original document; adjacent words in a summary 
			sentence are not very likely to come from sentences 
			that are far apart in the original document. We 
			use these heuristic rules to create a Hidden Markov 
			Model. The Viterbi algorithm (Viterbi, 1967) is used 
			to efficiently find the most likely document position 
			for each word in the summary sentence. 
			Figure 2 shows sample output of the program. 
			For the given summary sentence, the program cor- 
			rectly identified that the sentence was combined 
			from four sentences in the input article. It also di- 
			vided the summary sentence into phrases and pin- 
			pointed the exact document origin of each phrase. 
			A phrase in the summary sentence is annotated as 
			(FNUM:SNUM actual-text), where FNUM is the se- 
			quential number of the phrase and SNUM is the 
			number of the document sentence where the phrase 
			comes from. SNUM = -1 means that the compo- 
			nent does not come from the original document. The 
			phrases in the document sentences are annotated as 
			(FNUM actual-text). 
			4.2 Sentence reduction 
			The task of the sentence reduction module, de- 
			scribed in detail in (Jing, 2000), is to remove extra- 
			neous phrases from extracted sentences. The goal of 
			reduction is to "reduce without major loss"; that is, 
			we want to remove as many extraneous phrases as 
			possible from an extracted sentence so that it can be 
			concise, but without detracting from the main idea 
			that the sentence conveys. Ideally, we want to re- 
			move a phrase from an extracted sentence only if it 
			is irrelavant to the main topic. 
			Our reduction module makes decisions based on 
			multiple sources of knowledge: 
			(1) Grammar checking. In this step, we mark 
			which components of a sentence or a phrase are 
			obligatory to keep it grammatically correct. To do 
			this, we traverse the sentence parse tree, produced 
			by the English Slot Grammar(ESG) parser devel- 
			oped at IBM (McCord, 1990), in top-down order 
			and mark for each node in the parse tree, which 
			of its children are obligatory. The main source of 
			knowledge the system relies on in this step is a 
			large-scale, reusable lexicon we combined from mul- 
			tiple resources (Jing and McKeown, 1998). The lexi- 
			con contains subcategorizations for over 5,000 verbs. 
			This information is used to mark the obligatory ar- 
			guments of verb phrases. 
			(2) Context information. We use an extracted 
			sentence's local context in the article to decide which 
			components in the sentence are likely to be most 
			relevant to the main topic. We link the words in the 
			extracted sentence with words in its local context, 
			if they are repetitions, morphologically related, or 
			linked with each other in WordNet through certain 
			type of lexical relation, such as synonymy, antonymy, 
			or meronymy. Each word in the extracted sentence 
			gets an importance score, based on the number of 
			links it has with other words and the types of links. 
			Each phrase in the sentence is then assigned a score 
			181 
			Example 1: 
			Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give 
			parents a new and potentially revolutionary device to block out programs they don't 
			want their children to see. 
			Reduction program: The V-chip will give parents a new and potentially revolutionary device to 
			block out programs they don't want their children to see. 
			Professionals : The V-chip will give parents a device to block out programs they don't want 
			their children to see. 
			Example 2: 
			Original sentence : Sore and Hoffman's creation would allow broadcasters to insert 
			multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave 
			unexceptional portions o.f a show alone. 
			Reduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
			ings into a show. 
			Professionals : Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
			ings into a show. 
			Figure 3: Sample output of the 
			by adding up the scores of its children nodes in the 
			parse tree. This score indicates how important the 
			phrase is to the main topic in discussion. 
			(3) Corpus evidence. The program uses a cor- 
			pus of input articles and their corresponding reduced 
			forms in human-written abstracts to learn which 
			components of a sentence or a phrase can be re- 
			moved and how likely they are to be removed by 
			professionals. This corpus was created using the de- 
			composition program. We compute three types of 
			probabilities from this corpus: the probability that 
			a phrase is removed; the probability that a phrase is 
			reduced (i.e., the phrase is not removed as a whole, 
			but some components in the phrase are removed); 
			and the probability that a phrase is unchanged at 
			all (i.e., neither removed nor reduced). These cor- 
			pus probabilities help us capture human practice. 
			(4) Final decision. The final reduction decision 
			is based on the results from all the earlier steps. A 
			phrase is removed only if it is not grammatically 
			obligatory, not the focus of the local context (indi- 
			cated by a low context importance score), and has a 
			reasonable probability of being removed by humans. 
			The phrases we remove from an extracted sentence 
			include clauses, prepositional phrases, gerunds, and 
			to-infinitives. 
			The result of sentence reduction is a shortened 
			version of an extracted sentence 2. This shortened 
			text can be used directly as a summary, or it can 
			be fed to the sentence combination module to be 
			merged with other sentences. 
			Figure 3 shows two examples produced by the re- 
			duction program. The corresponding sentences in 
			human-written abstracts are also provided for com- 
			parison. 
			2It is actually also possible that the reduction program 
			decides no phrase in a sentence should be removed, thus the 
			result of reduction is the same as the input. 
			sentence reduction program 
			4.3 Sentence combination 
			To build the combination module, we first manu- 
			ally analyzed a corpus of combination examples pro- 
			duced by human professionals, automatically cre- 
			ated by the decomposition program, and identified 
			a list of combination operations. Table 1 shows the 
			combination operations. 
			To implement a combination operation, we need 
			to do two things: decide when to use which com- 
			bination operation, and implement the combining 
			actions. To decide when to use which operation, we 
			analyzed examples by humans and manually wrote 
			a set of rules. Two simple rules are shown in Fig- 
			ure 4. Sample outputs using these two simple rules 
			are shown in Figure 5. We are currently exploring 
			using machine learning techniques to learn the com- 
			bination rules from our corpus. 
			The implementation of the combining actions in- 
			volves joining two parse trees, substituting a subtree 
			with another, or adding additional nodes. We im- 
			plemented these actions using a formalism based on 
			Tree Adjoining Grammar (Joshi, 1987). 
			4.4 Extraction Module 
			The extraction module is the front end of the sum- 
			marization system and its role is to extract key sen- 
			tences. Our method is primarily based on lexical re- 
			lations. First, we link words in a sentence with other 
			words in the article through repetitions, morpholog- 
			ical relations, or one of the lexical relations encoded 
			in WordNet, similar to step 2 in sentence reduction. 
			An importance score is computed for each word in a 
			sentence based on the number of lexical links it has 
			with other words, the type of links, and the direc- 
			tions of the links. 
			After assigning a score to each word in a sentence, 
			we then compute a score for a sentence by adding up 
			the scores for each word. This score is then normal- 
			182 
			Categories Combination Operations 
			Add descriptions or names for people or organizations 
			Aggregations 
			Substitute incoherent phrases 
			Substitute phrases with more general or specific information 
			add description (see Figure 5) 
			add name 
			extract common subjects or objects (see Figure 5) 
			change one sentence to a clause 
			add connectives (e.g., and or while) 
			add punctuations (e.g., ";") 
			substitute dangling anaphora 
			substitute dangling noun phrases 
			substitute adverbs (e.g., here) 
			remove connectives 
			substitute with more general information 
			substitute with more specific information 
			Mixed operations combination of any of above operations (see Figure 2) 
			Table 1: Combination operations 
			Rule 1: 
			IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- 
			tion of the person or the organization exists somewhere in the original article but is missing in the 
			summary)) 
			THEN" replace the phrase with the full name plus the full description 
			Rule 2: 
			IF: ((two sentences are close to each other in the original article) and (their subjects refer to the 
			same entity) and (at least one of the sentences is the reduced form resulting from sentence reduc- 
			tion)) 
			THEN: merge the two sentences by removing the subject in the second sentence, and then com- 
			bining it with the first sentence using connective "and". 
			Figure 4: Sample sentence combination rules 
			ized over the number of words a sentence contains. 
			The sentences with high scores are considered im- 
			portant. 
			The extraction system selects sentences based on 
			the importance computed as above, as well as other 
			indicators, including sentence positions, cue phrases, 
			and tf*idf scores. 
			5 Evaluation 
			Our evaluation includes separate evaluations of each 
			module and the final evaluations of the overall sys- 
			tem. 
			We evaluated the decomposition program by two 
			experiments, described in (Jing and McKeown, 
			1999). In the first experiment, we selected 50 
			human-written abstracts, consisting of 305 sentences 
			in total. A human subject then read the decomposi- 
			tion results of these sentences to judge whether they 
			are correct. 93.8% of the sentences were correctly 
			decomposed. In the second experiment, we tested 
			the system in a summary alignment task. We ran 
			the decomposition program to identify the source 
			document sentences that were used to construct the 
			sentences in human-written abstracts. Human sub- 
			jects were also asked to select the document sen- 
			tences that are semantlc-equivalent to the sentences 
			in the abstracts. We compared the set of sentences 
			identified by the program with the set of sentences 
			selected by the majority of human subjects, which is 
			used as the gold standard in the computation of pre- 
			cision and recall. The program achieved an average 
			81.5% precision, 78.5% recall, and 79.1% f-measure 
			for 10 documents. The average performance of 14 
			human judges is 88.8% precision, 84.4% recall, and 
			85.7% f-measure. Recently, we have also tested the 
			system on legal documents (the headnotes used by 
			Westlaw company), and the program works well on 
			those documents too. 
			The evaluation of sentence reduction (see (Jing, 
			2000) for details) used a corpus of 500 sentences and 
			their reduced forms in human-written abstracts. 400 
			sentences were used to compute corpus probabili- 
			ties and 100 sentences were used for testing. The 
			results show that 81.3% of the reduction decisions 
			made by the system agreed with those of humans. 
			The humans reduced the length of the 500 sentences 
			by 44.2% on average, and the system reduced the 
			length of the 100 test sentences by 32.7%. 
			The evaluation of sentence combination module 
			is not as straightforward as that of decomposition 
			or reduction since combination happens later in the 
			pipeline and it depends on the output from prior 
			183 
			Example 1: add descriptions or names for people or organization 
			Original document sentences: 
			"We're trying to prove that there are big benefits to the patients by involving them more deeply in 
			their treatment", said Paul Clayton, Chairman of the Department dealing with comput- 
			erized medical information at Columbia. 
			"The economic payoff from breaking into health care records is a lot less than for 
			banks", said Clayton at Columbia. 
			Combined sentence: 
			"The economic payoff from breaking into health care records is a lot less than for banks", said Paul 
			Clayton, Chairman of the Department dealing with computerized medical information at Columbia. 
			Professional: (the same) 
			Example 2: extract common subjects 
			Original document sentences: 
			The new measure is an echo of the original bad idea, blurred just enough to cloud prospects 
			both for enforcement and for court review. 
			Unlike the 1996 act, this one applies only to commercial Web sites - thus sidestepping 
			1996 objections to the burden such regulations would pose for museums, libraries and freewheeling 
			conversation deemed "indecent" by somebody somewhere. 
			The new version also replaces the vague "indecency" standard, to which the court objected, 
			with the better-defined one of material ruled "harmful to minors." 
			Combined sentences: 
			The new measure is an echo of the original bad idea. 
			The new version applies only to commercial web sites and replaces the vague "indecency" standard 
			with the better-defined one of material ruled "harmful to minors." 
			Professional: 
			While the new law replaces the "indecency" standard with "harmful to minors" and now only 
			applies to commercial Web sites, the "new measure is an echo of the original bad idea." 
			Figure 5: Sample output of the sentence combination program 
			modules. To evaluate just the combination compo- 
			nent, we assume that the system makes the same 
			reduction decision as humans and the co-reference 
			system has a perfect performance. This involves 
			manual tagging of some examples to prepare for the 
			evaluation; this preparation is in progress. The eval- 
			uation of sentence combination will focus on the ac- 
			cessment of combination rules. 
			The overM1 system evMuation includes both in- 
			trinsic and extrinsic evaluation. In the intrinsic evM- 
			uation, we asked human subjects to compare the 
			quality of extraction-based summaries and their re- 
			vised versions produced by our sentence reduction 
			and combination modules. We selected 20 docu- 
			ments; three different automatic summarizers were 
			used to generate a summary for each document, pro- 
			ducing 60 summaries in total. These summaries 
			are all extraction-based. We then ran our sentence 
			reduction and sentence combination system to re- 
			vise the summaries, producing a revised version for 
			each summary. We presented human subjects with 
			the full documents, the extraction-based summaries, 
			and their revised versions, and asked them to com- 
			pare the extraction-based summaries and their re- 
			vised versions. The human subjects were asked to 
			score the conciseness of the summaries (extraction- 
			based or revised) based on a scale from 0 to 10 - 
			the higher the score, the more concise a summary is. 
			They were also asked to score the coherence of the 
			summaries based on a scale from 0 to 10. On aver- 
			age, the extraction-based summaries have a score of 
			4.2 for conciseness, while the revised summaries have 
			a score of 7.9 (an improvement of 88%). The average 
			improvement for the three systems are 78%, 105%, 
			and 88% respectively. The revised summaries are 
			on average 41% shorter than the original extraction- 
			based summaries. For summary coherence, the aver- 
			age score for the extraction-based summaries is 3.9, 
			while the average score for the revised summaries is 
			6.1 (an improvement of 56%). The average improve- 
			ment for the three systems are 69%, 57%, and 53% 
			respectively. 
			We are preparing a task-based evaluation, in 
			which we will use the data from the Summariza- 
			tion EvMuation Conference (Mani et al., 1998) and 
			compare how our revised summaries can influence 
			humans' performance in tasks like text categoriza- 
			tion and ad-hoc retrieval. 
			6 Related work 
			(Mani et al., 1999) addressed the problem of revising 
			summaries to improve their quality. They suggested 
			three types of operations: elimination, aggregation, 
			and smoothing. The goal of the elimination opera- 
			tion is similar to that of the sentence reduction op- 
			184 
			eration in our system. The difference is that while 
			elimination always removes parentheticals, sentence- 
			initial PPs and certain adverbial phrases for every 
			extracted sentence, our sentence reduction module 
			aims to make reduction decisions according to each 
			case and removes a sentence component only if it 
			considers it appropriate to do so. The goal of the 
			aggregation operation and the smoothing operation 
			is similar to that of the sentence combination op- 
			eration in our system. However, the combination 
			operations and combination rules that we derived 
			from corpus analysis are significantly different from 
			those used in the above system, which mostly came 
			from operations in traditional natural language gen- 
			eration. 
			2 Cut and paste in summarization 
			2.1 Related work in professional 
			summarizing 
			Professionals take two opposite positions on whether 
			a summary should be produced by cutting and past- 
			ing the original text. One school of scholars is 
			opposed; "(use) your own words... Do not keep 
			too close to the words before you", states an early 
			book on abstracting for American high school stu- 
			dents (Thurber, 1924). Another study, however, 
			shows that professional abstractors actually rely on 
			cutting and pasting to produce summaries: "Their 
			professional role tells abstractors to avoid inventing 
			anything. They follow the author as closely as pos- 
			sible and reintegrate the most important points of 
			a document in a shorter text" (Endres-Niggemeyer 
			et al., 1998). Some studies are somewhere in be- 
			tween: "summary language may or may not follow 
			that of author's" (Fidel, 1986). Other guidelines or 
			books on abstracting (ANSI, 1997; Cremmins, 1982) 
			do not discuss the issue. 
			Our cut and paste based summarization is a com- 
			putational model; we make no claim that humans 
			use the same cut and paste operations. 
			2.2 Cut and paste operations 
			We manually analyzed 30 articles and their corre- 
			sponding human-written summaries; the articles and 
			their summaries come from different domains ( 15 
			general news reports, 5 from the medical domain, 
			10 from the legal domain) and the summaries were 
			written by professionals from different organizations. 
			We found that reusing article text for summarization 
			is almost universal in the corpus we studied. We de- 
			fined six operations that can be used alone, sequen- 
			tially, or simultaneously to transform selected sen- 
			tences from an article into the corresponding sum- 
			mary sentences in its human-written abstract: 
			(1) sentence reduction 
			Remove extraneous phrases from a selected sen- 
			tence, as in the following example 1: 
			1 All the examples in this section were produced by human 
			professionals 
			The deleted material can be at any granularity: a 
			word, a phrase, or a clause. Multiple components 
			can be removed. 
			(2) sentence combination 
			Merge material from several sentences. It can be 
			used together with sentence reduction, as illustrated 
			in the following example, which also uses paraphras- 
			ing: 
			Text Sentence 1: But it also raises serious 
			questions about the privacy of such highly 
			personal information wafting about the digital 
			world. 
			Text Sentence 2: The issue thus fits squarely 
			into the broader debate about privacy and se- 
			curity on the internet, whether it involves pro- 
			tecting credit card number or keeping children 
			from offensive information. 
			Summary sentence: But it also raises the is- 
			sue of privacy of such personal information 
			and this issue hits the head on the nail in the 
			broader debate about privacy and security on 
			the internet. 
			(3) syntactic transformation 
			In both sentence reduction and combination, syn- 
			tactic transformations may be involved. For exam- 
			ple, the position of the subject in a sentence may be 
			moved from the end to the front. 
			(4) lexical paraphrasing 
			Replace phrases with their paraphrases. For in- 
			stance, the summaries substituted point out with 
			note, and fits squarely into with a more picturesque 
			description hits the head on the nail in the previous 
			examples. 
			(5) generalization or specification 
			Replace phrases or clauses with more general or 
			specific descriptions. Examples of generalization 
			and specification include: 
			Generalization: "a proposed new law that 
			would require Web publishers to obtain 
			parental consent before collecting personal in- 
			formation from children" --+ "legislation to 
			protect children's privacy on-line" 
			Specification: "the White House's top drug 
			official" ~ "Gen. Barry R. McCaffrey, the 
			White House's top drug official" 
			179 
			p . . . . . 
			,_e_ yr _', - 
			I , Co-reference ~, 
			I . . . . . . . . . I 
			,]WordNet'l 
			~ned ie~ -~ 
			Input~icle . 
			I ~ 
			I Sentenc i extractinl ) 
			extracteikey sentenc~ 
			Cut and paste based generation 
			[ Sentence reduction ] 
			I Sentence combinatio~ 
			Output summary 
			Figure 1: System architecture 
			(6) reordering 
			Change the order of extracted sentences. For in- 
			stance, place an ending sentence in an article at the 
			beginning of an abstract. 
			In human-written abstracts, there are, of course, 
			sentences that are not based on cut and paste, but 
			completely written from scratch. We used our de- 
			composition program to automatically analyze 300 
			human-written abstracts, and found that 19% of sen- 
			tences in the abstracts were written from scratch. 
			There are also other cut and paste operations not 
			listed here due to their infrequent occurrence. 
			3 System architecture 
			The architecture of our cut and paste based text 
			summarization system is shown in Figure 1. Input 
			to the system is a single document from any domain. 
			In the first stage, extraction, key sentences in the ar- 
			ticle are identified, as in most current summarizers. 
			In the second stage, cut and paste based generation, a 
			sentence reduction module and a sentence combina- 
			tion module implement the operations we observed 
			in human-written abstracts. 
			The cut and paste based component receives as 
			input not only the extracted key sentences, but also 
			the original article. This component can be ported 
			to other single-document summarizers to serve as 
			the generation component, since most current sum- 
			marizers extract key sentences - exactly what the 
			extraction module in our system does. 
			Other resources and tools in the summarization 
			system include a corpus of articles and their human- 
			written abstracts, the automatic decomposition pro- 
			gram, a syntactic parser, a co-reference resolution 
			system, the WordNet lexical database, and a large- 
			scale lexicon we combined from multiple resources. 
			The components in dotted lines are existing tools or 
			resources; all the others were developed by ourselves. 
			4 Major components 
			The main focus of our work is on decomposition of 
			summaries, sentence reduction, and sentence com- 
			bination. We also describe the sentence extraction 
			module, although it is not the main focus of our 
			work. 
			4.1 Decomposition of human-written 
			summary sentences 
			The decomposition program, see (Jing and McKe- 
			own, 1999) for details, is used to analyze the con- 
			struction of sentences in human-written abstracts. 
			The results from decomposition are used to build 
			the training and testing corpora for sentence reduc- 
			tion and sentence combination. 
			The decomposition program answers three ques- 
			tions about a sentence in a human-written abstract: 
			(1) Is the sentence constructed by cutting and past- 
			ing phrases from the input article? (2) If so, what 
			phrases in the sentence come from the original arti- 
			cle? (3) Where in the article do these phrases come 
			from? 
			We used a Hidden Markov Model (Baum, 1972) 
			solution to the decomposition problem. We first 
			mathematically formulated the problem, reducing it 
			to a problem of finding, for each word in a summary 
			180 
			Summary sentence: 
			(F0:S1 arthur b sackler vice president for law and public policy of time warner inc ) 
			(FI:S-1 and) (F2:S0 a member of the direct marketing association told ) (F3:$2 the com- 
			munications subcommittee of the senate commerce committee ) (F4:S-1 that legislation ) 
			(F5:Slto protect ) (F6:$4 children' s ) (F7:$4 privacy ) (F8:$4 online ) (F9:S0 could destroy 
			the spontaneous nature that makes the internet unique ) 
			Source document sentences: 
			Sentence 0: a proposed new law that would require web publishers to obtain parental consent before 
			collecting personal information from children (F9 could destroy the spontaneous nature that 
			makes the internet unique ) (F2 a member of the direct marketing association told) a 
			senate panel thursday 
			Sentence 1:(F0 arthur b sackler vice president for law and public policy of time warner 
			inc ) said the association supported efforts (F5 to protect ) children online but he urged lawmakers 
			to find some middle ground that also allows for interactivity on the internet 
			Sentence 2: for example a child's e-mail address is necessary in order to respond to inquiries such 
			as updates on mark mcguire's and sammy sosa's home run figures this year or updates of an online 
			magazine sackler said in testimony to (F3 the communications subcommittee of the senate 
			commerce committee ) 
			Sentence 4: the subcommittee is considering the (F6 children's ) (F8 online ) (F7 privacy ) 
			protection act which was drafted on the recommendation of the federal trade commission 
			Figure 2: Sample output of the decomposition program 
			sentence, a document position that it most likely 
			comes from. The position of a word in a document 
			is uniquely identified by the position of the sentence 
			where the word appears, and the position of the word 
			within the sentence. Based on the observation of cut 
			and paste practice by humans, we produced a set of 
			general heuristic rules. Sample heuristic rules in- 
			clude: two adjacent words in a summary sentence 
			are most likely to come from two adjacent words in 
			the original document; adjacent words in a summary 
			sentence are not very likely to come from sentences 
			that are far apart in the original document. We 
			use these heuristic rules to create a Hidden Markov 
			Model. The Viterbi algorithm (Viterbi, 1967) is used 
			to efficiently find the most likely document position 
			for each word in the summary sentence. 
			Figure 2 shows sample output of the program. 
			For the given summary sentence, the program cor- 
			rectly identified that the sentence was combined 
			from four sentences in the input article. It also di- 
			vided the summary sentence into phrases and pin- 
			pointed the exact document origin of each phrase. 
			A phrase in the summary sentence is annotated as 
			(FNUM:SNUM actual-text), where FNUM is the se- 
			quential number of the phrase and SNUM is the 
			number of the document sentence where the phrase 
			comes from. SNUM = -1 means that the compo- 
			nent does not come from the original document. The 
			phrases in the document sentences are annotated as 
			(FNUM actual-text). 
			4.2 Sentence reduction 
			The task of the sentence reduction module, de- 
			scribed in detail in (Jing, 2000), is to remove extra- 
			neous phrases from extracted sentences. The goal of 
			reduction is to "reduce without major loss"; that is, 
			we want to remove as many extraneous phrases as 
			possible from an extracted sentence so that it can be 
			concise, but without detracting from the main idea 
			that the sentence conveys. Ideally, we want to re- 
			move a phrase from an extracted sentence only if it 
			is irrelavant to the main topic. 
			Our reduction module makes decisions based on 
			multiple sources of knowledge: 
			(1) Grammar checking. In this step, we mark 
			which components of a sentence or a phrase are 
			obligatory to keep it grammatically correct. To do 
			this, we traverse the sentence parse tree, produced 
			by the English Slot Grammar(ESG) parser devel- 
			oped at IBM (McCord, 1990), in top-down order 
			and mark for each node in the parse tree, which 
			of its children are obligatory. The main source of 
			knowledge the system relies on in this step is a 
			large-scale, reusable lexicon we combined from mul- 
			tiple resources (Jing and McKeown, 1998). The lexi- 
			con contains subcategorizations for over 5,000 verbs. 
			This information is used to mark the obligatory ar- 
			guments of verb phrases. 
			(2) Context information. We use an extracted 
			sentence's local context in the article to decide which 
			components in the sentence are likely to be most 
			relevant to the main topic. We link the words in the 
			extracted sentence with words in its local context, 
			if they are repetitions, morphologically related, or 
			linked with each other in WordNet through certain 
			type of lexical relation, such as synonymy, antonymy, 
			or meronymy. Each word in the extracted sentence 
			gets an importance score, based on the number of 
			links it has with other words and the types of links. 
			Each phrase in the sentence is then assigned a score 
			181 
			Example 1: 
			Original sentence : When it arrives sometime next year in new TV sets, the V-chip will give 
			parents a new and potentially revolutionary device to block out programs they don't 
			want their children to see. 
			Reduction program: The V-chip will give parents a new and potentially revolutionary device to 
			block out programs they don't want their children to see. 
			Professionals : The V-chip will give parents a device to block out programs they don't want 
			their children to see. 
			Example 2: 
			Original sentence : Sore and Hoffman's creation would allow broadcasters to insert 
			multiple ratings into a show, enabling the V-chip to filter out racy or violent material but leave 
			unexceptional portions o.f a show alone. 
			Reduction Program: Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
			ings into a show. 
			Professionals : Som and Hoffman's creation would allow broadcasters to insert multiple rat- 
			ings into a show. 
			Figure 3: Sample output of the 
			by adding up the scores of its children nodes in the 
			parse tree. This score indicates how important the 
			phrase is to the main topic in discussion. 
			(3) Corpus evidence. The program uses a cor- 
			pus of input articles and their corresponding reduced 
			forms in human-written abstracts to learn which 
			components of a sentence or a phrase can be re- 
			moved and how likely they are to be removed by 
			professionals. This corpus was created using the de- 
			composition program. We compute three types of 
			probabilities from this corpus: the probability that 
			a phrase is removed; the probability that a phrase is 
			reduced (i.e., the phrase is not removed as a whole, 
			but some components in the phrase are removed); 
			and the probability that a phrase is unchanged at 
			all (i.e., neither removed nor reduced). These cor- 
			pus probabilities help us capture human practice. 
			(4) Final decision. The final reduction decision 
			is based on the results from all the earlier steps. A 
			phrase is removed only if it is not grammatically 
			obligatory, not the focus of the local context (indi- 
			cated by a low context importance score), and has a 
			reasonable probability of being removed by humans. 
			The phrases we remove from an extracted sentence 
			include clauses, prepositional phrases, gerunds, and 
			to-infinitives. 
			The result of sentence reduction is a shortened 
			version of an extracted sentence 2. This shortened 
			text can be used directly as a summary, or it can 
			be fed to the sentence combination module to be 
			merged with other sentences. 
			Figure 3 shows two examples produced by the re- 
			duction program. The corresponding sentences in 
			human-written abstracts are also provided for com- 
			parison. 
			2It is actually also possible that the reduction program 
			decides no phrase in a sentence should be removed, thus the 
			result of reduction is the same as the input. 
			sentence reduction program 
			4.3 Sentence combination 
			To build the combination module, we first manu- 
			ally analyzed a corpus of combination examples pro- 
			duced by human professionals, automatically cre- 
			ated by the decomposition program, and identified 
			a list of combination operations. Table 1 shows the 
			combination operations. 
			To implement a combination operation, we need 
			to do two things: decide when to use which com- 
			bination operation, and implement the combining 
			actions. To decide when to use which operation, we 
			analyzed examples by humans and manually wrote 
			a set of rules. Two simple rules are shown in Fig- 
			ure 4. Sample outputs using these two simple rules 
			are shown in Figure 5. We are currently exploring 
			using machine learning techniques to learn the com- 
			bination rules from our corpus. 
			The implementation of the combining actions in- 
			volves joining two parse trees, substituting a subtree 
			with another, or adding additional nodes. We im- 
			plemented these actions using a formalism based on 
			Tree Adjoining Grammar (Joshi, 1987). 
			4.4 Extraction Module 
			The extraction module is the front end of the sum- 
			marization system and its role is to extract key sen- 
			tences. Our method is primarily based on lexical re- 
			lations. First, we link words in a sentence with other 
			words in the article through repetitions, morpholog- 
			ical relations, or one of the lexical relations encoded 
			in WordNet, similar to step 2 in sentence reduction. 
			An importance score is computed for each word in a 
			sentence based on the number of lexical links it has 
			with other words, the type of links, and the direc- 
			tions of the links. 
			After assigning a score to each word in a sentence, 
			we then compute a score for a sentence by adding up 
			the scores for each word. This score is then normal- 
			182 
			Categories Combination Operations 
			Add descriptions or names for people or organizations 
			Aggregations 
			Substitute incoherent phrases 
			Substitute phrases with more general or specific information 
			add description (see Figure 5) 
			add name 
			extract common subjects or objects (see Figure 5) 
			change one sentence to a clause 
			add connectives (e.g., and or while) 
			add punctuations (e.g., ";") 
			substitute dangling anaphora 
			substitute dangling noun phrases 
			substitute adverbs (e.g., here) 
			remove connectives 
			substitute with more general information 
			substitute with more specific information 
			Mixed operations combination of any of above operations (see Figure 2) 
			Table 1: Combination operations 
			Rule 1: 
			IF: ((a person or an organization is mentioned the first time) and (the full name or the full descrip- 
			tion of the person or the organization exists somewhere in the original article but is missing in the 
			summary)) 
			THEN" replace the phrase with the full name plus the full description 
			Rule 2: 
			IF: ((two sentences are close to each other in the original article) and (their subjects refer to the 
			same entity) and (at least one of the sentences is the reduced form resulting from sentence reduc- 
			tion)) 
			THEN: merge the two sentences by removing the subject in the second sentence, and then com- 
			bining it with the first sentence using connective "and". 
			Figure 4: Sample sentence combination rules 
			ized over the number of words a sentence contains. 
			The sentences with high scores are considered im- 
			portant. 
			The extraction system selects sentences based on 
			the importance computed as above, as well as other 
			indicators, including sentence positions, cue phrases, 
			and tf*idf scores. 
			5 Evaluation 
			Our evaluation includes separate evaluations of each 
			module and the final evaluations of the overall sys- 
			tem. 
			We evaluated the decomposition program by two 
			experiments, described in (Jing and McKeown, 
			1999). In the first experiment, we selected 50 
			human-written abstracts, consisting of 305 sentences 
			in total. A human subject then read the decomposi- 
			tion results of these sentences to judge whether they 
			are correct. 93.8% of the sentences were correctly 
			decomposed. In the second experiment, we tested 
			the system in a summary alignment task. We ran 
			the decomposition program to identify the source 
			document sentences that were used to construct the 
			sentences in human-written abstracts. Human sub- 
			jects were also asked to select the document sen- 
			tences that are semantlc-equivalent to the sentences 
			in the abstracts. We compared the set of sentences 
			identified by the program with the set of sentences 
			selected by the majority of human subjects, which is 
			used as the gold standard in the computation of pre- 
			cision and recall. The program achieved an average 
			81.5% precision, 78.5% recall, and 79.1% f-measure 
			for 10 documents. The average performance of 14 
			human judges is 88.8% precision, 84.4% recall, and 
			85.7% f-measure. Recently, we have also tested the 
			system on legal documents (the headnotes used by 
			Westlaw company), and the program works well on 
			those documents too. 
			The evaluation of sentence reduction (see (Jing, 
			2000) for details) used a corpus of 500 sentences and 
			their reduced forms in human-written abstracts. 400 
			sentences were used to compute corpus probabili- 
			ties and 100 sentences were used for testing. The 
			results show that 81.3% of the reduction decisions 
			made by the system agreed with those of humans. 
			The humans reduced the length of the 500 sentences 
			by 44.2% on average, and the system reduced the 
			length of the 100 test sentences by 32.7%. 
			The evaluation of sentence combination module 
			is not as straightforward as that of decomposition 
			or reduction since combination happens later in the 
			pipeline and it depends on the output from prior 
			183 
			Example 1: add descriptions or names for people or organization 
			Original document sentences: 
			"We're trying to prove that there are big benefits to the patients by involving them more deeply in 
			their treatment", said Paul Clayton, Chairman of the Department dealing with comput- 
			erized medical information at Columbia. 
			"The economic payoff from breaking into health care records is a lot less than for 
			banks", said Clayton at Columbia. 
			Combined sentence: 
			"The economic payoff from breaking into health care records is a lot less than for banks", said Paul 
			Clayton, Chairman of the Department dealing with computerized medical information at Columbia. 
			Professional: (the same) 
			Example 2: extract common subjects 
			Original document sentences: 
			The new measure is an echo of the original bad idea, blurred just enough to cloud prospects 
			both for enforcement and for court review. 
			Unlike the 1996 act, this one applies only to commercial Web sites - thus sidestepping 
			1996 objections to the burden such regulations would pose for museums, libraries and freewheeling 
			conversation deemed "indecent" by somebody somewhere. 
			The new version also replaces the vague "indecency" standard, to which the court objected, 
			with the better-defined one of material ruled "harmful to minors." 
			Combined sentences: 
			The new measure is an echo of the original bad idea. 
			The new version applies only to commercial web sites and replaces the vague "indecency" standard 
			with the better-defined one of material ruled "harmful to minors." 
			Professional: 
			While the new law replaces the "indecency" standard with "harmful to minors" and now only 
			applies to commercial Web sites, the "new measure is an echo of the original bad idea." 
			Figure 5: Sample output of the sentence combination program 
			modules. To evaluate just the combination compo- 
			nent, we assume that the system makes the same 
			reduction decision as humans and the co-reference 
			system has a perfect performance. This involves 
			manual tagging of some examples to prepare for the 
			evaluation; this preparation is in progress. The eval- 
			uation of sentence combination will focus on the ac- 
			cessment of combination rules. 
			The overM1 system evMuation includes both in- 
			trinsic and extrinsic evaluation. In the intrinsic evM- 
			uation, we asked human subjects to compare the 
			quality of extraction-based summaries and their re- 
			vised versions produced by our sentence reduction 
			and combination modules. We selected 20 docu- 
			ments; three different automatic summarizers were 
			used to generate a summary for each document, pro- 
			ducing 60 summaries in total. These summaries 
			are all extraction-based. We then ran our sentence 
			reduction and sentence combination system to re- 
			vise the summaries, producing a revised version for 
			each summary. We presented human subjects with 
			the full documents, the extraction-based summaries, 
			and their revised versions, and asked them to com- 
			pare the extraction-based summaries and their re- 
			vised versions. The human subjects were asked to 
			score the conciseness of the summaries (extraction- 
			based or revised) based on a scale from 0 to 10 - 
			the higher the score, the more concise a summary is. 
			They were also asked to score the coherence of the 
			summaries based on a scale from 0 to 10. On aver- 
			age, the extraction-based summaries have a score of 
			4.2 for conciseness, while the revised summaries have 
			a score of 7.9 (an improvement of 88%). The average 
			improvement for the three systems are 78%, 105%, 
			and 88% respectively. The revised summaries are 
			on average 41% shorter than the original extraction- 
			based summaries. For summary coherence, the aver- 
			age score for the extraction-based summaries is 3.9, 
			while the average score for the revised summaries is 
			6.1 (an improvement of 56%). The average improve- 
			ment for the three systems are 69%, 57%, and 53% 
			respectively. 
			We are preparing a task-based evaluation, in 
			which we will use the data from the Summariza- 
			tion EvMuation Conference (Mani et al., 1998) and 
			compare how our revised summaries can influence 
			humans' performance in tasks like text categoriza- 
			tion and ad-hoc retrieval. 
			6 Related work 
			(Mani et al., 1999) addressed the problem of revising 
			summaries to improve their quality. They suggested 
			three types of operations: elimination, aggregation, 
			and smoothing. The goal of the elimination opera- 
			tion is similar to that of the sentence reduction op- 
			184 
			eration in our system. The difference is that while 
			elimination always removes parentheticals, sentence- 
			initial PPs and certain adverbial phrases for every 
			extracted sentence, our sentence reduction module 
			aims to make reduction decisions according to each 
			case and removes a sentence component only if it 
			considers it appropriate to do so. The goal of the 
			aggregation operation and the smoothing operation 
			is similar to that of the sentence combination op- 
			eration in our system. However, the combination 
			operations and combination rules that we derived 
			from corpus analysis are significantly different from 
			those used in the above system, which mostly came 
			from operations in traditional natural language gen- 
			eration. 
			II. INDUSTRIAL CONTEXT 
			Figure 1 presents the context of this work in VOCA- 
			GEN project. Our industrial partner Script&Go1 develop an 
			application for the construction management dedicated to 
			touch devices and wishes to set up an oral dialogue module 
			to facilitate on construction site seizure. The second industrial 
			partner (Tykomz) develops a vocal recognition suite based on 
			toolkit sphynx 4 [1]. This toolkit includes hierarchical agglom- 
			erative clustering methods using well-known measures such as 
			BIC and CLR and provides elementary tools, such as segment 
			and cluster generators, decoder and model trainers. Fitting 
			those elementary tools together is an easy way of developing 
			a specific diarization system. To work, it is necessary to build 
			a model of knowledge, i.e. a model describing the expressions 
			that must be recognized by the program. To improve the 
			performance of the system, this knowledge model must be 
			1http://www.scriptandgo.com/en/ 
			Fig. 1. figure describing the context of the project 
			powered by a domain-specific vocabulary. For example, in the 
			sentence "there is a stain of paint in the kitchen", the system 
			must understand that it is a stain of paint and that the kitchen is 
			a room. To our knowledge, there is no ontology or taxonomy 
			specific to the construction industry in French. A version is 
			under development by [2] but ontology is in English and very 
			generic. We therefore choose to extract useful knowledge from 
			textual data, and then, in a second step, to organize them. 
			III. RELATED WORKS 
			The goal of ontology learning (OL) is to build knowledge 
			models from text. OL use NLP knowledge extraction tools 
			to extract terminology (concepts) and links between them 
			(relationships). The main approaches found in the literature are 
			rule-based systems, statistical or learning based approaches. 
			The reference in the field of rule-based systems was de- 
			veloped by [3]. General Architecture for Text Engineering 
			(GATE) is a Java collection of tools initially developed at the 
			University of Sheffield since 1995. An alternative is offered by 
			the existing semi-automatic ontology learning text2onto [4]. 
			More recently, [5] developed UIMA, a system that can be 
			positioned as an alternative to GATE. Amongst other things, 
			UIMA makes possible to generate rules from a collection of 
			annotated documents. Exit, introduced by [6] is an iterative 
			approach that finds the terms in an incremental way. 
			[7] with TERMINAE is certainly the oldest statistic ap- 
			proach. Developed for French and based on lexical frequen- 
			cies, it requires pre-processing with TermoStat [8] and ANNIE 
			(GATE). [9] presents a method for extracting terminology spe- 
			cific to a domain from a corpus of domain-specific text, where 
			no external general domain reference corpus is required. They 
			present an adaptation of the classic tf-idf as ranking measure 
			and use different filters to produce a specific terminology. 
			More recently, the efficiency of ranking measure like mutual 
			information developed for statistical approach is discussed in 
			[10] and [11]. [12] proposes Termolator a terminology extrac- 
			tion system using a chunking procedure, and using internet 
			queries for ranking candidate terms. Approach is interesting 
			but the authors emphasize the fact that the runtime for each 
			queries is a limiting factor to produce a relevant ranking. 
			Closer to our work, [13] presents an approach combining 
			linguistic pattern and Z-score to extract terminology in the 
			field of nanotechnology. [14] propose TAXI, which combines 
			statistics and learning approach. TAXI is a system for building 
			a taxonomy using 2 corpora, a generic, the other specific. 
			It ranks the relevance of candidates by measure (frequency- 
			based), and by learning with SVM. [15], [16] present TexSIS, 
			a bilingual terminology extraction system with chunk-based 
			alignment method for the generation of candidate terms. After 
			the corpus alignment step, they use an approach combining 
			log likelihood measure and Mutual Expectation measure [17] 
			to rank candidate terms in each language. In the same or- 
			der, [18], [19] present an approach to extract grammatical 
			terminology from linguistic corpora. They compare a series 
			of well-established statistical measures that have been used 
			in similar automatic term extraction tasks and conclude that 
			corpus-comparing methods perform better than metrics that 
			are not based on corpus comparison. [20] and [21] present 
			methods with word embeddings. With a small data set for 
			learning phase, they improve the term extraction results in 
			n-gram terms. However, these papers involve labelled data 
			sets for learning phase, which is the main difference with 
			our proposed approach. The originality of our approach is to 
			combine a lexico-syntactical and a statistical approach while 
			using external resources. 
			IV. RESOURCES AND STATISTICS 
			First experiments were carried out using technical reports 
			collected from some customers from our industrial partners 
			who will be called as NC collection thereafter. Each document 
			contains all the non-compliance that was found on one work 
			site and describe solutions to resolve it. However heterogeneity 
			of the formats as well as the artificial repetition of the informa- 
			tion between two reports found in the same construction site 
			made the term extraction quite difficult. An insightful analysis 
			of those reports reveals vocabulary richness, however, difficult 
			to exploit given numerous misspellings, typing shortcuts, very 
			"telegraphic" style with verbs in the infinitive, little punctu- 
			ation, few determinants, etc. As a consequence, we used a 
			collection of technical specifications called CCTP2. CCTPs are 
			available online on public sector websites3. Several thousand 
			documents were collected by our industrial partner using an 
			automatic web collecting process. Figure 2 presents some key 
			descriptive statistics of theses collections. 
			Collection NC CCTP 
			Total number of documents 58 402 3665 
			Without pre-processing 
			Total number of words 130 309 230 962 734 
			Total number of different words 93 000 20 6264 
			Average words/document 125.3 63 018.48 
			Fig. 2. statistics of the collection. 
			2The technical specifications book (CCTP in French) is a contractual 
			document that gathers the technical clauses of a public contract in the field 
			of construction. 
			3For example, https://www.marches-publics.gouv.fr/ or 
			http://marchespublics.normandie.fr/. 
			ranking 
			CCTP 
			CR 1 06/02 
			Lot A : 
			Siphon independant 
			sous evier 
			CR 2 15/02 
			Lot A : 
			- Siphon independant 
			sous evier 
			Lot B : 
			- Pose des plinthes 
			sur la cloison 
			CR 3 22/02 
			Lot A : 
			- Siphon independant 
			sous evier 
			Lot B : 
			- suppression de la 
			cloison entre wc et sdb 
			words ngrams 
			extraction 
			pruning 
			pre-processing 
			citernea 
			u 
			devant entree 
			marche 
			s 
			chez techniplan 
			coulage par defaut 
			proposit 
			ions 
			d' auge 
			typologi 
			e 
			des citerneaux 
			espace d' activite 
			vision depuis ascenseur 
			vaissell 
			e 
			sous bar 
			mise en place 
			coulisse 
			au 
			pour douchette 
			terminology 
			  
			 
			 
			Fig. 3. System overview 
			V. METHODOLOGY 
			A. System Overview 
			Figure 3 presents an overview of the system designed and 
			implemented: steps are further explained in Sections V-B to 
			V-E. In Step 1 pre-processing of raw information extracted 
			from CCTP collection takes place; this is required for nor- 
			malizing the entire set of documents. In Step 2 n-grams are 
			extracted (by using measures). 1,2,3 grams are extracted. In 
			Step 3, n-grams are filtered by using linguistic patterns and 
			Internet queries. Finally, in Step 4 a ranking is applied to the 
			filtered n-grams. 
			B. Normalization, pre-processing and word ngrams extraction 
			In Step 1, a text normalization is performed to improve the 
			quality of the process. We remove special characters such as 
			"/" or "()". Different pretreatments are done to reduce noise 
			in the model: we remove numbers (numeric and/or textual), 
			special symbols. "." are tagged with a special character to 
			not create artificial n-grams. Specific words (including named 
			entities) like company names, dates, etc. are normalized and 
			will be removed in the next module. We do not include a 
			stop list to keep n-grams with prepositions, for the purpose 
			described in the remainder. Then, we tokenize the entire 
			collection before using TreeTagger [22] to get the part-of- 
			speech tags and lemmas of each word. After this step we 
			transform all vocabulary from the CCTP collection into 1- 
			grams, 2-grams and 3-grams. Special characters or normalized 
			words resulting from the previous processing are discarded. N- 
			grams with a very low frequency (2) are also discarded. 
			C. Linguistic patterns module 
			We use grammatical labels generated in the previous step 
			(section V-B) and linguistic patterns to retrieve collocations 
			such as NOUN-NOUN, NOUN-PREP-NOUN. These patterns 
			are frequently found in the literature [6] to capture specific 
			words in French like "carte de cr 
			edit"(credit card) and discard 
			3-gram like 'cr 
			editer sa carte' (credit his card) with the 
			pattern VERB-PREP-NOUN. Among frequent patterns found 
			in literature, those patterns have been selected according to the 
			statistics obtained from a knowledge model of another field 
			(agriculture), given by one of our industrial partners. Figure 
			4 presents main patterns we selected using this knowledge 
			model. The sum of the percentages is less than 100% because 
			Number Percentage 
			1-grams 1360 65.24% 
			NOUN 1037 76.25% 
			VERB 194 14.26% 
			ADJ 120 8.82% 
			2-grams 390 19.57% 
			NOUN-NOUN 346 88.72% 
			ADJ-NOUN 11 2.82% 
			PREP-NOUN 7 1,79% 
			VERB-NOUN 5 1,28% 
			3-grams 188 9.43% 
			NOUN-NOUN-NOUN 150 79.79% 
			PREP-NOUN-NOUN 15 7.98% 
			NOUN-PREP-NOUN 6 3,19% 
			VERB-NOUN-NOUN 6 3,19% 
			Fig. 4. Distribution of linguistic patterns according to the knowledge model. 
			patterns with a very low frequency are not included in the 
			table. We observed that the noun based patterns are the most 
			frequent patterns, whatever the size of the n-gram. The other 
			selected patterns also contain nouns, but they are n-grams 
			with verbs, adjectives or prepositions. Therefore, we have 
			configured our system to keep only the ngrams corresponding 
			to these patterns. 
			D. Pruning step 
			This step uses the Internet to prune n-grams for which no 
			information is returned after querying Bing4 search engine. 
			We count the number of links in the result pages that contain 
			exactly the ngram. We save the number of exact matches 
			between the ngram and the title and snippet of each result. 
			We keep only the n-grams whose number of matches exceeds 
			a defined threshold. We varied this threshold between 1 and 
			50 and results presented in Section VI-B have been obtained 
			with a threshold of 10. 
			E. Ranking step 
			We tested several measures as provided in [6], [16] like 
			maximum likelihood estimation or mutual information in 
			order to rank selected n-grams by quality but results were 
			disappointing. We finally use classical Z score [23] with 
			twenty years of the French newspaper Le Monde5 as generic 
			collection. This metric considers word frequencies weighted 
			over two different corpora, in order to assign high values to 
			words having much higher or lower frequencies than expected 
			in a generic collection. We defined it as follows : 
			p1 
			= a0 
			/b0 (1) 
			p2 
			= a1 
			/b1 (2) 
			4https://www.bing.com/ 
			5http://www.islrn.org/resources/421-401-527-366-2/ 
			p = (a0 
			+ a1 
			)/(b0 
			+ b1 
			) (3) 
			ZS 
			core = 
			p1 
			- p2 
			(p  (1 - p)  ( 1 
			b0 
			+ 1 
			b1 
			) 
			(4) 
			Where a is the lexical unit considered (1-gram, 2-gram or 
			3-gram), a0 the frequency of a in the CCTP collection, b0 the 
			total size in words of CCTP collection, b1 the frequency of a 
			in the collection Le Monde. 
			VI. EXPERIMENTS AND RESULTS 
			A. Experimental protocol 
			We have made a manual evaluation on all 3 grams retained 
			by the system. Manual evaluation was realized by 6 specialists 
			in the field of construction. Each specialist evaluating one third 
			of the results. 5144 3-grams were evaluated with this method 
			and each n-gram was evaluated by 2 different specialists. 
			For each n-grams, the specialist can choose between three 
			possibilities: 
			* 0 3-gram is irrelevant 
			* 1 3-gram is relevant but does not belong to the domain 
			* 2 3-gram is relevant and belongs to the domain 
			Evaluation was done in two steps and we use Kappa 
			measure6 [24] and inter-annotator agreement at the end of the 
			first step to show the difficulty of the task. At the end of the 
			first step, we obtained a Kappa score of 0.62 and a global 
			inter-annotator agreement of 0.74, which is quite good as 
			explained in [25]. The difficulty of the task was to distinguish 
			the domain-specific vocabulary from the generic vocabulary 
			used in the field of construction. Each disagreement was re- 
			evaluated in the second step by a pair of experts. Figure 5 
			shows the final results of the evaluation. 
			B. Results 
			In this section, we present the results obtained during the 
			manual evaluation of the 3-grams retained by the system. We 
			only compute the accuracy and the error rate, because we are 
			not able to compute the recall for this collection7. We have 
			merged the assessments of each expert using two different 
			evaluation rules: 
			* a strict evaluation where a n-gram is considered correct 
			if both experts have rated it relevant and in the domain . 
			* a flexible evaluation where a n-gram is considered correct 
			if both experts consider it relevant and at least one of the 
			experts consider it in the domain. 
			strict evaluation flexible evaluation 
			accuracy 0.77 0.91 
			error rate 0.23 0.09 
			Fig. 5. Results of manual evaluation on the 3-grams. 
			6We use general formula as follows :  = A0-Ae 
			1-Ae 
			where A0 = observed 
			agreement and Ae = expected (chance) agreement. 
			7Indeed, we do not know every the relevant terms existing in the corpus, 
			so we cannot estimate the recall for the collection of terms we automatically 
			extract. 
			Strict evaluation shows good quality results (0.77). Anal- 
			ysis of the results shows that the main error is related to 
			"incomplete n-grams". For example, the 3-gram "personne 
			 
			a mobilit 
			e" (person with mobility) is not relevant while the 
			4-gram "personne  
			a mobilit 
			e r 
			eduite" (person with reduced 
			mobility ) can belong to the field of construction. Some errors 
			can also be traced back to the CCTP documents. For example, 
			"engin de guerre" (war machine) is a term which does not 
			belong to the field but a law relating to the presence of war 
			machine on the building sites is reported in every CCTP. The 
			flexible evaluation shows very good results (0.91) and the 
			difficulty of assessing class of some terms such as "absence de 
			remise" which has 2 distinct meanings in French (no outhouse 
			and no discount). The first meaning is relevant in the field of 
			construction but not the second. 
			II. INDUSTRIAL CONTEXT 
			Figure 1 presents the context of this work in VOCA- 
			GEN project. Our industrial partner Script&Go1 develop an 
			application for the construction management dedicated to 
			touch devices and wishes to set up an oral dialogue module 
			to facilitate on construction site seizure. The second industrial 
			partner (Tykomz) develops a vocal recognition suite based on 
			toolkit sphynx 4 [1]. This toolkit includes hierarchical agglom- 
			erative clustering methods using well-known measures such as 
			BIC and CLR and provides elementary tools, such as segment 
			and cluster generators, decoder and model trainers. Fitting 
			those elementary tools together is an easy way of developing 
			a specific diarization system. To work, it is necessary to build 
			a model of knowledge, i.e. a model describing the expressions 
			that must be recognized by the program. To improve the 
			performance of the system, this knowledge model must be 
			1http://www.scriptandgo.com/en/ 
			Fig. 1. figure describing the context of the project 
			powered by a domain-specific vocabulary. For example, in the 
			sentence "there is a stain of paint in the kitchen", the system 
			must understand that it is a stain of paint and that the kitchen is 
			a room. To our knowledge, there is no ontology or taxonomy 
			specific to the construction industry in French. A version is 
			under development by [2] but ontology is in English and very 
			generic. We therefore choose to extract useful knowledge from 
			textual data, and then, in a second step, to organize them. 
			III. RELATED WORKS 
			The goal of ontology learning (OL) is to build knowledge 
			models from text. OL use NLP knowledge extraction tools 
			to extract terminology (concepts) and links between them 
			(relationships). The main approaches found in the literature are 
			rule-based systems, statistical or learning based approaches. 
			The reference in the field of rule-based systems was de- 
			veloped by [3]. General Architecture for Text Engineering 
			(GATE) is a Java collection of tools initially developed at the 
			University of Sheffield since 1995. An alternative is offered by 
			the existing semi-automatic ontology learning text2onto [4]. 
			More recently, [5] developed UIMA, a system that can be 
			positioned as an alternative to GATE. Amongst other things, 
			UIMA makes possible to generate rules from a collection of 
			annotated documents. Exit, introduced by [6] is an iterative 
			approach that finds the terms in an incremental way. 
			[7] with TERMINAE is certainly the oldest statistic ap- 
			proach. Developed for French and based on lexical frequen- 
			cies, it requires pre-processing with TermoStat [8] and ANNIE 
			(GATE). [9] presents a method for extracting terminology spe- 
			cific to a domain from a corpus of domain-specific text, where 
			no external general domain reference corpus is required. They 
			present an adaptation of the classic tf-idf as ranking measure 
			and use different filters to produce a specific terminology. 
			More recently, the efficiency of ranking measure like mutual 
			information developed for statistical approach is discussed in 
			[10] and [11]. [12] proposes Termolator a terminology extrac- 
			tion system using a chunking procedure, and using internet 
			queries for ranking candidate terms. Approach is interesting 
			but the authors emphasize the fact that the runtime for each 
			queries is a limiting factor to produce a relevant ranking. 
			Closer to our work, [13] presents an approach combining 
			linguistic pattern and Z-score to extract terminology in the 
			field of nanotechnology. [14] propose TAXI, which combines 
			statistics and learning approach. TAXI is a system for building 
			a taxonomy using 2 corpora, a generic, the other specific. 
			It ranks the relevance of candidates by measure (frequency- 
			based), and by learning with SVM. [15], [16] present TexSIS, 
			a bilingual terminology extraction system with chunk-based 
			alignment method for the generation of candidate terms. After 
			the corpus alignment step, they use an approach combining 
			log likelihood measure and Mutual Expectation measure [17] 
			to rank candidate terms in each language. In the same or- 
			der, [18], [19] present an approach to extract grammatical 
			terminology from linguistic corpora. They compare a series 
			of well-established statistical measures that have been used 
			in similar automatic term extraction tasks and conclude that 
			corpus-comparing methods perform better than metrics that 
			are not based on corpus comparison. [20] and [21] present 
			methods with word embeddings. With a small data set for 
			learning phase, they improve the term extraction results in 
			n-gram terms. However, these papers involve labelled data 
			sets for learning phase, which is the main difference with 
			our proposed approach. The originality of our approach is to 
			combine a lexico-syntactical and a statistical approach while 
			using external resources. 
			IV. RESOURCES AND STATISTICS 
			First experiments were carried out using technical reports 
			collected from some customers from our industrial partners 
			who will be called as NC collection thereafter. Each document 
			contains all the non-compliance that was found on one work 
			site and describe solutions to resolve it. However heterogeneity 
			of the formats as well as the artificial repetition of the informa- 
			tion between two reports found in the same construction site 
			made the term extraction quite difficult. An insightful analysis 
			of those reports reveals vocabulary richness, however, difficult 
			to exploit given numerous misspellings, typing shortcuts, very 
			"telegraphic" style with verbs in the infinitive, little punctu- 
			ation, few determinants, etc. As a consequence, we used a 
			collection of technical specifications called CCTP2. CCTPs are 
			available online on public sector websites3. Several thousand 
			documents were collected by our industrial partner using an 
			automatic web collecting process. Figure 2 presents some key 
			descriptive statistics of theses collections. 
			Collection NC CCTP 
			Total number of documents 58 402 3665 
			Without pre-processing 
			Total number of words 130 309 230 962 734 
			Total number of different words 93 000 20 6264 
			Average words/document 125.3 63 018.48 
			Fig. 2. statistics of the collection. 
			2The technical specifications book (CCTP in French) is a contractual 
			document that gathers the technical clauses of a public contract in the field 
			of construction. 
			3For example, https://www.marches-publics.gouv.fr/ or 
			http://marchespublics.normandie.fr/. 
			ranking 
			CCTP 
			CR 1 06/02 
			Lot A : 
			Siphon independant 
			sous evier 
			CR 2 15/02 
			Lot A : 
			- Siphon independant 
			sous evier 
			Lot B : 
			- Pose des plinthes 
			sur la cloison 
			CR 3 22/02 
			Lot A : 
			- Siphon independant 
			sous evier 
			Lot B : 
			- suppression de la 
			cloison entre wc et sdb 
			words ngrams 
			extraction 
			pruning 
			pre-processing 
			citernea 
			u 
			devant entree 
			marche 
			s 
			chez techniplan 
			coulage par defaut 
			proposit 
			ions 
			d' auge 
			typologi 
			e 
			des citerneaux 
			espace d' activite 
			vision depuis ascenseur 
			vaissell 
			e 
			sous bar 
			mise en place 
			coulisse 
			au 
			pour douchette 
			terminology 
			  
			 
			 
			Fig. 3. System overview 
			V. METHODOLOGY 
			A. System Overview 
			Figure 3 presents an overview of the system designed and 
			implemented: steps are further explained in Sections V-B to 
			V-E. In Step 1 pre-processing of raw information extracted 
			from CCTP collection takes place; this is required for nor- 
			malizing the entire set of documents. In Step 2 n-grams are 
			extracted (by using measures). 1,2,3 grams are extracted. In 
			Step 3, n-grams are filtered by using linguistic patterns and 
			Internet queries. Finally, in Step 4 a ranking is applied to the 
			filtered n-grams. 
			B. Normalization, pre-processing and word ngrams extraction 
			In Step 1, a text normalization is performed to improve the 
			quality of the process. We remove special characters such as 
			"/" or "()". Different pretreatments are done to reduce noise 
			in the model: we remove numbers (numeric and/or textual), 
			special symbols. "." are tagged with a special character to 
			not create artificial n-grams. Specific words (including named 
			entities) like company names, dates, etc. are normalized and 
			will be removed in the next module. We do not include a 
			stop list to keep n-grams with prepositions, for the purpose 
			described in the remainder. Then, we tokenize the entire 
			collection before using TreeTagger [22] to get the part-of- 
			speech tags and lemmas of each word. After this step we 
			transform all vocabulary from the CCTP collection into 1- 
			grams, 2-grams and 3-grams. Special characters or normalized 
			words resulting from the previous processing are discarded. N- 
			grams with a very low frequency (2) are also discarded. 
			C. Linguistic patterns module 
			We use grammatical labels generated in the previous step 
			(section V-B) and linguistic patterns to retrieve collocations 
			such as NOUN-NOUN, NOUN-PREP-NOUN. These patterns 
			are frequently found in the literature [6] to capture specific 
			words in French like "carte de cr 
			edit"(credit card) and discard 
			3-gram like 'cr 
			editer sa carte' (credit his card) with the 
			pattern VERB-PREP-NOUN. Among frequent patterns found 
			in literature, those patterns have been selected according to the 
			statistics obtained from a knowledge model of another field 
			(agriculture), given by one of our industrial partners. Figure 
			4 presents main patterns we selected using this knowledge 
			model. The sum of the percentages is less than 100% because 
			Number Percentage 
			1-grams 1360 65.24% 
			NOUN 1037 76.25% 
			VERB 194 14.26% 
			ADJ 120 8.82% 
			2-grams 390 19.57% 
			NOUN-NOUN 346 88.72% 
			ADJ-NOUN 11 2.82% 
			PREP-NOUN 7 1,79% 
			VERB-NOUN 5 1,28% 
			3-grams 188 9.43% 
			NOUN-NOUN-NOUN 150 79.79% 
			PREP-NOUN-NOUN 15 7.98% 
			NOUN-PREP-NOUN 6 3,19% 
			VERB-NOUN-NOUN 6 3,19% 
			Fig. 4. Distribution of linguistic patterns according to the knowledge model. 
			patterns with a very low frequency are not included in the 
			table. We observed that the noun based patterns are the most 
			frequent patterns, whatever the size of the n-gram. The other 
			selected patterns also contain nouns, but they are n-grams 
			with verbs, adjectives or prepositions. Therefore, we have 
			configured our system to keep only the ngrams corresponding 
			to these patterns. 
			D. Pruning step 
			This step uses the Internet to prune n-grams for which no 
			information is returned after querying Bing4 search engine. 
			We count the number of links in the result pages that contain 
			exactly the ngram. We save the number of exact matches 
			between the ngram and the title and snippet of each result. 
			We keep only the n-grams whose number of matches exceeds 
			a defined threshold. We varied this threshold between 1 and 
			50 and results presented in Section VI-B have been obtained 
			with a threshold of 10. 
			E. Ranking step 
			We tested several measures as provided in [6], [16] like 
			maximum likelihood estimation or mutual information in 
			order to rank selected n-grams by quality but results were 
			disappointing. We finally use classical Z score [23] with 
			twenty years of the French newspaper Le Monde5 as generic 
			collection. This metric considers word frequencies weighted 
			over two different corpora, in order to assign high values to 
			words having much higher or lower frequencies than expected 
			in a generic collection. We defined it as follows : 
			p1 
			= a0 
			/b0 (1) 
			p2 
			= a1 
			/b1 (2) 
			4https://www.bing.com/ 
			5http://www.islrn.org/resources/421-401-527-366-2/ 
			p = (a0 
			+ a1 
			)/(b0 
			+ b1 
			) (3) 
			ZS 
			core = 
			p1 
			- p2 
			(p  (1 - p)  ( 1 
			b0 
			+ 1 
			b1 
			) 
			(4) 
			Where a is the lexical unit considered (1-gram, 2-gram or 
			3-gram), a0 the frequency of a in the CCTP collection, b0 the 
			total size in words of CCTP collection, b1 the frequency of a 
			in the collection Le Monde. 
			VI. EXPERIMENTS AND RESULTS 
			A. Experimental protocol 
			We have made a manual evaluation on all 3 grams retained 
			by the system. Manual evaluation was realized by 6 specialists 
			in the field of construction. Each specialist evaluating one third 
			of the results. 5144 3-grams were evaluated with this method 
			and each n-gram was evaluated by 2 different specialists. 
			For each n-grams, the specialist can choose between three 
			possibilities: 
			* 0 3-gram is irrelevant 
			* 1 3-gram is relevant but does not belong to the domain 
			* 2 3-gram is relevant and belongs to the domain 
			Evaluation was done in two steps and we use Kappa 
			measure6 [24] and inter-annotator agreement at the end of the 
			first step to show the difficulty of the task. At the end of the 
			first step, we obtained a Kappa score of 0.62 and a global 
			inter-annotator agreement of 0.74, which is quite good as 
			explained in [25]. The difficulty of the task was to distinguish 
			the domain-specific vocabulary from the generic vocabulary 
			used in the field of construction. Each disagreement was re- 
			evaluated in the second step by a pair of experts. Figure 5 
			shows the final results of the evaluation. 
			B. Results 
			In this section, we present the results obtained during the 
			manual evaluation of the 3-grams retained by the system. We 
			only compute the accuracy and the error rate, because we are 
			not able to compute the recall for this collection7. We have 
			merged the assessments of each expert using two different 
			evaluation rules: 
			* a strict evaluation where a n-gram is considered correct 
			if both experts have rated it relevant and in the domain . 
			* a flexible evaluation where a n-gram is considered correct 
			if both experts consider it relevant and at least one of the 
			experts consider it in the domain. 
			strict evaluation flexible evaluation 
			accuracy 0.77 0.91 
			error rate 0.23 0.09 
			Fig. 5. Results of manual evaluation on the 3-grams. 
			6We use general formula as follows :  = A0-Ae 
			1-Ae 
			where A0 = observed 
			agreement and Ae = expected (chance) agreement. 
			7Indeed, we do not know every the relevant terms existing in the corpus, 
			so we cannot estimate the recall for the collection of terms we automatically 
			extract. 
			Strict evaluation shows good quality results (0.77). Anal- 
			ysis of the results shows that the main error is related to 
			"incomplete n-grams". For example, the 3-gram "personne 
			 
			a mobilit 
			e" (person with mobility) is not relevant while the 
			4-gram "personne  
			a mobilit 
			e r 
			eduite" (person with reduced 
			mobility ) can belong to the field of construction. Some errors 
			can also be traced back to the CCTP documents. For example, 
			"engin de guerre" (war machine) is a term which does not 
			belong to the field but a law relating to the presence of war 
			machine on the building sites is reported in every CCTP. The 
			flexible evaluation shows very good results (0.91) and the 
			difficulty of assessing class of some terms such as "absence de 
			remise" which has 2 distinct meanings in French (no outhouse 
			and no discount). The first meaning is relevant in the field of 
			construction but not the second. 
			II. RELATED WORKS 
			The main characteristic of the approach presented in this 
			paper is to only have to provide the labels of the classes to 
			be predicted. This method does not need to have a tagged 
			data set to predict the different classes, so it is closer to an 
			unsupervised (clustering) or semi-supervised learning method 
			than a supervised. The main idea of clustering is to group 
			untagged data into a number of clusters, such that similar ex- 
			amples are grouped together and different ones are separated. 
			In clustering, the number of classes and the distribution of 
			instances between classes are unknown and the goal is to find 
			meaningful clusters. 
			One kind of clustering methods is the partitioning-based 
			one. The k-means algorithm [3] is one of the most popu- 
			lar partitioning-based algorithms because it provides a good 
			compromise between the quality of the solution obtained and 
			its computational complexity [4]. K-means aims to find k 
			centroids, one for each cluster, minimizing the sum of the 
			distances of each instance of data from its respective centroid. 
			We can cite other partitioning-based algorithms such as k- 
			medoids or PAM (Partition Around Medoids), which is an 
			evolution of k-means [5]. Hierarchical approaches produce 
			clusters by recursively partitioning data backwards or upwards. 
			For example, in a hierarchical ascending classification or 
			CAH [6], each example from the initial dataset represents a 
			cluster. Then, the clusters are merged, according to a similarity 
			measure, until the desired tree structure is obtained. The result 
			of this clustering method is called a dendrogram. Density- 
			based methods like the EM algorithm [7] assume that the data 
			belonging to each cluster is derived from a specific probability 
			distribution [8]. The idea is to grow a cluster as the density in 
			the neighborhood of the cluster exceeds a predefined threshold. 
			Model-based classification methods like self-organizing 
			map - SOM [9] are focus on finding features to represent each 
			cluster. The most used methods are decision trees and neural 
			networks. Approaches based on semi-supervised learning such 
			as label propagation algorithm [10] are similar to the method 
			proposed in this paper because they consist in using a learning 
			dataset consisting of a few labelled data points to build a 
			model for labelling a larger number of unlabelled data. Closer 
			to the theme of our collection, [11] and [12] use supervised 
			approaches to automatically detect suicidal people in social 
			networks. They extract specific features like word distribution 
			statistics or sentiments to train different machine-learning clas- 
			sifiers and compare performance of machine-learning models 
			against the judgments of psychiatric trainees and mental health 
			professionals. More recently, CLEF challenge in 2018 consists 
			of performing a task on early risk detection of depression on 
			texts written in Social Media1. However, these papers and this 
			task involve tagged data sets, which is the main difference 
			with our proposed approach (we do not have tagged data set). 
			III. RESOURCES AND STATISTICS 
			The association provided a collection of conversations be- 
			tween volunteers and callers between 2005 and 2015, which 
			is called "METICS collection" henceforth. 
			To reduce noise in the collection, we removed all the 
			discussions containing fewer than 15 exchanges between a 
			caller and a person from the association, these exchanges are 
			generally unrepresentative (connection problem, request for 
			information, etc.). We observe particular linguistic phenomena 
			like emoticons2, acronyms, mistakes (spelling, typography, 
			glued words) and an explosive lexical creativity [13]. These 
			phenomena have their origin in the mode of communication 
			(direct or semi-direct), the speed of the composition of the 
			message or in the technological constraints of input imposed 
			by the material (mobile terminal, tablet, etc.). In addition, we 
			used a subset of the collection of the French newspaper, Le 
			Monde to validate our method on a tagged corpus. We only 
			keep articles on television, politics, art, science or economics. 
			Figure 1 presents some descriptive statistics of these two 
			collections. 
			IV. METHODOLOGY 
			A. System Overview 
			Figure 2 presents an overview of the system, each step will 
			be detailed in the rest of the section. In the first step (mod- 
			ule x), we apply different linguistic pre-processing to each 
			discussion. The next module (y) creates a word embedding 
			model with these discussions while the third module (z) uses 
			this model to create specific vectors. The last module ({) 
			performs a prediction for each discussion before separating 
			the collection into clusters based on the predicted class. 
			1http://early.irlab.org/ 
			2Symbols used in written messages to express emotions, e.g. smile or 
			sadness 
			Collection METICS Le-Monde 
			Total number of documents 17 594 205 661 
			Without pre-processing 
			Total number of words 12 276 973 87 122 002 
			Total number of different words 158 361 419 579 
			Average words/document 698 424 
			With pre-processing 
			Total number of words 4 529 793 41 425 938 
			Total number of different words 120 684 419 006 
			Average words/document 257 201 
			Fig. 1. Statistics of both collections. 
			Conversations 
			or documents 
			Word2vec 
			model 
			Prediction 
			Class 1 
			pre-processing 
			Specific 
			vectors 
			creation 
			Model 1 
			 
			  
			 
			Class 2 
			Model 2 
			Class n 
			Model n 
			Cluster 1 Cluster 2 Cluster n 
			Fig. 2. System overview 
			B. Normalization and pre-processing 
			We first extract the textual content of each discussion. In 
			step x, a text normalization is performed to improve the 
			quality of the process. We remove accents, special characters 
			such as "-","/" or "()". Different linguistic processes are used 
			to reduce noise in the model: we remove numbers (numeric 
			and/or textual), special symbols and terms contained in a stop- 
			list adapted to our problem. A lemmatization process was 
			incorporated during the first experiments but it was inefficient 
			considering the typographical variations described in Section 
			III. 
			C. word2vec model 
			In the next step we build a word embedding model using 
			word2vec [14]. We project each word of our corpus in a vector 
			space in order to obtain a semantic representation of these. 
			In this way, words appearing in similar contexts will have a 
			relatively close vector representation. In addition to semantic 
			information, one advantage of such modeling is the production 
			of vector representations of words, depending on the context 
			in which they are encountered. Some words close to a term t in 
			a model learned from a corpus c1 may be very different from 
			those from a model learned from a corpus c2. For example, 
			we observe in figure 3 that the first eight words close to the 
			term "teen" vary according to the corpus used. This example 
			also shows that the use of a generic model like Le Monde in 
			French or Wikipedia is irrelevant in our case, since the corpus 
			corpus words 
			METICS teenager, young, 15years, kid, 
			school, problem , spoiled, teen, 
			Le-Monde sitcom, radio, compote, hearing 
			boy, styx, scamp, rebel 
			Fig. 3. Eight words closest to the term "teenager" according to the type of 
			corpus in learning. 
			of the association is noisy and contains a number of apocopes, 
			abbreviations or acronyms. Different parameters were tested 
			and the configuration with the best results was kept3. 
			D. Specific vectors creation and cluster predictions 
			In this step, we build vectors containing terms that are 
			selected using the word2vec model described in step IV-C. 
			For each theme in the collection, we build a specific linguistic 
			model by performing a word embedding to reconstruct the 
			linguistic context of each theme. We observe, for example, 
			that the terms closest to the thematic "work" are: "unemploy- 
			ment", "job", "stress". Similarly, for the "addiction" theme, 
			we observe the terms: "cannabis", "alcoholism", "drugs" and 
			"heroin". We used this context subsequently to construct a 
			vector, containing the distance distc 
			(i) between each term i 
			and the theme c. Each of these models is independent, so 
			the same term can appear in several models. In this way, 
			we observed that the word "stress" is present in the vector 
			"suicide" and in that of "work", however, the associated weight 
			is different. We varied the size of these vectors between 20 
			and 1000 and the best results were obtained with a size of 400. 
			In the last step {, the system computes an Sc score for each 
			discussion and for each cluster according to each linguistic 
			model such as: 
			Sc 
			(d) = 
			n 
			i=1 
			tf(i) * distc 
			(i) (1) 
			with i the considered term, tf (i) frequency of i in the 
			collection, and distc 
			(i) is the distance between the term i and 
			the thematic c. In the end, the class with the highest score is 
			chosen. 
			V. EXPERIMENTS AND RESULTS 
			A. Experimental protocol 
			To evaluate the quality of the obtained clusters, we used a 
			subset of the texts of the Le-Monde newspaper, described in 
			Section III, each article having a label according to the theme. 
			For these experiments, we configured the specific vectors (SV) 
			approach with the optimal parameters, as defined in Sections 
			IV-C and IV-D. We also tested the specific vectors without 
			weighting to test the particular influence of this parameter. To 
			highlight the difficulty of the task, we compare our system 
			with a baseline which consists in a random draw, and with 
			3The best results were obtained with the following parameter values: vector 
			size: 700, sliding window size: 5, minimum frequency: 10, vectorization 
			method: skip grams, and a soft hierarchical max function for the model 
			learning. 
			the k-means algorithm [3], commonly used in the literature, 
			as mentioned in Section II. To feed the k-means algorithm, we 
			transformed our initial collection into a bag of words matrix 
			[15] where each conversation is described by the frequency 
			of its words. Each of the experiments was evaluated using the 
			classic measures of Precision, Recall and F-measure, averaged 
			over all classes (with beta = 1 in order not to privilege 
			precision or recall [16]). Since the k-means algorithm does not 
			associate a tag with the final clusters, we have exhaustively 
			calculated the set of solutions to keep only the one yielding 
			the highest F-score. 
			B. Results 
			Prec. Recall F-score 
			Without pre-processing 
			Baseline 0.18 0.16 0.17 
			k-means 0.23 0.20 0.22 
			Without weighting 0.54 0.50 0.52 
			Specific Vectors 0.53 0.54 0.53 
			With pre-processing 
			k-means 0.30 0.21 0.25 
			Without weighting 0.55 0.51 0.53 
			Specific Vectors 0.54 0.54 0.54 
			Fig. 4. Results obtained by each system. 
			Figure 4 presents a summary of the results obtained with 
			each systems. We first observe that baseline scores are very 
			low, but remain relatively close to the theoretical random (0.2) 
			given by the number of classes. Linguistic pre-treatments are 
			not very efficient individually, but improve overall the results 
			of other experiments. The k-means algorithm obtains slightly 
			better results in terms of F-score, but remains weak. Specific 
			vectors get excellent results that outperform other systems with 
			an F-score of 0.54. The execution without weighting improve 
			slightly the recall. 
			C. Cluster Analysis 
			Initial objective of this work was the exploration of the 
			METICS collection, we apply the whole process with the 
			specific vectors approach to automatically categorize all the 
			conversations. We use the Latent Dirichlet Allocation [17] in 
			order to obtain the main topic of each cluster. Figure 5 presents 
			average weight of each thematic keywords according to each 
			clusters. 
			In figure 5, fear, shrink and trust are present designations 
			for each cluster with a largely significant rank; yet, does 
			the writer still express fear when he writes, "I'm afraid of 
			being sick"? Do these designations not participate in opening 
			and constructing spheres of meanings around these pivotal 
			words? Conversely, with a lower rank, but also significant, the 
			designations of thing, difficult, and problem are more vague, 
			but more reformulating to take up the elements involved in 
			writing what is wrong. 
			Fig. 5. Distribution of discursive routines by cluster. 
			II. RELATED WORKS 
			The main characteristic of the approach presented in this 
			paper is to only have to provide the labels of the classes to 
			be predicted. This method does not need to have a tagged 
			data set to predict the different classes, so it is closer to an 
			unsupervised (clustering) or semi-supervised learning method 
			than a supervised. The main idea of clustering is to group 
			untagged data into a number of clusters, such that similar ex- 
			amples are grouped together and different ones are separated. 
			In clustering, the number of classes and the distribution of 
			instances between classes are unknown and the goal is to find 
			meaningful clusters. 
			One kind of clustering methods is the partitioning-based 
			one. The k-means algorithm [3] is one of the most popu- 
			lar partitioning-based algorithms because it provides a good 
			compromise between the quality of the solution obtained and 
			its computational complexity [4]. K-means aims to find k 
			centroids, one for each cluster, minimizing the sum of the 
			distances of each instance of data from its respective centroid. 
			We can cite other partitioning-based algorithms such as k- 
			medoids or PAM (Partition Around Medoids), which is an 
			evolution of k-means [5]. Hierarchical approaches produce 
			clusters by recursively partitioning data backwards or upwards. 
			For example, in a hierarchical ascending classification or 
			CAH [6], each example from the initial dataset represents a 
			cluster. Then, the clusters are merged, according to a similarity 
			measure, until the desired tree structure is obtained. The result 
			of this clustering method is called a dendrogram. Density- 
			based methods like the EM algorithm [7] assume that the data 
			belonging to each cluster is derived from a specific probability 
			distribution [8]. The idea is to grow a cluster as the density in 
			the neighborhood of the cluster exceeds a predefined threshold. 
			Model-based classification methods like self-organizing 
			map - SOM [9] are focus on finding features to represent each 
			cluster. The most used methods are decision trees and neural 
			networks. Approaches based on semi-supervised learning such 
			as label propagation algorithm [10] are similar to the method 
			proposed in this paper because they consist in using a learning 
			dataset consisting of a few labelled data points to build a 
			model for labelling a larger number of unlabelled data. Closer 
			to the theme of our collection, [11] and [12] use supervised 
			approaches to automatically detect suicidal people in social 
			networks. They extract specific features like word distribution 
			statistics or sentiments to train different machine-learning clas- 
			sifiers and compare performance of machine-learning models 
			against the judgments of psychiatric trainees and mental health 
			professionals. More recently, CLEF challenge in 2018 consists 
			of performing a task on early risk detection of depression on 
			texts written in Social Media1. However, these papers and this 
			task involve tagged data sets, which is the main difference 
			with our proposed approach (we do not have tagged data set). 
			III. RESOURCES AND STATISTICS 
			The association provided a collection of conversations be- 
			tween volunteers and callers between 2005 and 2015, which 
			is called "METICS collection" henceforth. 
			To reduce noise in the collection, we removed all the 
			discussions containing fewer than 15 exchanges between a 
			caller and a person from the association, these exchanges are 
			generally unrepresentative (connection problem, request for 
			information, etc.). We observe particular linguistic phenomena 
			like emoticons2, acronyms, mistakes (spelling, typography, 
			glued words) and an explosive lexical creativity [13]. These 
			phenomena have their origin in the mode of communication 
			(direct or semi-direct), the speed of the composition of the 
			message or in the technological constraints of input imposed 
			by the material (mobile terminal, tablet, etc.). In addition, we 
			used a subset of the collection of the French newspaper, Le 
			Monde to validate our method on a tagged corpus. We only 
			keep articles on television, politics, art, science or economics. 
			Figure 1 presents some descriptive statistics of these two 
			collections. 
			IV. METHODOLOGY 
			A. System Overview 
			Figure 2 presents an overview of the system, each step will 
			be detailed in the rest of the section. In the first step (mod- 
			ule x), we apply different linguistic pre-processing to each 
			discussion. The next module (y) creates a word embedding 
			model with these discussions while the third module (z) uses 
			this model to create specific vectors. The last module ({) 
			performs a prediction for each discussion before separating 
			the collection into clusters based on the predicted class. 
			1http://early.irlab.org/ 
			2Symbols used in written messages to express emotions, e.g. smile or 
			sadness 
			Collection METICS Le-Monde 
			Total number of documents 17 594 205 661 
			Without pre-processing 
			Total number of words 12 276 973 87 122 002 
			Total number of different words 158 361 419 579 
			Average words/document 698 424 
			With pre-processing 
			Total number of words 4 529 793 41 425 938 
			Total number of different words 120 684 419 006 
			Average words/document 257 201 
			Fig. 1. Statistics of both collections. 
			Conversations 
			or documents 
			Word2vec 
			model 
			Prediction 
			Class 1 
			pre-processing 
			Specific 
			vectors 
			creation 
			Model 1 
			 
			  
			 
			Class 2 
			Model 2 
			Class n 
			Model n 
			Cluster 1 Cluster 2 Cluster n 
			Fig. 2. System overview 
			B. Normalization and pre-processing 
			We first extract the textual content of each discussion. In 
			step x, a text normalization is performed to improve the 
			quality of the process. We remove accents, special characters 
			such as "-","/" or "()". Different linguistic processes are used 
			to reduce noise in the model: we remove numbers (numeric 
			and/or textual), special symbols and terms contained in a stop- 
			list adapted to our problem. A lemmatization process was 
			incorporated during the first experiments but it was inefficient 
			considering the typographical variations described in Section 
			III. 
			C. word2vec model 
			In the next step we build a word embedding model using 
			word2vec [14]. We project each word of our corpus in a vector 
			space in order to obtain a semantic representation of these. 
			In this way, words appearing in similar contexts will have a 
			relatively close vector representation. In addition to semantic 
			information, one advantage of such modeling is the production 
			of vector representations of words, depending on the context 
			in which they are encountered. Some words close to a term t in 
			a model learned from a corpus c1 may be very different from 
			those from a model learned from a corpus c2. For example, 
			we observe in figure 3 that the first eight words close to the 
			term "teen" vary according to the corpus used. This example 
			also shows that the use of a generic model like Le Monde in 
			French or Wikipedia is irrelevant in our case, since the corpus 
			corpus words 
			METICS teenager, young, 15years, kid, 
			school, problem , spoiled, teen, 
			Le-Monde sitcom, radio, compote, hearing 
			boy, styx, scamp, rebel 
			Fig. 3. Eight words closest to the term "teenager" according to the type of 
			corpus in learning. 
			of the association is noisy and contains a number of apocopes, 
			abbreviations or acronyms. Different parameters were tested 
			and the configuration with the best results was kept3. 
			D. Specific vectors creation and cluster predictions 
			In this step, we build vectors containing terms that are 
			selected using the word2vec model described in step IV-C. 
			For each theme in the collection, we build a specific linguistic 
			model by performing a word embedding to reconstruct the 
			linguistic context of each theme. We observe, for example, 
			that the terms closest to the thematic "work" are: "unemploy- 
			ment", "job", "stress". Similarly, for the "addiction" theme, 
			we observe the terms: "cannabis", "alcoholism", "drugs" and 
			"heroin". We used this context subsequently to construct a 
			vector, containing the distance distc 
			(i) between each term i 
			and the theme c. Each of these models is independent, so 
			the same term can appear in several models. In this way, 
			we observed that the word "stress" is present in the vector 
			"suicide" and in that of "work", however, the associated weight 
			is different. We varied the size of these vectors between 20 
			and 1000 and the best results were obtained with a size of 400. 
			In the last step {, the system computes an Sc score for each 
			discussion and for each cluster according to each linguistic 
			model such as: 
			Sc 
			(d) = 
			n 
			i=1 
			tf(i) * distc 
			(i) (1) 
			with i the considered term, tf (i) frequency of i in the 
			collection, and distc 
			(i) is the distance between the term i and 
			the thematic c. In the end, the class with the highest score is 
			chosen. 
			V. EXPERIMENTS AND RESULTS 
			A. Experimental protocol 
			To evaluate the quality of the obtained clusters, we used a 
			subset of the texts of the Le-Monde newspaper, described in 
			Section III, each article having a label according to the theme. 
			For these experiments, we configured the specific vectors (SV) 
			approach with the optimal parameters, as defined in Sections 
			IV-C and IV-D. We also tested the specific vectors without 
			weighting to test the particular influence of this parameter. To 
			highlight the difficulty of the task, we compare our system 
			with a baseline which consists in a random draw, and with 
			3The best results were obtained with the following parameter values: vector 
			size: 700, sliding window size: 5, minimum frequency: 10, vectorization 
			method: skip grams, and a soft hierarchical max function for the model 
			learning. 
			the k-means algorithm [3], commonly used in the literature, 
			as mentioned in Section II. To feed the k-means algorithm, we 
			transformed our initial collection into a bag of words matrix 
			[15] where each conversation is described by the frequency 
			of its words. Each of the experiments was evaluated using the 
			classic measures of Precision, Recall and F-measure, averaged 
			over all classes (with beta = 1 in order not to privilege 
			precision or recall [16]). Since the k-means algorithm does not 
			associate a tag with the final clusters, we have exhaustively 
			calculated the set of solutions to keep only the one yielding 
			the highest F-score. 
			B. Results 
			Prec. Recall F-score 
			Without pre-processing 
			Baseline 0.18 0.16 0.17 
			k-means 0.23 0.20 0.22 
			Without weighting 0.54 0.50 0.52 
			Specific Vectors 0.53 0.54 0.53 
			With pre-processing 
			k-means 0.30 0.21 0.25 
			Without weighting 0.55 0.51 0.53 
			Specific Vectors 0.54 0.54 0.54 
			Fig. 4. Results obtained by each system. 
			Figure 4 presents a summary of the results obtained with 
			each systems. We first observe that baseline scores are very 
			low, but remain relatively close to the theoretical random (0.2) 
			given by the number of classes. Linguistic pre-treatments are 
			not very efficient individually, but improve overall the results 
			of other experiments. The k-means algorithm obtains slightly 
			better results in terms of F-score, but remains weak. Specific 
			vectors get excellent results that outperform other systems with 
			an F-score of 0.54. The execution without weighting improve 
			slightly the recall. 
			C. Cluster Analysis 
			Initial objective of this work was the exploration of the 
			METICS collection, we apply the whole process with the 
			specific vectors approach to automatically categorize all the 
			conversations. We use the Latent Dirichlet Allocation [17] in 
			order to obtain the main topic of each cluster. Figure 5 presents 
			average weight of each thematic keywords according to each 
			clusters. 
			In figure 5, fear, shrink and trust are present designations 
			for each cluster with a largely significant rank; yet, does 
			the writer still express fear when he writes, "I'm afraid of 
			being sick"? Do these designations not participate in opening 
			and constructing spheres of meanings around these pivotal 
			words? Conversely, with a lower rank, but also significant, the 
			designations of thing, difficult, and problem are more vague, 
			but more reformulating to take up the elements involved in 
			writing what is wrong. 
			Fig. 5. Distribution of discursive routines by cluster. 
			2. Performance Measure, Corpora for Evaluation, and Intended Markup 
			A standard practice for measuring the performance of a system for the class of tasks 
			with which we are concerned in this article is to calculate its error rate: 
			error rate = 
			incorrectly assigned 
			all assigned by system 
			This single measure gives enough information, provided that the system does not 
			leave unassigned word tokens that it is intended to handle. Obviously, we want the 
			system to handle all cases as accurately as possible. Sometimes, however, it is beneficial 
			to assign only cases in which the system is confident enough, leaving the rest to be 
			handled by other methods. In this case apart from the error rate (which corresponds 
			to precision or accuracy as 1-error rate) we also measure the system's coverage or 
			recall 
			coverage = 
			correctly assigned 
			all to be assigned 
			2.1 Corpora for Evaluation 
			There are two corpora normally used for evaluation in a number of text-processing 
			tasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ) 
			corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). 
			The Brown corpus represents general English. It contains over one million word tokens 
			and is composed from 15 subcorpora that belong to different genres and domains, 
			ranging from news wire texts and scientific papers to fiction and transcribed speech. 
			The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and 
			ungrammatical sentences with complex internal structure. Altogether there are about 
			500 documents in the Brown corpus, with an average length of 2,300 word tokens. 
			The WSJ corpus represents journalistic news wire style. Its size is also over a 
			million word tokens, and the documents it contains are rich in abbreviations and 
			proper names, but they are much shorter than those in the Brown corpus. Altogether 
			there are about 2,500 documents in the WSJ corpus, with an average length of about 
			500 word tokens. 
			Documents in the Penn Treebank are segmented into paragraphs and sentences. 
			Sentences are further segmented into word tokens annotated with part-of-speech (POS) 
			information. POS information can be used to distinguish between proper names and 
			common words. We considered proper nouns (NNP), plural proper nouns (NNPS), and 
			proper adjectives2 (JJP) to signal proper names, and all other categories were consid- 
			ered to signal common words or punctuation. Since proper adjectives are not included 
			in the Penn Treebank tag set, we had to identify and retag them ourselves with the 
			help of a gazetteer. 
			Abbreviations in the Penn Treebank are tokenized together with their trailing pe- 
			riods, whereas fullstops and other sentence boundary punctuation are tokenized as 
			separate tokens. This gives all necessary information for the evaluation in all our three 
			2 These are adjectives derived from proper nouns (e.g. "American"). 
			292 
			Computational Linguistics Volume 28, Number 3 
			tasks: the sentence boundary disambiguation task, the capitalized word disambigua- 
			tion task, and the abbreviation identification task. 
			2.2 Tokenization Convention and Corpora Markup 
			For easier handling of potential sentence boundary punctuation, we developed a new 
			tokenization convention for periods. In the traditional Penn Treebank schema, abbrevi- 
			ations are tokenized together with their trailing periods, and thus stand-alone periods 
			unambiguously signal the end of a sentence. We decided to treat periods and all other 
			potential sentence termination punctuation as "first-class citizens" and adopted a con- 
			vention always to tokenize a period (and other punctuation) as a separate token when 
			it is followed by a white space, line break, or punctuation. In the original Penn Tree- 
			bank format, periods are unambiguous, whereas in our new convention they can take 
			on one of the three tags: fullstop (.), part of abbreviation (A) or both (*). 
			To generate the new format from the Penn Treebank, we had to split final periods 
			from abbreviations, mark them as separate tokens and assign them with A or * tags 
			according to whether or not the abbreviation was the last token in a sentence. We 
			applied a similar tokenization convention to the case in which several (usually three) 
			periods signal ellipsis in a sentence. Again, sometimes such constructions occur within 
			a sentence and sometimes at a sentence break. We decided to treat such constructions 
			similarly to abbreviations, tokenize all periods but the last together in a single token, 
			and tokenize the last period separately and tag it with A or * according to whether 
			or not the ellipsis was the last token in a sentence. We treated periods in numbers 
			(e.g., 14.534) or inside acronyms (e.g., Y.M.C.A.) as part of tokens rather than separate 
			periods. 
			In all our experiments we treated embedded sentence boundaries in the same way 
			as normal sentence boundaries. An embedded sentence boundary occurs when there 
			is a sentence inside a sentence. This can be a quoted direct-speech subsentence inside a 
			sentence, a subsentence embedded in brackets, etc. We considered closing punctuation 
			of such sentences equal to closing punctuation of normal sentences. 
			We also specially marked word tokens in positions where they were ambiguously 
			capitalized if such word tokens occurred in one of the following contexts: 
			* the first token in a sentence 
			* following a separately tokenized period, question mark, exclamation 
			mark, semicolon, colon, opening quote, closing quote, opening bracket, 
			or closed bracket 
			* occurring in a sentence with all words capitalized 
			All described transformations were performed automatically by applying a simple 
			Perl script. We found quite a few infelicities in the original tokenization and tagging, 
			however, which we had to correct by hand. We also converted both our corpora from 
			their original Penn Treebank format into an XML format where each word token is 
			represented as an XML element (W) with the attribute C holding its POS information 
			and attribute A set to Y for ambiguously capitalized words. An example of such a 
			markup is displayed in Figure 1. 
			3. Our Approach to Sentence Boundary Disambiguation 
			If we had at our disposal entirely correct information on whether or not each word 
			preceding a period was an abbreviation and whether or not each capitalized word 
			293 
			Mikheev Periods, Capitalized Words, etc. 
			...<W C=RB>soon</W><W C='.'>.</W> <W A=Y C=NNP>Mr</W><W C=A>.</W>... 
			...<W C=VBN>said</W> <W C=NNP>Mr</W><W C=A>.</W> 
			<W A=Y C=NNP>Brown</W>... 
			...<W C=','>,</W> <W C=NNP>Tex</W><W C='*'>.</W> 
			<W A=Y C=JJP>American</W>... 
			Figure 1 
			Example of the new tokenization and markup generated from the Penn Treebank format. 
			Tokens are represented as XML elements W, where the attribute C holds POS information. 
			Proper names are tagged as NNP, NNPS and JJP. Periods are tagged as . (fullstop), A (part of 
			abbreviation), * (a fullstop and part of abbreviation at the same time). Ambiguously 
			capitalized words are marked with A = Y. 
			that follows a period was a proper name, we could apply a very simple set of rules 
			to disambiguate sentence boundaries: 
			* If a period follows a nonabbreviation, it is sentence terminal (.). 
			* If a period follows an abbreviation and is the last token in a text passage 
			(paragraph, document, etc.), it is sentence terminal and part of the 
			abbreviation (*). 
			* If a period follows an abbreviation and is not followed by a capitalized 
			word, it is part of the abbreviation and is not sentence terminal (A). 
			* If a period follows an abbreviation and is followed by a capitalized word 
			that is not a proper name, it is sentence terminal and part of the 
			abbreviation (*). 
			It is a trivial matter to extend these rules to allow for brackets and quotation marks 
			between the period and the following word. To handle other sentence termination 
			punctuation such as question and exclamation marks and semicolons, this rule set 
			also needs to include corresponding rules. The entire rule set for sentence boundary 
			disambiguation that was used in our experiments is listed in Appendix A. 
			3.1 Ideal Case: Upper Bound for Our SBD Approach 
			The estimates from the Brown corpus and the WSJ corpus (section 3) show that the 
			application of the SBD rule set described above together with the information on 
			abbreviations and proper names marked up in the corpora produces very accurate 
			results (error rate less than 0.0001%), but it leaves unassigned the outcome of the case 
			in which an abbreviation is followed by a proper name. This is a truly ambiguous case, 
			and to deal with this situation in general, one should encode detailed information 
			about the words participating in such contexts. For instance, honorific abbreviations 
			such as Mr. or Dr. when followed by a proper name almost certainly do not end a 
			sentence, whereas the abbreviations of U.S. states such as Mo., Cal., and Ore., when 
			followed directly by a proper name, most likely end a sentence. Obviously encoding 
			this kind of information into the system requires detailed analysis of the domain lexica, 
			is not robust to unseen abbreviations, and is labor intensive. 
			To make our method robust to unseen words, we opted for a crude but simple 
			solution. If such ambiguous cases are always resolved as "not sentence boundary" (A), 
			this produces, by our measure, an error rate of less than 3%. Estimates from the Brown 
			294 
			Computational Linguistics Volume 28, Number 3 
			Table 1 
			Estimates of the upper and lower bound error rates on the SBD task for our method. Three 
			estimated categories are sentence boundaries, ambiguously capitalized words, and 
			abbreviations. 
			Brown Corpus WSJ Corpus 
			SBD Amb. Cap. Abbreviations SBD Amb. Cap. Abbreviations 
			Number of 59,539 58,957 4,657 53,043 54,537 16,317 
			resolved instances 
			A Upper Bound: 0.01% 0.0% 0.0% 0.13% 0.0% 0.0% 
			All correct proper 
			names 
			All correct abbrs. 
			B Lower Bound: 2.00% 7.4% 10.8% 4.10% 15.0% 9.6% 
			Lookup proper 
			names 
			Guessed abbrs. 
			C Lookup proper 1.20% 7.4% 0.0% 2.34% 15.0% 0.0% 
			names 
			All correct abbrs. 
			D All correct proper 0.45% 0.0% 10.8% 1.96% 0.0% 9.6% 
			names 
			Guessed abbrs. 
			corpus and the WSJ corpus showed that such ambiguous cases constitute only 5-7% 
			of all potential sentence boundaries. This translates into a relatively small impact of 
			the crude strategy on the overall error rate on sentence boundaries. This impact was 
			measured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presented 
			in row A of Table 1. Although this overly simplistic strategy extracts a small penalty 
			from the performance, we decided to use it because it is very general and independent 
			of domain-specific knowledge. 
			The SBD handling strategy described above is simple, robust, and well perform- 
			ing, but it relies on the assumption that we have entirely correct information about 
			abbreviations and proper names, as can be seen in row A of the table. The main dif- 
			ficulty is that when dealing with real-world texts, we have to identify abbreviations 
			and proper names ourselves. Thus estimates based on the application of our method 
			when using 100% correctly disambiguated capitalized words and abbreviations can be 
			considered as the upper bound for the SBD approach, that is, the top performance we 
			can achieve. 
			3.2 Worst Case: Lower Bound for Our SBD Approach 
			We can also estimate the lower bound for this approach applying very simple strategies 
			to the identification of proper names and abbreviations. 
			The simplest strategy for deciding whether or not a capitalized word in an ambigu- 
			ous position is a proper name is to apply a lexical-lookup strategy (possibly enhanced 
			with a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, words 
			not listed as known common words for a language are usually marked as proper 
			names. The application of this strategy produced a 7.4% error rate on the Brown 
			corpus and a 15% error rate on the WSJ corpus. The difference in error rates can be 
			explained by the observation that the WSJ corpus contains a higher percentage of orga- 
			nization names and person names, which often coincide with common English words, 
			295 
			Mikheev Periods, Capitalized Words, etc. 
			and it contains more words in titles with all important words capitalized, which we 
			also consider as ambiguously capitalized. 
			The simplest strategy for deciding whether a word that is followed by a period 
			is an abbreviation or a regular word is to apply well-known heuristics based on the 
			observation that single-word abbreviations are short and normally do not include 
			vowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation 
			unless it is written in all capital letters and can stand for an acronym or a proper name 
			(e.g., BBC). A span of single letters separated by periods forms an abbreviation too 
			(e.g., Y.M.C.A.). A single letter followed by a period is also a very likely abbreviation. 
			There is also an additional heuristic that classifies as abbreviations short words (with 
			length less than five characters) that are followed by a period and then by a comma, a 
			lower-cased word, or a number. All other words are considered to be nonabbreviations. 
			These heuristics are reasonably accurate. On the WSJ corpus they misrecognized 
			as abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate was 
			significantly higher: 1.6%. The major source for these errors were single letters that 
			stand for mathematical symbols in the scientific subcorpora of the Brown Corpus (e.g., 
			point T or triangle F). The major shortcoming of these abbreviation-guessing heuristics, 
			however, comes from the fact that they failed to identify about 9.5% of abbreviations. 
			This brings the overall error rate of the abbreviation-guessing heuristics to about 10%. 
			Combining the information produced by the lexical-lookup approach to proper 
			name identification with the abbreviation-guessing heuristics feeding the SBD rule set 
			gave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on the 
			SBD task. This can be interpreted as the lower bound to our SBD approach. Here we 
			see how errors in the identification of proper names and abbreviations propagated 
			themselves into errors on sentence boundaries. Row B of Table 1 displays a summary 
			for the lower-bound results. 
			3.3 Major Findings 
			We also measured the importance of each of the two knowledge sources (abbreviations 
			and proper names) separately. First, we applied the SBD rule set when all abbreviations 
			were correctly identified (using the information presented in the corpus) but applying 
			the lexical lookup strategy to proper-name identification (row C of Table 1). Then, we 
			applied the SBD rule set when all proper names were correctly identified (using the 
			information presented in the corpus) but applying the guessing heuristics to handle 
			abbreviations (row D of the table). In general, when a knowledge source returned 
			100% accurate information this significantly improved performance on the SBD task 
			measured against the lower-bound error rate. We also see that proper names have a 
			higher impact on the SBD task than abbreviations. 
			Since the upper bound of our SBD approach is high and the lower bound is far 
			from being acceptable, our main strategy for sentence boundary disambiguation will be to 
			invest in the disambiguation of capitalized words and abbreviations that then feed our SBD 
			rule set. 
			4. Document-Centered Approach to Proper Name and Abbreviation Handling 
			As we discussed above, virtually any common word can potentially act as a proper 
			name or part of a multiword proper name. The same applies to abbreviations: there is 
			no fixed list of abbreviations, and almost any short word can be used as an abbrevia- 
			tion. Fortunately, there is a mitigating factor too: important words are typically used 
			in a document more than once and in different contexts. Some of these contexts create 
			296 
			Computational Linguistics Volume 28, Number 3 
			ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually 
			unambiguously introduced at least once in the text unless they are part of common 
			knowledge presupposed to be possessed by the readers. 
			This observation can be applied to a broader class of tasks. For example, people 
			are often referred to by their surnames (e.g., Black) but are usually introduced at least 
			once in the text either with their first name (John Black) or with their title/profession 
			affiliation (Mr. Black, President Bush), and it is only when their names are common 
			knowledge that they do not need an introduction (e.g., Castro, Gorbachev). Thus our 
			suggestion is to look at the unambiguous usages of the words in question in the entire document. 
			In the case of proper name identification, we are not concerned with the semantic 
			class of a name (e.g., whether it is a person's name or a location), but rather we simply 
			want to distinguish whether a capitalized word in a particular occurrence acts as a 
			proper name (or part of a multiword proper name). If we restrict our scope to a single 
			sentence, we might find that there is just not enough information to make a reliable 
			decision. For instance, Riders in the sentence Riders rode all over the green is equally likely 
			to be a proper noun, a plural proper noun, or a plural common noun. But if in the 
			same text we find John Riders, this sharply increases the likelihood that the proper noun 
			interpretation is the correct one, and conversely if we find many riders, this suggests 
			the plural-noun interpretation. 
			The above reasoning can be summarized as follows: if we detect that a word is 
			used capitalized in an unambiguous context, this increases the chances that this word 
			acts as a proper name in ambiguous positions in the same document. And conversely 
			if a word is seen only lower-cased, this increases the chances that it should be treated 
			as a common word even when used capitalized in ambiguous positions in the same 
			document. (This, of course, is only a general principle and will be further elaborated 
			elsewhere in the article.) 
			The same logic applies to abbreviations. Although a short word followed by a 
			period is a potential abbreviation, the same word occurring in the same document 
			in a different context can be unambiguously classified as a regular word if it is used 
			without a trailing period, or it can be unambiguously classified as an abbreviation if 
			it is used with a trailing period and is followed by a lower-cased word or a comma. 
			This information gives us a better chance of assigning these potential abbreviations 
			correctly in nonobvious contexts. 
			We call such style of processing a document-centered approach (DCA), since in- 
			formation for the disambiguation of an individual word token is derived from the 
			entire document rather than from its immediate local context. Essentially the system 
			collects suggestive instances of usage for target words from each document under 
			processing and applies this information on the fly to the processing of the document, 
			in a manner similar to instance-based learning. This differentiates DCA from the tradi- 
			tional corpus-based approach, in which learning is applied prior to processing, which 
			is usually performed with supervision over multiple documents of the training corpus. 
			5. Building Support Resources 
			Our method requires only four word lists. Each list is a collection of words that belong 
			to a single type, but at the same time, a word can belong to multiple lists. Since we 
			have four lists, we have four types: 
			* common word (as opposed to proper name) 
			* common word that is a frequent sentence starter 
			297 
			Mikheev Periods, Capitalized Words, etc. 
			* frequent proper name 
			* abbreviation (as opposed to regular word) 
			These four lists can be acquired completely automatically from raw (unlabeled) texts. 
			For the development of these lists we used a collection of texts of about 300,000 words 
			derived from the New York Times (NYT) corpus that was supplied as training data for 
			the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these 
			texts because the approach described in this article was initially designed to be part 
			of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed 
			for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact 
			that this corpus does not have to be annotated in any way and that a corpus of similar 
			size can be easily collected from on-line sources (including the Internet) makes this 
			resource cheap to obtain. 
			The first list on which our method relies is a list of common words. This list 
			includes common words for a given language, but no supplementary information such 
			as POS or morphological information is required to be present in this list. A variety 
			of such lists for many languages are already available (e.g., Burnage 1990). Words in 
			such lists are usually supplemented with morphological and POS information (which 
			is not required by our method). We do not have to rely on pre-existing resources, 
			however. A list of common words can be easily obtained automatically from a raw 
			(unannotated in any way) text collection by simply collecting and counting lower- 
			cased words in it. We generated such list from the NYT collection. To account for 
			potential spelling and capitalization errors, we included in the list of common words 
			only words that occurred lower-cased at least three times in the NYT texts. The list 
			of common words that we developed from the NYT collection contained about 15,000 
			English words. 
			The second list on which our method relies is a frequent-starters list, a list of 
			common words most frequently used in sentence-starting positions. This list can also 
			be obtained completely automatically from an unannotated corpus by applying the 
			lexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a 
			7-15% error rate. We applied the list of common words over the NYT text collec- 
			tion to tag capitalized words in sentence-starting positions as common words and as 
			proper names: if a capitalized word was found in the list of common words, it was 
			tagged as a common word: otherwise it was tagged as a proper name. Of course, such 
			tagging was far from perfect, but it was good enough for our purposes. We included in 
			the frequent-starters list only the 200 most frequent sentence-starting common words. 
			This was more than a safe threshold to ensure that no wrongly tagged words were 
			added to this list. As one might predict, the most frequent sentence-starting common 
			word was The. This list also included some adverbs, such as However, Suddenly, and 
			Once; some prepositions, such as In, To, and By; and even a few verbs: Let, Have, Do, etc. 
			The third list on which our method relies is a list of single-word proper names that 
			coincide with common words. For instance, the word Japan is much more likely to be 
			used as a proper name (name of a country) rather than a verb, and therefore it needs to 
			be included in this list. We included in the proper name list 200 words that were most 
			frequently seen in the NYT text collection as single capitalized words in unambiguous 
			positions and that at the same time were present in the list of common words. For 
			instance, the word The can frequently be seen capitalized in unambiguous positions, 
			but it is always followed by another capitalized word, so we do not count it as a 
			candidate. On the other hand the word China is often seen capitalized in unambiguous 
			positions where it is not preceded or followed by other capitalized words. Since china 
			298 
			Computational Linguistics Volume 28, Number 3 
			Table 2 
			Error rates for different combinations of the abbreviation identification methods, including 
			combinations of guessing heuristics (GH), lexical lookup (LL), and the document-centered 
			approach (DCA). 
			Abbreviation Identification Method WSJ Brown 
			A GH 9.6% 10.8% 
			B LL 12.6% 11.9% 
			C GH + LL 1.2% 2.1% 
			D GH + DCA 6.6% 8.9% 
			E GH + DCA + LL 0.8% 1.2% 
			is also listed among common words and is much less frequently used in this way, we 
			include it in the proper name list. 
			The fourth list on which our method relies is a list of known abbreviations. 
			Again, we induced this list completely automatically from an unannotated corpus. 
			We applied the abbreviation-guessing heuristics described in Section 6 to our NYT 
			text collection and then extracted the 270 most frequent abbreviations: all abbrevi- 
			ations that appeared five times or more. This list included honorific abbreviations 
			(Mr, Dr), corporate designators (Ltd, Co), month name abbreviations (Jan, Feb), ab- 
			breviations of names of U.S. states (Ala, Cal), measure unit abbreviations (ft, kg), etc. 
			Although we described these abbreviations in groups, this information was not en- 
			coded in the list; the only information this list provides is that a word is a known 
			abbreviation. 
			Among these four lists the first three reflect general language regularities and 
			usually do not require modification for handling texts from a new domain. The ab- 
			breviation list, however, is much more domain dependent and for better performance 
			needs to be reinduced for a new domain. Since the compilation of all four lists does not 
			require data preannotated in any way, it is very easy to specialize the above-described 
			lists to a particular domain: we can simply rebuild the lists using a domain-specific 
			corpus. This process is completely automatic and does not require any human labor 
			apart from collecting a raw domain-specific corpus. Since all cutoff thresholds that 
			we applied here were chosen by intuition, however, different domains might require 
			some new settings. 
			6. Recognizing Abbreviations 
			The answer to the question of whether or not a particular word token is an abbreviation 
			or a regular word largely solves the sentence boundary problem. In the Brown corpus 
			92% of potential sentence boundaries come after regular words. The WSJ corpus is 
			richer with abbreviations, and only 83% of sentences in that corpus end with a regular 
			word followed by a period. In Section 3 we described the heuristics for abbreviation 
			guessing and pointed out that although these heuristics are reasonably accurate, they 
			fail to identify about 9.5% of abbreviations. Since unidentified abbreviations are then 
			treated as regular words, the overall error rate of the guessing heuristics was measured 
			at about 10% (row A of Table 2). Thus, to improve this error rate, we need first of all 
			to improve the coverage of the abbreviation-handling strategy. 
			A standard way to do this is to use the guessing heuristics in conjunction with a 
			list of known abbreviations. We decided to use the list of 270 abbreviations described 
			in Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e., 
			299 
			Mikheev Periods, Capitalized Words, etc. 
			only when a token was found in the list of 270 known abbreviations was it marked 
			as an abbreviation). This gave us an unexpectedly high error rate of about 12%, as 
			displayed in row B of Table 2. When we investigated the reason for the high error 
			rate, we found that the majority of single letters and spans of single letters sepa- 
			rated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus were 
			not present in our abbreviation list and therefore were not recognized as abbrevia- 
			tions. 
			Such cases, however, are handled well by the abbreviation-guessing heuristics. 
			When we applied the abbreviation list together with the abbreviation-guessing heuris- 
			tics (row C of Table 2), this gave us a very strong performance on the WSJ corpus: 
			an error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This can 
			be explained by the fact that we collected our abbreviation list from a corpus of news 
			articles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con- 
			tained many abbreviations found in that corpus. The Brown corpus, in contrast, ranges 
			across several different domains and sublanguages, which makes it more difficult to 
			compile a list from a single corpus to cover it. 
			6.1 Unigram DCA 
			The abbreviation-guessing heuristics supplemented with a list of abbreviations are 
			accurate, but they still can miss some abbreviations. For instance, if an abbreviation 
			like sec or Okla. is followed by a capitalized word and is not listed in the list of 
			abbreviations, the guessing heuristics will not uncover them. We also would like to 
			boost the abbreviation handling with a domain-independent method that enables the 
			system to function even when the abbreviation list is not much of a help. Thus, in 
			addition to the list of known abbreviations and the guessing heuristics, we decided to 
			apply the DCA as described below. 
			Each word of length four characters or less that is followed by a period is treated as 
			a potential abbreviation. First, the system collects unigrams of potential abbreviations 
			in unambiguous contexts from the document under processing. If a potential abbre- 
			viation is used elsewhere in the document without a trailing period, we can conclude 
			that it in fact is not an abbreviation but rather a regular word (nonabbreviation). To 
			decide whether a potential abbreviation is really an abbreviation, we look for contexts 
			in which it is followed by a period and then by a lower-cased word, a number, or a 
			comma. 
			For instance, the word Kong followed by a period and then by a capitalized word 
			cannot be safely classified as a regular word (nonabbreviation), and therefore it is a 
			potential abbreviation. But if in the same document we detect a context lived in Hong 
			Kong in 1993, this indicates that Kong in this document is normally written without 
			a trailing period and hence is not an abbreviation. Having established that, we can 
			apply this information to nonevident contexts and classify Kong as a regular word 
			throughout the document. However, if we detect a context such as Kong., said, this 
			indicates that in this document, Kong is normally written with a trailing period and 
			hence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation 
			in all its occurrences within the same document. 
			6.2 Bigram DCA 
			The DCA relies on the assumption that there is a consistency in writing within the 
			same document. Different authors can write Mr or Dr with or without a trailing period, 
			but we assume that the same author (the author of a particular document) writes 
			it in the same way consistently. A situation can arise, however, in which the same 
			potential abbreviation is used as a regular word and as an abbreviation within the same 
			300 
			Computational Linguistics Volume 28, Number 3 
			document. This is usually the case when an abbreviation coincides with a regular word, 
			for example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle 
			this problem, the system can collect from a document not only unigrams of potential 
			abbreviations in unambiguous contexts but also their bigrams with the preceding 
			word. Of course, as in the case with unigrams, the bigrams are collected on the fly 
			and completely automatically. 
			For instance, if the system finds a context vitamin C is, it stores the bigram vita- 
			min C and the unigram C with the information that C is a regular word. If in the 
			same document the system also detects a context John C. later said, it stores the bi- 
			gram John C and the unigram C with the information that C is an abbreviation. Here 
			we have conflicting information for the word C: it was detected to act as a regu- 
			lar word and as an abbreviation within the same document, so there is not enough 
			information to resolve ambiguous cases purely using the unigram. Some cases, how- 
			ever, can still be resolved on the basis of the bigrams. The system will assign C as a 
			regular word (nonabbreviation) in an ambiguous context such as vitamin C. Research 
			because of the stored vitamin C bigram. Obviously from such a short context, it is 
			difficult even for a human to make a confident decision, but the evidence gathered 
			from the entire document can influence this decision with a high degree of confi- 
			dence. 
			6.3 Resulting Approach 
			When neither unigrams nor bigrams can help to resolve an ambiguous context for a 
			potential abbreviation, the system decides in favor of the more frequent category for 
			that abbreviation. If the word In was detected to act as a regular word (preposition) five 
			times in the current document and two times as abbreviation (for the state Indiana), in 
			a context in which neither of the bigrams collected from the document can be applied, 
			In is assigned as a regular word (nonabbreviation). The last-resort strategy is to assign 
			all nonresolved cases as nonabbreviations. 
			Row D of Table 2 shows the results when we applied the abbreviation-guessing 
			heuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rate 
			of the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impact 
			was somewhat smaller, about 18%. This can be explained by the fact that abbreviations 
			in the WSJ corpus have a much higher repetition rate, which is very important for 
			the DCA. 
			We also applied the DCA together with the lexical lookup and the guessing heuris- 
			tics. This reduced the error rate on abbreviation identification by about 30% in com- 
			parison with the list and guessing heuristics configuration, as can be seen in row E of 
			Table 2. 
			7. Disambiguating Capitalized Words 
			The second key task of our approach is the disambiguation of capitalized words that 
			follow a potential sentence boundary punctuation sign. Apart from being an important 
			component in the task of text normalization, information about whether or not a 
			capitalized word that follows a period is a common word is crucial for the SBD task, 
			as we showed in Section 3. We tackle capitalized words in a similar fashion as we 
			tackled the abbreviations: through a document-centered approach that analyzes on 
			the fly the distribution of ambiguously capitalized words in the entire document. This 
			is implemented as a cascade of simple strategies, which were briefly described in 
			Mikheev (1999). 
			301 
			Mikheev Periods, Capitalized Words, etc. 
			7.1 The Sequence Strategy 
			The first DCA strategy for the disambiguation of ambiguous capitalized words is to 
			explore sequences of words extracted from contexts in which the same words are used 
			unambiguously with respect to their capitalization. We call this the sequence strategy. 
			The rationale behind this strategy is that if there is a phrase of two or more capitalized 
			words starting from an unambiguous position (e.g., following a lower-cased word), 
			the system can be reasonably confident that even when the same phrase starts from an 
			unreliable position (e.g., after a period), all its words still have to be grouped together 
			and hence are proper nouns. Moreover, this applies not just to the exact replication of 
			the capitalized phrase, but to any partial ordering of its words of size two characters 
			or more preserving their sequence. 
			For instance, if a phrase Rocket Systems Development Co. is found in a document 
			starting from an unambiguous position (e.g., after a lower-cased word, a number, or a 
			comma), the system collects it and also generates its partial-order subphrases: Rocket 
			Systems, Rocket Systems Co., Rocket Co., Systems Development, etc. If then in the same 
			document Rocket Systems is found in an ambiguous position (e.g., after a period), the 
			system will assign the word Rocket as a proper noun because it is part of a multiword 
			proper name that was seen in the unambiguous context. 
			A span of capitalized words can also internally include alpha-numerals, abbrevia- 
			tions with internal periods, symbols, and lower-cased words of length three characters 
			or shorter. This enables the system to capture phrases like A & M and The Phantom of 
			the Opera. Partial orders from such phrases are generated in a similar way, but with 
			the restriction that every generated subphrase should start and end with a capitalized 
			word. 
			The sequence strategy can also be applied to disambiguate common words. Since 
			in the case of common words the system cannot determine boundaries of a phrase, 
			only bigrams of the lower-cased words with their following words are collected from 
			the document. For instance, from a context continental suppliers of Mercury, the sys- 
			tem collects three bigrams: continental suppliers, suppliers of, and of Mercury. When the 
			system encounters the phrase Continental suppliers after a period, it can now use the 
			information that in the previously stored bigram continental suppliers, the word token 
			continental was written lower-cased and therefore was unambiguously used as a com- 
			mon word. On this basis the system can assign the ambiguous capitalized word token 
			Continental as a common word. 
			Row A of Table 3 displays the results obtained in the application of the sequence 
			strategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremely 
			useful when we are dealing with names of organizations, since many of them are 
			multiword phrases composed of common words. For instance, the words Rocket and 
			Insurance can be used both as proper names and common words within the same 
			document. The sequence strategy maintains contexts of the usages of such words 
			within the same document, and thus it can disambiguate such usages in the ambiguous 
			positions matching surrounding words. And indeed, the error rate of this strategy 
			when applied to proper names was below 1%, with coverage of about 9-12%. 
			For tagging common words the sequence strategy was also very accurate (error 
			rate less than 0.3%), covering 17% of ambiguous capitalized common words on the 
			WSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpus 
			can be explained by the fact that the documents in that corpus are in general longer 
			than those in the WSJ corpus, which enables more word bigrams to be collected from 
			a document. 
			Dual application of the sequence strategy contributes to its robustness against po- 
			tential capitalization errors in the document. The negative evidence (not proper name) 
			302 
			Computational Linguistics Volume 28, Number 3 
			Table 3 
			First part: Error rates of different individual strategies for capitalized-word disambiguation. 
			Second part: Error rates of the overall cascading application of the individual strategies. 
			Strategy Word Class Error Rate Coverage 
			WSJ Brown WSJ Brown 
			A Sequence strategy Proper Names 0.12% 0.97% 12.6% 8.82% 
			Sequence strategy Common Words 0.28% 0.21% 17.68% 26.5% 
			B Frequent-list lookup strategy Proper Names 0.49% 0.16% 2.62% 6.54% 
			Frequent-list lookup strategy Common Words 0.21% 0.14% 64.62% 61.20% 
			C Single-word assignment strategy Proper Names 3.18% 1.96% 18.77% 34.13% 
			Single-word assignment strategy Common Words 6.51% 2.87% 3.07% 4.78% 
			D Cascading DCA Proper/Common 1.10% 0.76% 84.12% 91.76% 
			E Cascading DCA Proper/Common 4.88% 2.83% 100.0% 100.0% 
			and lexical lookup 
			is used together with the positive evidence (proper name) and blocks assignment when 
			conflicts are found. For instance, if the system detects a capitalized phrase The President 
			in an unambiguous position, then the sequence strategy will treat the word the as part 
			of the proper name The President even when this phrase follows a period. If in the 
			same document, however, the system detects alternative evidence (e.g., the President, 
			where the is not part of the proper name), it then will block as unsafe the assignment 
			of The as a proper name in ambiguous usages of The President. 
			7.2 Frequent-List Lookup Strategy 
			The frequent-list lookup strategy applies lookup of ambiguously capitalized words 
			in two word lists. The first list contains common words that are frequently found 
			in sentence-starting positions, and the other list contains the most frequent proper 
			names. Both these lists can be compiled completely automatically, as explained in 
			section 5. Thus, if an ambiguous capitalized word is found in the list of frequent 
			sentence-starting common words, it is assigned as a common word, and if it is found 
			in the list of frequent proper names, it is assigned as a proper name. For instance, 
			the word token The when used after a period will be recognized as a common word, 
			because The is a frequent sentence-starting common word. The Word token Japan in a 
			similar context will be recognized as a proper name, because Japan is a member of the 
			frequent-proper-name list. 
			Note, however, that this strategy is applied after the sequence strategy and thus, a 
			word listed in one of the lists will not necessarily be marked according to its list class. 
			The list lookup assignment is applied only to the ambiguously capitalized words that 
			have not been handled by the sequence strategy. 
			Row B of Table 3 displays the results of the application of the frequent-list lookup 
			strategy to the Brown corpus and the WSJ corpus. The frequent-list lookup strategy 
			produced an error rate of less than 0.5%. A few wrong assignments came from phrases 
			like Mr. A and Mrs. Someone and words in titles like I've Got a Dog, where A, Someone, 
			and I were recognized as common words although they were tagged as proper nouns 
			in the text. The frequent-list lookup strategy is not very effective for proper names, 
			where it covered under 7% of candidates in the Brown corpus and under 3% in the 
			WSJ corpus, but it is extremely effective for common words: it covered over 60% of 
			ambiguous capitalized common words. 
			303 
			Mikheev Periods, Capitalized Words, etc. 
			7.3 Single-Word Assignment 
			The sequence strategy is accurate, but it covers only 9-12% of proper names in ambigu- 
			ous positions. The frequent-list lookup strategy is mostly effective for common words. 
			To boost the coverage on the proper name category, we introduced another DCA 
			strategy. We call this strategy single-word assignment, and it can be summarized as 
			follows: if a word in the current document is seen capitalized in an unambiguous po- 
			sition and at the same time it is not used lower-cased anywhere in the document, this 
			word in this particular document is very likely to stand for a proper name even when 
			used capitalized in ambiguous positions. And conversely, if a word in the current 
			document is used only lower-cased (except in ambiguous positions), it is extremely 
			unlikely that this word will act as a proper name in an ambiguous position and thus, 
			such a word can be marked as a common word. 
			Note that by the time single-word assignment is implemented, the sequence strat- 
			egy and the frequent-list lookup strategy have been already applied and all high- 
			frequency sentence-initial words have been assigned. This ordering is important, be- 
			cause even if a high-frequency common word is observed in a document only as a 
			proper name (usually as part of a multiword proper name), it is still not safe to mark 
			it as a proper name in ambiguous positions. 
			Row C of Table 3 displays the results of the application of the single-word assign- 
			ment strategy to the Brown corpus and the WSJ corpus. The single-word assignment 
			strategy is useful for proper-name identification: although it is not as accurate as the 
			sequence strategy, it still produces a reasonable error rate at the same time boosting the 
			coverage considerably (19-34%). On common words this method is not as effective, 
			with an error rate as high as 6.61% on the WSJ corpus and a coverage below 5%. 
			The single-word-assignment strategy handles well the so-called unknown-word 
			problem, which arises when domain-specific lexica are missing from a general vocab- 
			ulary. Since our system is not equipped with a general vocabulary but rather builds a 
			document-specific vocabulary on the fly," important domain-specific words are iden- 
			tified and treated similarly to all other words. 
			A generally difficult case for the single-word assignment strategy arises when a 
			word is used both as a proper name and as a common word in the same document, es- 
			pecially when one of these usages occurs only in an ambiguous position. For instance, 
			in a document about steel, the only occurrence of Steel Company happened to start 
			a sentence. This produced an erroneous assignment of the word Steel as a common 
			word. Another example: in a document about the Acting Judge, the word acting in a 
			sentence Acting on behalf. . . was wrongly classified as a proper name. These difficulties, 
			however, often are compensated for by the sequence strategy, which is applied prior 
			to the single-word assignment strategy and tackles such cases using n-grams of words. 
			7.4 Quotes, Brackets, and "After Abbr." Heuristic 
			Capitalized words in quotes and brackets do not directly contribute to our primary 
			task of sentence boundary disambiguation, but they still present a case of ambiguity 
			for the task of capitalized-word disambiguation. To tackle them we applied two simple 
			heuristics: 
			* If a single capitalized word is used in quotes or brackets it is a proper 
			noun (e.g., John (Cool) Lee). 
			* If there is a lowercased word, a number, or a comma that is followed by 
			an opening bracket and then by a capitalized word, this capitalized word 
			is a proper noun (e.g., . . . happened (Moscow News reported yesterday) but. . . ). 
			304 
			Computational Linguistics Volume 28, Number 3 
			These heuristics are reasonably accurate: they achieved under 2% error rate on our 
			two test corpora, but they covered only about 6-7% of proper names. 
			When we studied the distribution of capitalized words after capitalized abbrevi- 
			ations, we uncovered an interesting empirical fact. A capitalized word that follows 
			a capitalized abbreviation is almost certainly a proper name unless it is listed in the 
			list of frequent sentence-starting common words (i.e., it is not The, However, etc.). The 
			error rate of this heuristic is about 0.8% and, not surprisingly, in 99.5% of cases the 
			abbreviation and the following proper name belonged to the same sentence. Naturally, 
			the coverage of this "after abbr." heuristic depends on the proportion of capitalized 
			abbreviations in the text. In our two corpora this heuristic disambiguated about 20% 
			of ambiguous capitalized proper names. 
			7.5 Tagging Proper Names: The Overall Performance 
			In general, the cascading application of the above-described strategies achieved an 
			error rate of about 1%, but it left unclassified about 9% of ambiguous capitalized 
			words in the Brown corpus and 15% of such words in the WSJ corpus. Row D of 
			Table 3 displays the results of the application of the cascading application of the 
			capitalized-word disambiguation strategies to the Brown corpus and the WSJ corpus. 
			For the proper-name category, the most productive strategy was single-word as- 
			signment, followed by the "after abbr." strategy, and then the sequence strategy. For 
			common words, the most productive was the frequent-list lookup strategy, followed 
			by the sequence strategy. 
			Since our system left unassigned 10-15% of ambiguous capitalized words, we have 
			to decide what to do with them. To keep our system simple and domain independent, 
			we opted for the lexical-lookup strategy that we evaluated in Section 3. This strategy, 
			of course, is not very accurate, but it is applied only to the unassigned words. Row E 
			of Table 3 displays the results of applying the lexical-lookup strategy after the DCA 
			methods. We see that the error rate went up in comparison to the DCA-only method 
			by more than three times (2.9% on the Brown corpus and 4.9% on the WSJ corpus), 
			but no unassigned ambiguous capitalized words are left in the text. 
			8. Putting It All Together: Assigning Sentence Boundaries 
			After abbreviations have been identified and capitalized words have been classified 
			into proper names and common words, the system can carry out the assignments 
			of sentence boundaries using the SBD rule set described in Section 3 and listed in 
			Appendix A. This rule set makes use of the observation that if we have at our dis- 
			posal unambiguous (but not necessarily correct) information as to whether a particular 
			word that precedes a period is an abbreviation and whether the word that follows 
			this period is a proper name, then in mixed-case texts we can easily assign a pe- 
			riod (and other potential sentence termination punctuation) as a sentence break or 
			not. 
			The only ambiguous outcome is generated by the configuration in which an ab- 
			breviation is followed by a proper name. We decided to handle this case by applying 
			a crude and simple strategy of always resolving it as "not sentence boundary." On one 
			hand, this makes our method simple and robust, but on the other hand, it imposes 
			some penalty on its performance. 
			Row A of Table 4 summarizes the upper bound for our SBD approach: when we 
			have entirely correct information on the abbreviations and proper names, as explained 
			in Section 3.1. There the erroneous assignments come only from the crude treatment 
			of abbreviations that are followed by proper names. 
			305 
			Mikheev Periods, Capitalized Words, etc. 
			Table 4 
			Error rates measured on the SBD, capitalized-word disambiguation, and abbreviation 
			identification tasks achieved by different methods described in this article. 
			Method Brown Corpus WSJ Corpus 
			SBD Capitalized Abbreviations SBD Capitalized Abbreviations 
			words words 
			A Upper bound 0.01% 0.0% 0.0% 0.13% 0.0% 0.0% 
			B Lower bound 2.00% 7.40% 10.8% 4.10% 15.0% 9.6% 
			C Best quoted 0.20% 3.15% -- 0.50% 4.72% -- 
			D DCA 0.28% 2.83% 0.8% 0.45% 4.88% 1.2% 
			E DCA (no abbreviations 0.65% 2.89% 8.9% 1.41% 4.92% 6.6% 
			lexicon) 
			F POS tagger 0.25% 3.15% 1.2% 0.39% 4.72% 2.1% 
			G POS tagger + DCA 0.20% 1.87% 0.8% 0.31% 3.22% 1.2% 
			Row B of Table 4 summarizes the lower-bound results. The lower bound for our 
			approach was estimated by applying the lexical-lookup strategy for capitalized-word 
			disambiguation together with the abbreviation-guessing heuristics to feed the SBD 
			rule set, as described in Section 3.2. Here we see a significant impact of the infelicities 
			in the disambiguation of capitalized words and abbreviations on the performance of 
			the SBD rule set. 
			Row C of Table 4 summarizes the highest results known to us (for all three tasks) 
			produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-the- 
			art machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% 
			measured on the Brown corpus and the WSJ corpus. The best performance on the 
			WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 
			1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best 
			performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who 
			trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of 
			capitalized words, the most widespread method is POS tagging, which achieves about 
			a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported 
			in Mikheev (2000). We are not aware of any studies devoted to the identification of 
			abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ 
			corpus. 
			In row D of Table 4, we summarized our main results: the results obtained by the 
			application of our SBD rule set, which uses the information provided by the DCA to 
			capitalized word disambiguation applied together with lexical lookup (as described 
			in Section 7.5), and the abbreviation-handling strategy, which included the guessing 
			heuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). As 
			can be seen in the table, the performance of this system is almost indistinguishable 
			from the best previously quoted results. On proper-name disambiguation, it achieved 
			a 2.83% error rate on the Brown corpus and a 4.88% error rate on the WSJ corpus. 
			On the SBD task, it achieved a 0.28% error rate on the Brown corpus and a 0.45% 
			error rate on the WSJ corpus. If we compare these results with the upper bound 
			for our SBD approach, we can see that the infelicities in proper-name and abbrevia- 
			tion identification introduced an increase of about 0.3% in the error rate on the SBD 
			task. 
			To test the adaptability of our approach to a completely new domain, we applied 
			our system in a configuration in which it was not equipped with the list of 270 abbre- 
			306 
			Computational Linguistics Volume 28, Number 3 
			viations, since this list is the only domain-sensitive resource in our system. The results 
			for this configuration are summarized in row E of Table 4. The error rate increase of 
			5-7% on the abbreviation handling introduced about a twofold increase in the SBD 
			error rate on the Brown corpus (a 0.65% error rate) and about a threefold increase on 
			the WSJ corpus (1.41%). But these results are still comparable to those of the majority 
			of currently used sentence splitters. 
			9. Detecting Limits for the DCA 
			Since our DCA method relies on the assumption that the words it tries to disam- 
			biguate occur multiple times in a document, its performance clearly should depend 
			on the length of the document: very short documents possibly do not provide enough 
			disambiguation clues, whereas very long documents possibly contain too many clues 
			that cancel each other. 
			As noted in Section 2.1, the average length of the documents in the Brown corpus 
			is about 2,300 words. Also, the documents in that corpus are distributed very densely 
			around their mean. Thus not much can be inferred about the dependency of the perfor- 
			mance of the method on document length apart from the observation that documents 
			2,000-3,000 words long are handled well by our approach. In the WSJ corpus, the aver- 
			age length of the document is about 500 words, and therefore we could investigate the 
			effect of short documents on the performance. We divided documents into six groups 
			according to their length and plotted the error rate for the SBD and capitalized-word 
			disambiguation tasks as well as the number of documents in a group, as shown in 
			Figure 2. As can be seen in the figure, short documents (50 words or less) have the 
			highest average error rate both for the SBD task (1.63) and for the capitalized-word 
			disambiguation task (5.25). For documents 50 to 100 words long, the error rate is still 
			a bit higher than normal, and for longer documents the error rate stabilizes around 
			1.5 for the capitalized-word disambiguation task and 0.3 for the SBD task. The error 
			rate on documents 2,000 words long and higher is almost identical to that registered 
			on the Brown corpus on documents of the same length. 
			Thus here we can conclude that the proposed approach tends not to be very 
			effective for documents shorter than 50 words (one to three sentences), but it handles 
			well documents up to 4,000 words long. Since our corpora did not contain documents 
			significantly longer than that, we could not estimate whether or when the performance 
			of our method significantly deteriorates on longer documents. We also evaluated the 
			performance of the method on different subcorpora of the Brown corpus: the most 
			difficult subdomains proved to be scientific texts, spoken-language transcripts, and 
			journalistic texts, whereas fiction was the easiest genre for the system. 
			10. Incorporating DCA into a POS Tagger 
			To test our hypothesis that DCA can be used as a complement to a local-context 
			approach, we combined our main configuration (evaluated in row D of Table 4) with 
			a POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was also 
			trained to disambiguate sentence boundaries. 
			10.1 Training a POS Tagger 
			In our markup convention (Section 2), periods are tokenized as separate tokens re- 
			gardless of whether they stand for fullstops or belong to abbreviations. Consequently 
			a POS tagger can naturally treat them similarly to any other ambiguous words. There 
			is, however, one difference in the implementation of such a tagger. Normally, a POS 
			307 
			Mikheev Periods, Capitalized Words, etc. 
			x 
			x 
			x 
			x 
			x x 
			x 
			o 
			o 
			o 
			o o 
			o 
			o 
			c 
			c 
			c c 
			c 
			c 
			c 
			Number of Documents 
			Cap.Word error rate 
			SBD error rate 
			50 100 200 500 1000 2000 3000 
			1 
			2 
			3 
			200 
			600 
			Doc. Length 
			Error Rate 
			Number of Docs 
			400 
			Figure 2 
			Distribution of the error rate and the number of documents across the document length 
			(measured in word tokens) in the WSJ corpus. 
			tagger operates on text spans that form a sentence. This requires resolving sentence 
			boundaries before tagging. We see no good reason, however, why such text spans 
			should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden 
			Markov Model [HMM] [Kupiec 1992], Brill's [Brill 1995a], and MaxEnt [Ratnaparkhi 
			1996]) do not attempt to parse an entire sentence and operate only in the local win- 
			dow of two to three tokens. The only reason why taggers traditionally operate on 
			the sentence level is that a sentence naturally represents a text span in which POS 
			information does not depend on the previous and following history. 
			This issue can be also addressed by breaking the text into short text spans at 
			positions where the previous tagging history does not affect current decisions. For 
			instance, a bigram tagger operates within a window of two tokens, and thus a se- 
			quence of word tokens can be terminated at an unambiguous word token, since this 
			unambiguous word token will be the only history used in tagging of the next token. 
			At the same time since this token is unambiguous, it is not affected by the history. 
			A trigram tagger operates within a window of three tokens, and thus a sequence of 
			word tokens can be terminated when two unambiguous words follow each other. 
			Using Penn Treebank with our tokenization convention (Section 2), we trained a 
			trigram HMM POS tagger. Words were clustered into ambiguity classes (Kupiec 1992) 
			according to the sets of POS tags they can take on. The tagger predictions were based 
			on the ambiguity class of the current word, abbreviation/capitalization information, 
			308 
			Computational Linguistics Volume 28, Number 3 
			and trigrams of POS tags: 
			P(t1 . . . tn 
			O1 . . . On) = argmax 
			i=n 
			i=1 
			P(Oi 
			| ti)  P(ti 
			| ti-1 
			ti-2 
			ai-1) 
			where ti 
			is a disambiguated POS tag of the ith word, ai 
			is the abbreviation flag of 
			the ith word, and Oi 
			is the observation at the ith position, which in our case is 
			the ambiguity class the word belongs to, its capitalization, and its abbreviation flag 
			(AmbClassi 
			, ai 
			, Capi). Since the abbreviation flag of the previous word strongly influ- 
			ences period disambiguation, it was included in the standard trigram model. 
			We decided to train the tagger with the minimum of preannotated resources. First, 
			we used 20,000 tagged words to "bootstrap" the training process, because purely un- 
			supervised techniques, at least for the HMM class of taggers, yield lower precision. 
			We also used our DCA system to assign capitalized words, abbreviations, and sen- 
			tence breaks, retaining only cases assigned by the strategies with an accuracy not less 
			than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch 
			[Baum and Petrie 1966] or Brill's [Brill 1995b]) enable regularities to be induced for 
			word classes which contain many entries, exploiting the fact that individual words that 
			belong to a POS class occur in different ambiguity patterns. Counting all possible POS 
			combinations in these ambiguity patterns over multiple patterns usually produces the 
			right combinations as the most frequent. Periods as many other closed-class words 
			cannot be successfully covered by such technique. 
			After bootstrapping we applied the forward-backward (Baum-Welch) algorithm 
			(Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, with- 
			out using the annotation available in the Brown corpus and the WSJ corpus. For 
			evaluation purposes we trained (and bootstrapped) our tagger on the Brown corpus 
			and applied it to the WSJ corpus and vice versa. We preferred this method to tenfold 
			cross-validation because it allowed us to produce only two tagging models instead of 
			twenty and also enabled us to test the tagger in harsher conditions, that is, when it is 
			applied to texts that are very distant from the ones on which it was trained. 
			The overall performance of the tagger was close to 96%, which is a bit lower 
			than the best quoted results. This can be accounted for by the fact that training and 
			evaluation were performed on two very different text corpora, as explained above. 
			The performance of the tagger on our target categories (periods and proper names) 
			was very close to that of the DCA method, as can be seen in row F of Table 4. 
			10.2 POS Tagger and the DCA 
			We felt that the DCA method could be used as a complement to the POS tagger, since 
			these techniques employ different types of information: in-document distribution and 
			local context. Thus, a hybrid system can deliver at least two advantages. First, 10-15% 
			of the ambiguous capitalized words unassigned by the DCA can be assigned using 
			a standard POS-tagging method based on the local syntactic context rather than the 
			inaccurate lexical-lookup approach. Second, the local context can correct some of the 
			errors made by the DCA. 
			To implement this hybrid approach we incorporated the DCA system into the 
			POS tagger. We modified the tagger model by incorporating the DCA predictions 
			using linear interpolation: 
			P(combined) =   P(tagger) + (1 - )  P(DCA Strategy) 
			where P(DCA Strategy) is the accuracy of a specific DCA strategy and P(tagger) is the 
			probability assigned by the tagger's model. Although it was possible to estimate an 
			309 
			Mikheev Periods, Capitalized Words, etc. 
			optimal value for  from the tagged corpus, we decided simply to set it to be 0.5 (i.e., 
			giving similar weight to both sources of information). Instead of using the SBD rule 
			set described in Section 3, in this configuration, period assignments were handled by 
			the tagger's model. 
			Row G of Table 4 displays the results of the application of the hybrid system. We 
			see an improvement on proper-name recognition in comparison to the DCA or POS- 
			tagging approaches (rows D and F) by about a 30-40% cut in the error rate: an overall 
			error rate of 1.87% on the Brown corpus and of 3.22% on the WSJ corpus. In turn this 
			enabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus 
			and a 0.31% error rate on the WSJ corpus, which corresponds to about a 20% cut in 
			the error rate in comparison to the DCA or the POS-tagging approaches alone. 
			Thus, although for applications that rely on POS tagging it probably makes more 
			sense to have a single system that assigns both POS tags and sentence boundaries, 
			there is still a clear benefit in using the DCA method because 
			* the DCA method incorporated into the POS tagger significantly reduced 
			the error rate on the target categories (periods and proper names). 
			* the DCA method is domain independent, whereas taggers usually need 
			to be trained for each specific domain to obtain best results. 
			* the DCA system was used in resource preparation for training the tagger. 
			* the DCA system is significantly faster than the tagger, does not require 
			resource development, and for tasks that do not require full POS 
			information, it is a preferable solution. 
			So in general, the DCA method can be seen as an enhancer for a POS tagger and 
			also as a lightweight alternative to such a tagger when full POS information is not 
			required. 
			11. Further Experiments 
			11.1 The Cache Extension 
			One of the features of the method advocated in this article is that the system collects 
			suggestive instances of usage for target words from each document, then applies this 
			information during the second pass through the document (actual processing), and 
			then "forgets" what it has learned before handling another document. The main rea- 
			son for not carrying over the information that has been inferred from one document 
			to process another document is that in general we do not know whether this new 
			document comes from the same corpus as the first document, and thus the regular- 
			ities that have been identified in the first document might not be useful, but rather 
			harmful, when applied to that new document. When we are dealing with documents 
			of reasonable length, this "forgetful" behavior does not matter much, because such 
			documents usually contain enough disambiguation clues. As we showed in Section 8, 
			however, when short documents of one to three sentences are being processed, quite 
			often there are not enough disambiguation clues within the document itself, which 
			leads to inferior performance. 
			To improve the performance on short documents, we introduced a special caching 
			module that propagates some information identified in previously processed docu- 
			ments to the processing of a new one. To propagate features of individual words from 
			one document to processing another one is a risky strategy, since words are very 
			310 
			Computational Linguistics Volume 28, Number 3 
			ambiguous. Word sequences, however, are much more stable and can be propagated 
			across documents. We decided to accumulate in our cache all multiword proper names 
			and lower-cased word bigrams induced by the sequence strategy (Section 7.1). These 
			word sequences are used by the sequence strategy exactly as are word sequences in- 
			duced on the fly, and then the induced on-the-fly sequences are added to the cache. 
			We also add to the cache the bigrams of abbreviations and regular words induced by 
			the abbreviation-handling module, as explained in Section 6. These bigrams are used 
			together with the bigrams induced on the fly. This strategy proved to be quite useful: 
			it covered another 2% of unresolved cases (before applying the lexical lookup), with 
			an error rate of less than 1%. 
			11.2 Handling Russian News 
			To test how easy it is to apply the DCA to a new language, we tested it on a corpus 
			of British Broadcasting Corporation (BBC) news in Russian. We collected this corpus 
			from the Internet http://news.bbc.co.uk/hi/russian/world/default.htm over a period of 30 
			days. This gave us a corpus of 300 short documents (one or two paragraphs each). 
			We automatically created the supporting resources from 364,000 documents from the 
			Russian corpus of the European Corpus Initiative, using the method described in 
			section 5. 
			Since, unlike English, Russian is a highly inflected language, we had to deal with 
			the case normalization issue. Before using the DCA method, we applied a Russian 
			morphological processor (Mikheev and Liubushkina 1995) to convert each word in 
			the text to its main form: nominative case singular for nouns and adjectives, infinitive 
			for verbs, etc. For words that could be normalized to several main forms (polysemy), 
			when secondary forms of different words coincided, we retained all the main forms. 
			Since the documents in the BBC news corpus were rather short, we applied the cache 
			module, as described in Section 11.1. This allowed us to reuse information across the 
			documents. 
			Russian proved to be a simpler case than English for our tasks. First, on average, 
			Russian words are longer than English words: thus the identification of abbreviations 
			is simpler. Second, proper names in Russian coincide less frequently with common 
			words; this makes the disambiguation of capitalized words in ambiguous positions 
			easier. The overall performance reached a 0.1% error rate on sentence boundaries and 
			a 1.8% error rate on ambiguous capitalized words, with the coverage on both tasks 
			at 100%. 
			12. Related Research 
			12.1 Research in Nonlocal Context 
			The use of nonlocal context and dynamic adaptation have been studied in language 
			modeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model 
			that works as a kind of short-term memory by which the probability of the most re- 
			cent n words is increased over the probability of a general-purpose bigram or trigram 
			model. Within certain limits, such a model can adapt itself to changes in word frequen- 
			cies, depending on the topic of the text passage. The DCA system is similar in spirit 
			to such dynamic adaptation: it applies word n-grams collected on the fly from the 
			document under processing and favors them more highly than the default assignment 
			based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. 
			Clarkson and Robinson (1997) developed a way of incorporating standard n-grams 
			into the cache model, using mixtures of language models and also exponentially de- 
			caying the weight for the cache prediction depending on the recency of the word's last 
			311 
			Mikheev Periods, Capitalized Words, etc. 
			occurrence. In our experiments we applied simple linear interpolation to incorporate 
			the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted 
			for not propagating it from one document for processing of another. For handling very 
			long documents with our method, however, the information decay strategy seems to 
			be the right way to proceed. 
			Mani and MacMillan (1995) pointed out that little attention had been paid in the 
			named-entity recognition field to the discourse properties of proper names. They pro- 
			posed that proper names be viewed as linguistic expressions whose interpretation 
			often depends on the discourse context, advocating text-driven processing rather than 
			reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal dis- 
			course context and does not heavily rely on pre-existing word lists. It has been applied 
			not only to the identification of proper names, as described in this article, but also to 
			their classification (Mikheev, Grover, and Moens 1998). 
			Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit 
			only one sense in a document or discourse ("one sense per discourse"). Since then 
			this idea has been applied to several tasks, including word sense disambiguation 
			(Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, 
			Church, and Yarowsky's observation is also used in our DCA, especially for the iden- 
			tification of abbreviations. In capitalized-word disambiguation, however, we use this 
			assumption with caution and first apply strategies that rely not just on single words 
			but on words together with their local contexts (n-grams). This is similar to "one sense 
			per collocation" idea of Yarowsky (1993). 
			The description of the EAGLE workbench for linguistic engineering (Baldwin et al. 
			1997) mentions a case normalization module that uses a heuristic in which a capitalized 
			word in an ambiguous position should be rewritten without capitalization if it is found 
			lower-cased in the same document. This heuristic also employs a database of bigrams 
			and unigrams of lower-cased and capitalized words found in unambiguous positions. 
			It is quite similar to our method for capitalized-word disambiguation. The description 
			of the EAGLE case normalization module provided by Baldwin et al. is, however, very 
			brief and provides no performance evaluation or other details. 
			12.2 Research in Text Preprocessing 
			12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD sys- 
			tems: rule based and machine learning. The rule-based systems use manually built 
			rules that are usually encoded in terms of regular-expression grammars supplemented 
			with lists of abbreviations, common words, proper names, etc. To put together a few 
			rules is fast and easy, but to develop a rule-based system with good performance is 
			quite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen et 
			al. 1995) contains a sentence-splitting module that employs over 100 regular-expression 
			rules written in Flex. Another well-acknowledged shortcoming of rule-based systems 
			is that such systems are usually closely tailored to a particular corpus or sublanguage 
			and are not easily portable across domains. 
			Automatically trainable software is generally seen as a way of producing sys- 
			tems that are quickly retrainable for a new corpus, for a new domain, or even for 
			another language. Thus, the second class of SBD systems employs machine learning 
			techniques such as decision tree classifiers (Riley 1989), neural networks (Palmer and 
			Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Ma- 
			chine learning systems treat the SBD task as a classification problem, using features 
			such as word spelling, capitalization, suffix, and word class found in the local con- 
			text of a potential sentence-terminating punctuation sign. Although training of such 
			312 
			Computational Linguistics Volume 28, Number 3 
			systems is completely automatic, the majority of machine learning approaches to the 
			SBD task require labeled examples for training. This implies an investment in the data 
			annotation phase. 
			The main difference between the existing machine learning and rule-based meth- 
			ods for the SBD task and our approach is that we decomposed the SBD task into 
			several subtasks. We decided to tackle the SBD task through the disambiguation of 
			the period preceding and following words and then feed this information into a very 
			simple SBD rule set. In contrast, the standard practice in building SBD software is to 
			disambiguate configurations of a period with its ambiguous local context in a single 
			step, either by encoding disambiguation clues into the rules or inferring a classifier 
			that accounts for the ambiguity of the words on the left and on the right of the period. 
			Our approach to SBD is closer in spirit to machine learning methods because its 
			retargeting does not require rule reengineering and can be done completely automat- 
			ically. Unlike traditional machine learning SBD approaches, however, our approach 
			does not require annotated data for training. 
			12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words 
			is usually handled by POS taggers, which treat capitalized words in the same way 
			as other categories, that is, by accounting for the immediate syntactic context and 
			using estimates collected from a training corpus. As Church (1988) rightly pointed 
			out, however, "Proper nouns and capitalized words are particularly problematic: some 
			capitalized words are proper nouns and some are not. Estimates from the Brown 
			Corpus can be misleading. For example, the capitalized word `Acts' is found twice in 
			the Brown Corpus, both times as a proper noun (in a title). It would be misleading to 
			infer from this evidence that the word `Acts' is always a proper noun." 
			In the information extraction field, the disambiguation of ambiguous capitalized 
			words has always been tightly linked to the classification of proper names into seman- 
			tic classes such as person name, location, and company name. Named-entity recogni- 
			tion systems usually use sets of complex hand-crafted rules that employ a gazetteer 
			and a local context (Krupa and Hausman 1998). In some systems such dependencies 
			are learned from labeled examples (Bikel et al. 1997). The advantage of the named- 
			entity approach is that the system not only identifies proper names but also determines 
			their semantic class. The disadvantage is in the cost of building a wide-coverage set of 
			contextual clues manually or producing annotated training data. Also, the contextual 
			clues are usually highly specific to the domain and text genre, making such systems 
			very difficult to port. 
			Both POS taggers and named-entity recognizers are normally built using the local- 
			context paradigm. In contrast, we opted for a method that relies on the entire distri- 
			bution of a word in a document. Although it is possible to train some classes of POS 
			taggers without supervision, this usually leads to suboptimal performance. Thus the 
			majority of taggers are trained using at least some labeled data. Named-entity recog- 
			nition systems are usually hand-crafted or trained from labeled data. As was shown 
			above, our method does not require supervised training. 
			12.2.3 Disambiguation of Abbreviations. Not much information has been published 
			on abbreviation identification. One of the better-known approaches is described in 
			Grefenstette and Tapanainen (1994), which suggested that abbreviations first be ex- 
			tracted from a corpus using abbreviation-guessing heuristics akin to those described 
			in Section 6 and then reused in further processing. This is similar to our treatment of 
			abbreviation handling, but our strategy is applied on the document rather than corpus 
			level. The main reason for restricting abbreviation discovery to a single document is 
			313 
			Mikheev Periods, Capitalized Words, etc. 
			that this does not presuppose the existence of a corpus in which the current document 
			is similar to other documents. 
			Park and Byrd (2001) recently described a hybrid method for finding abbrevia- 
			tions and their definitions. This method first applies an "abbreviation recognizer" that 
			generates a set of "candidate abbreviations" for a document. Then for this set of can- 
			didates the system tries to find in the text their definitions (e.g., United Kingdom for 
			UK). The abbreviation recognizer for these purposes is allowed to overgenerate signif- 
			icantly. There is no harm (apart from the performance issues) in proposing too many 
			candidate abbreviations, because only those that can be linked to their definitions will 
			be retained. Therefore the abbreviation recognizer treats as a candidate any token of 
			two to ten characters that contains at least one capital letter. Candidates then are fil- 
			tered through a set of known common words and proper names. At the same time 
			many good abbreviations and acronyms are filtered out because not for all of them 
			will definitions exist in the current document. 
			In our task we are interested in finding all and only abbreviations that end with 
			a period (proper abbreviations rather than acronyms), regardless of whether they can 
			be linked to their definitions in the current document or not. Therefore, in our method 
			we cannot tolerate candidate overgeneration or excessive filtering and had to develop 
			more selective methods for finding abbreviations in text. 
			2. Performance Measure, Corpora for Evaluation, and Intended Markup 
			A standard practice for measuring the performance of a system for the class of tasks 
			with which we are concerned in this article is to calculate its error rate: 
			error rate = 
			incorrectly assigned 
			all assigned by system 
			This single measure gives enough information, provided that the system does not 
			leave unassigned word tokens that it is intended to handle. Obviously, we want the 
			system to handle all cases as accurately as possible. Sometimes, however, it is beneficial 
			to assign only cases in which the system is confident enough, leaving the rest to be 
			handled by other methods. In this case apart from the error rate (which corresponds 
			to precision or accuracy as 1-error rate) we also measure the system's coverage or 
			recall 
			coverage = 
			correctly assigned 
			all to be assigned 
			2.1 Corpora for Evaluation 
			There are two corpora normally used for evaluation in a number of text-processing 
			tasks: the Brown corpus (Francis and Kucera 1982) and the Wall Street Journal (WSJ) 
			corpus, both part of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). 
			The Brown corpus represents general English. It contains over one million word tokens 
			and is composed from 15 subcorpora that belong to different genres and domains, 
			ranging from news wire texts and scientific papers to fiction and transcribed speech. 
			The Brown corpus is rich in out-of-vocabulary (unknown) words, spelling errors, and 
			ungrammatical sentences with complex internal structure. Altogether there are about 
			500 documents in the Brown corpus, with an average length of 2,300 word tokens. 
			The WSJ corpus represents journalistic news wire style. Its size is also over a 
			million word tokens, and the documents it contains are rich in abbreviations and 
			proper names, but they are much shorter than those in the Brown corpus. Altogether 
			there are about 2,500 documents in the WSJ corpus, with an average length of about 
			500 word tokens. 
			Documents in the Penn Treebank are segmented into paragraphs and sentences. 
			Sentences are further segmented into word tokens annotated with part-of-speech (POS) 
			information. POS information can be used to distinguish between proper names and 
			common words. We considered proper nouns (NNP), plural proper nouns (NNPS), and 
			proper adjectives2 (JJP) to signal proper names, and all other categories were consid- 
			ered to signal common words or punctuation. Since proper adjectives are not included 
			in the Penn Treebank tag set, we had to identify and retag them ourselves with the 
			help of a gazetteer. 
			Abbreviations in the Penn Treebank are tokenized together with their trailing pe- 
			riods, whereas fullstops and other sentence boundary punctuation are tokenized as 
			separate tokens. This gives all necessary information for the evaluation in all our three 
			2 These are adjectives derived from proper nouns (e.g. "American"). 
			292 
			Computational Linguistics Volume 28, Number 3 
			tasks: the sentence boundary disambiguation task, the capitalized word disambigua- 
			tion task, and the abbreviation identification task. 
			2.2 Tokenization Convention and Corpora Markup 
			For easier handling of potential sentence boundary punctuation, we developed a new 
			tokenization convention for periods. In the traditional Penn Treebank schema, abbrevi- 
			ations are tokenized together with their trailing periods, and thus stand-alone periods 
			unambiguously signal the end of a sentence. We decided to treat periods and all other 
			potential sentence termination punctuation as "first-class citizens" and adopted a con- 
			vention always to tokenize a period (and other punctuation) as a separate token when 
			it is followed by a white space, line break, or punctuation. In the original Penn Tree- 
			bank format, periods are unambiguous, whereas in our new convention they can take 
			on one of the three tags: fullstop (.), part of abbreviation (A) or both (*). 
			To generate the new format from the Penn Treebank, we had to split final periods 
			from abbreviations, mark them as separate tokens and assign them with A or * tags 
			according to whether or not the abbreviation was the last token in a sentence. We 
			applied a similar tokenization convention to the case in which several (usually three) 
			periods signal ellipsis in a sentence. Again, sometimes such constructions occur within 
			a sentence and sometimes at a sentence break. We decided to treat such constructions 
			similarly to abbreviations, tokenize all periods but the last together in a single token, 
			and tokenize the last period separately and tag it with A or * according to whether 
			or not the ellipsis was the last token in a sentence. We treated periods in numbers 
			(e.g., 14.534) or inside acronyms (e.g., Y.M.C.A.) as part of tokens rather than separate 
			periods. 
			In all our experiments we treated embedded sentence boundaries in the same way 
			as normal sentence boundaries. An embedded sentence boundary occurs when there 
			is a sentence inside a sentence. This can be a quoted direct-speech subsentence inside a 
			sentence, a subsentence embedded in brackets, etc. We considered closing punctuation 
			of such sentences equal to closing punctuation of normal sentences. 
			We also specially marked word tokens in positions where they were ambiguously 
			capitalized if such word tokens occurred in one of the following contexts: 
			* the first token in a sentence 
			* following a separately tokenized period, question mark, exclamation 
			mark, semicolon, colon, opening quote, closing quote, opening bracket, 
			or closed bracket 
			* occurring in a sentence with all words capitalized 
			All described transformations were performed automatically by applying a simple 
			Perl script. We found quite a few infelicities in the original tokenization and tagging, 
			however, which we had to correct by hand. We also converted both our corpora from 
			their original Penn Treebank format into an XML format where each word token is 
			represented as an XML element (W) with the attribute C holding its POS information 
			and attribute A set to Y for ambiguously capitalized words. An example of such a 
			markup is displayed in Figure 1. 
			3. Our Approach to Sentence Boundary Disambiguation 
			If we had at our disposal entirely correct information on whether or not each word 
			preceding a period was an abbreviation and whether or not each capitalized word 
			293 
			Mikheev Periods, Capitalized Words, etc. 
			...<W C=RB>soon</W><W C='.'>.</W> <W A=Y C=NNP>Mr</W><W C=A>.</W>... 
			...<W C=VBN>said</W> <W C=NNP>Mr</W><W C=A>.</W> 
			<W A=Y C=NNP>Brown</W>... 
			...<W C=','>,</W> <W C=NNP>Tex</W><W C='*'>.</W> 
			<W A=Y C=JJP>American</W>... 
			Figure 1 
			Example of the new tokenization and markup generated from the Penn Treebank format. 
			Tokens are represented as XML elements W, where the attribute C holds POS information. 
			Proper names are tagged as NNP, NNPS and JJP. Periods are tagged as . (fullstop), A (part of 
			abbreviation), * (a fullstop and part of abbreviation at the same time). Ambiguously 
			capitalized words are marked with A = Y. 
			that follows a period was a proper name, we could apply a very simple set of rules 
			to disambiguate sentence boundaries: 
			* If a period follows a nonabbreviation, it is sentence terminal (.). 
			* If a period follows an abbreviation and is the last token in a text passage 
			(paragraph, document, etc.), it is sentence terminal and part of the 
			abbreviation (*). 
			* If a period follows an abbreviation and is not followed by a capitalized 
			word, it is part of the abbreviation and is not sentence terminal (A). 
			* If a period follows an abbreviation and is followed by a capitalized word 
			that is not a proper name, it is sentence terminal and part of the 
			abbreviation (*). 
			It is a trivial matter to extend these rules to allow for brackets and quotation marks 
			between the period and the following word. To handle other sentence termination 
			punctuation such as question and exclamation marks and semicolons, this rule set 
			also needs to include corresponding rules. The entire rule set for sentence boundary 
			disambiguation that was used in our experiments is listed in Appendix A. 
			3.1 Ideal Case: Upper Bound for Our SBD Approach 
			The estimates from the Brown corpus and the WSJ corpus (section 3) show that the 
			application of the SBD rule set described above together with the information on 
			abbreviations and proper names marked up in the corpora produces very accurate 
			results (error rate less than 0.0001%), but it leaves unassigned the outcome of the case 
			in which an abbreviation is followed by a proper name. This is a truly ambiguous case, 
			and to deal with this situation in general, one should encode detailed information 
			about the words participating in such contexts. For instance, honorific abbreviations 
			such as Mr. or Dr. when followed by a proper name almost certainly do not end a 
			sentence, whereas the abbreviations of U.S. states such as Mo., Cal., and Ore., when 
			followed directly by a proper name, most likely end a sentence. Obviously encoding 
			this kind of information into the system requires detailed analysis of the domain lexica, 
			is not robust to unseen abbreviations, and is labor intensive. 
			To make our method robust to unseen words, we opted for a crude but simple 
			solution. If such ambiguous cases are always resolved as "not sentence boundary" (A), 
			this produces, by our measure, an error rate of less than 3%. Estimates from the Brown 
			294 
			Computational Linguistics Volume 28, Number 3 
			Table 1 
			Estimates of the upper and lower bound error rates on the SBD task for our method. Three 
			estimated categories are sentence boundaries, ambiguously capitalized words, and 
			abbreviations. 
			Brown Corpus WSJ Corpus 
			SBD Amb. Cap. Abbreviations SBD Amb. Cap. Abbreviations 
			Number of 59,539 58,957 4,657 53,043 54,537 16,317 
			resolved instances 
			A Upper Bound: 0.01% 0.0% 0.0% 0.13% 0.0% 0.0% 
			All correct proper 
			names 
			All correct abbrs. 
			B Lower Bound: 2.00% 7.4% 10.8% 4.10% 15.0% 9.6% 
			Lookup proper 
			names 
			Guessed abbrs. 
			C Lookup proper 1.20% 7.4% 0.0% 2.34% 15.0% 0.0% 
			names 
			All correct abbrs. 
			D All correct proper 0.45% 0.0% 10.8% 1.96% 0.0% 9.6% 
			names 
			Guessed abbrs. 
			corpus and the WSJ corpus showed that such ambiguous cases constitute only 5-7% 
			of all potential sentence boundaries. This translates into a relatively small impact of 
			the crude strategy on the overall error rate on sentence boundaries. This impact was 
			measured at 0.01% on the Brown corpus and at 0.13% on the WSJ corpus, as presented 
			in row A of Table 1. Although this overly simplistic strategy extracts a small penalty 
			from the performance, we decided to use it because it is very general and independent 
			of domain-specific knowledge. 
			The SBD handling strategy described above is simple, robust, and well perform- 
			ing, but it relies on the assumption that we have entirely correct information about 
			abbreviations and proper names, as can be seen in row A of the table. The main dif- 
			ficulty is that when dealing with real-world texts, we have to identify abbreviations 
			and proper names ourselves. Thus estimates based on the application of our method 
			when using 100% correctly disambiguated capitalized words and abbreviations can be 
			considered as the upper bound for the SBD approach, that is, the top performance we 
			can achieve. 
			3.2 Worst Case: Lower Bound for Our SBD Approach 
			We can also estimate the lower bound for this approach applying very simple strategies 
			to the identification of proper names and abbreviations. 
			The simplest strategy for deciding whether or not a capitalized word in an ambigu- 
			ous position is a proper name is to apply a lexical-lookup strategy (possibly enhanced 
			with a morphological word guesser, e.g., Mikheev [1997]). Using this strategy, words 
			not listed as known common words for a language are usually marked as proper 
			names. The application of this strategy produced a 7.4% error rate on the Brown 
			corpus and a 15% error rate on the WSJ corpus. The difference in error rates can be 
			explained by the observation that the WSJ corpus contains a higher percentage of orga- 
			nization names and person names, which often coincide with common English words, 
			295 
			Mikheev Periods, Capitalized Words, etc. 
			and it contains more words in titles with all important words capitalized, which we 
			also consider as ambiguously capitalized. 
			The simplest strategy for deciding whether a word that is followed by a period 
			is an abbreviation or a regular word is to apply well-known heuristics based on the 
			observation that single-word abbreviations are short and normally do not include 
			vowels (Mr., Dr., kg.). Thus a word without vowels can be guessed to be an abbreviation 
			unless it is written in all capital letters and can stand for an acronym or a proper name 
			(e.g., BBC). A span of single letters separated by periods forms an abbreviation too 
			(e.g., Y.M.C.A.). A single letter followed by a period is also a very likely abbreviation. 
			There is also an additional heuristic that classifies as abbreviations short words (with 
			length less than five characters) that are followed by a period and then by a comma, a 
			lower-cased word, or a number. All other words are considered to be nonabbreviations. 
			These heuristics are reasonably accurate. On the WSJ corpus they misrecognized 
			as abbreviations only 0.2% of tokens. On the Brown corpus the misrecognition rate was 
			significantly higher: 1.6%. The major source for these errors were single letters that 
			stand for mathematical symbols in the scientific subcorpora of the Brown Corpus (e.g., 
			point T or triangle F). The major shortcoming of these abbreviation-guessing heuristics, 
			however, comes from the fact that they failed to identify about 9.5% of abbreviations. 
			This brings the overall error rate of the abbreviation-guessing heuristics to about 10%. 
			Combining the information produced by the lexical-lookup approach to proper 
			name identification with the abbreviation-guessing heuristics feeding the SBD rule set 
			gave us a 2.0% error rate on the Brown corpus and 4.1% on the WSJ corpus on the 
			SBD task. This can be interpreted as the lower bound to our SBD approach. Here we 
			see how errors in the identification of proper names and abbreviations propagated 
			themselves into errors on sentence boundaries. Row B of Table 1 displays a summary 
			for the lower-bound results. 
			3.3 Major Findings 
			We also measured the importance of each of the two knowledge sources (abbreviations 
			and proper names) separately. First, we applied the SBD rule set when all abbreviations 
			were correctly identified (using the information presented in the corpus) but applying 
			the lexical lookup strategy to proper-name identification (row C of Table 1). Then, we 
			applied the SBD rule set when all proper names were correctly identified (using the 
			information presented in the corpus) but applying the guessing heuristics to handle 
			abbreviations (row D of the table). In general, when a knowledge source returned 
			100% accurate information this significantly improved performance on the SBD task 
			measured against the lower-bound error rate. We also see that proper names have a 
			higher impact on the SBD task than abbreviations. 
			Since the upper bound of our SBD approach is high and the lower bound is far 
			from being acceptable, our main strategy for sentence boundary disambiguation will be to 
			invest in the disambiguation of capitalized words and abbreviations that then feed our SBD 
			rule set. 
			4. Document-Centered Approach to Proper Name and Abbreviation Handling 
			As we discussed above, virtually any common word can potentially act as a proper 
			name or part of a multiword proper name. The same applies to abbreviations: there is 
			no fixed list of abbreviations, and almost any short word can be used as an abbrevia- 
			tion. Fortunately, there is a mitigating factor too: important words are typically used 
			in a document more than once and in different contexts. Some of these contexts create 
			296 
			Computational Linguistics Volume 28, Number 3 
			ambiguity, but some do not. Furthermore, ambiguous words and phrases are usually 
			unambiguously introduced at least once in the text unless they are part of common 
			knowledge presupposed to be possessed by the readers. 
			This observation can be applied to a broader class of tasks. For example, people 
			are often referred to by their surnames (e.g., Black) but are usually introduced at least 
			once in the text either with their first name (John Black) or with their title/profession 
			affiliation (Mr. Black, President Bush), and it is only when their names are common 
			knowledge that they do not need an introduction (e.g., Castro, Gorbachev). Thus our 
			suggestion is to look at the unambiguous usages of the words in question in the entire document. 
			In the case of proper name identification, we are not concerned with the semantic 
			class of a name (e.g., whether it is a person's name or a location), but rather we simply 
			want to distinguish whether a capitalized word in a particular occurrence acts as a 
			proper name (or part of a multiword proper name). If we restrict our scope to a single 
			sentence, we might find that there is just not enough information to make a reliable 
			decision. For instance, Riders in the sentence Riders rode all over the green is equally likely 
			to be a proper noun, a plural proper noun, or a plural common noun. But if in the 
			same text we find John Riders, this sharply increases the likelihood that the proper noun 
			interpretation is the correct one, and conversely if we find many riders, this suggests 
			the plural-noun interpretation. 
			The above reasoning can be summarized as follows: if we detect that a word is 
			used capitalized in an unambiguous context, this increases the chances that this word 
			acts as a proper name in ambiguous positions in the same document. And conversely 
			if a word is seen only lower-cased, this increases the chances that it should be treated 
			as a common word even when used capitalized in ambiguous positions in the same 
			document. (This, of course, is only a general principle and will be further elaborated 
			elsewhere in the article.) 
			The same logic applies to abbreviations. Although a short word followed by a 
			period is a potential abbreviation, the same word occurring in the same document 
			in a different context can be unambiguously classified as a regular word if it is used 
			without a trailing period, or it can be unambiguously classified as an abbreviation if 
			it is used with a trailing period and is followed by a lower-cased word or a comma. 
			This information gives us a better chance of assigning these potential abbreviations 
			correctly in nonobvious contexts. 
			We call such style of processing a document-centered approach (DCA), since in- 
			formation for the disambiguation of an individual word token is derived from the 
			entire document rather than from its immediate local context. Essentially the system 
			collects suggestive instances of usage for target words from each document under 
			processing and applies this information on the fly to the processing of the document, 
			in a manner similar to instance-based learning. This differentiates DCA from the tradi- 
			tional corpus-based approach, in which learning is applied prior to processing, which 
			is usually performed with supervision over multiple documents of the training corpus. 
			5. Building Support Resources 
			Our method requires only four word lists. Each list is a collection of words that belong 
			to a single type, but at the same time, a word can belong to multiple lists. Since we 
			have four lists, we have four types: 
			* common word (as opposed to proper name) 
			* common word that is a frequent sentence starter 
			297 
			Mikheev Periods, Capitalized Words, etc. 
			* frequent proper name 
			* abbreviation (as opposed to regular word) 
			These four lists can be acquired completely automatically from raw (unlabeled) texts. 
			For the development of these lists we used a collection of texts of about 300,000 words 
			derived from the New York Times (NYT) corpus that was supplied as training data for 
			the 7th Message Understanding Conference (MUC-7) (Chinchor 1998). We used these 
			texts because the approach described in this article was initially designed to be part 
			of a named-entity recognition system (Mikheev, Grover, and Moens 1998) developed 
			for MUC-7. Although the corpus size of 300,000 words can be seen as large, the fact 
			that this corpus does not have to be annotated in any way and that a corpus of similar 
			size can be easily collected from on-line sources (including the Internet) makes this 
			resource cheap to obtain. 
			The first list on which our method relies is a list of common words. This list 
			includes common words for a given language, but no supplementary information such 
			as POS or morphological information is required to be present in this list. A variety 
			of such lists for many languages are already available (e.g., Burnage 1990). Words in 
			such lists are usually supplemented with morphological and POS information (which 
			is not required by our method). We do not have to rely on pre-existing resources, 
			however. A list of common words can be easily obtained automatically from a raw 
			(unannotated in any way) text collection by simply collecting and counting lower- 
			cased words in it. We generated such list from the NYT collection. To account for 
			potential spelling and capitalization errors, we included in the list of common words 
			only words that occurred lower-cased at least three times in the NYT texts. The list 
			of common words that we developed from the NYT collection contained about 15,000 
			English words. 
			The second list on which our method relies is a frequent-starters list, a list of 
			common words most frequently used in sentence-starting positions. This list can also 
			be obtained completely automatically from an unannotated corpus by applying the 
			lexical-lookup strategy. As discussed in Section 3.2, this strategy performs with a 
			7-15% error rate. We applied the list of common words over the NYT text collec- 
			tion to tag capitalized words in sentence-starting positions as common words and as 
			proper names: if a capitalized word was found in the list of common words, it was 
			tagged as a common word: otherwise it was tagged as a proper name. Of course, such 
			tagging was far from perfect, but it was good enough for our purposes. We included in 
			the frequent-starters list only the 200 most frequent sentence-starting common words. 
			This was more than a safe threshold to ensure that no wrongly tagged words were 
			added to this list. As one might predict, the most frequent sentence-starting common 
			word was The. This list also included some adverbs, such as However, Suddenly, and 
			Once; some prepositions, such as In, To, and By; and even a few verbs: Let, Have, Do, etc. 
			The third list on which our method relies is a list of single-word proper names that 
			coincide with common words. For instance, the word Japan is much more likely to be 
			used as a proper name (name of a country) rather than a verb, and therefore it needs to 
			be included in this list. We included in the proper name list 200 words that were most 
			frequently seen in the NYT text collection as single capitalized words in unambiguous 
			positions and that at the same time were present in the list of common words. For 
			instance, the word The can frequently be seen capitalized in unambiguous positions, 
			but it is always followed by another capitalized word, so we do not count it as a 
			candidate. On the other hand the word China is often seen capitalized in unambiguous 
			positions where it is not preceded or followed by other capitalized words. Since china 
			298 
			Computational Linguistics Volume 28, Number 3 
			Table 2 
			Error rates for different combinations of the abbreviation identification methods, including 
			combinations of guessing heuristics (GH), lexical lookup (LL), and the document-centered 
			approach (DCA). 
			Abbreviation Identification Method WSJ Brown 
			A GH 9.6% 10.8% 
			B LL 12.6% 11.9% 
			C GH + LL 1.2% 2.1% 
			D GH + DCA 6.6% 8.9% 
			E GH + DCA + LL 0.8% 1.2% 
			is also listed among common words and is much less frequently used in this way, we 
			include it in the proper name list. 
			The fourth list on which our method relies is a list of known abbreviations. 
			Again, we induced this list completely automatically from an unannotated corpus. 
			We applied the abbreviation-guessing heuristics described in Section 6 to our NYT 
			text collection and then extracted the 270 most frequent abbreviations: all abbrevi- 
			ations that appeared five times or more. This list included honorific abbreviations 
			(Mr, Dr), corporate designators (Ltd, Co), month name abbreviations (Jan, Feb), ab- 
			breviations of names of U.S. states (Ala, Cal), measure unit abbreviations (ft, kg), etc. 
			Although we described these abbreviations in groups, this information was not en- 
			coded in the list; the only information this list provides is that a word is a known 
			abbreviation. 
			Among these four lists the first three reflect general language regularities and 
			usually do not require modification for handling texts from a new domain. The ab- 
			breviation list, however, is much more domain dependent and for better performance 
			needs to be reinduced for a new domain. Since the compilation of all four lists does not 
			require data preannotated in any way, it is very easy to specialize the above-described 
			lists to a particular domain: we can simply rebuild the lists using a domain-specific 
			corpus. This process is completely automatic and does not require any human labor 
			apart from collecting a raw domain-specific corpus. Since all cutoff thresholds that 
			we applied here were chosen by intuition, however, different domains might require 
			some new settings. 
			6. Recognizing Abbreviations 
			The answer to the question of whether or not a particular word token is an abbreviation 
			or a regular word largely solves the sentence boundary problem. In the Brown corpus 
			92% of potential sentence boundaries come after regular words. The WSJ corpus is 
			richer with abbreviations, and only 83% of sentences in that corpus end with a regular 
			word followed by a period. In Section 3 we described the heuristics for abbreviation 
			guessing and pointed out that although these heuristics are reasonably accurate, they 
			fail to identify about 9.5% of abbreviations. Since unidentified abbreviations are then 
			treated as regular words, the overall error rate of the guessing heuristics was measured 
			at about 10% (row A of Table 2). Thus, to improve this error rate, we need first of all 
			to improve the coverage of the abbreviation-handling strategy. 
			A standard way to do this is to use the guessing heuristics in conjunction with a 
			list of known abbreviations. We decided to use the list of 270 abbreviations described 
			in Section 5. First we applied only the lexical-lookup strategy to our two corpora (i.e., 
			299 
			Mikheev Periods, Capitalized Words, etc. 
			only when a token was found in the list of 270 known abbreviations was it marked 
			as an abbreviation). This gave us an unexpectedly high error rate of about 12%, as 
			displayed in row B of Table 2. When we investigated the reason for the high error 
			rate, we found that the majority of single letters and spans of single letters sepa- 
			rated by periods (e.g. Y.M.C.A.) found in the Brown corpus and the WSJ corpus were 
			not present in our abbreviation list and therefore were not recognized as abbrevia- 
			tions. 
			Such cases, however, are handled well by the abbreviation-guessing heuristics. 
			When we applied the abbreviation list together with the abbreviation-guessing heuris- 
			tics (row C of Table 2), this gave us a very strong performance on the WSJ corpus: 
			an error rate of 1.2%. On the Brown corpus, the error rate was higher: 2.1%. This can 
			be explained by the fact that we collected our abbreviation list from a corpus of news 
			articles that is not too dissimilar to the texts in the WSJ corpus and thus, this list con- 
			tained many abbreviations found in that corpus. The Brown corpus, in contrast, ranges 
			across several different domains and sublanguages, which makes it more difficult to 
			compile a list from a single corpus to cover it. 
			6.1 Unigram DCA 
			The abbreviation-guessing heuristics supplemented with a list of abbreviations are 
			accurate, but they still can miss some abbreviations. For instance, if an abbreviation 
			like sec or Okla. is followed by a capitalized word and is not listed in the list of 
			abbreviations, the guessing heuristics will not uncover them. We also would like to 
			boost the abbreviation handling with a domain-independent method that enables the 
			system to function even when the abbreviation list is not much of a help. Thus, in 
			addition to the list of known abbreviations and the guessing heuristics, we decided to 
			apply the DCA as described below. 
			Each word of length four characters or less that is followed by a period is treated as 
			a potential abbreviation. First, the system collects unigrams of potential abbreviations 
			in unambiguous contexts from the document under processing. If a potential abbre- 
			viation is used elsewhere in the document without a trailing period, we can conclude 
			that it in fact is not an abbreviation but rather a regular word (nonabbreviation). To 
			decide whether a potential abbreviation is really an abbreviation, we look for contexts 
			in which it is followed by a period and then by a lower-cased word, a number, or a 
			comma. 
			For instance, the word Kong followed by a period and then by a capitalized word 
			cannot be safely classified as a regular word (nonabbreviation), and therefore it is a 
			potential abbreviation. But if in the same document we detect a context lived in Hong 
			Kong in 1993, this indicates that Kong in this document is normally written without 
			a trailing period and hence is not an abbreviation. Having established that, we can 
			apply this information to nonevident contexts and classify Kong as a regular word 
			throughout the document. However, if we detect a context such as Kong., said, this 
			indicates that in this document, Kong is normally written with a trailing period and 
			hence is an abbreviation. This gives us grounds for classifying Kong as an abbreviation 
			in all its occurrences within the same document. 
			6.2 Bigram DCA 
			The DCA relies on the assumption that there is a consistency in writing within the 
			same document. Different authors can write Mr or Dr with or without a trailing period, 
			but we assume that the same author (the author of a particular document) writes 
			it in the same way consistently. A situation can arise, however, in which the same 
			potential abbreviation is used as a regular word and as an abbreviation within the same 
			300 
			Computational Linguistics Volume 28, Number 3 
			document. This is usually the case when an abbreviation coincides with a regular word, 
			for example Sun. (meaning Sunday) and Sun (the name of a newspaper). To tackle 
			this problem, the system can collect from a document not only unigrams of potential 
			abbreviations in unambiguous contexts but also their bigrams with the preceding 
			word. Of course, as in the case with unigrams, the bigrams are collected on the fly 
			and completely automatically. 
			For instance, if the system finds a context vitamin C is, it stores the bigram vita- 
			min C and the unigram C with the information that C is a regular word. If in the 
			same document the system also detects a context John C. later said, it stores the bi- 
			gram John C and the unigram C with the information that C is an abbreviation. Here 
			we have conflicting information for the word C: it was detected to act as a regu- 
			lar word and as an abbreviation within the same document, so there is not enough 
			information to resolve ambiguous cases purely using the unigram. Some cases, how- 
			ever, can still be resolved on the basis of the bigrams. The system will assign C as a 
			regular word (nonabbreviation) in an ambiguous context such as vitamin C. Research 
			because of the stored vitamin C bigram. Obviously from such a short context, it is 
			difficult even for a human to make a confident decision, but the evidence gathered 
			from the entire document can influence this decision with a high degree of confi- 
			dence. 
			6.3 Resulting Approach 
			When neither unigrams nor bigrams can help to resolve an ambiguous context for a 
			potential abbreviation, the system decides in favor of the more frequent category for 
			that abbreviation. If the word In was detected to act as a regular word (preposition) five 
			times in the current document and two times as abbreviation (for the state Indiana), in 
			a context in which neither of the bigrams collected from the document can be applied, 
			In is assigned as a regular word (nonabbreviation). The last-resort strategy is to assign 
			all nonresolved cases as nonabbreviations. 
			Row D of Table 2 shows the results when we applied the abbreviation-guessing 
			heuristics together with the DCA. On the WSJ corpus, the DCA reduced the error rate 
			of the guessing heuristics alone (row A) by about 30%; on the Brown corpus its impact 
			was somewhat smaller, about 18%. This can be explained by the fact that abbreviations 
			in the WSJ corpus have a much higher repetition rate, which is very important for 
			the DCA. 
			We also applied the DCA together with the lexical lookup and the guessing heuris- 
			tics. This reduced the error rate on abbreviation identification by about 30% in com- 
			parison with the list and guessing heuristics configuration, as can be seen in row E of 
			Table 2. 
			7. Disambiguating Capitalized Words 
			The second key task of our approach is the disambiguation of capitalized words that 
			follow a potential sentence boundary punctuation sign. Apart from being an important 
			component in the task of text normalization, information about whether or not a 
			capitalized word that follows a period is a common word is crucial for the SBD task, 
			as we showed in Section 3. We tackle capitalized words in a similar fashion as we 
			tackled the abbreviations: through a document-centered approach that analyzes on 
			the fly the distribution of ambiguously capitalized words in the entire document. This 
			is implemented as a cascade of simple strategies, which were briefly described in 
			Mikheev (1999). 
			301 
			Mikheev Periods, Capitalized Words, etc. 
			7.1 The Sequence Strategy 
			The first DCA strategy for the disambiguation of ambiguous capitalized words is to 
			explore sequences of words extracted from contexts in which the same words are used 
			unambiguously with respect to their capitalization. We call this the sequence strategy. 
			The rationale behind this strategy is that if there is a phrase of two or more capitalized 
			words starting from an unambiguous position (e.g., following a lower-cased word), 
			the system can be reasonably confident that even when the same phrase starts from an 
			unreliable position (e.g., after a period), all its words still have to be grouped together 
			and hence are proper nouns. Moreover, this applies not just to the exact replication of 
			the capitalized phrase, but to any partial ordering of its words of size two characters 
			or more preserving their sequence. 
			For instance, if a phrase Rocket Systems Development Co. is found in a document 
			starting from an unambiguous position (e.g., after a lower-cased word, a number, or a 
			comma), the system collects it and also generates its partial-order subphrases: Rocket 
			Systems, Rocket Systems Co., Rocket Co., Systems Development, etc. If then in the same 
			document Rocket Systems is found in an ambiguous position (e.g., after a period), the 
			system will assign the word Rocket as a proper noun because it is part of a multiword 
			proper name that was seen in the unambiguous context. 
			A span of capitalized words can also internally include alpha-numerals, abbrevia- 
			tions with internal periods, symbols, and lower-cased words of length three characters 
			or shorter. This enables the system to capture phrases like A & M and The Phantom of 
			the Opera. Partial orders from such phrases are generated in a similar way, but with 
			the restriction that every generated subphrase should start and end with a capitalized 
			word. 
			The sequence strategy can also be applied to disambiguate common words. Since 
			in the case of common words the system cannot determine boundaries of a phrase, 
			only bigrams of the lower-cased words with their following words are collected from 
			the document. For instance, from a context continental suppliers of Mercury, the sys- 
			tem collects three bigrams: continental suppliers, suppliers of, and of Mercury. When the 
			system encounters the phrase Continental suppliers after a period, it can now use the 
			information that in the previously stored bigram continental suppliers, the word token 
			continental was written lower-cased and therefore was unambiguously used as a com- 
			mon word. On this basis the system can assign the ambiguous capitalized word token 
			Continental as a common word. 
			Row A of Table 3 displays the results obtained in the application of the sequence 
			strategy to the Brown corpus and the WSJ corpus. The sequence strategy is extremely 
			useful when we are dealing with names of organizations, since many of them are 
			multiword phrases composed of common words. For instance, the words Rocket and 
			Insurance can be used both as proper names and common words within the same 
			document. The sequence strategy maintains contexts of the usages of such words 
			within the same document, and thus it can disambiguate such usages in the ambiguous 
			positions matching surrounding words. And indeed, the error rate of this strategy 
			when applied to proper names was below 1%, with coverage of about 9-12%. 
			For tagging common words the sequence strategy was also very accurate (error 
			rate less than 0.3%), covering 17% of ambiguous capitalized common words on the 
			WSJ corpus and 25% on the Brown corpus. The higher coverage on the Brown corpus 
			can be explained by the fact that the documents in that corpus are in general longer 
			than those in the WSJ corpus, which enables more word bigrams to be collected from 
			a document. 
			Dual application of the sequence strategy contributes to its robustness against po- 
			tential capitalization errors in the document. The negative evidence (not proper name) 
			302 
			Computational Linguistics Volume 28, Number 3 
			Table 3 
			First part: Error rates of different individual strategies for capitalized-word disambiguation. 
			Second part: Error rates of the overall cascading application of the individual strategies. 
			Strategy Word Class Error Rate Coverage 
			WSJ Brown WSJ Brown 
			A Sequence strategy Proper Names 0.12% 0.97% 12.6% 8.82% 
			Sequence strategy Common Words 0.28% 0.21% 17.68% 26.5% 
			B Frequent-list lookup strategy Proper Names 0.49% 0.16% 2.62% 6.54% 
			Frequent-list lookup strategy Common Words 0.21% 0.14% 64.62% 61.20% 
			C Single-word assignment strategy Proper Names 3.18% 1.96% 18.77% 34.13% 
			Single-word assignment strategy Common Words 6.51% 2.87% 3.07% 4.78% 
			D Cascading DCA Proper/Common 1.10% 0.76% 84.12% 91.76% 
			E Cascading DCA Proper/Common 4.88% 2.83% 100.0% 100.0% 
			and lexical lookup 
			is used together with the positive evidence (proper name) and blocks assignment when 
			conflicts are found. For instance, if the system detects a capitalized phrase The President 
			in an unambiguous position, then the sequence strategy will treat the word the as part 
			of the proper name The President even when this phrase follows a period. If in the 
			same document, however, the system detects alternative evidence (e.g., the President, 
			where the is not part of the proper name), it then will block as unsafe the assignment 
			of The as a proper name in ambiguous usages of The President. 
			7.2 Frequent-List Lookup Strategy 
			The frequent-list lookup strategy applies lookup of ambiguously capitalized words 
			in two word lists. The first list contains common words that are frequently found 
			in sentence-starting positions, and the other list contains the most frequent proper 
			names. Both these lists can be compiled completely automatically, as explained in 
			section 5. Thus, if an ambiguous capitalized word is found in the list of frequent 
			sentence-starting common words, it is assigned as a common word, and if it is found 
			in the list of frequent proper names, it is assigned as a proper name. For instance, 
			the word token The when used after a period will be recognized as a common word, 
			because The is a frequent sentence-starting common word. The Word token Japan in a 
			similar context will be recognized as a proper name, because Japan is a member of the 
			frequent-proper-name list. 
			Note, however, that this strategy is applied after the sequence strategy and thus, a 
			word listed in one of the lists will not necessarily be marked according to its list class. 
			The list lookup assignment is applied only to the ambiguously capitalized words that 
			have not been handled by the sequence strategy. 
			Row B of Table 3 displays the results of the application of the frequent-list lookup 
			strategy to the Brown corpus and the WSJ corpus. The frequent-list lookup strategy 
			produced an error rate of less than 0.5%. A few wrong assignments came from phrases 
			like Mr. A and Mrs. Someone and words in titles like I've Got a Dog, where A, Someone, 
			and I were recognized as common words although they were tagged as proper nouns 
			in the text. The frequent-list lookup strategy is not very effective for proper names, 
			where it covered under 7% of candidates in the Brown corpus and under 3% in the 
			WSJ corpus, but it is extremely effective for common words: it covered over 60% of 
			ambiguous capitalized common words. 
			303 
			Mikheev Periods, Capitalized Words, etc. 
			7.3 Single-Word Assignment 
			The sequence strategy is accurate, but it covers only 9-12% of proper names in ambigu- 
			ous positions. The frequent-list lookup strategy is mostly effective for common words. 
			To boost the coverage on the proper name category, we introduced another DCA 
			strategy. We call this strategy single-word assignment, and it can be summarized as 
			follows: if a word in the current document is seen capitalized in an unambiguous po- 
			sition and at the same time it is not used lower-cased anywhere in the document, this 
			word in this particular document is very likely to stand for a proper name even when 
			used capitalized in ambiguous positions. And conversely, if a word in the current 
			document is used only lower-cased (except in ambiguous positions), it is extremely 
			unlikely that this word will act as a proper name in an ambiguous position and thus, 
			such a word can be marked as a common word. 
			Note that by the time single-word assignment is implemented, the sequence strat- 
			egy and the frequent-list lookup strategy have been already applied and all high- 
			frequency sentence-initial words have been assigned. This ordering is important, be- 
			cause even if a high-frequency common word is observed in a document only as a 
			proper name (usually as part of a multiword proper name), it is still not safe to mark 
			it as a proper name in ambiguous positions. 
			Row C of Table 3 displays the results of the application of the single-word assign- 
			ment strategy to the Brown corpus and the WSJ corpus. The single-word assignment 
			strategy is useful for proper-name identification: although it is not as accurate as the 
			sequence strategy, it still produces a reasonable error rate at the same time boosting the 
			coverage considerably (19-34%). On common words this method is not as effective, 
			with an error rate as high as 6.61% on the WSJ corpus and a coverage below 5%. 
			The single-word-assignment strategy handles well the so-called unknown-word 
			problem, which arises when domain-specific lexica are missing from a general vocab- 
			ulary. Since our system is not equipped with a general vocabulary but rather builds a 
			document-specific vocabulary on the fly," important domain-specific words are iden- 
			tified and treated similarly to all other words. 
			A generally difficult case for the single-word assignment strategy arises when a 
			word is used both as a proper name and as a common word in the same document, es- 
			pecially when one of these usages occurs only in an ambiguous position. For instance, 
			in a document about steel, the only occurrence of Steel Company happened to start 
			a sentence. This produced an erroneous assignment of the word Steel as a common 
			word. Another example: in a document about the Acting Judge, the word acting in a 
			sentence Acting on behalf. . . was wrongly classified as a proper name. These difficulties, 
			however, often are compensated for by the sequence strategy, which is applied prior 
			to the single-word assignment strategy and tackles such cases using n-grams of words. 
			7.4 Quotes, Brackets, and "After Abbr." Heuristic 
			Capitalized words in quotes and brackets do not directly contribute to our primary 
			task of sentence boundary disambiguation, but they still present a case of ambiguity 
			for the task of capitalized-word disambiguation. To tackle them we applied two simple 
			heuristics: 
			* If a single capitalized word is used in quotes or brackets it is a proper 
			noun (e.g., John (Cool) Lee). 
			* If there is a lowercased word, a number, or a comma that is followed by 
			an opening bracket and then by a capitalized word, this capitalized word 
			is a proper noun (e.g., . . . happened (Moscow News reported yesterday) but. . . ). 
			304 
			Computational Linguistics Volume 28, Number 3 
			These heuristics are reasonably accurate: they achieved under 2% error rate on our 
			two test corpora, but they covered only about 6-7% of proper names. 
			When we studied the distribution of capitalized words after capitalized abbrevi- 
			ations, we uncovered an interesting empirical fact. A capitalized word that follows 
			a capitalized abbreviation is almost certainly a proper name unless it is listed in the 
			list of frequent sentence-starting common words (i.e., it is not The, However, etc.). The 
			error rate of this heuristic is about 0.8% and, not surprisingly, in 99.5% of cases the 
			abbreviation and the following proper name belonged to the same sentence. Naturally, 
			the coverage of this "after abbr." heuristic depends on the proportion of capitalized 
			abbreviations in the text. In our two corpora this heuristic disambiguated about 20% 
			of ambiguous capitalized proper names. 
			7.5 Tagging Proper Names: The Overall Performance 
			In general, the cascading application of the above-described strategies achieved an 
			error rate of about 1%, but it left unclassified about 9% of ambiguous capitalized 
			words in the Brown corpus and 15% of such words in the WSJ corpus. Row D of 
			Table 3 displays the results of the application of the cascading application of the 
			capitalized-word disambiguation strategies to the Brown corpus and the WSJ corpus. 
			For the proper-name category, the most productive strategy was single-word as- 
			signment, followed by the "after abbr." strategy, and then the sequence strategy. For 
			common words, the most productive was the frequent-list lookup strategy, followed 
			by the sequence strategy. 
			Since our system left unassigned 10-15% of ambiguous capitalized words, we have 
			to decide what to do with them. To keep our system simple and domain independent, 
			we opted for the lexical-lookup strategy that we evaluated in Section 3. This strategy, 
			of course, is not very accurate, but it is applied only to the unassigned words. Row E 
			of Table 3 displays the results of applying the lexical-lookup strategy after the DCA 
			methods. We see that the error rate went up in comparison to the DCA-only method 
			by more than three times (2.9% on the Brown corpus and 4.9% on the WSJ corpus), 
			but no unassigned ambiguous capitalized words are left in the text. 
			8. Putting It All Together: Assigning Sentence Boundaries 
			After abbreviations have been identified and capitalized words have been classified 
			into proper names and common words, the system can carry out the assignments 
			of sentence boundaries using the SBD rule set described in Section 3 and listed in 
			Appendix A. This rule set makes use of the observation that if we have at our dis- 
			posal unambiguous (but not necessarily correct) information as to whether a particular 
			word that precedes a period is an abbreviation and whether the word that follows 
			this period is a proper name, then in mixed-case texts we can easily assign a pe- 
			riod (and other potential sentence termination punctuation) as a sentence break or 
			not. 
			The only ambiguous outcome is generated by the configuration in which an ab- 
			breviation is followed by a proper name. We decided to handle this case by applying 
			a crude and simple strategy of always resolving it as "not sentence boundary." On one 
			hand, this makes our method simple and robust, but on the other hand, it imposes 
			some penalty on its performance. 
			Row A of Table 4 summarizes the upper bound for our SBD approach: when we 
			have entirely correct information on the abbreviations and proper names, as explained 
			in Section 3.1. There the erroneous assignments come only from the crude treatment 
			of abbreviations that are followed by proper names. 
			305 
			Mikheev Periods, Capitalized Words, etc. 
			Table 4 
			Error rates measured on the SBD, capitalized-word disambiguation, and abbreviation 
			identification tasks achieved by different methods described in this article. 
			Method Brown Corpus WSJ Corpus 
			SBD Capitalized Abbreviations SBD Capitalized Abbreviations 
			words words 
			A Upper bound 0.01% 0.0% 0.0% 0.13% 0.0% 0.0% 
			B Lower bound 2.00% 7.40% 10.8% 4.10% 15.0% 9.6% 
			C Best quoted 0.20% 3.15% -- 0.50% 4.72% -- 
			D DCA 0.28% 2.83% 0.8% 0.45% 4.88% 1.2% 
			E DCA (no abbreviations 0.65% 2.89% 8.9% 1.41% 4.92% 6.6% 
			lexicon) 
			F POS tagger 0.25% 3.15% 1.2% 0.39% 4.72% 2.1% 
			G POS tagger + DCA 0.20% 1.87% 0.8% 0.31% 3.22% 1.2% 
			Row B of Table 4 summarizes the lower-bound results. The lower bound for our 
			approach was estimated by applying the lexical-lookup strategy for capitalized-word 
			disambiguation together with the abbreviation-guessing heuristics to feed the SBD 
			rule set, as described in Section 3.2. Here we see a significant impact of the infelicities 
			in the disambiguation of capitalized words and abbreviations on the performance of 
			the SBD rule set. 
			Row C of Table 4 summarizes the highest results known to us (for all three tasks) 
			produced by automatic systems on the Brown corpus and the WSJ corpus. State-of-the- 
			art machine learning and rule-based SBD systems achieve an error rate of 0.8-1.5% 
			measured on the Brown corpus and the WSJ corpus. The best performance on the 
			WSJ corpus was achieved by a combination of the SATZ system (Palmer and Hearst 
			1997) with the Alembic system (Aberdeen et al. 1995): a 0.5% error rate. The best 
			performance on the Brown corpus, a 0.2% error rate, was reported by Riley (1989), who 
			trained a decision tree classifier on a 25-million-word corpus. In the disambiguation of 
			capitalized words, the most widespread method is POS tagging, which achieves about 
			a 3% error rate on the Brown corpus and a 5% error rate on the WSJ corpus, as reported 
			in Mikheev (2000). We are not aware of any studies devoted to the identification of 
			abbreviations with comprehensive evaluation on either the Brown corpus or the WSJ 
			corpus. 
			In row D of Table 4, we summarized our main results: the results obtained by the 
			application of our SBD rule set, which uses the information provided by the DCA to 
			capitalized word disambiguation applied together with lexical lookup (as described 
			in Section 7.5), and the abbreviation-handling strategy, which included the guessing 
			heuristics, the DCA, and the list of 270 abbreviations (as described in Section 6). As 
			can be seen in the table, the performance of this system is almost indistinguishable 
			from the best previously quoted results. On proper-name disambiguation, it achieved 
			a 2.83% error rate on the Brown corpus and a 4.88% error rate on the WSJ corpus. 
			On the SBD task, it achieved a 0.28% error rate on the Brown corpus and a 0.45% 
			error rate on the WSJ corpus. If we compare these results with the upper bound 
			for our SBD approach, we can see that the infelicities in proper-name and abbrevia- 
			tion identification introduced an increase of about 0.3% in the error rate on the SBD 
			task. 
			To test the adaptability of our approach to a completely new domain, we applied 
			our system in a configuration in which it was not equipped with the list of 270 abbre- 
			306 
			Computational Linguistics Volume 28, Number 3 
			viations, since this list is the only domain-sensitive resource in our system. The results 
			for this configuration are summarized in row E of Table 4. The error rate increase of 
			5-7% on the abbreviation handling introduced about a twofold increase in the SBD 
			error rate on the Brown corpus (a 0.65% error rate) and about a threefold increase on 
			the WSJ corpus (1.41%). But these results are still comparable to those of the majority 
			of currently used sentence splitters. 
			9. Detecting Limits for the DCA 
			Since our DCA method relies on the assumption that the words it tries to disam- 
			biguate occur multiple times in a document, its performance clearly should depend 
			on the length of the document: very short documents possibly do not provide enough 
			disambiguation clues, whereas very long documents possibly contain too many clues 
			that cancel each other. 
			As noted in Section 2.1, the average length of the documents in the Brown corpus 
			is about 2,300 words. Also, the documents in that corpus are distributed very densely 
			around their mean. Thus not much can be inferred about the dependency of the perfor- 
			mance of the method on document length apart from the observation that documents 
			2,000-3,000 words long are handled well by our approach. In the WSJ corpus, the aver- 
			age length of the document is about 500 words, and therefore we could investigate the 
			effect of short documents on the performance. We divided documents into six groups 
			according to their length and plotted the error rate for the SBD and capitalized-word 
			disambiguation tasks as well as the number of documents in a group, as shown in 
			Figure 2. As can be seen in the figure, short documents (50 words or less) have the 
			highest average error rate both for the SBD task (1.63) and for the capitalized-word 
			disambiguation task (5.25). For documents 50 to 100 words long, the error rate is still 
			a bit higher than normal, and for longer documents the error rate stabilizes around 
			1.5 for the capitalized-word disambiguation task and 0.3 for the SBD task. The error 
			rate on documents 2,000 words long and higher is almost identical to that registered 
			on the Brown corpus on documents of the same length. 
			Thus here we can conclude that the proposed approach tends not to be very 
			effective for documents shorter than 50 words (one to three sentences), but it handles 
			well documents up to 4,000 words long. Since our corpora did not contain documents 
			significantly longer than that, we could not estimate whether or when the performance 
			of our method significantly deteriorates on longer documents. We also evaluated the 
			performance of the method on different subcorpora of the Brown corpus: the most 
			difficult subdomains proved to be scientific texts, spoken-language transcripts, and 
			journalistic texts, whereas fiction was the easiest genre for the system. 
			10. Incorporating DCA into a POS Tagger 
			To test our hypothesis that DCA can be used as a complement to a local-context 
			approach, we combined our main configuration (evaluated in row D of Table 4) with 
			a POS tagger. Unlike other POS taggers, this POS tagger (Mikheev 2000) was also 
			trained to disambiguate sentence boundaries. 
			10.1 Training a POS Tagger 
			In our markup convention (Section 2), periods are tokenized as separate tokens re- 
			gardless of whether they stand for fullstops or belong to abbreviations. Consequently 
			a POS tagger can naturally treat them similarly to any other ambiguous words. There 
			is, however, one difference in the implementation of such a tagger. Normally, a POS 
			307 
			Mikheev Periods, Capitalized Words, etc. 
			x 
			x 
			x 
			x 
			x x 
			x 
			o 
			o 
			o 
			o o 
			o 
			o 
			c 
			c 
			c c 
			c 
			c 
			c 
			Number of Documents 
			Cap.Word error rate 
			SBD error rate 
			50 100 200 500 1000 2000 3000 
			1 
			2 
			3 
			200 
			600 
			Doc. Length 
			Error Rate 
			Number of Docs 
			400 
			Figure 2 
			Distribution of the error rate and the number of documents across the document length 
			(measured in word tokens) in the WSJ corpus. 
			tagger operates on text spans that form a sentence. This requires resolving sentence 
			boundaries before tagging. We see no good reason, however, why such text spans 
			should necessarily be sentences, since the majority of tagging paradigms (e.g., Hidden 
			Markov Model [HMM] [Kupiec 1992], Brill's [Brill 1995a], and MaxEnt [Ratnaparkhi 
			1996]) do not attempt to parse an entire sentence and operate only in the local win- 
			dow of two to three tokens. The only reason why taggers traditionally operate on 
			the sentence level is that a sentence naturally represents a text span in which POS 
			information does not depend on the previous and following history. 
			This issue can be also addressed by breaking the text into short text spans at 
			positions where the previous tagging history does not affect current decisions. For 
			instance, a bigram tagger operates within a window of two tokens, and thus a se- 
			quence of word tokens can be terminated at an unambiguous word token, since this 
			unambiguous word token will be the only history used in tagging of the next token. 
			At the same time since this token is unambiguous, it is not affected by the history. 
			A trigram tagger operates within a window of three tokens, and thus a sequence of 
			word tokens can be terminated when two unambiguous words follow each other. 
			Using Penn Treebank with our tokenization convention (Section 2), we trained a 
			trigram HMM POS tagger. Words were clustered into ambiguity classes (Kupiec 1992) 
			according to the sets of POS tags they can take on. The tagger predictions were based 
			on the ambiguity class of the current word, abbreviation/capitalization information, 
			308 
			Computational Linguistics Volume 28, Number 3 
			and trigrams of POS tags: 
			P(t1 . . . tn 
			O1 . . . On) = argmax 
			i=n 
			i=1 
			P(Oi 
			| ti)  P(ti 
			| ti-1 
			ti-2 
			ai-1) 
			where ti 
			is a disambiguated POS tag of the ith word, ai 
			is the abbreviation flag of 
			the ith word, and Oi 
			is the observation at the ith position, which in our case is 
			the ambiguity class the word belongs to, its capitalization, and its abbreviation flag 
			(AmbClassi 
			, ai 
			, Capi). Since the abbreviation flag of the previous word strongly influ- 
			ences period disambiguation, it was included in the standard trigram model. 
			We decided to train the tagger with the minimum of preannotated resources. First, 
			we used 20,000 tagged words to "bootstrap" the training process, because purely un- 
			supervised techniques, at least for the HMM class of taggers, yield lower precision. 
			We also used our DCA system to assign capitalized words, abbreviations, and sen- 
			tence breaks, retaining only cases assigned by the strategies with an accuracy not less 
			than 99.8%. This was done because purely unsupervised techniques (e.g., Baum-Welch 
			[Baum and Petrie 1966] or Brill's [Brill 1995b]) enable regularities to be induced for 
			word classes which contain many entries, exploiting the fact that individual words that 
			belong to a POS class occur in different ambiguity patterns. Counting all possible POS 
			combinations in these ambiguity patterns over multiple patterns usually produces the 
			right combinations as the most frequent. Periods as many other closed-class words 
			cannot be successfully covered by such technique. 
			After bootstrapping we applied the forward-backward (Baum-Welch) algorithm 
			(Baum and Petrie 1966) and trained our tagger in the unsupervised mode, that is, with- 
			out using the annotation available in the Brown corpus and the WSJ corpus. For 
			evaluation purposes we trained (and bootstrapped) our tagger on the Brown corpus 
			and applied it to the WSJ corpus and vice versa. We preferred this method to tenfold 
			cross-validation because it allowed us to produce only two tagging models instead of 
			twenty and also enabled us to test the tagger in harsher conditions, that is, when it is 
			applied to texts that are very distant from the ones on which it was trained. 
			The overall performance of the tagger was close to 96%, which is a bit lower 
			than the best quoted results. This can be accounted for by the fact that training and 
			evaluation were performed on two very different text corpora, as explained above. 
			The performance of the tagger on our target categories (periods and proper names) 
			was very close to that of the DCA method, as can be seen in row F of Table 4. 
			10.2 POS Tagger and the DCA 
			We felt that the DCA method could be used as a complement to the POS tagger, since 
			these techniques employ different types of information: in-document distribution and 
			local context. Thus, a hybrid system can deliver at least two advantages. First, 10-15% 
			of the ambiguous capitalized words unassigned by the DCA can be assigned using 
			a standard POS-tagging method based on the local syntactic context rather than the 
			inaccurate lexical-lookup approach. Second, the local context can correct some of the 
			errors made by the DCA. 
			To implement this hybrid approach we incorporated the DCA system into the 
			POS tagger. We modified the tagger model by incorporating the DCA predictions 
			using linear interpolation: 
			P(combined) =   P(tagger) + (1 - )  P(DCA Strategy) 
			where P(DCA Strategy) is the accuracy of a specific DCA strategy and P(tagger) is the 
			probability assigned by the tagger's model. Although it was possible to estimate an 
			309 
			Mikheev Periods, Capitalized Words, etc. 
			optimal value for  from the tagged corpus, we decided simply to set it to be 0.5 (i.e., 
			giving similar weight to both sources of information). Instead of using the SBD rule 
			set described in Section 3, in this configuration, period assignments were handled by 
			the tagger's model. 
			Row G of Table 4 displays the results of the application of the hybrid system. We 
			see an improvement on proper-name recognition in comparison to the DCA or POS- 
			tagging approaches (rows D and F) by about a 30-40% cut in the error rate: an overall 
			error rate of 1.87% on the Brown corpus and of 3.22% on the WSJ corpus. In turn this 
			enabled better tagging of sentence boundaries: a 0.20% error rate on the Brown corpus 
			and a 0.31% error rate on the WSJ corpus, which corresponds to about a 20% cut in 
			the error rate in comparison to the DCA or the POS-tagging approaches alone. 
			Thus, although for applications that rely on POS tagging it probably makes more 
			sense to have a single system that assigns both POS tags and sentence boundaries, 
			there is still a clear benefit in using the DCA method because 
			* the DCA method incorporated into the POS tagger significantly reduced 
			the error rate on the target categories (periods and proper names). 
			* the DCA method is domain independent, whereas taggers usually need 
			to be trained for each specific domain to obtain best results. 
			* the DCA system was used in resource preparation for training the tagger. 
			* the DCA system is significantly faster than the tagger, does not require 
			resource development, and for tasks that do not require full POS 
			information, it is a preferable solution. 
			So in general, the DCA method can be seen as an enhancer for a POS tagger and 
			also as a lightweight alternative to such a tagger when full POS information is not 
			required. 
			11. Further Experiments 
			11.1 The Cache Extension 
			One of the features of the method advocated in this article is that the system collects 
			suggestive instances of usage for target words from each document, then applies this 
			information during the second pass through the document (actual processing), and 
			then "forgets" what it has learned before handling another document. The main rea- 
			son for not carrying over the information that has been inferred from one document 
			to process another document is that in general we do not know whether this new 
			document comes from the same corpus as the first document, and thus the regular- 
			ities that have been identified in the first document might not be useful, but rather 
			harmful, when applied to that new document. When we are dealing with documents 
			of reasonable length, this "forgetful" behavior does not matter much, because such 
			documents usually contain enough disambiguation clues. As we showed in Section 8, 
			however, when short documents of one to three sentences are being processed, quite 
			often there are not enough disambiguation clues within the document itself, which 
			leads to inferior performance. 
			To improve the performance on short documents, we introduced a special caching 
			module that propagates some information identified in previously processed docu- 
			ments to the processing of a new one. To propagate features of individual words from 
			one document to processing another one is a risky strategy, since words are very 
			310 
			Computational Linguistics Volume 28, Number 3 
			ambiguous. Word sequences, however, are much more stable and can be propagated 
			across documents. We decided to accumulate in our cache all multiword proper names 
			and lower-cased word bigrams induced by the sequence strategy (Section 7.1). These 
			word sequences are used by the sequence strategy exactly as are word sequences in- 
			duced on the fly, and then the induced on-the-fly sequences are added to the cache. 
			We also add to the cache the bigrams of abbreviations and regular words induced by 
			the abbreviation-handling module, as explained in Section 6. These bigrams are used 
			together with the bigrams induced on the fly. This strategy proved to be quite useful: 
			it covered another 2% of unresolved cases (before applying the lexical lookup), with 
			an error rate of less than 1%. 
			11.2 Handling Russian News 
			To test how easy it is to apply the DCA to a new language, we tested it on a corpus 
			of British Broadcasting Corporation (BBC) news in Russian. We collected this corpus 
			from the Internet http://news.bbc.co.uk/hi/russian/world/default.htm over a period of 30 
			days. This gave us a corpus of 300 short documents (one or two paragraphs each). 
			We automatically created the supporting resources from 364,000 documents from the 
			Russian corpus of the European Corpus Initiative, using the method described in 
			section 5. 
			Since, unlike English, Russian is a highly inflected language, we had to deal with 
			the case normalization issue. Before using the DCA method, we applied a Russian 
			morphological processor (Mikheev and Liubushkina 1995) to convert each word in 
			the text to its main form: nominative case singular for nouns and adjectives, infinitive 
			for verbs, etc. For words that could be normalized to several main forms (polysemy), 
			when secondary forms of different words coincided, we retained all the main forms. 
			Since the documents in the BBC news corpus were rather short, we applied the cache 
			module, as described in Section 11.1. This allowed us to reuse information across the 
			documents. 
			Russian proved to be a simpler case than English for our tasks. First, on average, 
			Russian words are longer than English words: thus the identification of abbreviations 
			is simpler. Second, proper names in Russian coincide less frequently with common 
			words; this makes the disambiguation of capitalized words in ambiguous positions 
			easier. The overall performance reached a 0.1% error rate on sentence boundaries and 
			a 1.8% error rate on ambiguous capitalized words, with the coverage on both tasks 
			at 100%. 
			12. Related Research 
			12.1 Research in Nonlocal Context 
			The use of nonlocal context and dynamic adaptation have been studied in language 
			modeling for speech recognition. Kuhn and de Mori (1998) proposed a cache model 
			that works as a kind of short-term memory by which the probability of the most re- 
			cent n words is increased over the probability of a general-purpose bigram or trigram 
			model. Within certain limits, such a model can adapt itself to changes in word frequen- 
			cies, depending on the topic of the text passage. The DCA system is similar in spirit 
			to such dynamic adaptation: it applies word n-grams collected on the fly from the 
			document under processing and favors them more highly than the default assignment 
			based on prebuilt lists. But unlike the cache model, it uses a multipass strategy. 
			Clarkson and Robinson (1997) developed a way of incorporating standard n-grams 
			into the cache model, using mixtures of language models and also exponentially de- 
			caying the weight for the cache prediction depending on the recency of the word's last 
			311 
			Mikheev Periods, Capitalized Words, etc. 
			occurrence. In our experiments we applied simple linear interpolation to incorporate 
			the DCA system into a POS tagger. Instead of decaying nonlocal information, we opted 
			for not propagating it from one document for processing of another. For handling very 
			long documents with our method, however, the information decay strategy seems to 
			be the right way to proceed. 
			Mani and MacMillan (1995) pointed out that little attention had been paid in the 
			named-entity recognition field to the discourse properties of proper names. They pro- 
			posed that proper names be viewed as linguistic expressions whose interpretation 
			often depends on the discourse context, advocating text-driven processing rather than 
			reliance on pre-existing lists. The DCA outlined in this article also uses nonlocal dis- 
			course context and does not heavily rely on pre-existing word lists. It has been applied 
			not only to the identification of proper names, as described in this article, but also to 
			their classification (Mikheev, Grover, and Moens 1998). 
			Gale, Church, and Yarowsky (1992) showed that words strongly tend to exhibit 
			only one sense in a document or discourse ("one sense per discourse"). Since then 
			this idea has been applied to several tasks, including word sense disambiguation 
			(Yarowsky 1995) and named-entity recognition (Cucerzan and Yarowsky 1999). Gale, 
			Church, and Yarowsky's observation is also used in our DCA, especially for the iden- 
			tification of abbreviations. In capitalized-word disambiguation, however, we use this 
			assumption with caution and first apply strategies that rely not just on single words 
			but on words together with their local contexts (n-grams). This is similar to "one sense 
			per collocation" idea of Yarowsky (1993). 
			The description of the EAGLE workbench for linguistic engineering (Baldwin et al. 
			1997) mentions a case normalization module that uses a heuristic in which a capitalized 
			word in an ambiguous position should be rewritten without capitalization if it is found 
			lower-cased in the same document. This heuristic also employs a database of bigrams 
			and unigrams of lower-cased and capitalized words found in unambiguous positions. 
			It is quite similar to our method for capitalized-word disambiguation. The description 
			of the EAGLE case normalization module provided by Baldwin et al. is, however, very 
			brief and provides no performance evaluation or other details. 
			12.2 Research in Text Preprocessing 
			12.2.1 Sentence Boundary Disambiguation. There exist two large classes of SBD sys- 
			tems: rule based and machine learning. The rule-based systems use manually built 
			rules that are usually encoded in terms of regular-expression grammars supplemented 
			with lists of abbreviations, common words, proper names, etc. To put together a few 
			rules is fast and easy, but to develop a rule-based system with good performance is 
			quite a labor-consuming enterprise. For instance, the Alembic workbench (Aberdeen et 
			al. 1995) contains a sentence-splitting module that employs over 100 regular-expression 
			rules written in Flex. Another well-acknowledged shortcoming of rule-based systems 
			is that such systems are usually closely tailored to a particular corpus or sublanguage 
			and are not easily portable across domains. 
			Automatically trainable software is generally seen as a way of producing sys- 
			tems that are quickly retrainable for a new corpus, for a new domain, or even for 
			another language. Thus, the second class of SBD systems employs machine learning 
			techniques such as decision tree classifiers (Riley 1989), neural networks (Palmer and 
			Hearst 1994), and maximum-entropy modeling (Reynar and Ratnaparkhi 1997). Ma- 
			chine learning systems treat the SBD task as a classification problem, using features 
			such as word spelling, capitalization, suffix, and word class found in the local con- 
			text of a potential sentence-terminating punctuation sign. Although training of such 
			312 
			Computational Linguistics Volume 28, Number 3 
			systems is completely automatic, the majority of machine learning approaches to the 
			SBD task require labeled examples for training. This implies an investment in the data 
			annotation phase. 
			The main difference between the existing machine learning and rule-based meth- 
			ods for the SBD task and our approach is that we decomposed the SBD task into 
			several subtasks. We decided to tackle the SBD task through the disambiguation of 
			the period preceding and following words and then feed this information into a very 
			simple SBD rule set. In contrast, the standard practice in building SBD software is to 
			disambiguate configurations of a period with its ambiguous local context in a single 
			step, either by encoding disambiguation clues into the rules or inferring a classifier 
			that accounts for the ambiguity of the words on the left and on the right of the period. 
			Our approach to SBD is closer in spirit to machine learning methods because its 
			retargeting does not require rule reengineering and can be done completely automat- 
			ically. Unlike traditional machine learning SBD approaches, however, our approach 
			does not require annotated data for training. 
			12.2.2 Disambiguation of Capitalized Words. Disambiguation of capitalized words 
			is usually handled by POS taggers, which treat capitalized words in the same way 
			as other categories, that is, by accounting for the immediate syntactic context and 
			using estimates collected from a training corpus. As Church (1988) rightly pointed 
			out, however, "Proper nouns and capitalized words are particularly problematic: some 
			capitalized words are proper nouns and some are not. Estimates from the Brown 
			Corpus can be misleading. For example, the capitalized word `Acts' is found twice in 
			the Brown Corpus, both times as a proper noun (in a title). It would be misleading to 
			infer from this evidence that the word `Acts' is always a proper noun." 
			In the information extraction field, the disambiguation of ambiguous capitalized 
			words has always been tightly linked to the classification of proper names into seman- 
			tic classes such as person name, location, and company name. Named-entity recogni- 
			tion systems usually use sets of complex hand-crafted rules that employ a gazetteer 
			and a local context (Krupa and Hausman 1998). In some systems such dependencies 
			are learned from labeled examples (Bikel et al. 1997). The advantage of the named- 
			entity approach is that the system not only identifies proper names but also determines 
			their semantic class. The disadvantage is in the cost of building a wide-coverage set of 
			contextual clues manually or producing annotated training data. Also, the contextual 
			clues are usually highly specific to the domain and text genre, making such systems 
			very difficult to port. 
			Both POS taggers and named-entity recognizers are normally built using the local- 
			context paradigm. In contrast, we opted for a method that relies on the entire distri- 
			bution of a word in a document. Although it is possible to train some classes of POS 
			taggers without supervision, this usually leads to suboptimal performance. Thus the 
			majority of taggers are trained using at least some labeled data. Named-entity recog- 
			nition systems are usually hand-crafted or trained from labeled data. As was shown 
			above, our method does not require supervised training. 
			12.2.3 Disambiguation of Abbreviations. Not much information has been published 
			on abbreviation identification. One of the better-known approaches is described in 
			Grefenstette and Tapanainen (1994), which suggested that abbreviations first be ex- 
			tracted from a corpus using abbreviation-guessing heuristics akin to those described 
			in Section 6 and then reused in further processing. This is similar to our treatment of 
			abbreviation handling, but our strategy is applied on the document rather than corpus 
			level. The main reason for restricting abbreviation discovery to a single document is 
			313 
			Mikheev Periods, Capitalized Words, etc. 
			that this does not presuppose the existence of a corpus in which the current document 
			is similar to other documents. 
			Park and Byrd (2001) recently described a hybrid method for finding abbrevia- 
			tions and their definitions. This method first applies an "abbreviation recognizer" that 
			generates a set of "candidate abbreviations" for a document. Then for this set of can- 
			didates the system tries to find in the text their definitions (e.g., United Kingdom for 
			UK). The abbreviation recognizer for these purposes is allowed to overgenerate signif- 
			icantly. There is no harm (apart from the performance issues) in proposing too many 
			candidate abbreviations, because only those that can be linked to their definitions will 
			be retained. Therefore the abbreviation recognizer treats as a candidate any token of 
			two to ten characters that contains at least one capital letter. Candidates then are fil- 
			tered through a set of known common words and proper names. At the same time 
			many good abbreviations and acronyms are filtered out because not for all of them 
			will definitions exist in the current document. 
			In our task we are interested in finding all and only abbreviations that end with 
			a period (proper abbreviations rather than acronyms), regardless of whether they can 
			be linked to their definitions in the current document or not. Therefore, in our method 
			we cannot tolerate candidate overgeneration or excessive filtering and had to develop 
			more selective methods for finding abbreviations in text. 


Conclusion :
null			The rate of information growth due to the World Wide Web has called for a need 
			to develop efficient and accurate summarization systems. Although research on 
			summarization started about 50 years ago, there is still a long trail to walk in 
			this field. Over time, attention has drifted from summarizing scientific articles to 
			news articles, electronic mail messages, advertisements, and blogs. Both abstractive 
			and extractive approaches have been attempted, depending on the application at 
			hand. Usually, abstractive summarization requires heavy machinery for language 
			generation and is difficult to replicate or extend to broader domains. In contrast, 
			simple extraction of sentences have produced satisfactory results in large-scale ap- 
			plications, specially in multi-document summarization. The recent popularity of 
			effective newswire summarization systems confirms this claim. 
			This survey emphasizes extractive approaches to summarization using statisti- 
			cal methods. A distinction has been made between single document and multi- 
			document summarization. Since a lot of interesting work is being done far from 
			the mainstream research in this field, we have chosen to include a brief discussion 
			on some methods that we found relevant to future research, even if they focus only 
			on small details related to a general summarization process and not on building an 
			entire summarization system. 
			Finally, some recent trends in automatic evaluation of summarization systems 
			have been surveyed. The low inter-annotator agreement figures observed during 
			manual evaluations suggest that the future of this research area heavily depends on 
			the ability to find efficient ways of automatically evaluating these systems and on 
			the development of measures that are objective enough to be commonly accepted 
			by the research community. 
			The rate of information growth due to the World Wide Web has called for a need 
			to develop efficient and accurate summarization systems. Although research on 
			summarization started about 50 years ago, there is still a long trail to walk in 
			this field. Over time, attention has drifted from summarizing scientific articles to 
			news articles, electronic mail messages, advertisements, and blogs. Both abstractive 
			and extractive approaches have been attempted, depending on the application at 
			hand. Usually, abstractive summarization requires heavy machinery for language 
			generation and is difficult to replicate or extend to broader domains. In contrast, 
			simple extraction of sentences have produced satisfactory results in large-scale ap- 
			plications, specially in multi-document summarization. The recent popularity of 
			effective newswire summarization systems confirms this claim. 
			This survey emphasizes extractive approaches to summarization using statisti- 
			cal methods. A distinction has been made between single document and multi- 
			document summarization. Since a lot of interesting work is being done far from 
			the mainstream research in this field, we have chosen to include a brief discussion 
			on some methods that we found relevant to future research, even if they focus only 
			on small details related to a general summarization process and not on building an 
			entire summarization system. 
			Finally, some recent trends in automatic evaluation of summarization systems 
			have been surveyed. The low inter-annotator agreement figures observed during 
			manual evaluations suggest that the future of this research area heavily depends on 
			the ability to find efficient ways of automatically evaluating these systems and on 
			the development of measures that are objective enough to be commonly accepted 
			by the research community. 
			In this paper we presented WiSeBE, a semi-automatic multi-reference sentence 
			boundary evaluation protocol based on the necessity of having a more reliable 
			way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric 
			which not only evaluates the performance of a system against all references, but 
			also takes into account the agreement between them. According to your point 
			of view, this inclusivity is very important given the difficulties that are present 
			when working with spoken language and the possible disagreements that a task 
			like SBD could provoke. 
			WiSeBE shows to be correlated with standard SBD metrics, however we 
			want to measure its correlation with extrinsic evaluations techniques like auto- 
			matic summarization and machine translation. 
			In this paper we presented WiSeBE, a semi-automatic multi-reference sentence 
			boundary evaluation protocol based on the necessity of having a more reliable 
			way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric 
			which not only evaluates the performance of a system against all references, but 
			also takes into account the agreement between them. According to your point 
			of view, this inclusivity is very important given the difficulties that are present 
			when working with spoken language and the possible disagreements that a task 
			like SBD could provoke. 
			WiSeBE shows to be correlated with standard SBD metrics, however we 
			want to measure its correlation with extrinsic evaluations techniques like auto- 
			matic summarization and machine translation. 
			We think that this work means an important step 
			for the RST research in Spanish, and that the RST 
			Spanish Treebank will be useful to carry out 
			diverse researches about RST in this language, 
			from a descriptive point of view (ex. analysis of 
			texts from different domains or genres) and an 
			applied point of view (development of discourse 
			parsers and NLP applications, like automatic 
			summarization, automatic translation, IE, etc.). 
			For the moment the corpus' size is acceptable 
			and, though the percentage of double-annotated 
			texts is not very high, we think that having 10 
			annotators (using the same annotation manual) 
			avoids the bias of only one annotator. In addition, 
			the corpus includes texts of diverse domains and 
			genres, which provides us with a heterogeneous 
			Spanish corpus. Moreover, the corpus interface 
			that we have designed allows the user to select a 
			subcorpus and to analyze it statistically. In 
			addition, we think that it is essential to release a 
			free corpus, on-line and dynamic, that is, in 
			continuous growth. Nevertheless, we are conscious 
			that our work still has certain limitations, which we 
			will try to solve in the future. In the short term, we 
			have 5 aims: 
			a) To add one more annotator for the test corpus 
			and to measure inter-annotator agreement. 
			b) To use more agreement measures, like kappa. 
			c) To reach a consensus on the annotation of the 
			test corpus, in order to establish a set of texts 
			considered as a preliminary gold standard. 
			d) To finish and to evaluate the IE tool. 
			e) To analyze the corpus to extract linguistic 
			patterns for the automatic relations detection. 
			In the long term, we consider other aims: 
			f) To increase the corpus, by adding non- 
			specialized texts, and new domains and genres. 
			g) To annotate all the texts by 3 people, to get a 
			representative gold-standard for Spanish (this aim 
			will depend on the funding of the project). 
			8 
			We think that this work means an important step 
			for the RST research in Spanish, and that the RST 
			Spanish Treebank will be useful to carry out 
			diverse researches about RST in this language, 
			from a descriptive point of view (ex. analysis of 
			texts from different domains or genres) and an 
			applied point of view (development of discourse 
			parsers and NLP applications, like automatic 
			summarization, automatic translation, IE, etc.). 
			For the moment the corpus' size is acceptable 
			and, though the percentage of double-annotated 
			texts is not very high, we think that having 10 
			annotators (using the same annotation manual) 
			avoids the bias of only one annotator. In addition, 
			the corpus includes texts of diverse domains and 
			genres, which provides us with a heterogeneous 
			Spanish corpus. Moreover, the corpus interface 
			that we have designed allows the user to select a 
			subcorpus and to analyze it statistically. In 
			addition, we think that it is essential to release a 
			free corpus, on-line and dynamic, that is, in 
			continuous growth. Nevertheless, we are conscious 
			that our work still has certain limitations, which we 
			will try to solve in the future. In the short term, we 
			have 5 aims: 
			a) To add one more annotator for the test corpus 
			and to measure inter-annotator agreement. 
			b) To use more agreement measures, like kappa. 
			c) To reach a consensus on the annotation of the 
			test corpus, in order to establish a set of texts 
			considered as a preliminary gold standard. 
			d) To finish and to evaluate the IE tool. 
			e) To analyze the corpus to extract linguistic 
			patterns for the automatic relations detection. 
			In the long term, we consider other aims: 
			f) To increase the corpus, by adding non- 
			specialized texts, and new domains and genres. 
			g) To annotate all the texts by 3 people, to get a 
			representative gold-standard for Spanish (this aim 
			will depend on the funding of the project). 
			8 
			This paper presents a novel architecture for text 
			summarization using cut and paste techniques ob- 
			served in human-written abstracts. In order to auto- 
			matically analyze a large quantity of human-written 
			abstracts, we developed a decomposition program. 
			The automatic decomposition allows us to build 
			large corpora for studying sentence reduction and 
			sentence combination, which are two effective op- 
			erations in cut and paste. We developed a sentence 
			reduction module that makes reduction decisions us- 
			ing multiple sources of knowledge. We also investi- 
			gated possible sentence combination operations and 
			implemented the combination module. A sentence 
			extraction module was developed and used as the 
			front end of the summarization system. 
			We are preparing the task-based evaluation of the 
			overall system. We also plan to evaluate the porta- 
			bility of the system by testing it on another corpus. 
			We will also extend the system to query-based sum- 
			marization and investigate whether the system can 
			be modified for multiple document summarization. 
			This paper presents a novel architecture for text 
			summarization using cut and paste techniques ob- 
			served in human-written abstracts. In order to auto- 
			matically analyze a large quantity of human-written 
			abstracts, we developed a decomposition program. 
			The automatic decomposition allows us to build 
			large corpora for studying sentence reduction and 
			sentence combination, which are two effective op- 
			erations in cut and paste. We developed a sentence 
			reduction module that makes reduction decisions us- 
			ing multiple sources of knowledge. We also investi- 
			gated possible sentence combination operations and 
			implemented the combination module. A sentence 
			extraction module was developed and used as the 
			front end of the summarization system. 
			We are preparing the task-based evaluation of the 
			overall system. We also plan to evaluate the porta- 
			bility of the system by testing it on another corpus. 
			We will also extend the system to query-based sum- 
			marization and investigate whether the system can 
			be modified for multiple document summarization. 
			The paper reports our experiments and results for building 
			a precise and large terminology for the construction domain. 
			Collecting terminology is indeed the first step towards a 
			complete knowledge model containing both concepts and 
			relationships. During our work we were faced to several 
			problems: finding resources and selecting them for building 
			an appropriate corpus, thinking and developing pre-processing 
			for cleaning those resources, experimenting distinct measures 
			for n-grams and selecting the most appropriate, improving 
			results by adding linguistic patterns and Internet queries. The 
			current results are quite promising according to the evaluation 
			of the extracted terminology carried out by 6 experts in the 
			field. As a perspective, we will develop generic modules and 
			guidelines for adapting these pre-processing modules to other 
			languages. Most importantly, the results of our work are useful 
			for extracting taxonomical and non-taxonomical relationships. 
			For the both purposes, we are currently working on SemEval 
			collection [26]. Applying our method to other domain corpora 
			and datasets is another future direction for this research. 
			The paper reports our experiments and results for building 
			a precise and large terminology for the construction domain. 
			Collecting terminology is indeed the first step towards a 
			complete knowledge model containing both concepts and 
			relationships. During our work we were faced to several 
			problems: finding resources and selecting them for building 
			an appropriate corpus, thinking and developing pre-processing 
			for cleaning those resources, experimenting distinct measures 
			for n-grams and selecting the most appropriate, improving 
			results by adding linguistic patterns and Internet queries. The 
			current results are quite promising according to the evaluation 
			of the extracted terminology carried out by 6 experts in the 
			field. As a perspective, we will develop generic modules and 
			guidelines for adapting these pre-processing modules to other 
			languages. Most importantly, the results of our work are useful 
			for extracting taxonomical and non-taxonomical relationships. 
			For the both purposes, we are currently working on SemEval 
			collection [26]. Applying our method to other domain corpora 
			and datasets is another future direction for this research. 
			In this article, we presented an unsupervised approach to 
			exploring a collection of stories about human distress. This 
			approach uses a word embedding model to build vectors 
			containing only vocabulary from the linguistic context of the 
			model. We evaluated the quality of the approach on a col- 
			lection labeled with classical measures. The detailed analysis 
			showed very good results (average Fscore of 0.54) compared 
			to the other systems tested. This method of analysis has also 
			made it possible to highlight semantic universes and thematic 
			groupings. We first intend to study in more detail the influence 
			of each of the parameters on the results obtained. We are also 
			planning to be able to assign several tags to each discussion, 
			which would allow thematic overlaps to be taken into account. 
			The analysis reinforces the cluster approach to highlight the 
			defining features of this type of speech production and to 
			reveal its inner workings. This entry by the discursive routines 
			is only one example which will then make it possible to 
			approach other explorations with a particular focus on the 
			argumentative forms and on the forms of intensity. 
			In this article, we presented an unsupervised approach to 
			exploring a collection of stories about human distress. This 
			approach uses a word embedding model to build vectors 
			containing only vocabulary from the linguistic context of the 
			model. We evaluated the quality of the approach on a col- 
			lection labeled with classical measures. The detailed analysis 
			showed very good results (average Fscore of 0.54) compared 
			to the other systems tested. This method of analysis has also 
			made it possible to highlight semantic universes and thematic 
			groupings. We first intend to study in more detail the influence 
			of each of the parameters on the results obtained. We are also 
			planning to be able to assign several tags to each discussion, 
			which would allow thematic overlaps to be taken into account. 
			The analysis reinforces the cluster approach to highlight the 
			defining features of this type of speech production and to 
			reveal its inner workings. This entry by the discursive routines 
			is only one example which will then make it possible to 
			approach other explorations with a particular focus on the 
			argumentative forms and on the forms of intensity. 


Discussion :
null			In this paper we have described SMMR, a scal- 
			able sentence scoring method based on MMR that 
			achieves very promising results. An important as- 
			pect of our sentence scoring method is that it does 
			not requires re-ranking nor linguistic knowledge, 
			which makes it a simple and fast approach to the 
			issue of update summarization. It was pointed out 
			at the DUC 2007 workshop that Question Answer- 
			ing and query-oriented summarization have been 
			converging on a common task. The value added 
			by summarization lies in the linguistic quality. Ap- 
			proaches mixing IR techniques are well suited for 
			query-oriented summarization but they require in- 
			tensive work for making the summary fluent and 
			coherent. Among the others, this is a point that we 
			think is worthy of further investigation. 
			In this paper we have described SMMR, a scal- 
			able sentence scoring method based on MMR that 
			achieves very promising results. An important as- 
			pect of our sentence scoring method is that it does 
			not requires re-ranking nor linguistic knowledge, 
			which makes it a simple and fast approach to the 
			issue of update summarization. It was pointed out 
			at the DUC 2007 workshop that Question Answer- 
			ing and query-oriented summarization have been 
			converging on a common task. The value added 
			by summarization lies in the linguistic quality. Ap- 
			proaches mixing IR techniques are well suited for 
			query-oriented summarization but they require in- 
			tensive work for making the summary fluent and 
			coherent. Among the others, this is a point that we 
			think is worthy of further investigation. 
			5.1 RGA R 
			and Fleiss' Kappa correlation 
			In Sect. 3 we described the WiSeBE score and how it relies on the RGAR 
			value 
			to scale the performance of CT 
			over RW 
			. RGAR 
			can intuitively be consider an 
			agreement value over all elements of R. To test this hypothesis, we computed 
			the Pearson correlation coefficient (PCC) [21] between RGAR 
			and the Fleiss' 
			Kappa [4] of each video in the dataset (R 
			). 
			A linear correlation between RGAR 
			and R 
			can be observed in Table 6. This 
			is confirmed by a PCC value equal to 0.890, which means a very strong positive 
			linear correlation between them. 
			5.2 F 1mean vs. W iSeBE 
			Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean 
			. 
			While it is true that they show a linear correlation, WiSeBE may produce a 
			128 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 5. WiSeBE evaluation 
			Transcript System F1mean 
			F1RW 
			RGAR 
			WiSeBE 
			v1 S1 0.432 0.495 0.691 0.342 
			S2 0.480 0.513 0.354 
			v2 S1 0.578 0.659 0.688 0.453 
			S2 0.549 0.595 0.409 
			v3 S1 0.270 0.303 0.684 0.207 
			S2 0.325 0.400 0.274 
			v4 S1 0.505 0.593 0.578 0.342 
			S2 0.735 0.800 0.462 
			v5 S1 0.592 0.614 0.767 0.471 
			S2 0.499 0.500 0.383 
			v6 S1 0.443 0.550 0.541 0.298 
			S2 0.457 0.535 0.289 
			v7 S1 0.518 0.592 0.617 0.366 
			S2 0.539 0.606 0.374 
			v8 S1 0.429 0.494 0.525 0.259 
			S2 0.487 0.508 0.267 
			v9 S1 0.459 0.569 0.604 0.344 
			S2 0.541 0.667 0.403 
			v10 S1 0.582 0.581 0.619 0.359 
			S2 0.487 0.545 0.338 
			Mean scores S1 0.481 0.545 0.631 0.344 
			S2 0.510 0.567 0.355 
			Table 6. Agreement within dataset 
			Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 
			RGAR 
			0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619 
			R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718 
			different system ranking than F1mean 
			given the integral multi-reference principle 
			it follows. However, what we consider the most profitable about WiSeBE is the 
			twofold inclusion of all available references it performs. First, the construction of 
			RW 
			to provide a more inclusive reference against to whom be evaluated and then, 
			the computation of RGAR 
			, which scales the result depending of the agreement 
			between references. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 129 
			5.1 RGA R 
			and Fleiss' Kappa correlation 
			In Sect. 3 we described the WiSeBE score and how it relies on the RGAR 
			value 
			to scale the performance of CT 
			over RW 
			. RGAR 
			can intuitively be consider an 
			agreement value over all elements of R. To test this hypothesis, we computed 
			the Pearson correlation coefficient (PCC) [21] between RGAR 
			and the Fleiss' 
			Kappa [4] of each video in the dataset (R 
			). 
			A linear correlation between RGAR 
			and R 
			can be observed in Table 6. This 
			is confirmed by a PCC value equal to 0.890, which means a very strong positive 
			linear correlation between them. 
			5.2 F 1mean vs. W iSeBE 
			Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean 
			. 
			While it is true that they show a linear correlation, WiSeBE may produce a 
			128 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			Table 5. WiSeBE evaluation 
			Transcript System F1mean 
			F1RW 
			RGAR 
			WiSeBE 
			v1 S1 0.432 0.495 0.691 0.342 
			S2 0.480 0.513 0.354 
			v2 S1 0.578 0.659 0.688 0.453 
			S2 0.549 0.595 0.409 
			v3 S1 0.270 0.303 0.684 0.207 
			S2 0.325 0.400 0.274 
			v4 S1 0.505 0.593 0.578 0.342 
			S2 0.735 0.800 0.462 
			v5 S1 0.592 0.614 0.767 0.471 
			S2 0.499 0.500 0.383 
			v6 S1 0.443 0.550 0.541 0.298 
			S2 0.457 0.535 0.289 
			v7 S1 0.518 0.592 0.617 0.366 
			S2 0.539 0.606 0.374 
			v8 S1 0.429 0.494 0.525 0.259 
			S2 0.487 0.508 0.267 
			v9 S1 0.459 0.569 0.604 0.344 
			S2 0.541 0.667 0.403 
			v10 S1 0.582 0.581 0.619 0.359 
			S2 0.487 0.545 0.338 
			Mean scores S1 0.481 0.545 0.631 0.344 
			S2 0.510 0.567 0.355 
			Table 6. Agreement within dataset 
			Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10 
			RGAR 
			0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619 
			R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718 
			different system ranking than F1mean 
			given the integral multi-reference principle 
			it follows. However, what we consider the most profitable about WiSeBE is the 
			twofold inclusion of all available references it performs. First, the construction of 
			RW 
			to provide a more inclusive reference against to whom be evaluated and then, 
			the computation of RGAR 
			, which scales the result depending of the agreement 
			between references. 
			WiSeBE: Window-Based Sentence Boundary Evaluation 129 
			In this article we presented an approach that tackles three important aspects of text nor- 
			malization: sentence boundary disambiguation, disambiguation of capitalized words 
			when they are used in positions where capitalization is expected, and identification of 
			abbreviations. The major distinctive features of our approach can be summarized as 
			follows: 
			* We tackle the sentence boundary task only after we have fully 
			disambiguated the word on the left and the word on the right of a 
			potential sentence boundary punctuation sign. 
			* To disambiguate capitalized words and abbreviations, we use 
			information distributed across the entire document rather than their 
			immediate local context. 
			* Our approach does not require manual rule construction or data 
			annotation for training. Instead, it relies on four word lists that can be 
			generated completely automatically from a raw (unlabeled) corpus. 
			In this approach we do not try to resolve each ambiguous word occurrence individu- 
			ally. Instead, the system scans the entire document for the contexts in which the words 
			in question are used unambiguously, and this gives it grounds, acting by analogy, for 
			resolving ambiguous contexts. 
			We deliberately shaped our approach so that it largely does not rely on precom- 
			piled statistics, because the most interesting events are inherently infrequent and hence 
			are difficult to collect reliable statistics for. At the same time precompiled statistics 
			would be smoothed across multiple documents rather than targeted to a specific docu- 
			ment. By collecting suggestive instances of usage for target words from each particular 
			document on the fly, rather than relying on preacquired resources smoothed across the 
			entire document collection, our approach is robust to domain shifts and new lexica 
			and closely targeted to each document. 
			314 
			Computational Linguistics Volume 28, Number 3 
			A significant advantage of this approach is that it can be targeted to new domains 
			completely automatically, without human intervention. The four word lists that our 
			system uses for its operation can be generated automatically from a raw corpus and 
			require no human annotation. Although some SBD systems can be trained on relatively 
			small sets of labeled examples, their performance in such cases is somewhat lower than 
			their optimal performance. For instance, Palmer and Hearst (1997) report that the SATZ 
			system (decision tree variant) was trained on a set of about 800 labeled periods, which 
			corresponds to a corpus of about 16,000 words. This is a relatively small training set 
			that can be manually marked in a few hours' time. But the error rate (1.5%) of the 
			decision tree classifier trained on this small sample was about 50% higher than that 
			when trained on 6,000 labeled examples (1.0%). 
			The performance of our system does not depend on the availability of labeled 
			training examples. For its "training," it uses a raw (unannotated in any way) corpus 
			of texts. Although it needs such a corpus to be relatively large (a few hundred thousand 
			words), this is normally not a problem, since when the system is targeted to a new 
			domain, such a corpus is usually already available at no extra cost. Therefore there is no 
			trade-off between the amount of human labor and the performance of the system. This 
			not only makes retargeting of such system easier but also enables it to be operational 
			in a completely autonomous way: it needs only to be pointed to texts from a new 
			domain, and then it can retarget itself automatically. 
			Although the DCA requires two passes through a document, the simplicity of the 
			underlying algorithms makes it reasonably fast. It processes about 3,000 words per 
			second using a Pentium II 400 MHz processor. This includes identification of abbre- 
			viations, disambiguation of capitalized words, and then disambiguation of sentence 
			boundaries. This is comparable to the speed of other preprocessing systems.3 The oper- 
			ational speed is about 10% higher than the training speed because, apart from applying 
			the system to the training corpus, training also involves collecting, thresholding, and 
			sorting of the word lists--all done automatically but at extra time cost. Training on 
			the 300,000-word NYT text collection took about two minutes. 
			Despite its simplicity, the performance of our approach was on the level with 
			the previously highest reported results on the same test collections. The error rate 
			on sentence boundaries in the Brown corpus was not significantly worse than the 
			lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus 
			our system performed slightly better than the combination of the Alembic and SATZ 
			systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although 
			these error rates seem to be very small, they are quite significant. Unlike general POS 
			tagging, in which it is unfair to expect an error rate of less than 2% because even human 
			annotators have a disagreement rate of about 3%, sentence boundaries are much less 
			ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate 
			of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, 
			one error in 200 periods means that there is one error in every two documents in the 
			Brown corpus and one error in every four documents in the WSJ corpus. 
			With all its strong points, there are a number of restrictions to the proposed ap- 
			proach. First, in its present form it is suitable only for processing of reasonably "well- 
			behaved" texts that consistently use capitalization (mixed case) and do not contain 
			much noisy data. Thus, for instance, we do not expect our system to perform well 
			on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on 
			3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average 
			sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha 
			3000). 
			315 
			Mikheev Periods, Capitalized Words, etc. 
			optical character reader-generated texts. We noted in Section 8 that very short doc- 
			uments of one to three sentences also present a difficulty for our approach. This is 
			where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger 
			reported in Mikheev (2000), which do not heavily rely on word capitalization and are 
			not sensitive to document length, have an advantage. 
			Our DCA uses information derived from the entire document and thus can be 
			used as a complement to approaches based on the local context. When we incorpo- 
			rated the DCA system into a POS tagger (Section 8), we measured a 30-35% cut in the 
			error rate on proper-name identification in comparison to DCA or the POS-tagging 
			approaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20% 
			error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre- 
			sponds to about a 20% cut in the error rate in comparison to DCA or the POS-tagging 
			approaches alone. 
			We also investigated the portability of our approach to other languages and ob- 
			tained encouraging results on a corpus of news in Russian. This strongly suggests that 
			the DCA method can be applied to the majority of European languages, since they 
			share the same principles of capitalization and word abbreviation. Obvious exceptions, 
			though, are German and some Scandinavian languages in which capitalization is used 
			for things other than proper-name and sentence start signaling. This does not mean, 
			however, that the DCA in general is not suitable for preprocessing of German texts--it 
			just needs to be applied with different disambiguation clues. 
			Initially the system described in this article was developed as a text normalization 
			module for a named-entity recognition system (Mikheev, Grover, and Moens 1998) that 
			participated in MUC-7. There the ability to identify proper names with high accuracy 
			proved to be instrumental in enabling the entire system to achieve a very high level of 
			performance. Since then this text normalization module has been used in several other 
			systems, and its ability to be adapted easily to new domains enabled rapid develop- 
			ment of text analysis capabilities in medical, legal, and law enforcement domains. 
			In this article we presented an approach that tackles three important aspects of text nor- 
			malization: sentence boundary disambiguation, disambiguation of capitalized words 
			when they are used in positions where capitalization is expected, and identification of 
			abbreviations. The major distinctive features of our approach can be summarized as 
			follows: 
			* We tackle the sentence boundary task only after we have fully 
			disambiguated the word on the left and the word on the right of a 
			potential sentence boundary punctuation sign. 
			* To disambiguate capitalized words and abbreviations, we use 
			information distributed across the entire document rather than their 
			immediate local context. 
			* Our approach does not require manual rule construction or data 
			annotation for training. Instead, it relies on four word lists that can be 
			generated completely automatically from a raw (unlabeled) corpus. 
			In this approach we do not try to resolve each ambiguous word occurrence individu- 
			ally. Instead, the system scans the entire document for the contexts in which the words 
			in question are used unambiguously, and this gives it grounds, acting by analogy, for 
			resolving ambiguous contexts. 
			We deliberately shaped our approach so that it largely does not rely on precom- 
			piled statistics, because the most interesting events are inherently infrequent and hence 
			are difficult to collect reliable statistics for. At the same time precompiled statistics 
			would be smoothed across multiple documents rather than targeted to a specific docu- 
			ment. By collecting suggestive instances of usage for target words from each particular 
			document on the fly, rather than relying on preacquired resources smoothed across the 
			entire document collection, our approach is robust to domain shifts and new lexica 
			and closely targeted to each document. 
			314 
			Computational Linguistics Volume 28, Number 3 
			A significant advantage of this approach is that it can be targeted to new domains 
			completely automatically, without human intervention. The four word lists that our 
			system uses for its operation can be generated automatically from a raw corpus and 
			require no human annotation. Although some SBD systems can be trained on relatively 
			small sets of labeled examples, their performance in such cases is somewhat lower than 
			their optimal performance. For instance, Palmer and Hearst (1997) report that the SATZ 
			system (decision tree variant) was trained on a set of about 800 labeled periods, which 
			corresponds to a corpus of about 16,000 words. This is a relatively small training set 
			that can be manually marked in a few hours' time. But the error rate (1.5%) of the 
			decision tree classifier trained on this small sample was about 50% higher than that 
			when trained on 6,000 labeled examples (1.0%). 
			The performance of our system does not depend on the availability of labeled 
			training examples. For its "training," it uses a raw (unannotated in any way) corpus 
			of texts. Although it needs such a corpus to be relatively large (a few hundred thousand 
			words), this is normally not a problem, since when the system is targeted to a new 
			domain, such a corpus is usually already available at no extra cost. Therefore there is no 
			trade-off between the amount of human labor and the performance of the system. This 
			not only makes retargeting of such system easier but also enables it to be operational 
			in a completely autonomous way: it needs only to be pointed to texts from a new 
			domain, and then it can retarget itself automatically. 
			Although the DCA requires two passes through a document, the simplicity of the 
			underlying algorithms makes it reasonably fast. It processes about 3,000 words per 
			second using a Pentium II 400 MHz processor. This includes identification of abbre- 
			viations, disambiguation of capitalized words, and then disambiguation of sentence 
			boundaries. This is comparable to the speed of other preprocessing systems.3 The oper- 
			ational speed is about 10% higher than the training speed because, apart from applying 
			the system to the training corpus, training also involves collecting, thresholding, and 
			sorting of the word lists--all done automatically but at extra time cost. Training on 
			the 300,000-word NYT text collection took about two minutes. 
			Despite its simplicity, the performance of our approach was on the level with 
			the previously highest reported results on the same test collections. The error rate 
			on sentence boundaries in the Brown corpus was not significantly worse than the 
			lowest quoted before (Riley 1989: 0.28% vs. 0.20% error rate). On the WSJ corpus 
			our system performed slightly better than the combination of the Alembic and SATZ 
			systems described in Palmer and Hearst (1997) (0.44% vs. 0.5% error rate). Although 
			these error rates seem to be very small, they are quite significant. Unlike general POS 
			tagging, in which it is unfair to expect an error rate of less than 2% because even human 
			annotators have a disagreement rate of about 3%, sentence boundaries are much less 
			ambiguous (with a disagreement of about 1 in 5,000). This shows that an error rate 
			of 1 in 200 (0.5%) is still far from reaching the disagreement level. On the other hand, 
			one error in 200 periods means that there is one error in every two documents in the 
			Brown corpus and one error in every four documents in the WSJ corpus. 
			With all its strong points, there are a number of restrictions to the proposed ap- 
			proach. First, in its present form it is suitable only for processing of reasonably "well- 
			behaved" texts that consistently use capitalization (mixed case) and do not contain 
			much noisy data. Thus, for instance, we do not expect our system to perform well 
			on single-cased texts (e.g., texts written in all capital or all lower-cased letters) or on 
			3 Palmer and Hearst (1997) report a speed of over 10,000 sentences a minute, which with their average 
			sentence length of 20 words equals over 3,000 words per second, but on a slower machine (DEC Alpha 
			3000). 
			315 
			Mikheev Periods, Capitalized Words, etc. 
			optical character reader-generated texts. We noted in Section 8 that very short doc- 
			uments of one to three sentences also present a difficulty for our approach. This is 
			where robust syntactic systems like SATZ (Palmer and Hearst 1997) or the POS tagger 
			reported in Mikheev (2000), which do not heavily rely on word capitalization and are 
			not sensitive to document length, have an advantage. 
			Our DCA uses information derived from the entire document and thus can be 
			used as a complement to approaches based on the local context. When we incorpo- 
			rated the DCA system into a POS tagger (Section 8), we measured a 30-35% cut in the 
			error rate on proper-name identification in comparison to DCA or the POS-tagging 
			approaches alone. This in turn enabled better tagging of sentence boundaries: a 0.20% 
			error rate on the Brown corpus and a 0.31% error rate on the WSJ corpus, which corre- 
			sponds to about a 20% cut in the error rate in comparison to DCA or the POS-tagging 
			approaches alone. 
			We also investigated the portability of our approach to other languages and ob- 
			tained encouraging results on a corpus of news in Russian. This strongly suggests that 
			the DCA method can be applied to the majority of European languages, since they 
			share the same principles of capitalization and word abbreviation. Obvious exceptions, 
			though, are German and some Scandinavian languages in which capitalization is used 
			for things other than proper-name and sentence start signaling. This does not mean, 
			however, that the DCA in general is not suitable for preprocessing of German texts--it 
			just needs to be applied with different disambiguation clues. 
			Initially the system described in this article was developed as a text normalization 
			module for a named-entity recognition system (Mikheev, Grover, and Moens 1998) that 
			participated in MUC-7. There the ability to identify proper names with high accuracy 
			proved to be instrumental in enabling the entire system to achieve a very high level of 
			performance. Since then this text normalization module has been used in several other 
			systems, and its ability to be adapted easily to new domains enabled rapid develop- 
			ment of text analysis capabilities in medical, legal, and law enforcement domains. 


Reference :
null			Boudin, F. and J.M. Torres-Moreno. 2007. A Co- 
			sine Maximization-Minimization approach for User- 
			Oriented Multi-Document Update Summarization. 
			In Recent Advances in Natural Language Processing 
			(RANLP), pages 81-87. 
			Carbonell, J. and J. Goldstein. 1998. The use of MMR, 
			diversity-based reranking for reordering documents 
			and producing summaries. In 21st annual interna- 
			tional ACM SIGIR conference on Research and de- 
			velopment in information retrieval, pages 335-336. 
			ACM Press New York, NY, USA. 
			Hachey, B., G. Murray, and D. Reitter. 2005. The 
			Embra System at DUC 2005: Query-oriented Multi- 
			document Summarization with a Very Large Latent 
			Semantic Space. In Document Understanding Con- 
			ference (DUC). 
			Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC's 
			GISTexter at DUC 2007: Machine Reading for Up- 
			date Summarization. In Document Understanding 
			Conference (DUC). 
			Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and 
			S. Ye. 2007. NUS at DUC 2007: Using Evolu- 
			tionary Models of Text. In Document Understanding 
			Conference (DUC). 
			Lin, C.Y. 2004. Rouge: A Package for Automatic 
			Evaluation of Summaries. In Workshop on Text Sum- 
			marization Branches Out, pages 25-26. 
			Mani, I. and M.T. Maybury. 1999. Advances in Auto- 
			matic Text Summarization. MIT Press. 
			Mani, I. and G. Wilson. 2000. Robust temporal pro- 
			cessing of news. In 38th Annual Meeting on Asso- 
			ciation for Computational Linguistics, pages 69-76. 
			Association for Computational Linguistics Morris- 
			town, NJ, USA. 
			Murray, G., S. Renals, and J. Carletta. 2005. Extractive 
			Summarization of Meeting Recordings. In Ninth Eu- 
			ropean Conference on Speech Communication and 
			Technology. ISCA. 
			Salton, G., A. Wong, and C. S. Yang. 1975. A vector 
			space model for automatic indexing. Communica- 
			tions of the ACM, 18(11):613-620. 
			Swan, R. and J. Allan. 2000. Automatic generation 
			of overview timelines. In 23rd annual international 
			ACM SIGIR conference on Research and develop- 
			ment in information retrieval, pages 49-56. 
			Winkler, W. E. 1999. The state of record linkage and 
			current research problems. In Survey Methods Sec- 
			tion, pages 73-79. 
			Witte, R., R. Krestel, and S. Bergler. 2007. Generat- 
			ing Update Summaries for DUC 2007. In Document 
			Understanding Conference (DUC). 
			Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS 
			at DUC 2005: Understanding documents via con- 
			cept links. In Document Understanding Conference 
			(DUC). 
			26 
			Boudin, F. and J.M. Torres-Moreno. 2007. A Co- 
			sine Maximization-Minimization approach for User- 
			Oriented Multi-Document Update Summarization. 
			In Recent Advances in Natural Language Processing 
			(RANLP), pages 81-87. 
			Carbonell, J. and J. Goldstein. 1998. The use of MMR, 
			diversity-based reranking for reordering documents 
			and producing summaries. In 21st annual interna- 
			tional ACM SIGIR conference on Research and de- 
			velopment in information retrieval, pages 335-336. 
			ACM Press New York, NY, USA. 
			Hachey, B., G. Murray, and D. Reitter. 2005. The 
			Embra System at DUC 2005: Query-oriented Multi- 
			document Summarization with a Very Large Latent 
			Semantic Space. In Document Understanding Con- 
			ference (DUC). 
			Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC's 
			GISTexter at DUC 2007: Machine Reading for Up- 
			date Summarization. In Document Understanding 
			Conference (DUC). 
			Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, and 
			S. Ye. 2007. NUS at DUC 2007: Using Evolu- 
			tionary Models of Text. In Document Understanding 
			Conference (DUC). 
			Lin, C.Y. 2004. Rouge: A Package for Automatic 
			Evaluation of Summaries. In Workshop on Text Sum- 
			marization Branches Out, pages 25-26. 
			Mani, I. and M.T. Maybury. 1999. Advances in Auto- 
			matic Text Summarization. MIT Press. 
			Mani, I. and G. Wilson. 2000. Robust temporal pro- 
			cessing of news. In 38th Annual Meeting on Asso- 
			ciation for Computational Linguistics, pages 69-76. 
			Association for Computational Linguistics Morris- 
			town, NJ, USA. 
			Murray, G., S. Renals, and J. Carletta. 2005. Extractive 
			Summarization of Meeting Recordings. In Ninth Eu- 
			ropean Conference on Speech Communication and 
			Technology. ISCA. 
			Salton, G., A. Wong, and C. S. Yang. 1975. A vector 
			space model for automatic indexing. Communica- 
			tions of the ACM, 18(11):613-620. 
			Swan, R. and J. Allan. 2000. Automatic generation 
			of overview timelines. In 23rd annual international 
			ACM SIGIR conference on Research and develop- 
			ment in information retrieval, pages 49-56. 
			Winkler, W. E. 1999. The state of record linkage and 
			current research problems. In Survey Methods Sec- 
			tion, pages 73-79. 
			Witte, R., R. Krestel, and S. Bergler. 2007. Generat- 
			ing Update Summaries for DUC 2007. In Document 
			Understanding Conference (DUC). 
			Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUS 
			at DUC 2005: Understanding documents via con- 
			cept links. In Document Understanding Conference 
			(DUC). 
			26 
			Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla- 
			tions of Mathematical Monographs). Oxford University Press. [20] 
			Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable 
			summarizer with knowledge acquired from robust nlp techniques. In Mani, I. 
			and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 
			71-80. MIT Press. [4, 5] 
			Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization. 
			In Proceedings ISTS'97. [8] 
			Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the 
			context of multi-document summarization. In Proceedings of ACL '99. [12, 13, 
			14, 16] 
			Baxendale, P. (1958). Machine-made index for technical literature - an experiment. 
			IBM Journal of Research Development, 2(4):354-361. [2, 3, 5] 
			Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The 
			mathematics of statistical machine translation: parameter estimation. Comput. 
			Linguist., 19(2):263-311. [18] 
			Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and 
			Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05: 
			Proceedings of the 22nd international conference on Machine learning, pages 89- 
			96, New York, NY, USA. ACM. [8] 
			Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking 
			for reordering documents and producing summaries. In Proceedings of SIGIR '98, 
			pages 335-336, New York, NY, USA. [12, 14, 15] 
			Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. 
			PhD thesis, University of Pennsylvania. [13, 20] 
			Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov 
			models. In Proceedings of SIGIR '01, pages 406-407, New York, NY, USA. [6] 
			Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25] 
			Daum 
			e III, H. and Marcu, D. (2002). A noisy-channel model for document com- 
			pression. In Proceedings of the Conference of the Association of Computational 
			Linguistics (ACL 2002). [20] 
			Daum 
			e III, H. and Marcu, D. (2004). A tree-position kernel for document compres- 
			sion. In Proceedings of DUC2004. [20] 
			Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the 
			ACM, 16(2):264-285. [2, 3, 4] 
			28 
			Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu- 
			tions. IEEE Transactions on Information Theory, 49(7):1858-1860. [26] 
			Evans, D. K. (2005). Similarity-based multilingual multi-document summarization. 
			Technical Report CUCS-014-05, Columbia University. [12, 17] 
			Gous, A. (1999). Spherical subfamily models. [20, 21] 
			Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for 
			natural language processing and information retrieval. In Proc. 17th International 
			Conf. on Machine Learning, pages 351-358. Morgan Kaufmann, San Francisco, 
			CA. [20] 
			Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In 
			Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza- 
			tion, pages 81-94. MIT Press. [17] 
			Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen- 
			tence compression. In AAAI/IAAI, pages 703-710. [19] 
			Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer. 
			In Proceedings SIGIR '95, pages 68-73, New York, NY, USA. [4] 
			Lebanon, G. (2006). Sequential document representations and simplicial curves. In 
			Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. [22] 
			Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words 
			framework for document representation. J. Mach. Learn. Res., 8:2405-2441. [21, 
			22] 
			Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of 
			CIKM '99, pages 55-62, New York, NY, USA. [5] 
			Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In 
			Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro- 
			ceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain. [8, 23, 24, 
			25] 
			Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap- 
			proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL 
			'06, pages 463-470, Morristown, NJ, USA. [25, 26] 
			Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of 
			the Fifth conference on Applied natural language processing, pages 283-290, San 
			Francisco, CA, USA. [5] 
			Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In 
			Proceedings of the ACL-02 Workshop on Automatic Summarization, pages 45-51, 
			Morristown, NJ, USA. [23] 
			29 
			Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of 
			Research Development, 2(2):159-165. [2, 3, 6, 8] 
			Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search 
			and matching. In AAAI/IAAI, pages 622-628. [15, 16] 
			Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In 
			Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages 
			206-215, Montreal, Canada. [9, 10, 20] 
			Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of 
			natural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst. 
			[10] 
			McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E. 
			(1999). Towards multidocument summarization by reformulation: Progress and 
			prospects. In AAAI/IAAI, pages 453-460. [11, 12, 13, 14, 16] 
			McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news 
			articles. In Proceedings of SIGIR '95, pages 74-82, Seattle, Washington. [8, 11, 
			12] 
			Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM, 
			38(11):39-41. [4, 9] 
			Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned 
			from the document understanding conference. In Proceedings of AAAI 2005, 
			Pittsburgh, USA. [7] 
			Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical 
			structure extraction. In Proceedings of Coling '94, pages 344-348, Morristown, 
			NJ, USA. [9] 
			Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings 
			of the ACL'02 Workshop on Automatic Summarization, pages 1-8, Morristown, 
			NJ, USA. [7] 
			Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for 
			automatic evaluation of machine translation. In Proceedings of ACL '02, pages 
			311-318, Morristown, NJ, USA. [24] 
			Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue 
			on summarization. Computational Linguistics., 28(4):399-408. [1, 2] 
			Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization 
			of multiple documents: sentence extraction, utility-based evaluation, and user 
			studies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages 
			21-30, Morristown, NJ, USA. [12, 16, 17] 
			30 
			Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza- 
			tion of multiple documents. Information Processing and Management 40 (2004), 
			40:919-938. [16, 17] 
			Radev, D. R. and McKeown, K. (1998). Generating natural language summaries 
			from multiple on-line sources. Computational Linguistics, 24(3):469-500. [12] 
			Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in 
			automatic information. In Proceedings of SIGIR '88, pages 147-160, New York, 
			NY, USA. [15] 
			Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic 
			indexing. Communications of the ACM, 18:229-237. [20] 
			Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving 
			hard satisfiability problems. In AAAI, pages 440-446. [11] 
			Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document 
			summarization by combining RankNet and third-party sources. In Proceedings of 
			the EMNLP-CoNLL, pages 448-457. [7, 8] 
			Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract): 
			a statistical approach to generating highly condensed non-extractive summaries. 
			In Proceedings of SIGIR '99, pages 315-316, New York, NY, USA. [18] 
			31 
			Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla- 
			tions of Mathematical Monographs). Oxford University Press. [20] 
			Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainable 
			summarizer with knowledge acquired from robust nlp techniques. In Mani, I. 
			and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages 
			71-80. MIT Press. [4, 5] 
			Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization. 
			In Proceedings ISTS'97. [8] 
			Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in the 
			context of multi-document summarization. In Proceedings of ACL '99. [12, 13, 
			14, 16] 
			Baxendale, P. (1958). Machine-made index for technical literature - an experiment. 
			IBM Journal of Research Development, 2(4):354-361. [2, 3, 5] 
			Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). The 
			mathematics of statistical machine translation: parameter estimation. Comput. 
			Linguist., 19(2):263-311. [18] 
			Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., and 
			Hullender, G. (2005). Learning to rank using gradient descent. In ICML '05: 
			Proceedings of the 22nd international conference on Machine learning, pages 89- 
			96, New York, NY, USA. ACM. [8] 
			Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based reranking 
			for reordering documents and producing summaries. In Proceedings of SIGIR '98, 
			pages 335-336, New York, NY, USA. [12, 14, 15] 
			Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing. 
			PhD thesis, University of Pennsylvania. [13, 20] 
			Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markov 
			models. In Proceedings of SIGIR '01, pages 406-407, New York, NY, USA. [6] 
			Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25] 
			Daum 
			e III, H. and Marcu, D. (2002). A noisy-channel model for document com- 
			pression. In Proceedings of the Conference of the Association of Computational 
			Linguistics (ACL 2002). [20] 
			Daum 
			e III, H. and Marcu, D. (2004). A tree-position kernel for document compres- 
			sion. In Proceedings of DUC2004. [20] 
			Edmundson, H. P. (1969). New methods in automatic extracting. Journal of the 
			ACM, 16(2):264-285. [2, 3, 4] 
			28 
			Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu- 
			tions. IEEE Transactions on Information Theory, 49(7):1858-1860. [26] 
			Evans, D. K. (2005). Similarity-based multilingual multi-document summarization. 
			Technical Report CUCS-014-05, Columbia University. [12, 17] 
			Gous, A. (1999). Spherical subfamily models. [20, 21] 
			Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies for 
			natural language processing and information retrieval. In Proc. 17th International 
			Conf. on Machine Learning, pages 351-358. Morgan Kaufmann, San Francisco, 
			CA. [20] 
			Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. In 
			Mani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza- 
			tion, pages 81-94. MIT Press. [17] 
			Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen- 
			tence compression. In AAAI/IAAI, pages 703-710. [19] 
			Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer. 
			In Proceedings SIGIR '95, pages 68-73, New York, NY, USA. [4] 
			Lebanon, G. (2006). Sequential document representations and simplicial curves. In 
			Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. [22] 
			Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of words 
			framework for document representation. J. Mach. Learn. Res., 8:2405-2441. [21, 
			22] 
			Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings of 
			CIKM '99, pages 55-62, New York, NY, USA. [5] 
			Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. In 
			Marie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro- 
			ceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain. [8, 23, 24, 
			25] 
			Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap- 
			proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL 
			'06, pages 463-470, Morristown, NJ, USA. [25, 26] 
			Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings of 
			the Fifth conference on Applied natural language processing, pages 283-290, San 
			Francisco, CA, USA. [5] 
			Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. In 
			Proceedings of the ACL-02 Workshop on Automatic Summarization, pages 45-51, 
			Morristown, NJ, USA. [23] 
			29 
			Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal of 
			Research Development, 2(2):159-165. [2, 3, 6, 8] 
			Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph search 
			and matching. In AAAI/IAAI, pages 622-628. [15, 16] 
			Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. In 
			Proceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages 
			206-215, Montreal, Canada. [9, 10, 20] 
			Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation of 
			natural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst. 
			[10] 
			McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E. 
			(1999). Towards multidocument summarization by reformulation: Progress and 
			prospects. In AAAI/IAAI, pages 453-460. [11, 12, 13, 14, 16] 
			McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple news 
			articles. In Proceedings of SIGIR '95, pages 74-82, Seattle, Washington. [8, 11, 
			12] 
			Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM, 
			38(11):39-41. [4, 9] 
			Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learned 
			from the document understanding conference. In Proceedings of AAAI 2005, 
			Pittsburgh, USA. [7] 
			Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetorical 
			structure extraction. In Proceedings of Coling '94, pages 344-348, Morristown, 
			NJ, USA. [9] 
			Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedings 
			of the ACL'02 Workshop on Automatic Summarization, pages 1-8, Morristown, 
			NJ, USA. [7] 
			Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method for 
			automatic evaluation of machine translation. In Proceedings of ACL '02, pages 
			311-318, Morristown, NJ, USA. [24] 
			Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issue 
			on summarization. Computational Linguistics., 28(4):399-408. [1, 2] 
			Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarization 
			of multiple documents: sentence extraction, utility-based evaluation, and user 
			studies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages 
			21-30, Morristown, NJ, USA. [12, 16, 17] 
			30 
			Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza- 
			tion of multiple documents. Information Processing and Management 40 (2004), 
			40:919-938. [16, 17] 
			Radev, D. R. and McKeown, K. (1998). Generating natural language summaries 
			from multiple on-line sources. Computational Linguistics, 24(3):469-500. [12] 
			Salton, G. and Buckley, C. (1988). On the use of spreading activation methods in 
			automatic information. In Proceedings of SIGIR '88, pages 147-160, New York, 
			NY, USA. [15] 
			Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automatic 
			indexing. Communications of the ACM, 18:229-237. [20] 
			Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solving 
			hard satisfiability problems. In AAAI, pages 440-446. [11] 
			Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-document 
			summarization by combining RankNet and third-party sources. In Proceedings of 
			the EMNLP-CoNLL, pages 448-457. [7, 8] 
			Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract): 
			a statistical approach to generating highly condensed non-extractive summaries. 
			In Proceedings of SIGIR '99, pages 315-316, New York, NY, USA. [18] 
			31 
			1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- 
			nized speech for web presentation of large audio archive. In: 2012 35th International 
			Conference on Telecommunications and Signal Processing (TSP), pp. 441-445. 
			IEEE (2012) 
			2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over 
			a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, 
			A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134-138. Springer, Cham 
			(2016). https://doi.org/10.1007/978-3-319-41552-9 14 
			3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented 
			transcript based on word vector. In: LREC (2016) 
			4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull. 
			76(5), 378 (1971) 
			5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- 
			works. In: IEEE International Conference on Information Systems and Economic 
			Intelligence (2017) 
			6. Gonz 
			alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- 
			scripts informativeness study: an approach based on automatic summarization. In: 
			Conf 
			erence en Recherche d'Information et Applications (CORIA), Rennes, France, 
			May (2018) 
			7. Gonz 
			alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for 
			French with subword-level information vectors and convolutional neural networks. 
			arXiv preprint arXiv:1802.04559 (2018) 
			8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts. 
			In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium 
			ISCA Tutorial and Research Workshop (ITRW) (2000) 
			130 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni- 
			tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6), 
			82-97 (2012) 
			10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech 
			recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308-318 
			(2015) 
			11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com- 
			put. Linguist. 32(4), 485-525 (2006) 
			12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts 
			using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology 
			Workshop (SLT), pp. 433-440. IEEE (2016) 
			13. Kol 
			a 
			r, J., Lamel, L.: Development and evaluation of automatic punctuation for 
			French and english speech-to-text. In: Thirteenth Annual Conference of the Inter- 
			national Speech Communication Association (2012) 
			14. Kol 
			a 
			r, J.,  
			Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broad- 
			cast news speech. In: SPECOM 2004 (2004) 
			15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine 
			learning from imbalanced data for sentence boundary detection in speech. Comput. 
			Speech Lang. 20(4), 468-494 (2006) 
			16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran- 
			dom fields. In: Proceedings of the 2010 Conference on Empirical Methods in Natu- 
			ral Language Processing. pp. 177-186. Association for Computational Linguistics 
			(2010) 
			17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In: 
			Conference on Empirical Methods in Natural Language Processing (1996) 
			18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg- 
			mentation of speech for automatic summarization. In: 2006 IEEE International 
			Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I. 
			IEEE (2006) 
			19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro- 
			ceedings of the Fourth Conference on Applied Natural Language Processing, pp. 
			78-83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA, 
			USA (1994) 
			20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua- 
			tion. Comput. Linguist. 23(2), 241-267 (1997) 
			21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc. 
			R. Soc. Lond. 58, 240-242 (1895) 
			22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical 
			phrase-based translation. In: Proceedings of the International Workshop on Spoken 
			Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014) 
			23. Rott, M.,  
			Cerva, P.: Speech-to-text summarization using automatic phrase extrac- 
			tion from recognized text. In: Sojka, P., Hor 
			ak, A., Kope 
			cek, I., Pala, K. (eds.) TSD 
			2016. LNCS (LNAI), vol. 9924, pp. 101-108. Springer, Cham (2016). https://doi. 
			org/10.1007/978-3-319-45510-5 12 
			24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based 
			study. In: Proceedings of the Fourth International Conference on Spoken Language, 
			1996. ICSLP 1996, vol. 3, pp. 1868-1871. IEEE (1996) 
			25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In: 
			Proceedings of the sixth conference on Applied natural language processing, pp. 
			84-89. Association for Computational Linguistics (2000) 
			WiSeBE: Window-Based Sentence Boundary Evaluation 131 
			26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational 
			speech. In: Proceedings of the Fourth International Conference on Spoken Lan- 
			guage, 1996. ICSLP 1996, vol. 2, pp. 1005-1008. IEEE (1996) 
			27. Strassel, S.: Simple metadata annotation specification v5. 0, linguistic data consor- 
			tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE 
			V5.0.pdf 
			28. Tilk, O., Alum 
			ae, T.: Bidirectional recurrent neural network with attention mech- 
			anism for punctuation restoration. In: Interspeech 2016 (2016) 
			29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen- 
			tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704 
			(2017) 
			30. Ueffing, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation 
			prediction for spoken and written text. In: Interspeech, pp. 3097-3101 (2013) 
			31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disfluency removal for 
			improving spoken language translation. In: 2010 IEEE International Conference on 
			Acoustics Speech and Signal Processing (ICASSP), pp. 5214-5217. IEEE (2010) 
			32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network 
			approach for sentence boundary detection in broadcast news. In: Fifteenth Annual 
			Conference of the International Speech Communication Association (2014) 
			33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https:// 
			doi.org/10.1007/978-1-4471-5779-3 
			1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- 
			nized speech for web presentation of large audio archive. In: 2012 35th International 
			Conference on Telecommunications and Signal Processing (TSP), pp. 441-445. 
			IEEE (2012) 
			2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over 
			a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco, 
			A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134-138. Springer, Cham 
			(2016). https://doi.org/10.1007/978-3-319-41552-9 14 
			3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented 
			transcript based on word vector. In: LREC (2016) 
			4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull. 
			76(5), 378 (1971) 
			5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- 
			works. In: IEEE International Conference on Information Systems and Economic 
			Intelligence (2017) 
			6. Gonz 
			alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- 
			scripts informativeness study: an approach based on automatic summarization. In: 
			Conf 
			erence en Recherche d'Information et Applications (CORIA), Rennes, France, 
			May (2018) 
			7. Gonz 
			alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for 
			French with subword-level information vectors and convolutional neural networks. 
			arXiv preprint arXiv:1802.04559 (2018) 
			8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts. 
			In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium 
			ISCA Tutorial and Research Workshop (ITRW) (2000) 
			130 C.-E. Gonz 
			alez-Gallardo and J.-M. Torres-Moreno 
			9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni- 
			tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6), 
			82-97 (2012) 
			10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech 
			recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308-318 
			(2015) 
			11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com- 
			put. Linguist. 32(4), 485-525 (2006) 
			12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts 
			using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology 
			Workshop (SLT), pp. 433-440. IEEE (2016) 
			13. Kol 
			a 
			r, J., Lamel, L.: Development and evaluation of automatic punctuation for 
			French and english speech-to-text. In: Thirteenth Annual Conference of the Inter- 
			national Speech Communication Association (2012) 
			14. Kol 
			a 
			r, J.,  
			Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broad- 
			cast news speech. In: SPECOM 2004 (2004) 
			15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine 
			learning from imbalanced data for sentence boundary detection in speech. Comput. 
			Speech Lang. 20(4), 468-494 (2006) 
			16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran- 
			dom fields. In: Proceedings of the 2010 Conference on Empirical Methods in Natu- 
			ral Language Processing. pp. 177-186. Association for Computational Linguistics 
			(2010) 
			17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In: 
			Conference on Empirical Methods in Natural Language Processing (1996) 
			18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg- 
			mentation of speech for automatic summarization. In: 2006 IEEE International 
			Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I. 
			IEEE (2006) 
			19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro- 
			ceedings of the Fourth Conference on Applied Natural Language Processing, pp. 
			78-83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA, 
			USA (1994) 
			20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua- 
			tion. Comput. Linguist. 23(2), 241-267 (1997) 
			21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc. 
			R. Soc. Lond. 58, 240-242 (1895) 
			22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical 
			phrase-based translation. In: Proceedings of the International Workshop on Spoken 
			Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014) 
			23. Rott, M.,  
			Cerva, P.: Speech-to-text summarization using automatic phrase extrac- 
			tion from recognized text. In: Sojka, P., Hor 
			ak, A., Kope 
			cek, I., Pala, K. (eds.) TSD 
			2016. LNCS (LNAI), vol. 9924, pp. 101-108. Springer, Cham (2016). https://doi. 
			org/10.1007/978-3-319-45510-5 12 
			24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based 
			study. In: Proceedings of the Fourth International Conference on Spoken Language, 
			1996. ICSLP 1996, vol. 3, pp. 1868-1871. IEEE (1996) 
			25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In: 
			Proceedings of the sixth conference on Applied natural language processing, pp. 
			84-89. Association for Computational Linguistics (2000) 
			WiSeBE: Window-Based Sentence Boundary Evaluation 131 
			26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational 
			speech. In: Proceedings of the Fourth International Conference on Spoken Lan- 
			guage, 1996. ICSLP 1996, vol. 2, pp. 1005-1008. IEEE (1996) 
			27. Strassel, S.: Simple metadata annotation specification v5. 0, linguistic data consor- 
			tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE 
			V5.0.pdf 
			28. Tilk, O., Alum 
			ae, T.: Bidirectional recurrent neural network with attention mech- 
			anism for punctuation restoration. In: Interspeech 2016 (2016) 
			29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen- 
			tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704 
			(2017) 
			30. Ueffing, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation 
			prediction for spoken and written text. In: Interspeech, pp. 3097-3101 (2013) 
			31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disfluency removal for 
			improving spoken language translation. In: 2010 IEEE International Conference on 
			Acoustics Speech and Signal Processing (ICASSP), pp. 5214-5217. IEEE (2010) 
			32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network 
			approach for sentence boundary detection in broadcast news. In: Fifteenth Annual 
			Conference of the International Speech Communication Association (2014) 
			33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https:// 
			doi.org/10.1007/978-1-4471-5779-3 
			Ron Artstein, and Massimo Poesio. 2008. Survey 
			Article: Inter-Coder Agreement for Computational 
			Linguistics. Computational Linguistics, 34(4):555- 
			596. 
			Nadjet Bouayad-Agha, Leo Wanner, and Daniel 
			Nicklass. 2006. Discourse structuring of dynamic 
			content. Procesamiento del lenguaje natural, 37:207- 
			213. 
			M. Teresa Cabre (1999). La terminologia: 
			representacion y comunicacion. Barcelona: IULA- 
			UPF. 
			Lynn Carlson and Daniel Marcu. 2001. Discourse 
			Tagging Reference Manual. ISI Technical Report 
			ISITR-545. Los Angeles: University of Southern 
			California. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002a. RST Discourse Treebank. 
			Pennsylvania: Linguistic Data Consortium. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002b. Building a Discourse-Tagged 
			Corpus in the Framework of Rhetorical Structure 
			Theory. In Proceedings of the 2nd SIGDIAL 
			Workshop on Discourse and Dialogue, Eurospeech 
			2001. 
			Jacob Cohen. 1960. A coefficient of agreement for 
			nominal scales. Educational and Psychological 
			Measurement, 20(1):37-46 
			Iria da Cunha, Eric SanJuan, Juan-Manuel Torres- 
			Moreno, Marina Lloberes, and Irene Castellon. 2010. 
			Discourse Segmentation for Spanish based on 
			Shallow Parsing. Lecture Notes in Computer 
			Science, 6437:13-23. 
			Iria da Cunha, and Mikel Iruskieta. 2010. Comparing 
			rhetorical structures of different languages: The 
			influence of translation strategies. Discourse Studies, 
			12(5):563-598. 
			Iria da Cunha, Leo Wanner, and M. Teresa Cabre. 2007. 
			Summarization of specialized discourse: The case of 
			medical articles in Spanish. Terminology, 13(2):249- 
			286. 
			Dmitriy Dligach, Rodney D. Nielsen, and Martha 
			Palmer. 2010. To Annotate More Accurately or to 
			Annotate More. In Proceedings of the 4th Linguistic 
			Annotation Workshop (LAW-IV). 48th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Joseph L. Fleis. 1971. Measuring nominal scale 
			agreement among many raters. Psychological 
			Bulletin, 76(5):378-382. 
			Eduard Hovy. 2010. Annotation. A Tutorial. Presented 
			at the 48th Annual Meeting of the Association for 
			Computational Linguistics. 
			Nancy Ide and Pustejovsky, J. (2010). What Does 
			Interoperability Mean, anyway? Toward an 
			Operational Definition of Interoperability. In 
			Proceedings of the Second International Conference 
			on Global Interoperability for Language Resources 
			(ICGL 2010). 
			William C. Mann, and Sandra A. Thompson. 1988. 
			Rhetorical structure theory: Toward a functional 
			theory of text organization. Text, 8(3):243-281. 
			Daniel Marcu. 2000. The Theory and Practice of 
			Discourse Parsing Summarization. Massachusetts: 
			Institute of Technology. 
			Mitchell P. Marcus, Beatrice Santorini, Mary A. 
			Marcinkiewicz. 1993. Building a large annotated 
			corpus of English: the Penn Treenbank. 
			Computational Linguistics, 19(2):313-330. 
			Michael O'Donnell. 2000. RSTTOOL 2.4 - A markup 
			tool for rhetorical structure theory. In Proceedings of 
			the International Natural Language Generation 
			Conference. 253-256. 
			Martha Palmer, and Nianwen Xue. 2010. Linguistic 
			Annotation. Handbook of Computational Linguistics 
			and Natural Language Processing. 
			Martha Palmer, Randee Tangi, Stephanie Strassel, 
			Christiane Fellbaum, and Eduard Hovy (on-line). 
			Historical Development and Future Directions in 
			Data Resource Development. MINDS report. 
			http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf 
			Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
			Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
			OntoNotes: A Unified Relational Semantic 
			Representation. In Proceedings of the First IEEE 
			International Conference on Semantic Computing 
			(ICSC-07). 
			Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
			Miltsakaki, Livio Robaldo, Aravind Joshi, and 
			Bonnie Webber. 2008. The Penn Discourse Treebank 
			2.0. In Proceedings of the 6th International 
			Conference on Language Resources and Evaluation 
			(LREC 2008). 
			David Reitter, and Mandred Stede. 2003. Step by step: 
			underspecified markup in incremental rhetorical 
			analysis. In Proceedings of the 4th International 
			9 
			Workshop on Linguistically Interpreted Corpora 
			(LINC-03). 
			Magdalena Romera. 2004. Discourse Functional Units: 
			The Expression of Coherence Relations in Spoken 
			Spanish. Munich: LINCOM. 
			Thiago Alexandre Salgueiro Pardo, and Lucia Helena 
			Machado Rino. 2001. A summary planner based on a 
			three-level discourse model. In Proceedings of 
			Natural Language Processing Pacific Rim 
			Symposium. 533-538. 
			Thiago Alexandre Salgueiro Pardo, Maria das Gracas 
			Volpe Nunes, and Lucia Helena Machado Rino. 
			2008. DiZer: An Automatic Discourse Analyzer for 
			Brazilian Portuguese. Lecture Notes in Artificial 
			Intelligence, 3171:224-234. 
			Thiago Alexandre Salgueiro Pardo, and Eloize Rossi 
			Marques Seno. 2005. Rhetalho: um corpus de 
			referencia anotado retoricamente. In Anais do V 
			Encontro de Corpora. Sao Carlos-SP, Brasil. 
			Gerardo Sierra. 2008. Diseno de corpus textuales para 
			fines linguisticos. In Proceedings of the IX Encuentro 
			Internacional de Linguistica en el Noroeste 2. 445- 
			462. 
			Manfred Stede. 2004. The Potsdam commentary corpus. 
			In Proceedings of the Workshop on Discourse 
			Annotation, 42nd Meeting of the Association for 
			Computational Linguistics. 
			Maite Taboada. 2004. Building Coherence and 
			Cohesion: Task-Oriented Dialogue in English and 
			Spanish. Amsterdam/Philadelphia: John Benjamins. 
			Maite Taboada, and Jan Renkema. 2008. Discourse 
			Relations Reference Corpus [Corpus]. Simon Fraser 
			University and Tilburg University. 
			http://www.sfu.ca/rst/06tools/discourse_relations_cor 
			pus.html. 
			Maite Taboada, and William C. Mann. 2006a. 
			Rhetorical Structure Theory: Looking Back and 
			Moving Ahead. Discourse Studies, 8(3):423-459. 
			Maite Taboada, and William C. Mann. 2006b. 
			Applications of Rhetorical Structure Theory. 
			Discourse Studies, 8(4):567-588. 
			Milan Tofiloski, Julian Brooke, and Maite Taboada. 
			2009. A Syntactic and Lexical-Based Discourse 
			Segmenter. In Proceedings of the 47th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large 
			a Corpus Do We Need: Statistical Method Versus 
			Rule-based Method. In Proceedings of the Seventh 
			conference on International Language Resources and 
			Evaluation (LREC'10). 
			10 
			Ron Artstein, and Massimo Poesio. 2008. Survey 
			Article: Inter-Coder Agreement for Computational 
			Linguistics. Computational Linguistics, 34(4):555- 
			596. 
			Nadjet Bouayad-Agha, Leo Wanner, and Daniel 
			Nicklass. 2006. Discourse structuring of dynamic 
			content. Procesamiento del lenguaje natural, 37:207- 
			213. 
			M. Teresa Cabre (1999). La terminologia: 
			representacion y comunicacion. Barcelona: IULA- 
			UPF. 
			Lynn Carlson and Daniel Marcu. 2001. Discourse 
			Tagging Reference Manual. ISI Technical Report 
			ISITR-545. Los Angeles: University of Southern 
			California. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002a. RST Discourse Treebank. 
			Pennsylvania: Linguistic Data Consortium. 
			Lynn Carlson, Daniel Marcu, and Mary Ellen 
			Okurowski. 2002b. Building a Discourse-Tagged 
			Corpus in the Framework of Rhetorical Structure 
			Theory. In Proceedings of the 2nd SIGDIAL 
			Workshop on Discourse and Dialogue, Eurospeech 
			2001. 
			Jacob Cohen. 1960. A coefficient of agreement for 
			nominal scales. Educational and Psychological 
			Measurement, 20(1):37-46 
			Iria da Cunha, Eric SanJuan, Juan-Manuel Torres- 
			Moreno, Marina Lloberes, and Irene Castellon. 2010. 
			Discourse Segmentation for Spanish based on 
			Shallow Parsing. Lecture Notes in Computer 
			Science, 6437:13-23. 
			Iria da Cunha, and Mikel Iruskieta. 2010. Comparing 
			rhetorical structures of different languages: The 
			influence of translation strategies. Discourse Studies, 
			12(5):563-598. 
			Iria da Cunha, Leo Wanner, and M. Teresa Cabre. 2007. 
			Summarization of specialized discourse: The case of 
			medical articles in Spanish. Terminology, 13(2):249- 
			286. 
			Dmitriy Dligach, Rodney D. Nielsen, and Martha 
			Palmer. 2010. To Annotate More Accurately or to 
			Annotate More. In Proceedings of the 4th Linguistic 
			Annotation Workshop (LAW-IV). 48th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Joseph L. Fleis. 1971. Measuring nominal scale 
			agreement among many raters. Psychological 
			Bulletin, 76(5):378-382. 
			Eduard Hovy. 2010. Annotation. A Tutorial. Presented 
			at the 48th Annual Meeting of the Association for 
			Computational Linguistics. 
			Nancy Ide and Pustejovsky, J. (2010). What Does 
			Interoperability Mean, anyway? Toward an 
			Operational Definition of Interoperability. In 
			Proceedings of the Second International Conference 
			on Global Interoperability for Language Resources 
			(ICGL 2010). 
			William C. Mann, and Sandra A. Thompson. 1988. 
			Rhetorical structure theory: Toward a functional 
			theory of text organization. Text, 8(3):243-281. 
			Daniel Marcu. 2000. The Theory and Practice of 
			Discourse Parsing Summarization. Massachusetts: 
			Institute of Technology. 
			Mitchell P. Marcus, Beatrice Santorini, Mary A. 
			Marcinkiewicz. 1993. Building a large annotated 
			corpus of English: the Penn Treenbank. 
			Computational Linguistics, 19(2):313-330. 
			Michael O'Donnell. 2000. RSTTOOL 2.4 - A markup 
			tool for rhetorical structure theory. In Proceedings of 
			the International Natural Language Generation 
			Conference. 253-256. 
			Martha Palmer, and Nianwen Xue. 2010. Linguistic 
			Annotation. Handbook of Computational Linguistics 
			and Natural Language Processing. 
			Martha Palmer, Randee Tangi, Stephanie Strassel, 
			Christiane Fellbaum, and Eduard Hovy (on-line). 
			Historical Development and Future Directions in 
			Data Resource Development. MINDS report. 
			http://www-nlpir.nist.gov/MINDS/FINAL/data.web.pdf 
			Sameer Pradhan, Eduard Hovy, Mitch Marcus, Martha 
			Palmer, Lance Ramshaw, Ralph Weischedel. 2007. 
			OntoNotes: A Unified Relational Semantic 
			Representation. In Proceedings of the First IEEE 
			International Conference on Semantic Computing 
			(ICSC-07). 
			Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni 
			Miltsakaki, Livio Robaldo, Aravind Joshi, and 
			Bonnie Webber. 2008. The Penn Discourse Treebank 
			2.0. In Proceedings of the 6th International 
			Conference on Language Resources and Evaluation 
			(LREC 2008). 
			David Reitter, and Mandred Stede. 2003. Step by step: 
			underspecified markup in incremental rhetorical 
			analysis. In Proceedings of the 4th International 
			9 
			Workshop on Linguistically Interpreted Corpora 
			(LINC-03). 
			Magdalena Romera. 2004. Discourse Functional Units: 
			The Expression of Coherence Relations in Spoken 
			Spanish. Munich: LINCOM. 
			Thiago Alexandre Salgueiro Pardo, and Lucia Helena 
			Machado Rino. 2001. A summary planner based on a 
			three-level discourse model. In Proceedings of 
			Natural Language Processing Pacific Rim 
			Symposium. 533-538. 
			Thiago Alexandre Salgueiro Pardo, Maria das Gracas 
			Volpe Nunes, and Lucia Helena Machado Rino. 
			2008. DiZer: An Automatic Discourse Analyzer for 
			Brazilian Portuguese. Lecture Notes in Artificial 
			Intelligence, 3171:224-234. 
			Thiago Alexandre Salgueiro Pardo, and Eloize Rossi 
			Marques Seno. 2005. Rhetalho: um corpus de 
			referencia anotado retoricamente. In Anais do V 
			Encontro de Corpora. Sao Carlos-SP, Brasil. 
			Gerardo Sierra. 2008. Diseno de corpus textuales para 
			fines linguisticos. In Proceedings of the IX Encuentro 
			Internacional de Linguistica en el Noroeste 2. 445- 
			462. 
			Manfred Stede. 2004. The Potsdam commentary corpus. 
			In Proceedings of the Workshop on Discourse 
			Annotation, 42nd Meeting of the Association for 
			Computational Linguistics. 
			Maite Taboada. 2004. Building Coherence and 
			Cohesion: Task-Oriented Dialogue in English and 
			Spanish. Amsterdam/Philadelphia: John Benjamins. 
			Maite Taboada, and Jan Renkema. 2008. Discourse 
			Relations Reference Corpus [Corpus]. Simon Fraser 
			University and Tilburg University. 
			http://www.sfu.ca/rst/06tools/discourse_relations_cor 
			pus.html. 
			Maite Taboada, and William C. Mann. 2006a. 
			Rhetorical Structure Theory: Looking Back and 
			Moving Ahead. Discourse Studies, 8(3):423-459. 
			Maite Taboada, and William C. Mann. 2006b. 
			Applications of Rhetorical Structure Theory. 
			Discourse Studies, 8(4):567-588. 
			Milan Tofiloski, Julian Brooke, and Maite Taboada. 
			2009. A Syntactic and Lexical-Based Discourse 
			Segmenter. In Proceedings of the 47th Annual 
			Meeting of the Association for Computational 
			Linguistics. 
			Hai Zhao, Yan Song, and Chunyu Kit. 2010. How Large 
			a Corpus Do We Need: Statistical Method Versus 
			Rule-based Method. In Proceedings of the Seventh 
			conference on International Language Resources and 
			Evaluation (LREC'10). 
			10 
			upon work supported by the National Science Foun- 
			dation under Grant No. IRI 96-19124 and IRI 
			96-18797. Any opinions, findings, and conclusions 
			or recommendations expressed in this material are 
			those of the authors and do not necessarily reflect 
			the views of the National Science Foundation. 
			References 
			ANSI. 1997. Guidelines for abstracts. Technical Re- 
			port Z39.14-1997, NISO Press, Bethesda, Mary- 
			land. 
			L. Baum. 1972. An inequality and associated max- 
			imization technique in statistical estimation of 
			probabilistic functions of a markov process. In- 
			equalities, (3):1-8. 
			Edward T. Cremmins. 1982. The Art of Abstracting. 
			ISI Press, Philadelphia. 
			Brigitte Endres-Niggemeyer, Kai Haseloh, Jens 
			Mfiller, Simone Peist, Irene Santini de Sigel, 
			Alexander Sigel, Elisabeth Wansorra, Jan 
			Wheeler, and Brfinja Wollny. 1998. Summarizing 
			Information. Springer, Berlin. 
			Raya Fidel. 1986. Writing abstracts for free-text 
			searching. Journal of Documentation, 42(1):11- 
			21, March. 
			Hongyan Jing and Kathleen R. McKeown. 1998. 
			Combining multiple, large-scale resources in a 
			reusable lexicon for natural language generation. 
			In Proceedings of the 36th Annual Meeting of the 
			Association for Computational Linguistics and the 
			17th International Conference on Computational 
			Linguistics, volume 1, pages 607-613, Universit6 
			de Montreal, Quebec, Canada, August. 
			Hongyan Jing and Kathleen R. McKeown. 1999. 
			The decomposition of human-written summary 
			sentences. In Proceedings of the P2nd In- 
			ternational ACM SIGIR Conference on Re- 
			search and Development in Information Re- 
			trieval(SIGIR'99), pages 129-136, University of 
			Berkeley, CA, August. 
			Hongyan Jing. 2000. Sentence reduction for au- 
			tomatic text summarization. In Proceedings of 
			ANLP 2000. 
			Aravind.K. Joshi. 1987. Introduction to tree- 
			adjoining grammars. In A. Manaster-Ramis, ed- 
			itor, Mathematics of Language. John Benjamins, 
			Amsterdam. 
			Inderjeet Mani, David House, Gary Klein, Lynette 
			Hirschman, Leo Obrst, Therese Firmin, Michael 
			Chrzanowski, and Beth Sundheim. 1998. The 
			TIPSTER SUMMAC text summarization eval- 
			uation final report. Technical Report MTR 
			98W0000138, The MITRE Corporation. 
			Inderjeet Mani, Barbara Gates, and Erie Bloedorn. 
			1999. Improving summaries by revising them. In 
			Proceedings of the 37th Annual Meeting of the As- 
			sociation for Computational Linguistics(A CL '99), 
			pages 558-565, University of Maryland, Mary- 
			land, June. 
			Michael MeCord, 1990. English Slot Grammar. 
			IBM. 
			Samuel Thurber, editor. 1924. Prgcis Writing for 
			American Schools. The Atlantic Monthly Press, 
			INC., Boston. 
			A.J. Viterbi. 1967. Error bounds for convolution 
			codes and an asymptotically optimal decoding al- 
			gorithm. IEEE Transactions on Information The- 
			ory, 13:260-269. 
			185 
			upon work supported by the National Science Foun- 
			dation under Grant No. IRI 96-19124 and IRI 
			96-18797. Any opinions, findings, and conclusions 
			or recommendations expressed in this material are 
			those of the authors and do not necessarily reflect 
			the views of the National Science Foundation. 
			References 
			ANSI. 1997. Guidelines for abstracts. Technical Re- 
			port Z39.14-1997, NISO Press, Bethesda, Mary- 
			land. 
			L. Baum. 1972. An inequality and associated max- 
			imization technique in statistical estimation of 
			probabilistic functions of a markov process. In- 
			equalities, (3):1-8. 
			Edward T. Cremmins. 1982. The Art of Abstracting. 
			ISI Press, Philadelphia. 
			Brigitte Endres-Niggemeyer, Kai Haseloh, Jens 
			Mfiller, Simone Peist, Irene Santini de Sigel, 
			Alexander Sigel, Elisabeth Wansorra, Jan 
			Wheeler, and Brfinja Wollny. 1998. Summarizing 
			Information. Springer, Berlin. 
			Raya Fidel. 1986. Writing abstracts for free-text 
			searching. Journal of Documentation, 42(1):11- 
			21, March. 
			Hongyan Jing and Kathleen R. McKeown. 1998. 
			Combining multiple, large-scale resources in a 
			reusable lexicon for natural language generation. 
			In Proceedings of the 36th Annual Meeting of the 
			Association for Computational Linguistics and the 
			17th International Conference on Computational 
			Linguistics, volume 1, pages 607-613, Universit6 
			de Montreal, Quebec, Canada, August. 
			Hongyan Jing and Kathleen R. McKeown. 1999. 
			The decomposition of human-written summary 
			sentences. In Proceedings of the P2nd In- 
			ternational ACM SIGIR Conference on Re- 
			search and Development in Information Re- 
			trieval(SIGIR'99), pages 129-136, University of 
			Berkeley, CA, August. 
			Hongyan Jing. 2000. Sentence reduction for au- 
			tomatic text summarization. In Proceedings of 
			ANLP 2000. 
			Aravind.K. Joshi. 1987. Introduction to tree- 
			adjoining grammars. In A. Manaster-Ramis, ed- 
			itor, Mathematics of Language. John Benjamins, 
			Amsterdam. 
			Inderjeet Mani, David House, Gary Klein, Lynette 
			Hirschman, Leo Obrst, Therese Firmin, Michael 
			Chrzanowski, and Beth Sundheim. 1998. The 
			TIPSTER SUMMAC text summarization eval- 
			uation final report. Technical Report MTR 
			98W0000138, The MITRE Corporation. 
			Inderjeet Mani, Barbara Gates, and Erie Bloedorn. 
			1999. Improving summaries by revising them. In 
			Proceedings of the 37th Annual Meeting of the As- 
			sociation for Computational Linguistics(A CL '99), 
			pages 558-565, University of Maryland, Mary- 
			land, June. 
			Michael MeCord, 1990. English Slot Grammar. 
			IBM. 
			Samuel Thurber, editor. 1924. Prgcis Writing for 
			American Schools. The Atlantic Monthly Press, 
			INC., Boston. 
			A.J. Viterbi. 1967. Error bounds for convolution 
			codes and an asymptotically optimal decoding al- 
			gorithm. IEEE Transactions on Information The- 
			ory, 13:260-269. 
			185 
			[1] S. Meignier and T. Merlin, "Lium spkdiarization: an open source toolkit 
			for diarization," in in CMU SPUD Workshop, 2010. 
			[2] P. Pauwels and W. Terkaj, "Express to owl for construction industry: 
			Towards a recommendable and usable ifcowl ontology," Automation in 
			Construction, vol. 63, pp. 100-133, 03 2016. 
			[3] H. Cunningham, "Gate, a general architecture for text engineering," in 
			Computers and the Humanities, vol. 36, 2002, pp. 223-254. 
			[4] P. Cimiano and J. V 
			olker, "text2onto," in International conference on 
			application of natural language to information systems. Springer, 2005, 
			pp. 227-238. 
			[5] P. Kluegl, M. Toepfer, P.-D. Beck, G. Fette, and F. Puppe, "Uima ruta: 
			Rapid development of rule-based information extraction applications," 
			Natural Language Engineering, vol. 22, no. 1, p. 1-40, 2016. 
			[6] M. Roche and Y. Kodratoff, "Exit: Un syst 
			eme it 
			eratif pour l'extraction 
			de la terminologie du domaine  
			a partir de corpus sp 
			ecialis 
			es," in 
			Proceedings of JADT 4, 2004, pp. 946-956. 
			[7] B. Bi 
			ebow, S. Szulman, and A. J. B. Cl 
			ement, "Terminae: A linguistics- 
			based tool for the building of a domain ontology," in Knowledge 
			Acquisition, Modeling and Management, D. Fensel and R. Studer, Eds., 
			1999, pp. 49-66. 
			[8] P. Drouin, "Term extraction using non technical corpora as point of lever- 
			age," in John Benjamins Publishing Company: Amsterdam/Philadelphia, 
			n. Terminology, vol. 9, Ed., 2003, pp. 99-115. 
			[9] M. F. M. Chowdhury, A. M. Gliozzo, and S. M. Trewin, "Domain- 
			specific terminology extraction by boosting frequency metrics," Sep. 27 
			2018, uS Patent App. 15/469,766. 
			[10] G. Bouma, "Normalized (pointwise) mutual information in collocation 
			extraction," Proceedings of GSCL, 2009. 
			[11] Y. Bestgen, "Evaluation de mesures d'association pour les bigrammes et 
			les trigrammes au moyen du test exact de fisher," Proceedings of TALN 
			2017, pp. 10-19, 2017. 
			[12] A. L. Meyers, Y. He, Z. Glass, J. Ortega, S. Liao, A. Grieve-Smith, 
			R. Grishman, and O. Babko-Malaya, "The termolator: Terminology 
			recognition based on chunking, statistical and search-based scores," 
			Frontiers in Research Metrics and Analytics, vol. 3, p. 19, 2018. 
			[13] L. Gillam, M. Tariq, and K. Ahmad, "Terminology and the construction 
			of ontology," TERMINOLOGY, vol. 11, pp. 55-81, 2005. 
			[14] A. Panchenko, S. Faralli, E. Ruppert, S. Remus, H. Naets, C. Fairon, S. P. 
			Ponzetto, and C. Biemann, "TAXI at semeval-2016 task 13: a taxonomy 
			induction method based on lexico-syntactic patterns, substrings and 
			focused crawling," in Proceedings of the 10th International Workshop 
			on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, 
			USA, June 16-17, 2016, 2016, pp. 1320-1327. 
			[15] E. Lefever, L. Macken, and V. Hoste, "Language-independent 
			bilingual terminology extraction from a multilingual parallel corpus," 
			in Proceedings of the 12th Conference of the European Chapter 
			of the Association for Computational Linguistics, ser. EACL '09. 
			Stroudsburg, PA, USA: Association for Computational Linguistics, 
			2009, pp. 496-504. [Online]. Available: http://dl.acm.org/citation.cfm? 
			id=1609067.1609122 
			[16] L. Macken, E. Lefever, and V. Hoste, "Texsis: bilingual terminology 
			extraction from parallel corpora using chunk-based alignment," 
			Terminology, vol. 19, no. 1, pp. 1-30, 2013. [Online]. Available: 
			http://dx.doi.org/10.1075/term.19.1.01mac 
			[17] G. Dias and H.-J. Kaalep, "Automatic extraction of multiword units 
			for estonian : Phrasal verbs," in Languages in Development, 2003, p. 
			41:81-91. 
			[18] B. Daille, "Study and implementation of combined techniques for 
			automatic extraction of terminology," The Balancing Act: Combining 
			Symbolic and Statistical Approaches to Language, 12 2002. 
			[19] C. Lang, R. Schneider, and K. Suchowolec, "Extracting specialized 
			terminology from linguistic corpora," GRAMMAR AND CORPORA, p. 
			425, 2018. 
			[20] E. Amjadian, D. Inkpen, T. S. Paribakht, and F. Faez, "Local-global 
			vectors to improve unigram terminology extraction," in Proceedings of 
			the 5th International Workshop on Computational Terminology, 2016. 
			[21] G. Wohlgenannt and F. Minic, "Using word2vec to build a simple 
			ontology learning system." in International Semantic Web Conference 
			(Posters & Demos), 2016. 
			[22] H. Schmid, "Probabilistic part-of-speech tagging using decision trees," 
			1994. 
			[23] E. Altman, "Financial ratios, discriminant analysis and the predic- 
			tion of corporate bankruptcy." in The Journal of Finance, 23(4). 
			doi:10.2307/2978933, 1968, pp. 589-609. 
			[24] J. Cohen, "A Coefficient of Agreement for Nominal Scales," Educational 
			and Psychological Measurement, vol. 20, no. 1, p. 37, 1960. 
			[25] M. L. McHugh, "Interrater reliability: the kappa statistic," in Biochemia 
			medica, 2012. 
			[26] M. Apidianaki, S. M. Mohammad, J. May, E. Shutova, S. Bethard, 
			and M. Carpuat, "Proceedings of the 12th international workshop on 
			semantic evaluation," in Proceedings of The 12th International Workshop 
			on Semantic Evaluation. Association for Computational Linguistics, 
			2018. [Online]. Available: http://aclweb.org/anthology/S18-1000 
			[1] S. Meignier and T. Merlin, "Lium spkdiarization: an open source toolkit 
			for diarization," in in CMU SPUD Workshop, 2010. 
			[2] P. Pauwels and W. Terkaj, "Express to owl for construction industry: 
			Towards a recommendable and usable ifcowl ontology," Automation in 
			Construction, vol. 63, pp. 100-133, 03 2016. 
			[3] H. Cunningham, "Gate, a general architecture for text engineering," in 
			Computers and the Humanities, vol. 36, 2002, pp. 223-254. 
			[4] P. Cimiano and J. V 
			olker, "text2onto," in International conference on 
			application of natural language to information systems. Springer, 2005, 
			pp. 227-238. 
			[5] P. Kluegl, M. Toepfer, P.-D. Beck, G. Fette, and F. Puppe, "Uima ruta: 
			Rapid development of rule-based information extraction applications," 
			Natural Language Engineering, vol. 22, no. 1, p. 1-40, 2016. 
			[6] M. Roche and Y. Kodratoff, "Exit: Un syst 
			eme it 
			eratif pour l'extraction 
			de la terminologie du domaine  
			a partir de corpus sp 
			ecialis 
			es," in 
			Proceedings of JADT 4, 2004, pp. 946-956. 
			[7] B. Bi 
			ebow, S. Szulman, and A. J. B. Cl 
			ement, "Terminae: A linguistics- 
			based tool for the building of a domain ontology," in Knowledge 
			Acquisition, Modeling and Management, D. Fensel and R. Studer, Eds., 
			1999, pp. 49-66. 
			[8] P. Drouin, "Term extraction using non technical corpora as point of lever- 
			age," in John Benjamins Publishing Company: Amsterdam/Philadelphia, 
			n. Terminology, vol. 9, Ed., 2003, pp. 99-115. 
			[9] M. F. M. Chowdhury, A. M. Gliozzo, and S. M. Trewin, "Domain- 
			specific terminology extraction by boosting frequency metrics," Sep. 27 
			2018, uS Patent App. 15/469,766. 
			[10] G. Bouma, "Normalized (pointwise) mutual information in collocation 
			extraction," Proceedings of GSCL, 2009. 
			[11] Y. Bestgen, "Evaluation de mesures d'association pour les bigrammes et 
			les trigrammes au moyen du test exact de fisher," Proceedings of TALN 
			2017, pp. 10-19, 2017. 
			[12] A. L. Meyers, Y. He, Z. Glass, J. Ortega, S. Liao, A. Grieve-Smith, 
			R. Grishman, and O. Babko-Malaya, "The termolator: Terminology 
			recognition based on chunking, statistical and search-based scores," 
			Frontiers in Research Metrics and Analytics, vol. 3, p. 19, 2018. 
			[13] L. Gillam, M. Tariq, and K. Ahmad, "Terminology and the construction 
			of ontology," TERMINOLOGY, vol. 11, pp. 55-81, 2005. 
			[14] A. Panchenko, S. Faralli, E. Ruppert, S. Remus, H. Naets, C. Fairon, S. P. 
			Ponzetto, and C. Biemann, "TAXI at semeval-2016 task 13: a taxonomy 
			induction method based on lexico-syntactic patterns, substrings and 
			focused crawling," in Proceedings of the 10th International Workshop 
			on Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA, 
			USA, June 16-17, 2016, 2016, pp. 1320-1327. 
			[15] E. Lefever, L. Macken, and V. Hoste, "Language-independent 
			bilingual terminology extraction from a multilingual parallel corpus," 
			in Proceedings of the 12th Conference of the European Chapter 
			of the Association for Computational Linguistics, ser. EACL '09. 
			Stroudsburg, PA, USA: Association for Computational Linguistics, 
			2009, pp. 496-504. [Online]. Available: http://dl.acm.org/citation.cfm? 
			id=1609067.1609122 
			[16] L. Macken, E. Lefever, and V. Hoste, "Texsis: bilingual terminology 
			extraction from parallel corpora using chunk-based alignment," 
			Terminology, vol. 19, no. 1, pp. 1-30, 2013. [Online]. Available: 
			http://dx.doi.org/10.1075/term.19.1.01mac 
			[17] G. Dias and H.-J. Kaalep, "Automatic extraction of multiword units 
			for estonian : Phrasal verbs," in Languages in Development, 2003, p. 
			41:81-91. 
			[18] B. Daille, "Study and implementation of combined techniques for 
			automatic extraction of terminology," The Balancing Act: Combining 
			Symbolic and Statistical Approaches to Language, 12 2002. 
			[19] C. Lang, R. Schneider, and K. Suchowolec, "Extracting specialized 
			terminology from linguistic corpora," GRAMMAR AND CORPORA, p. 
			425, 2018. 
			[20] E. Amjadian, D. Inkpen, T. S. Paribakht, and F. Faez, "Local-global 
			vectors to improve unigram terminology extraction," in Proceedings of 
			the 5th International Workshop on Computational Terminology, 2016. 
			[21] G. Wohlgenannt and F. Minic, "Using word2vec to build a simple 
			ontology learning system." in International Semantic Web Conference 
			(Posters & Demos), 2016. 
			[22] H. Schmid, "Probabilistic part-of-speech tagging using decision trees," 
			1994. 
			[23] E. Altman, "Financial ratios, discriminant analysis and the predic- 
			tion of corporate bankruptcy." in The Journal of Finance, 23(4). 
			doi:10.2307/2978933, 1968, pp. 589-609. 
			[24] J. Cohen, "A Coefficient of Agreement for Nominal Scales," Educational 
			and Psychological Measurement, vol. 20, no. 1, p. 37, 1960. 
			[25] M. L. McHugh, "Interrater reliability: the kappa statistic," in Biochemia 
			medica, 2012. 
			[26] M. Apidianaki, S. M. Mohammad, J. May, E. Shutova, S. Bethard, 
			and M. Carpuat, "Proceedings of the 12th international workshop on 
			semantic evaluation," in Proceedings of The 12th International Workshop 
			on Semantic Evaluation. Association for Computational Linguistics, 
			2018. [Online]. Available: http://aclweb.org/anthology/S18-1000 
			[1] D. Fassin, "Et la souffrance devint sociale," in Critique. 680(1), 2004, 
			pp. 16-29. 
			[2] ----, "Souffrir par le social, gouverner par l' 
			ecoute," in Politix. 73(1), 
			2006, pp. 137-157. 
			[3] MacQueen, J., "Some methods for classification and analysis of multi- 
			variate observations," in Proceedings of the Fifth Berkeley Symposium 
			on Mathematical Statistics and Probability, Vol. 1: Statistics. USA: 
			University of California Press, 1967, pp. 281-297. 
			[4] D. Arthur and S. Vassilvitskii, "K-means++: The advantages of careful 
			seeding," Proceedings of the Eighteenth Annual ACM-SIAM Symposium 
			on Discrete Algorithms, pp. 1027-1035, 2007. 
			[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids. 
			Delft University of Technology : reports of the Faculty of 
			Technical Mathematics and Informatics, 1987. [Online]. Available: 
			https://books.google.fr/books?id=HK-4GwAACAAJ 
			[6] G. N. Lance and W. T. Williams, "A general theory of classificatory 
			sorting strategies1. hierarchical systems," The Computer Journal 4, pp. 
			373-380, 1967. 
			[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood 
			from incomplete data via the em algorithm," in Journal of the royal 
			society, series B, 1977, pp. 1-38. 
			[8] J. D. Banfield and A. E. Raftery, "Model-based gaussian and non- 
			gaussian clustering," in Biometrics, vol. 49, 1993, pp. 803-821. 
			[9] T. Kohonen, "Self-organized formation of topologically correct feature 
			maps," Biological Cybernetics, pp. 59-69, Jan 1982. 
			[10] U. N. Raghavan, R. Albert, and S. Kumara, "Near linear time algorithm 
			to detect community structures in large-scale networks." Physical review. 
			E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007. 
			[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner, 
			J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, "Sentiment analysis 
			of suicide notes: A shared task," Biomedical Informatics Insights, pp. 
			3-16, 2012. 
			[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Az 
			e, S. Bringay, and 
			P. Poncelet, "Mining twitter for suicide prevention," in Natural Language 
			Processing and Information Systems: 19th International Conference on 
			Applications of Natural Language to Information Systems, NLDB 2014, 
			Montpellier, France, June 18-20, 2014. Proceedings. Springer, 2014, 
			pp. 250-253. 
			[13] R. Kessler, J.-M. Torres, and M. El-B 
			eze, "Classification th 
			ematique de 
			courriel par des m 
			ethodes hybrides," Journ 
			ee ATALA sur les nouvelles 
			formes de communication  
			ecrite, 2004. 
			[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, "Distributed 
			representations of words and phrases and their compositionality," in 
			Proceedings of NIPS'13. USA: Curran Associates Inc., 2013, 
			pp. 3111-3119. [Online]. Available: http://dl.acm.org/citation.cfm?id= 
			2999792.2999959 
			[15] C. D. Manning and H. Sch 
			utze, Foundations of Statistical Natural 
			Language Processing. Cambridge, MA, USA: MIT Press, 1999. 
			[16] C. Goutte and E. Gaussier, " A Probabilistic Interpretation of Precision, 
			Recall and F-Score, with Implication for Evaluation," ECIR 2005, pp. 
			345-359, 2005. 
			[17] M. Hoffman, F. R. Bach, and D. M. Blei, "Online learning for latent 
			dirichlet allocation," in Advances in Neural Information Processing 
			Systems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, 
			and A. Culotta, Eds. 23, 2010, pp. 856-864. 
			[1] D. Fassin, "Et la souffrance devint sociale," in Critique. 680(1), 2004, 
			pp. 16-29. 
			[2] ----, "Souffrir par le social, gouverner par l' 
			ecoute," in Politix. 73(1), 
			2006, pp. 137-157. 
			[3] MacQueen, J., "Some methods for classification and analysis of multi- 
			variate observations," in Proceedings of the Fifth Berkeley Symposium 
			on Mathematical Statistics and Probability, Vol. 1: Statistics. USA: 
			University of California Press, 1967, pp. 281-297. 
			[4] D. Arthur and S. Vassilvitskii, "K-means++: The advantages of careful 
			seeding," Proceedings of the Eighteenth Annual ACM-SIAM Symposium 
			on Discrete Algorithms, pp. 1027-1035, 2007. 
			[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids. 
			Delft University of Technology : reports of the Faculty of 
			Technical Mathematics and Informatics, 1987. [Online]. Available: 
			https://books.google.fr/books?id=HK-4GwAACAAJ 
			[6] G. N. Lance and W. T. Williams, "A general theory of classificatory 
			sorting strategies1. hierarchical systems," The Computer Journal 4, pp. 
			373-380, 1967. 
			[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihood 
			from incomplete data via the em algorithm," in Journal of the royal 
			society, series B, 1977, pp. 1-38. 
			[8] J. D. Banfield and A. E. Raftery, "Model-based gaussian and non- 
			gaussian clustering," in Biometrics, vol. 49, 1993, pp. 803-821. 
			[9] T. Kohonen, "Self-organized formation of topologically correct feature 
			maps," Biological Cybernetics, pp. 59-69, Jan 1982. 
			[10] U. N. Raghavan, R. Albert, and S. Kumara, "Near linear time algorithm 
			to detect community structures in large-scale networks." Physical review. 
			E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007. 
			[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner, 
			J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, "Sentiment analysis 
			of suicide notes: A shared task," Biomedical Informatics Insights, pp. 
			3-16, 2012. 
			[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Az 
			e, S. Bringay, and 
			P. Poncelet, "Mining twitter for suicide prevention," in Natural Language 
			Processing and Information Systems: 19th International Conference on 
			Applications of Natural Language to Information Systems, NLDB 2014, 
			Montpellier, France, June 18-20, 2014. Proceedings. Springer, 2014, 
			pp. 250-253. 
			[13] R. Kessler, J.-M. Torres, and M. El-B 
			eze, "Classification th 
			ematique de 
			courriel par des m 
			ethodes hybrides," Journ 
			ee ATALA sur les nouvelles 
			formes de communication  
			ecrite, 2004. 
			[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, "Distributed 
			representations of words and phrases and their compositionality," in 
			Proceedings of NIPS'13. USA: Curran Associates Inc., 2013, 
			pp. 3111-3119. [Online]. Available: http://dl.acm.org/citation.cfm?id= 
			2999792.2999959 
			[15] C. D. Manning and H. Sch 
			utze, Foundations of Statistical Natural 
			Language Processing. Cambridge, MA, USA: MIT Press, 1999. 
			[16] C. Goutte and E. Gaussier, " A Probabilistic Interpretation of Precision, 
			Recall and F-Score, with Implication for Evaluation," ECIR 2005, pp. 
			345-359, 2005. 
			[17] M. Hoffman, F. R. Bach, and D. M. Blei, "Online learning for latent 
			dirichlet allocation," in Advances in Neural Information Processing 
			Systems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, 
			and A. Culotta, Eds. 23, 2010, pp. 856-864. 
			Aberdeen, John S., John D. Burger, David S. 
			Day, Lynette Hirschman, Patricia 
			Robinson, and Marc Vilain. 1995. "Mitre: 
			Description of the alembic system used 
			for MUC-6." In Proceedings of the Sixth 
			Message Understanding Conference (MUC-6), 
			Columbia, Maryland, November. Morgan 
			Kaufmann. 
			Baldwin, Breck, Christine Doran, Jeffrey 
			Reynar, Michael Niv, Bangalore Srinivas, 
			and Mark Wasson. 1997. "EAGLE: An 
			extensible architecture for general 
			linguistic engineering." In Proceedings of 
			Computer-Assisted Information Searching on 
			Internet (RIAO '97), Montreal, June. 
			Baum, Leonard E. and Ted Petrie. 1966. 
			Statistical inference for probabilistic 
			functions of finite Markov chains. Annals 
			of Mathematical Statistics 37:1559-1563. 
			Bikel, Daniel, Scott Miller, Richard 
			Schwartz, and Ralph Weischedel. 1997. 
			"Nymble: A high performance learning 
			name-finder." In Proceedings of the Fifth 
			Conference on Applied Natural Language 
			Processing (ANLP'97), pages 194-200. 
			Washington, D.C., Morgan Kaufmann. 
			Brill, Eric. 1995a. Transformation-based 
			error-driven learning and natural 
			language parsing: A case study in 
			part-of-speech tagging. Computational 
			Linguistics 21(4):543-565. 
			Brill, Eric. 1995b. "Unsupervised learning of 
			disambiguation rules for part of speech 
			tagging." In David Yarovsky and Kenneth 
			Church, editors, Proceedings of the Third 
			Workshop on Very Large Corpora, pages 
			1-13, Somerset, New Jersey. Association 
			for Computational Linguistics. 
			Burnage, Gavin. 1990. CELEX: A Guide for 
			Users. Centre for Lexical Information, 
			Nijmegen, Netherlands. 
			317 
			Mikheev Periods, Capitalized Words, etc. 
			Chinchor, Nancy. 1998. "Overview of 
			MUC-7." In Seventh Message Understanding 
			Conference (MUC-7): Proceedings of a 
			Conference Held in Fairfax, April. Morgan 
			Kaufmann. 
			Church, Kenneth. 1988. "A stochastic parts 
			program and noun-phrase parser for 
			unrestricted text." In Proceedings of the 
			Second ACL Conference on Applied Natural 
			Language Processing (ANLP'88), pages 
			136-143, Austin, Texas. 
			Church, Kenneth. 1995. "One term or two?" 
			In SIGIR'95, Proceedings of the 18th Annual 
			International ACM SIGIR Conference on 
			Research and Development in Information 
			Retrieval, pages 310-318, Seattle, 
			Washington, July. ACM Press. 
			Clarkson, Philip and Anthony J. Robinson. 
			1997. "Language model adaptation using 
			mixtures and an exponentially decaying 
			cache." In Proceedings IEEE International 
			Conference on Speech and Signal Processing, 
			Munich, Germany. 
			Cucerzan, Silviu and David Yarowsky. 1999. 
			"Language independent named entity 
			recognition combining morphological and 
			contextual evidence." In Proceedings of 
			Joint SIGDAT Conference on EMNLP and 
			VLC. 
			Francis, W. Nelson and Henry Kucera. 1982. 
			Frequency Analysis of English Usage: Lexicon 
			and Grammar. Houghton Mifflin, New 
			York. 
			Gale, William, Kenneth Church, and David 
			Yarowsky. 1992. "One sense per 
			discourse." In Proceedings of the Fourth 
			DARPA Speech and Natural Language 
			Workshop, pages 233-237. 
			Grefenstette, Gregory and Pasi Tapanainen. 
			1994. "What is a word, what is a 
			sentence? Problems of tokenization." In 
			The Proceedings of Third Conference on 
			Computational Lexicography and Text 
			Research (COMPLEX'94), Budapest, 
			Hungary. 
			Krupka, George R. and Kevin Hausman. 
			1998. Isoquest Inc.: Description of the 
			netowl extractor system as used for 
			MUC-7. In Proceedings of the Seventh 
			Message Understanding Conference (MUC-7), 
			Fairfax, VA. Morgan Kaufmann. 
			Kuhn, Roland and Renato de Mori. 1998. A 
			cache-based natural language model for 
			speech recognition. IEEE Transactions on 
			Pattern Analysis and Machine Intelligence 
			12:570-583. 
			Kupiec, Julian. 1992. Robust part-of-speech 
			tagging using a hidden Markov model. 
			Computer Speech and Language. 
			Mani, Inderjeet and T. Richard MacMillan. 
			1995. "Identifying unknown proper 
			names in newswire text." In B. Boguraev 
			and J. Pustejovsky, editors, Corpus 
			Processing for Lexical Acquisition. MIT Press, 
			Cambridge, Massachusetts, pages 41-59. 
			Marcus, Mitchell, Mary Ann Marcinkiewicz, 
			and Beatrice Santorini. 1993. Building a 
			large annotated corpus of English: The 
			Penn treebank. Computational Linguistics 
			19(2):313-329. 
			Mikheev, Andrei. 1997. Automatic rule 
			induction for unknown word guessing. 
			Computational Linguistics 23(3):405-423. 
			Mikheev, Andrei. 1999. A knowledge-free 
			method for capitalized word 
			disambiguation. In Proceedings of the 37th 
			Conference of the Association for 
			Computational Linguistics (ACL'99), pages 
			159-168, University of Maryland, College 
			Park. 
			Mikheev, Andrei. 2000. "Tagging sentence 
			boundaries." In Proceedings of the First 
			Meeting of the North American Chapter of the 
			Computational Linguistics (NAACL'2000), 
			pages 264-271, Seattle, Washington. 
			Morgan Kaufmann. 
			Mikheev, Andrei, Clair Grover, and Colin 
			Matheson. 1998. TTT: Text Tokenisation Tool. 
			Language Technology Group, University 
			of Edinburgh. Available at 
			http://www.ltg.ed.ac.uk/software/ttt/ 
			index.html. 
			Mikheev, Andrei, Clair Grover, and Marc 
			Moens. 1998. Description of the ltg 
			system used for MUC-7. In Seventh 
			Message Understanding Conference 
			(MUC-7): Proceedings of a Conference Held in 
			Fairfax, Virginia. Morgan Kaufmann. 
			Mikheev, Andrei and Liubov Liubushkina. 
			1995. Russian morphology: An 
			engineering approach. Natural Language 
			Engineering 1(3):235-260. 
			Palmer, David D. and Marti A. Hearst. 1994. 
			"Adaptive sentence boundary 
			disambiguation." In Proceedings of the 
			Fourth ACL Conference on Applied Natural 
			Language Processing (ANLP'94), pages 
			78-83, Stuttgart, Germany, October. 
			Morgan Kaufmann. 
			Palmer, David D. and Marti A. Hearst. 1997. 
			Adaptive multilingual sentence boundary 
			disambiguation. Computational Linguistics 
			23(2):241-269. 
			Park, Youngja and Roy J. Byrd. 2001. 
			"Hybrid text mining for finding 
			abbreviations and their definitions." In 
			Proceedings of the Conference on Empirical 
			Methods in Natural Language Processing 
			(EMLP'01), pages 16-19, Washington, 
			D.C. Morgan Kaufmann. 
			Ratnaparkhi, Adwait. 1996. "A maximum 
			entropy model for part-of-speech 
			318 
			Computational Linguistics Volume 28, Number 3 
			tagging." In Proceedings of Conference on 
			Empirical Methods in Natural Language 
			Processing, pages 133-142, University of 
			Pennsylvania, Philadelphia. 
			Reynar, Jeffrey C. and Adwait Ratnaparkhi. 
			1997. "A maximum entropy approach to 
			identifying sentence boundaries." In 
			Proceedings of the Fifth ACL Conference on 
			Applied Natural Language Processing 
			(ANLP'97), pages 16-19. Morgan 
			Kaufmann. 
			Riley, Michael D. 1989. "Some applications 
			of tree-based modeling to speech and 
			language indexing." In Proceedings of the 
			DARPA Speech and Natural Language 
			Workshop, pages 339-352. Morgan 
			Kaufmann. 
			Yarowsky, David. 1993. "One sense per 
			collocation." In Proceedings of ARPA 
			Human Language Technology Workshop '93, 
			pages 266-271, Princeton, New Jersey. 
			Yarowsky, David. 1995. "Unsupervised 
			word sense disambiguation rivaling 
			supervised methods." In Meeting of the 
			Association for Computational Linguistics 
			(ACL'95), pages 189-196. 
			Aberdeen, John S., John D. Burger, David S. 
			Day, Lynette Hirschman, Patricia 
			Robinson, and Marc Vilain. 1995. "Mitre: 
			Description of the alembic system used 
			for MUC-6." In Proceedings of the Sixth 
			Message Understanding Conference (MUC-6), 
			Columbia, Maryland, November. Morgan 
			Kaufmann. 
			Baldwin, Breck, Christine Doran, Jeffrey 
			Reynar, Michael Niv, Bangalore Srinivas, 
			and Mark Wasson. 1997. "EAGLE: An 
			extensible architecture for general 
			linguistic engineering." In Proceedings of 
			Computer-Assisted Information Searching on 
			Internet (RIAO '97), Montreal, June. 
			Baum, Leonard E. and Ted Petrie. 1966. 
			Statistical inference for probabilistic 
			functions of finite Markov chains. Annals 
			of Mathematical Statistics 37:1559-1563. 
			Bikel, Daniel, Scott Miller, Richard 
			Schwartz, and Ralph Weischedel. 1997. 
			"Nymble: A high performance learning 
			name-finder." In Proceedings of the Fifth 
			Conference on Applied Natural Language 
			Processing (ANLP'97), pages 194-200. 
			Washington, D.C., Morgan Kaufmann. 
			Brill, Eric. 1995a. Transformation-based 
			error-driven learning and natural 
			language parsing: A case study in 
			part-of-speech tagging. Computational 
			Linguistics 21(4):543-565. 
			Brill, Eric. 1995b. "Unsupervised learning of 
			disambiguation rules for part of speech 
			tagging." In David Yarovsky and Kenneth 
			Church, editors, Proceedings of the Third 
			Workshop on Very Large Corpora, pages 
			1-13, Somerset, New Jersey. Association 
			for Computational Linguistics. 
			Burnage, Gavin. 1990. CELEX: A Guide for 
			Users. Centre for Lexical Information, 
			Nijmegen, Netherlands. 
			317 
			Mikheev Periods, Capitalized Words, etc. 
			Chinchor, Nancy. 1998. "Overview of 
			MUC-7." In Seventh Message Understanding 
			Conference (MUC-7): Proceedings of a 
			Conference Held in Fairfax, April. Morgan 
			Kaufmann. 
			Church, Kenneth. 1988. "A stochastic parts 
			program and noun-phrase parser for 
			unrestricted text." In Proceedings of the 
			Second ACL Conference on Applied Natural 
			Language Processing (ANLP'88), pages 
			136-143, Austin, Texas. 
			Church, Kenneth. 1995. "One term or two?" 
			In SIGIR'95, Proceedings of the 18th Annual 
			International ACM SIGIR Conference on 
			Research and Development in Information 
			Retrieval, pages 310-318, Seattle, 
			Washington, July. ACM Press. 
			Clarkson, Philip and Anthony J. Robinson. 
			1997. "Language model adaptation using 
			mixtures and an exponentially decaying 
			cache." In Proceedings IEEE International 
			Conference on Speech and Signal Processing, 
			Munich, Germany. 
			Cucerzan, Silviu and David Yarowsky. 1999. 
			"Language independent named entity 
			recognition combining morphological and 
			contextual evidence." In Proceedings of 
			Joint SIGDAT Conference on EMNLP and 
			VLC. 
			Francis, W. Nelson and Henry Kucera. 1982. 
			Frequency Analysis of English Usage: Lexicon 
			and Grammar. Houghton Mifflin, New 
			York. 
			Gale, William, Kenneth Church, and David 
			Yarowsky. 1992. "One sense per 
			discourse." In Proceedings of the Fourth 
			DARPA Speech and Natural Language 
			Workshop, pages 233-237. 
			Grefenstette, Gregory and Pasi Tapanainen. 
			1994. "What is a word, what is a 
			sentence? Problems of tokenization." In 
			The Proceedings of Third Conference on 
			Computational Lexicography and Text 
			Research (COMPLEX'94), Budapest, 
			Hungary. 
			Krupka, George R. and Kevin Hausman. 
			1998. Isoquest Inc.: Description of the 
			netowl extractor system as used for 
			MUC-7. In Proceedings of the Seventh 
			Message Understanding Conference (MUC-7), 
			Fairfax, VA. Morgan Kaufmann. 
			Kuhn, Roland and Renato de Mori. 1998. A 
			cache-based natural language model for 
			speech recognition. IEEE Transactions on 
			Pattern Analysis and Machine Intelligence 
			12:570-583. 
			Kupiec, Julian. 1992. Robust part-of-speech 
			tagging using a hidden Markov model. 
			Computer Speech and Language. 
			Mani, Inderjeet and T. Richard MacMillan. 
			1995. "Identifying unknown proper 
			names in newswire text." In B. Boguraev 
			and J. Pustejovsky, editors, Corpus 
			Processing for Lexical Acquisition. MIT Press, 
			Cambridge, Massachusetts, pages 41-59. 
			Marcus, Mitchell, Mary Ann Marcinkiewicz, 
			and Beatrice Santorini. 1993. Building a 
			large annotated corpus of English: The 
			Penn treebank. Computational Linguistics 
			19(2):313-329. 
			Mikheev, Andrei. 1997. Automatic rule 
			induction for unknown word guessing. 
			Computational Linguistics 23(3):405-423. 
			Mikheev, Andrei. 1999. A knowledge-free 
			method for capitalized word 
			disambiguation. In Proceedings of the 37th 
			Conference of the Association for 
			Computational Linguistics (ACL'99), pages 
			159-168, University of Maryland, College 
			Park. 
			Mikheev, Andrei. 2000. "Tagging sentence 
			boundaries." In Proceedings of the First 
			Meeting of the North American Chapter of the 
			Computational Linguistics (NAACL'2000), 
			pages 264-271, Seattle, Washington. 
			Morgan Kaufmann. 
			Mikheev, Andrei, Clair Grover, and Colin 
			Matheson. 1998. TTT: Text Tokenisation Tool. 
			Language Technology Group, University 
			of Edinburgh. Available at 
			http://www.ltg.ed.ac.uk/software/ttt/ 
			index.html. 
			Mikheev, Andrei, Clair Grover, and Marc 
			Moens. 1998. Description of the ltg 
			system used for MUC-7. In Seventh 
			Message Understanding Conference 
			(MUC-7): Proceedings of a Conference Held in 
			Fairfax, Virginia. Morgan Kaufmann. 
			Mikheev, Andrei and Liubov Liubushkina. 
			1995. Russian morphology: An 
			engineering approach. Natural Language 
			Engineering 1(3):235-260. 
			Palmer, David D. and Marti A. Hearst. 1994. 
			"Adaptive sentence boundary 
			disambiguation." In Proceedings of the 
			Fourth ACL Conference on Applied Natural 
			Language Processing (ANLP'94), pages 
			78-83, Stuttgart, Germany, October. 
			Morgan Kaufmann. 
			Palmer, David D. and Marti A. Hearst. 1997. 
			Adaptive multilingual sentence boundary 
			disambiguation. Computational Linguistics 
			23(2):241-269. 
			Park, Youngja and Roy J. Byrd. 2001. 
			"Hybrid text mining for finding 
			abbreviations and their definitions." In 
			Proceedings of the Conference on Empirical 
			Methods in Natural Language Processing 
			(EMLP'01), pages 16-19, Washington, 
			D.C. Morgan Kaufmann. 
			Ratnaparkhi, Adwait. 1996. "A maximum 
			entropy model for part-of-speech 
			318 
			Computational Linguistics Volume 28, Number 3 
			tagging." In Proceedings of Conference on 
			Empirical Methods in Natural Language 
			Processing, pages 133-142, University of 
			Pennsylvania, Philadelphia. 
			Reynar, Jeffrey C. and Adwait Ratnaparkhi. 
			1997. "A maximum entropy approach to 
			identifying sentence boundaries." In 
			Proceedings of the Fifth ACL Conference on 
			Applied Natural Language Processing 
			(ANLP'97), pages 16-19. Morgan 
			Kaufmann. 
			Riley, Michael D. 1989. "Some applications 
			of tree-based modeling to speech and 
			language indexing." In Proceedings of the 
			DARPA Speech and Natural Language 
			Workshop, pages 339-352. Morgan 
			Kaufmann. 
			Yarowsky, David. 1993. "One sense per 
			collocation." In Proceedings of ARPA 
			Human Language Technology Workshop '93, 
			pages 266-271, Princeton, New Jersey. 
			Yarowsky, David. 1995. "Unsupervised 
			word sense disambiguation rivaling 
			supervised methods." In Meeting of the 
			Association for Computational Linguistics 
			(ACL'95), pages 189-196. 
