Nom du fichier :
			kessler94715.pdf
Titre :
			Extraction of terminology in the field of construction
Nombre d'auteur :
 			3
Auteurs :
			Remy Kessler Universite Bretagne Sud
			Nicolas Bechet Universite Bretagne Sud
			Giuseppe Berio Universite Bretagne Sud
Abstract :
			We describe a corpus analysis method to extract terminology from a collection of technical specifications in the field of construction. Using statistics and word n-grams analysis, we extract the terminology of the domain and then perform pruning steps with linguistic patterns and internet queries to improve the quality of the final terminology. Results are evaluated by using a manual evaluation carried out by 6 experts in the field.

Intro :
			The current era is increasingly influenced by the prominenceof smart data and mobile applications. The work presentedin this paper has been carried out in one industrial project(VOCAGEN) aiming at automating the production of struc-tured data from human machine dialogues. Specifically, thetargeted application drives dialogues with people working ina construction area for populating a database reporting keydata extracted from those dialogues. This application requirescomplex processing for both transcripting speeches but also fordriving dialogues. The first process is required for good speechrecognition in a noisy environment. The second processing isrequired because the database needs to be populated with bothright and complete data; indeed, people tend to apply a broad(colloquial) vocabulary and the transcripted words need to beused for filling in the corresponding data. Additionally, if somedata populate the database, additional data may be requiredfor completeness, thus the dialogue should enable to get thoseadditional data (e.g. if the word "room" is recognised and usedto populate the database, the location of the room must alsobe got; this can be done by driving the dialogue).The application provides people with "hand-free" device,enabling a complete, quick and standardized reporting. Firstusages of this application will be oriented to reporting failuresand problems in constructions.The two processing steps mentioned above require on theone side a "language model" (for transcripting the sentences)and on the other side a "knowledge model" for driving thedialogue and correctly understanding the meaning of the word.The knowledge model is mainly an ontology of the domain (inthis case, the construction domain) providing the standardizedconcepts and their relationships. As well-known, building suchknowledge models needs time and is costly; one of the earlierquestions raised by our industrial partners has been about"how to build, as automaticaly as possible, such a knowledgemodel". This question is closely related to the interest ofquickly adapting the application to other domains (than theconstruction one) for reaching new markets. We developed acomplete methodology and system for partially answering thequestion, focusing on how to extract a relevant terminologyfrom a collection of technical specifications.The rest of the paper is organized as follow. Section IIpresent context of the project. Related work are reviewed inSection III. Section IV presents collected resources and somestatistics about them. Section V describes the methodology de-veloped for extracting relevant terms from collected resources.The details about the evaluation are presented in Section VI-Aand results obtained, are given in Section VI-B.

Corps :			II. INDUSTRIAL CONTEXTFigure 1 presents the context of this work in VOCA-GEN project. Our industrial partner Script&Go1 develop anapplication for the construction management dedicated totouch devices and wishes to set up an oral dialogue moduleto facilitate on construction site seizure. The second industrialpartner (Tykomz) develops a vocal recognition suite based ontoolkit sphynx 4 [1]. This toolkit includes hierarchical agglom-erative clustering methods using well-known measures such asBIC and CLR and provides elementary tools, such as segmentand cluster generators, decoder and model trainers. Fittingthose elementary tools together is an easy way of developinga specific diarization system. To work, it is necessary to builda model of knowledge, i.e. a model describing the expressionsthat must be recognized by the program. To improve theperformance of the system, this knowledge model must be1http://www.scriptandgo.com/en/Fig. 1. figure describing the context of the projectpowered by a domain-specific vocabulary. For example, in thesentence "there is a stain of paint in the kitchen", the systemmust understand that it is a stain of paint and that the kitchen isa room. To our knowledge, there is no ontology or taxonomyspecific to the construction industry in French. A version isunder development by [2] but ontology is in English and verygeneric. We therefore choose to extract useful knowledge fromtextual data, and then, in a second step, to organize them.III. RELATED WORKSThe goal of ontology learning (OL) is to build knowledgemodels from text. OL use NLP knowledge extraction toolsto extract terminology (concepts) and links between them(relationships). The main approaches found in the literature arerule-based systems, statistical or learning based approaches.The reference in the field of rule-based systems was de-veloped by [3]. General Architecture for Text Engineering(GATE) is a Java collection of tools initially developed at theUniversity of Sheffield since 1995. An alternative is offered bythe existing semi-automatic ontology learning text2onto [4].More recently, [5] developed UIMA, a system that can bepositioned as an alternative to GATE. Amongst other things,UIMA makes possible to generate rules from a collection ofannotated documents. Exit, introduced by [6] is an iterativeapproach that finds the terms in an incremental way.[7] with TERMINAE is certainly the oldest statistic ap-proach. Developed for French and based on lexical frequen-cies, it requires pre-processing with TermoStat [8] and ANNIE(GATE). [9] presents a method for extracting terminology spe-cific to a domain from a corpus of domain-specific text, whereno external general domain reference corpus is required. Theypresent an adaptation of the classic tf-idf as ranking measureand use different filters to produce a specific terminology.More recently, the efficiency of ranking measure like mutualinformation developed for statistical approach is discussed in[10] and [11]. [12] proposes Termolator a terminology extrac-tion system using a chunking procedure, and using internetqueries for ranking candidate terms. Approach is interestingbut the authors emphasize the fact that the runtime for eachqueries is a limiting factor to produce a relevant ranking.Closer to our work, [13] presents an approach combininglinguistic pattern and Z-score to extract terminology in thefield of nanotechnology. [14] propose TAXI, which combinesstatistics and learning approach. TAXI is a system for buildinga taxonomy using 2 corpora, a generic, the other specific.It ranks the relevance of candidates by measure (frequency-based), and by learning with SVM. [15], [16] present TexSIS,a bilingual terminology extraction system with chunk-basedalignment method for the generation of candidate terms. Afterthe corpus alignment step, they use an approach combininglog likelihood measure and Mutual Expectation measure [17]to rank candidate terms in each language. In the same or-der, [18], [19] present an approach to extract grammaticalterminology from linguistic corpora. They compare a seriesof well-established statistical measures that have been usedin similar automatic term extraction tasks and conclude thatcorpus-comparing methods perform better than metrics thatare not based on corpus comparison. [20] and [21] presentmethods with word embeddings. With a small data set forlearning phase, they improve the term extraction results inn-gram terms. However, these papers involve labelled datasets for learning phase, which is the main difference withour proposed approach. The originality of our approach is tocombine a lexico-syntactical and a statistical approach whileusing external resources.IV. RESOURCES AND STATISTICSFirst experiments were carried out using technical reportscollected from some customers from our industrial partnerswho will be called as NC collection thereafter. Each documentcontains all the non-compliance that was found on one worksite and describe solutions to resolve it. However heterogeneityof the formats as well as the artificial repetition of the informa-tion between two reports found in the same construction sitemade the term extraction quite difficult. An insightful analysisof those reports reveals vocabulary richness, however, difficultto exploit given numerous misspellings, typing shortcuts, very"telegraphic" style with verbs in the infinitive, little punctu-ation, few determinants, etc. As a consequence, we used acollection of technical specifications called CCTP2. CCTPs areavailable online on public sector websites3. Several thousanddocuments were collected by our industrial partner using anautomatic web collecting process. Figure 2 presents some keydescriptive statistics of theses collections.Collection NC CCTPTotal number of documents 58 402 3665Without pre-processingTotal number of words 130 309 230 962 734Total number of different words 93 000 20 6264Average words/document 125.3 63 018.48Fig. 2. statistics of the collection.2The technical specifications book (CCTP in French) is a contractualdocument that gathers the technical clauses of a public contract in the fieldof construction.3For example, https://www.marches-publics.gouv.fr/ orhttp://marchespublics.normandie.fr/.rankingCCTPCR 1 06/02Lot A :Siphon independantsous evierCR 2 15/02Lot A :- Siphon independantsous evierLot B :- Pose des plinthessur la cloisonCR 3 22/02Lot A :- Siphon independantsous evierLot B :- suppression de lacloison entre wc et sdbwords ngramsextractionpruningpre-processingciterneaudevant entreemarcheschez techniplancoulage par defautpropositionsd' augetypologiedes citerneauxespace d' activitevision depuis ascenseurvaissellesous barmise en placecoulisseaupour douchetteterminology Fig. 3. System overviewV. METHODOLOGYA. System OverviewFigure 3 presents an overview of the system designed andimplemented: steps are further explained in Sections V-B toV-E. In Step 1 pre-processing of raw information extractedfrom CCTP collection takes place; this is required for nor-malizing the entire set of documents. In Step 2 n-grams areextracted (by using measures). 1,2,3 grams are extracted. InStep 3, n-grams are filtered by using linguistic patterns andInternet queries. Finally, in Step 4 a ranking is applied to thefiltered n-grams.B. Normalization, pre-processing and word ngrams extractionIn Step 1, a text normalization is performed to improve thequality of the process. We remove special characters such as"/" or "()". Different pretreatments are done to reduce noisein the model: we remove numbers (numeric and/or textual),special symbols. "." are tagged with a special character tonot create artificial n-grams. Specific words (including namedentities) like company names, dates, etc. are normalized andwill be removed in the next module. We do not include astop list to keep n-grams with prepositions, for the purposedescribed in the remainder. Then, we tokenize the entirecollection before using TreeTagger [22] to get the part-of-speech tags and lemmas of each word. After this step wetransform all vocabulary from the CCTP collection into 1-grams, 2-grams and 3-grams. Special characters or normalizedwords resulting from the previous processing are discarded. N-grams with a very low frequency (2) are also discarded.C. Linguistic patterns moduleWe use grammatical labels generated in the previous step(section V-B) and linguistic patterns to retrieve collocationssuch as NOUN-NOUN, NOUN-PREP-NOUN. These patternsare frequently found in the literature [6] to capture specificwords in French like "carte de credit"(credit card) and discard3-gram like 'crediter sa carte' (credit his card) with thepattern VERB-PREP-NOUN. Among frequent patterns foundin literature, those patterns have been selected according to thestatistics obtained from a knowledge model of another field(agriculture), given by one of our industrial partners. Figure4 presents main patterns we selected using this knowledgemodel. The sum of the percentages is less than 100% becauseNumber Percentage1-grams 1360 65.24%NOUN 1037 76.25%VERB 194 14.26%ADJ 120 8.82%2-grams 390 19.57%NOUN-NOUN 346 88.72%ADJ-NOUN 11 2.82%PREP-NOUN 7 1,79%VERB-NOUN 5 1,28%3-grams 188 9.43%NOUN-NOUN-NOUN 150 79.79%PREP-NOUN-NOUN 15 7.98%NOUN-PREP-NOUN 6 3,19%VERB-NOUN-NOUN 6 3,19%Fig. 4. Distribution of linguistic patterns according to the knowledge model.patterns with a very low frequency are not included in thetable. We observed that the noun based patterns are the mostfrequent patterns, whatever the size of the n-gram. The otherselected patterns also contain nouns, but they are n-gramswith verbs, adjectives or prepositions. Therefore, we haveconfigured our system to keep only the ngrams correspondingto these patterns.D. Pruning stepThis step uses the Internet to prune n-grams for which noinformation is returned after querying Bing4 search engine.We count the number of links in the result pages that containexactly the ngram. We save the number of exact matchesbetween the ngram and the title and snippet of each result.We keep only the n-grams whose number of matches exceedsa defined threshold. We varied this threshold between 1 and50 and results presented in Section VI-B have been obtainedwith a threshold of 10.E. Ranking stepWe tested several measures as provided in [6], [16] likemaximum likelihood estimation or mutual information inorder to rank selected n-grams by quality but results weredisappointing. We finally use classical Z score [23] withtwenty years of the French newspaper Le Monde5 as genericcollection. This metric considers word frequencies weightedover two different corpora, in order to assign high values towords having much higher or lower frequencies than expectedin a generic collection. We defined it as follows :p1= a0/b0 (1)p2= a1/b1 (2)4https://www.bing.com/5http://www.islrn.org/resources/421-401-527-366-2/p = (a0+ a1)/(b0+ b1) (3)ZScore =p1- p2(p  (1 - p)  ( 1b0+ 1b1)(4)Where a is the lexical unit considered (1-gram, 2-gram or3-gram), a0 the frequency of a in the CCTP collection, b0 thetotal size in words of CCTP collection, b1 the frequency of ain the collection Le Monde.VI. EXPERIMENTS AND RESULTSA. Experimental protocolWe have made a manual evaluation on all 3 grams retainedby the system. Manual evaluation was realized by 6 specialistsin the field of construction. Each specialist evaluating one thirdof the results. 5144 3-grams were evaluated with this methodand each n-gram was evaluated by 2 different specialists.For each n-grams, the specialist can choose between threepossibilities:* 0 3-gram is irrelevant* 1 3-gram is relevant but does not belong to the domain* 2 3-gram is relevant and belongs to the domainEvaluation was done in two steps and we use Kappameasure6 [24] and inter-annotator agreement at the end of thefirst step to show the difficulty of the task. At the end of thefirst step, we obtained a Kappa score of 0.62 and a globalinter-annotator agreement of 0.74, which is quite good asexplained in [25]. The difficulty of the task was to distinguishthe domain-specific vocabulary from the generic vocabularyused in the field of construction. Each disagreement was re-evaluated in the second step by a pair of experts. Figure 5shows the final results of the evaluation.B. ResultsIn this section, we present the results obtained during themanual evaluation of the 3-grams retained by the system. Weonly compute the accuracy and the error rate, because we arenot able to compute the recall for this collection7. We havemerged the assessments of each expert using two differentevaluation rules:* a strict evaluation where a n-gram is considered correctif both experts have rated it relevant and in the domain .* a flexible evaluation where a n-gram is considered correctif both experts consider it relevant and at least one of theexperts consider it in the domain.strict evaluation flexible evaluationaccuracy 0.77 0.91error rate 0.23 0.09Fig. 5. Results of manual evaluation on the 3-grams.6We use general formula as follows :  = A0-Ae1-Aewhere A0 = observedagreement and Ae = expected (chance) agreement.7Indeed, we do not know every the relevant terms existing in the corpus,so we cannot estimate the recall for the collection of terms we automaticallyextract.Strict evaluation shows good quality results (0.77). Anal-ysis of the results shows that the main error is related to"incomplete n-grams". For example, the 3-gram "personnea mobilite" (person with mobility) is not relevant while the4-gram "personne a mobilite reduite" (person with reducedmobility ) can belong to the field of construction. Some errorscan also be traced back to the CCTP documents. For example,"engin de guerre" (war machine) is a term which does notbelong to the field but a law relating to the presence of warmachine on the building sites is reported in every CCTP. Theflexible evaluation shows very good results (0.91) and thedifficulty of assessing class of some terms such as "absence deremise" which has 2 distinct meanings in French (no outhouseand no discount). The first meaning is relevant in the field ofconstruction but not the second.

Conclusion :
The paper reports our experiments and results for buildinga precise and large terminology for the construction domain.Collecting terminology is indeed the first step towards acomplete knowledge model containing both concepts andrelationships. During our work we were faced to severalproblems: finding resources and selecting them for buildingan appropriate corpus, thinking and developing pre-processingfor cleaning those resources, experimenting distinct measuresfor n-grams and selecting the most appropriate, improvingresults by adding linguistic patterns and Internet queries. Thecurrent results are quite promising according to the evaluationof the extracted terminology carried out by 6 experts in thefield. As a perspective, we will develop generic modules andguidelines for adapting these pre-processing modules to otherlanguages. Most importantly, the results of our work are usefulfor extracting taxonomical and non-taxonomical relationships.For the both purposes, we are currently working on SemEvalcollection [26]. Applying our method to other domain corporaand datasets is another future direction for this research.

Discussion :


Reference :
[1] S. Meignier and T. Merlin, "Lium spkdiarization: an open source toolkitfor diarization," in in CMU SPUD Workshop, 2010.[2] P. Pauwels and W. Terkaj, "Express to owl for construction industry:Towards a recommendable and usable ifcowl ontology," Automation inConstruction, vol. 63, pp. 100-133, 03 2016.[3] H. Cunningham, "Gate, a general architecture for text engineering," inComputers and the Humanities, vol. 36, 2002, pp. 223-254.[4] P. Cimiano and J. Volker, "text2onto," in International conference onapplication of natural language to information systems. Springer, 2005,pp. 227-238.[5] P. Kluegl, M. Toepfer, P.-D. Beck, G. Fette, and F. Puppe, "Uima ruta:Rapid development of rule-based information extraction applications,"Natural Language Engineering, vol. 22, no. 1, p. 1-40, 2016.[6] M. Roche and Y. Kodratoff, "Exit: Un systeme iteratif pour l'extractionde la terminologie du domaine a partir de corpus specialises," inProceedings of JADT 4, 2004, pp. 946-956.[7] B. Biebow, S. Szulman, and A. J. B. Clement, "Terminae: A linguistics-based tool for the building of a domain ontology," in KnowledgeAcquisition, Modeling and Management, D. Fensel and R. Studer, Eds.,1999, pp. 49-66.[8] P. Drouin, "Term extraction using non technical corpora as point of lever-age," in John Benjamins Publishing Company: Amsterdam/Philadelphia,n. Terminology, vol. 9, Ed., 2003, pp. 99-115.[9] M. F. M. Chowdhury, A. M. Gliozzo, and S. M. Trewin, "Domain-specific terminology extraction by boosting frequency metrics," Sep. 272018, uS Patent App. 15/469,766.[10] G. Bouma, "Normalized (pointwise) mutual information in collocationextraction," Proceedings of GSCL, 2009.[11] Y. Bestgen, "Evaluation de mesures d'association pour les bigrammes etles trigrammes au moyen du test exact de fisher," Proceedings of TALN2017, pp. 10-19, 2017.[12] A. L. Meyers, Y. He, Z. Glass, J. Ortega, S. Liao, A. Grieve-Smith,R. Grishman, and O. Babko-Malaya, "The termolator: Terminologyrecognition based on chunking, statistical and search-based scores,"Frontiers in Research Metrics and Analytics, vol. 3, p. 19, 2018.[13] L. Gillam, M. Tariq, and K. Ahmad, "Terminology and the constructionof ontology," TERMINOLOGY, vol. 11, pp. 55-81, 2005.[14] A. Panchenko, S. Faralli, E. Ruppert, S. Remus, H. Naets, C. Fairon, S. P.Ponzetto, and C. Biemann, "TAXI at semeval-2016 task 13: a taxonomyinduction method based on lexico-syntactic patterns, substrings andfocused crawling," in Proceedings of the 10th International Workshopon Semantic Evaluation, SemEval@NAACL-HLT 2016, San Diego, CA,USA, June 16-17, 2016, 2016, pp. 1320-1327.[15] E. Lefever, L. Macken, and V. Hoste, "Language-independentbilingual terminology extraction from a multilingual parallel corpus,"in Proceedings of the 12th Conference of the European Chapterof the Association for Computational Linguistics, ser. EACL '09.Stroudsburg, PA, USA: Association for Computational Linguistics,2009, pp. 496-504. [Online]. Available: http://dl.acm.org/citation.cfm?id=1609067.1609122[16] L. Macken, E. Lefever, and V. Hoste, "Texsis: bilingual terminologyextraction from parallel corpora using chunk-based alignment,"Terminology, vol. 19, no. 1, pp. 1-30, 2013. [Online]. Available:http://dx.doi.org/10.1075/term.19.1.01mac[17] G. Dias and H.-J. Kaalep, "Automatic extraction of multiword unitsfor estonian : Phrasal verbs," in Languages in Development, 2003, p.41:81-91.[18] B. Daille, "Study and implementation of combined techniques forautomatic extraction of terminology," The Balancing Act: CombiningSymbolic and Statistical Approaches to Language, 12 2002.[19] C. Lang, R. Schneider, and K. Suchowolec, "Extracting specializedterminology from linguistic corpora," GRAMMAR AND CORPORA, p.425, 2018.[20] E. Amjadian, D. Inkpen, T. S. Paribakht, and F. Faez, "Local-globalvectors to improve unigram terminology extraction," in Proceedings ofthe 5th International Workshop on Computational Terminology, 2016.[21] G. Wohlgenannt and F. Minic, "Using word2vec to build a simpleontology learning system." in International Semantic Web Conference(Posters & Demos), 2016.[22] H. Schmid, "Probabilistic part-of-speech tagging using decision trees,"1994.[23] E. Altman, "Financial ratios, discriminant analysis and the predic-tion of corporate bankruptcy." in The Journal of Finance, 23(4).doi:10.2307/2978933, 1968, pp. 589-609.[24] J. Cohen, "A Coefficient of Agreement for Nominal Scales," Educationaland Psychological Measurement, vol. 20, no. 1, p. 37, 1960.[25] M. L. McHugh, "Interrater reliability: the kappa statistic," in Biochemiamedica, 2012.[26] M. Apidianaki, S. M. Mohammad, J. May, E. Shutova, S. Bethard,and M. Carpuat, "Proceedings of the 12th international workshop onsemantic evaluation," in Proceedings of The 12th International Workshopon Semantic Evaluation. Association for Computational Linguistics,2018. [Online]. Available: http://aclweb.org/anthology/S18-1000