Nom du fichier :
			kesslerMETICS-ICDIM2019.pdf
Titre :
			A word embedding approach to explore a collection of discussions of people in psychological distress
Nombre d'auteur :
 			4
Auteurs :
			Remy Kessler
			Nicolas Bechet
			Gudrun Ledegen
			Frederic Pugniere-Saavedra
Abstract :
			In order to better adapt to society, an association has developed a web chat application that allows anyone to express and share their concerns and anguishes. Several thousand anonymous conversations have been gathered and form a new corpus of stories about human distress and social violence. We present a method of corpus analysis combining unsupervised learning and word embedding in order to bring out the themes of this particular collection. We compare this approach with a standard algorithm of the literature on a labeled corpus and obtain very good results. An interpretation of the obtained clusters collection confirms the interest of the method.

Intro :
			Since the nineties, social suffering has been a theme that hasreceived much attention from public and associative action.Among the consequences, there is an explosion of listeningplaces or socio-technical devices of communication whoseobjectives consist in moderating the various forms of sufferingby the liberation of the speech for a therapeutic purpose [1][2]. As part of the METICS project, a suicide preventionassociation developed an application of web chat to meetthis need. The web chat is an area that allows anyone toexpress and share with a volunteer listener their concerns andanguishes. The main specificity of this device is its anonymousnature. Protected by a pseudonym, the writers are invitedto discuss with a volunteer the problematic aspects of theirexistence. Several thousand anonymous conversations havebeen gathered and form a corpus of unpublished stories abouthuman distress. The purpose of the METICS project is to makevisible the ordinary forms of suffering usually removed fromcommon spaces and to grasp both its modes of enunciation anddigital support. In this study, we want to automatically identifythe reason for coming on the web chat for each participant.Indeed, even if the association provided us with the themeof all the conversations (work, loneliness, violence, racism,addictions, family, etc.), the original reason has not beenpreserved. In what follows, we first review some of the relatedwork in Section II. Section III presents the resources used andgives some statistics about the collection. An overview of thesystem and the strategy for identify the reason for comingon the web chat is given in Section IV. Section V presentsthe experimental protocol, an evaluation of our system and aninterpretation of the final results on the collection of humandistress.

Corps :			II. RELATED WORKSThe main characteristic of the approach presented in thispaper is to only have to provide the labels of the classes tobe predicted. This method does not need to have a taggeddata set to predict the different classes, so it is closer to anunsupervised (clustering) or semi-supervised learning methodthan a supervised. The main idea of clustering is to groupuntagged data into a number of clusters, such that similar ex-amples are grouped together and different ones are separated.In clustering, the number of classes and the distribution ofinstances between classes are unknown and the goal is to findmeaningful clusters.One kind of clustering methods is the partitioning-basedone. The k-means algorithm [3] is one of the most popu-lar partitioning-based algorithms because it provides a goodcompromise between the quality of the solution obtained andits computational complexity [4]. K-means aims to find kcentroids, one for each cluster, minimizing the sum of thedistances of each instance of data from its respective centroid.We can cite other partitioning-based algorithms such as k-medoids or PAM (Partition Around Medoids), which is anevolution of k-means [5]. Hierarchical approaches produceclusters by recursively partitioning data backwards or upwards.For example, in a hierarchical ascending classification orCAH [6], each example from the initial dataset represents acluster. Then, the clusters are merged, according to a similaritymeasure, until the desired tree structure is obtained. The resultof this clustering method is called a dendrogram. Density-based methods like the EM algorithm [7] assume that the databelonging to each cluster is derived from a specific probabilitydistribution [8]. The idea is to grow a cluster as the density inthe neighborhood of the cluster exceeds a predefined threshold.Model-based classification methods like self-organizingmap - SOM [9] are focus on finding features to represent eachcluster. The most used methods are decision trees and neuralnetworks. Approaches based on semi-supervised learning suchas label propagation algorithm [10] are similar to the methodproposed in this paper because they consist in using a learningdataset consisting of a few labelled data points to build amodel for labelling a larger number of unlabelled data. Closerto the theme of our collection, [11] and [12] use supervisedapproaches to automatically detect suicidal people in socialnetworks. They extract specific features like word distributionstatistics or sentiments to train different machine-learning clas-sifiers and compare performance of machine-learning modelsagainst the judgments of psychiatric trainees and mental healthprofessionals. More recently, CLEF challenge in 2018 consistsof performing a task on early risk detection of depression ontexts written in Social Media1. However, these papers and thistask involve tagged data sets, which is the main differencewith our proposed approach (we do not have tagged data set).III. RESOURCES AND STATISTICSThe association provided a collection of conversations be-tween volunteers and callers between 2005 and 2015, whichis called "METICS collection" henceforth.To reduce noise in the collection, we removed all thediscussions containing fewer than 15 exchanges between acaller and a person from the association, these exchanges aregenerally unrepresentative (connection problem, request forinformation, etc.). We observe particular linguistic phenomenalike emoticons2, acronyms, mistakes (spelling, typography,glued words) and an explosive lexical creativity [13]. Thesephenomena have their origin in the mode of communication(direct or semi-direct), the speed of the composition of themessage or in the technological constraints of input imposedby the material (mobile terminal, tablet, etc.). In addition, weused a subset of the collection of the French newspaper, LeMonde to validate our method on a tagged corpus. We onlykeep articles on television, politics, art, science or economics.Figure 1 presents some descriptive statistics of these twocollections.IV. METHODOLOGYA. System OverviewFigure 2 presents an overview of the system, each step willbe detailed in the rest of the section. In the first step (mod-ule x), we apply different linguistic pre-processing to eachdiscussion. The next module (y) creates a word embeddingmodel with these discussions while the third module (z) usesthis model to create specific vectors. The last module ({)performs a prediction for each discussion before separatingthe collection into clusters based on the predicted class.1http://early.irlab.org/2Symbols used in written messages to express emotions, e.g. smile orsadnessCollection METICS Le-MondeTotal number of documents 17 594 205 661Without pre-processingTotal number of words 12 276 973 87 122 002Total number of different words 158 361 419 579Average words/document 698 424With pre-processingTotal number of words 4 529 793 41 425 938Total number of different words 120 684 419 006Average words/document 257 201Fig. 1. Statistics of both collections.Conversationsor documentsWord2vecmodelPredictionClass 1pre-processingSpecificvectorscreationModel 1 Class 2Model 2Class nModel nCluster 1 Cluster 2 Cluster nFig. 2. System overviewB. Normalization and pre-processingWe first extract the textual content of each discussion. Instep x, a text normalization is performed to improve thequality of the process. We remove accents, special characterssuch as "-","/" or "()". Different linguistic processes are usedto reduce noise in the model: we remove numbers (numericand/or textual), special symbols and terms contained in a stop-list adapted to our problem. A lemmatization process wasincorporated during the first experiments but it was inefficientconsidering the typographical variations described in SectionIII.C. word2vec modelIn the next step we build a word embedding model usingword2vec [14]. We project each word of our corpus in a vectorspace in order to obtain a semantic representation of these.In this way, words appearing in similar contexts will have arelatively close vector representation. In addition to semanticinformation, one advantage of such modeling is the productionof vector representations of words, depending on the contextin which they are encountered. Some words close to a term t ina model learned from a corpus c1 may be very different fromthose from a model learned from a corpus c2. For example,we observe in figure 3 that the first eight words close to theterm "teen" vary according to the corpus used. This examplealso shows that the use of a generic model like Le Monde inFrench or Wikipedia is irrelevant in our case, since the corpuscorpus wordsMETICS teenager, young, 15years, kid,school, problem , spoiled, teen,Le-Monde sitcom, radio, compote, hearingboy, styx, scamp, rebelFig. 3. Eight words closest to the term "teenager" according to the type ofcorpus in learning.of the association is noisy and contains a number of apocopes,abbreviations or acronyms. Different parameters were testedand the configuration with the best results was kept3.D. Specific vectors creation and cluster predictionsIn this step, we build vectors containing terms that areselected using the word2vec model described in step IV-C.For each theme in the collection, we build a specific linguisticmodel by performing a word embedding to reconstruct thelinguistic context of each theme. We observe, for example,that the terms closest to the thematic "work" are: "unemploy-ment", "job", "stress". Similarly, for the "addiction" theme,we observe the terms: "cannabis", "alcoholism", "drugs" and"heroin". We used this context subsequently to construct avector, containing the distance distc(i) between each term iand the theme c. Each of these models is independent, sothe same term can appear in several models. In this way,we observed that the word "stress" is present in the vector"suicide" and in that of "work", however, the associated weightis different. We varied the size of these vectors between 20and 1000 and the best results were obtained with a size of 400.In the last step {, the system computes an Sc score for eachdiscussion and for each cluster according to each linguisticmodel such as:Sc(d) =ni=1tf(i) * distc(i) (1)with i the considered term, tf (i) frequency of i in thecollection, and distc(i) is the distance between the term i andthe thematic c. In the end, the class with the highest score ischosen.V. EXPERIMENTS AND RESULTSA. Experimental protocolTo evaluate the quality of the obtained clusters, we used asubset of the texts of the Le-Monde newspaper, described inSection III, each article having a label according to the theme.For these experiments, we configured the specific vectors (SV)approach with the optimal parameters, as defined in SectionsIV-C and IV-D. We also tested the specific vectors withoutweighting to test the particular influence of this parameter. Tohighlight the difficulty of the task, we compare our systemwith a baseline which consists in a random draw, and with3The best results were obtained with the following parameter values: vectorsize: 700, sliding window size: 5, minimum frequency: 10, vectorizationmethod: skip grams, and a soft hierarchical max function for the modellearning.the k-means algorithm [3], commonly used in the literature,as mentioned in Section II. To feed the k-means algorithm, wetransformed our initial collection into a bag of words matrix[15] where each conversation is described by the frequencyof its words. Each of the experiments was evaluated using theclassic measures of Precision, Recall and F-measure, averagedover all classes (with beta = 1 in order not to privilegeprecision or recall [16]). Since the k-means algorithm does notassociate a tag with the final clusters, we have exhaustivelycalculated the set of solutions to keep only the one yieldingthe highest F-score.B. ResultsPrec. Recall F-scoreWithout pre-processingBaseline 0.18 0.16 0.17k-means 0.23 0.20 0.22Without weighting 0.54 0.50 0.52Specific Vectors 0.53 0.54 0.53With pre-processingk-means 0.30 0.21 0.25Without weighting 0.55 0.51 0.53Specific Vectors 0.54 0.54 0.54Fig. 4. Results obtained by each system.Figure 4 presents a summary of the results obtained witheach systems. We first observe that baseline scores are verylow, but remain relatively close to the theoretical random (0.2)given by the number of classes. Linguistic pre-treatments arenot very efficient individually, but improve overall the resultsof other experiments. The k-means algorithm obtains slightlybetter results in terms of F-score, but remains weak. Specificvectors get excellent results that outperform other systems withan F-score of 0.54. The execution without weighting improveslightly the recall.C. Cluster AnalysisInitial objective of this work was the exploration of theMETICS collection, we apply the whole process with thespecific vectors approach to automatically categorize all theconversations. We use the Latent Dirichlet Allocation [17] inorder to obtain the main topic of each cluster. Figure 5 presentsaverage weight of each thematic keywords according to eachclusters.In figure 5, fear, shrink and trust are present designationsfor each cluster with a largely significant rank; yet, doesthe writer still express fear when he writes, "I'm afraid ofbeing sick"? Do these designations not participate in openingand constructing spheres of meanings around these pivotalwords? Conversely, with a lower rank, but also significant, thedesignations of thing, difficult, and problem are more vague,but more reformulating to take up the elements involved inwriting what is wrong.Fig. 5. Distribution of discursive routines by cluster.

Conclusion :
In this article, we presented an unsupervised approach toexploring a collection of stories about human distress. Thisapproach uses a word embedding model to build vectorscontaining only vocabulary from the linguistic context of themodel. We evaluated the quality of the approach on a col-lection labeled with classical measures. The detailed analysisshowed very good results (average Fscore of 0.54) comparedto the other systems tested. This method of analysis has alsomade it possible to highlight semantic universes and thematicgroupings. We first intend to study in more detail the influenceof each of the parameters on the results obtained. We are alsoplanning to be able to assign several tags to each discussion,which would allow thematic overlaps to be taken into account.The analysis reinforces the cluster approach to highlight thedefining features of this type of speech production and toreveal its inner workings. This entry by the discursive routinesis only one example which will then make it possible toapproach other explorations with a particular focus on theargumentative forms and on the forms of intensity.

Discussion :


Reference :
[1] D. Fassin, "Et la souffrance devint sociale," in Critique. 680(1), 2004,pp. 16-29.[2] ----, "Souffrir par le social, gouverner par l'ecoute," in Politix. 73(1),2006, pp. 137-157.[3] MacQueen, J., "Some methods for classification and analysis of multi-variate observations," in Proceedings of the Fifth Berkeley Symposiumon Mathematical Statistics and Probability, Vol. 1: Statistics. USA:University of California Press, 1967, pp. 281-297.[4] D. Arthur and S. Vassilvitskii, "K-means++: The advantages of carefulseeding," Proceedings of the Eighteenth Annual ACM-SIAM Symposiumon Discrete Algorithms, pp. 1027-1035, 2007.[5] L. Kaufman and P. Rousseeuw, Clustering by Means of Medoids.Delft University of Technology : reports of the Faculty ofTechnical Mathematics and Informatics, 1987. [Online]. Available:https://books.google.fr/books?id=HK-4GwAACAAJ[6] G. N. Lance and W. T. Williams, "A general theory of classificatorysorting strategies1. hierarchical systems," The Computer Journal 4, pp.373-380, 1967.[7] A. P. Dempster, N. M. Laird, and D. B. Rubin, "Maximum likelihoodfrom incomplete data via the em algorithm," in Journal of the royalsociety, series B, 1977, pp. 1-38.[8] J. D. Banfield and A. E. Raftery, "Model-based gaussian and non-gaussian clustering," in Biometrics, vol. 49, 1993, pp. 803-821.[9] T. Kohonen, "Self-organized formation of topologically correct featuremaps," Biological Cybernetics, pp. 59-69, Jan 1982.[10] U. N. Raghavan, R. Albert, and S. Kumara, "Near linear time algorithmto detect community structures in large-scale networks." Physical review.E, Statistical, nonlinear, and soft matter physics, p. 036106, 2007.[11] J. P. Pestian, P. Matykiewicz, M. Linn-Gust, B. South, O. Uzuner,J. Wiebe, K. B. Cohen, J. Hurdle, and C. Brew, "Sentiment analysisof suicide notes: A shared task," Biomedical Informatics Insights, pp.3-16, 2012.[12] A. Abboute, Y. Boudjeriou, G. Entringer, J. Aze, S. Bringay, andP. Poncelet, "Mining twitter for suicide prevention," in Natural LanguageProcessing and Information Systems: 19th International Conference onApplications of Natural Language to Information Systems, NLDB 2014,Montpellier, France, June 18-20, 2014. Proceedings. Springer, 2014,pp. 250-253.[13] R. Kessler, J.-M. Torres, and M. El-Beze, "Classification thematique decourriel par des methodes hybrides," Journee ATALA sur les nouvellesformes de communication ecrite, 2004.[14] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, "Distributedrepresentations of words and phrases and their compositionality," inProceedings of NIPS'13. USA: Curran Associates Inc., 2013,pp. 3111-3119. [Online]. Available: http://dl.acm.org/citation.cfm?id=2999792.2999959[15] C. D. Manning and H. Schutze, Foundations of Statistical NaturalLanguage Processing. Cambridge, MA, USA: MIT Press, 1999.[16] C. Goutte and E. Gaussier, " A Probabilistic Interpretation of Precision,Recall and F-Score, with Implication for Evaluation," ECIR 2005, pp.345-359, 2005.[17] M. Hoffman, F. R. Bach, and D. M. Blei, "Online learning for latentdirichlet allocation," in Advances in Neural Information ProcessingSystems, J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,and A. Culotta, Eds. 23, 2010, pp. 856-864.