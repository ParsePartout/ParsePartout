Nom du fichier :
			Boudin-Torres-2006.pdf
Titre :
			A Scalable MMR Approach to Sentence Scoring for Multi-Document Update Summarization
Nombre d'auteur :
 			3
Auteurs :
			Florian Boudin
			Marc El-Beze Laboratoire
			Juan-Manuel Torres-Moreno
Mails :
			florian.boudin@univ-avignon.fr
			marc.elbeze@univ-avignon.fr
			juan-manuel.torres@univ-avignon.fr
Abstract :
			We present SMMR, a scalable sentence scoring method for query-oriented update summarization. Sentences are scored thanks to a criterion combining query relevance and dissimilarity with already read documents (history). As the amount of data in history increases, non-redundancy is prioritized over query-relevance. We show that SMMR achieves promising results on the DUC 2007 update corpus.

Intro :
			Extensive experiments on query-oriented multi-document summarization have been carried outover the past few years. Most of the strategiesto produce summaries are based on an extrac-tion method, which identifies salient textual seg-ments, most often sentences, in documents. Sen-tences containing the most salient concepts are se-lected, ordered and assembled according to theirrelevance to produce summaries (also called ex-tracts) (Mani and Maybury, 1999).Recently emerged from the Document Under-standing Conference (DUC) 20071, update sum-marization attempts to enhance summarizationwhen more information about knowledge acquiredby the user is available. It asks the following ques-tion: has the user already read documents on thetopic? In the case of a positive answer, producingan extract focusing on only new facts is of inter-est. In this way, an important issue is introduced:c 2008. Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1Document Understanding Conferences are conductedsince 2000 by the National Institute of Standards and Tech-nology (NIST), http://www-nlpir.nist.govredundancy with previously read documents (his-tory) has to be removed from the extract.A natural way to go about update summarizationwould be extracting temporal tags (dates, elapsedtimes, temporal expressions...) (Mani and Wilson,2000) or to automatically construct the timelinefrom documents (Swan and Allan, 2000). Thesetemporal marks could be used to focus extracts onthe most recently written facts. However, most re-cently written facts are not necessarily new facts.Machine Reading (MR) was used by (Hickl etal., 2007) to construct knowledge representationsfrom clusters of documents. Sentences contain-ing "new" information (i.e. that could not be in-ferred by any previously considered document)are selected to generate summary. However, thishighly efficient approach (best system in DUC2007 update) requires large linguistic resources.(Witte et al., 2007) propose a rule-based systembased on fuzzy coreference cluster graphs. Again,this approach requires to manually write the sen-tence ranking scheme. Several strategies remain-ing on post-processing redundancy removal tech-niques have been suggested. Extracts constructedfrom history were used by (Boudin and Torres-Moreno, 2007) to minimize history's redundancy.(Lin et al., 2007) have proposed a modified Max-imal Marginal Relevance (MMR) (Carbonell andGoldstein, 1998) re-ranker during sentence selec-tion, constructing the summary by incrementallyre-ranking sentences.In this paper, we propose a scalable sentencescoring method for update summarization derivedfrom MMR. Motivated by the need for relevantnovelty, candidate sentences are selected accord-ing to a combined criterion of query relevance anddissimilarity with previously read sentences. Therest of the paper is organized as follows. Section 223introduces our proposed sentence scoring methodand Section 3 presents experiments and evaluatesour approach.

Corps :			2 MethodThe underlying idea of our method is that as thenumber of sentences in the history increases, thelikelihood to have redundant information withincandidate sentences also increases. We proposea scalable sentence scoring method derived fromMMR that, as the size of the history increases,gives more importance to non-redundancy that toquery relevance. We define H to represent the pre-viously read documents (history), Q to representthe query and s the candidate sentence. The fol-lowing subsections formally define the similaritymeasures and the scalable MMR scoring method.2.1 A query-oriented multi-documentsummarizerWe have first started by implementing a simplesummarizer for which the task is to produce query-focused summaries from clusters of documents.Each document is pre-processed: documents aresegmented into sentences, sentences are filtered(words which do not carry meaning are removedsuch as functional words or common words) andnormalized using a lemmas database (i.e. inflectedforms "go", "goes", "went", "gone"... are replacedby "go"). An N-dimensional term-space , whereN is the number of different terms found in thecluster, is constructed. Sentences are representedin  by vectors in which each component is theterm frequency within the sentence. Sentence scor-ing can be seen as a passage retrieval task in Infor-mation Retrieval (IR). Each sentence s is scored bycomputing a combination of two similarity mea-sures between the sentence and the query. The firstmeasure is the well known cosine angle (Salton etal., 1975) between the sentence and the query vec-torial representations in  (denoted respectively sand Q). The second similarity measure is basedon the Jaro-Winkler distance (Winkler, 1999). Theoriginal Jaro-Winkler measure, denoted JW, usesthe number of matching characters and transposi-tions to compute a similarity score between twoterms, giving more favourable ratings to terms thatmatch from the beginning. We have extended thismeasure to calculate the similarity between thesentence s and the query Q:JWe(s, Q) =1|Q|*qQmaxmSJW(q, m) (1)where S is the term set of s in which the termsm that already have maximized JW(q, m) are re-moved. The use of JWe smooths normalization andmisspelling errors. Each sentence s is scored usingthe linear combination:Sim1(s, Q) =  * cosine(s, Q)+ (1 - ) * JWe(s, Q) (2)where  = 0.7, optimally tuned on the past DUCsdata (2005 and 2006). The system produces a listof ranked sentences from which the summary isconstructed by arranging the high scored sentencesuntil the desired size is reached.2.2 A scalable MMR approachMMR re-ranking algorithm has been successfullyused in query-oriented summarization (Ye et al.,2005). It strives to reduce redundancy while main-taining query relevance in selected sentences. Thesummary is constructed incrementally from a listof ranked sentences, at each iteration the sentencewhich maximizes MMR is chosen:MMR = arg maxsS[  * Sim1(s, Q)- (1 - ) * maxsjESim2(s, sj) ] (3)where S is the set of candidates sentences and Eis the set of selected sentences.  represents aninterpolation coefficient between sentence's rele-vance and non-redundancy. Sim2(s, sj) is a nor-malized Longest Common Substring (LCS) mea-sure between sentences s and sj. Detecting sen-tence rehearsals, LCS is well adapted for redun-dancy removal.We propose an interpretation of MMR to tacklethe update summarization issue. Since Sim1 andSim2 are ranged in [0, 1], they can be seen as prob-abilities even though they are not. Just as rewriting(3) as (NR stands for Novelty Relevance):NR = arg maxsS[  * Sim1(s, Q)+ (1 - ) * (1 - maxshHSim2(s, sh)) ] (4)We can understand that (4) equates to an OR com-bination. But as we are looking for a more intu-itive AND and since the similarities are indepen-dent, we have to use the product combination. The24scoring method defined in (2) is modified into adouble maximization criterion in which the bestranked sentence will be the most relevant to thequery AND the most different to the sentences inH.SMMR(s) = Sim1(s, Q)* 1 - maxshHSim2(s, sh)f(H)(5)Decreasing  in (3) with the length of the sum-mary was suggested by (Murray et al., 2005) andsuccessfully used in the DUC 2005 by (Hacheyet al., 2005), thereby emphasizing the relevanceat the outset but increasingly prioritizing redun-dancy removal as the process continues. Sim-ilarly, we propose to follow this assumption inSMMR using a function denoted f that as theamount of data in history increases, prioritize non-redundancy (f(H)  0).3 ExperimentsThe method described in the previous section hasbeen implemented and evaluated by using theDUC 2007 update corpus2. The following subsec-tions present details of the different experimentswe have conducted.3.1 The DUC 2007 update corpusWe used for our experiments the DUC 2007 up-date competition data set. The corpus is composedof 10 topics, with 25 documents per topic. The up-date task goal was to produce short (100 words)multi-document update summaries of newswire ar-ticles under the assumption that the user has al-ready read a set of earlier articles. The purposeof each update summary will be to inform thereader of new information about a particular topic.Given a DUC topic and its 3 document clusters: A(10 documents), B (8 documents) and C (7 doc-uments), the task is to create from the documentsthree brief, fluent summaries that contribute to sat-isfying the information need expressed in the topicstatement.1. A summary of documents in cluster A.2. An update summary of documents in B, un-der the assumption that the reader has alreadyread documents in A.2More information about the DUC 2007 corpus is avail-able at http://duc.nist.gov/.3. An update summary of documents in C, un-der the assumption that the reader has alreadyread documents in A and B.Within a topic, the document clusters must be pro-cessed in chronological order. Our system gener-ates a summary for each cluster by arranging thehigh ranked sentences until the limit of 100 wordsis reached.3.2 EvaluationMost existing automated evaluation methods workby comparing the generated summaries to one ormore reference summaries (ideally, produced byhumans). To evaluate the quality of our generatedsummaries, we choose to use the ROUGE3 (Lin,2004) evaluation toolkit, that has been found to behighly correlated with human judgments. ROUGE-N is a n-gram recall measure calculated betweena candidate summary and a set of reference sum-maries. In our experiments ROUGE-1, ROUGE-2and ROUGE-SU4 will be computed.3.3 ResultsTable 1 reports the results obtained on the DUC2007 update data set for different sentence scor-ing methods. cosine + JWe stands for the scor-ing method defined in (2) and NR improves itwith sentence re-ranking defined in equation (4).SMMR is the combined adaptation we have pro-posed in (5). The function f(H) used in SMMR isthe simple rational function 1H, where H increaseswith the number of previous clusters (f(H) = 1for cluster A, 12for cluster B and 13for cluster C).This function allows to simply test the assumptionthat non-redundancy have to be favoured as thesize of history grows. Baseline results are obtainedon summaries generated by taking the leading sen-tences of the most recent documents of the cluster,up to 100 words (official baseline of DUC). Thetable also lists the three top performing systems atDUC 2007 and the lowest scored human reference.As we can see from these results, SMMR out-performs the other sentence scoring methods. Byways of comparison our system would have beenranked second at the DUC 2007 update competi-tion. Moreover, no post-processing was applied tothe selected sentences leaving an important marginof progress. Another interesting result is the highperformance of the non-update specific method(cosine + JWe) that could be due to the small size3ROUGE is available at http://haydn.isi.edu/ROUGE/.25of the corpus (little redundancy between clusters).ROUGE-1 ROUGE-2 ROUGE-SU4Baseline 0.26232 0.04543 0.082473rd system 0.35715 0.09622 0.132452nd system 0.36965 0.09851 0.13509cosine + JWe0.35905 0.10161 0.13701NR 0.36207 0.10042 0.13781SMMR 0.36323 0.10223 0.138861st system 0.37032 0.11189 0.14306Worst human 0.40497 0.10511 0.14779Table 1: ROUGE average recall scores computedon the DUC 2007 update corpus.

Conclusion :
N/A

Discussion :
In this paper we have described SMMR, a scal-able sentence scoring method based on MMR thatachieves very promising results. An important as-pect of our sentence scoring method is that it doesnot requires re-ranking nor linguistic knowledge,which makes it a simple and fast approach to theissue of update summarization. It was pointed outat the DUC 2007 workshop that Question Answer-ing and query-oriented summarization have beenconverging on a common task. The value addedby summarization lies in the linguistic quality. Ap-proaches mixing IR techniques are well suited forquery-oriented summarization but they require in-tensive work for making the summary fluent andcoherent. Among the others, this is a point that wethink is worthy of further investigation.

Reference :
Boudin, F. and J.M. Torres-Moreno. 2007. A Co-sine Maximization-Minimization approach for User-Oriented Multi-Document Update Summarization.In Recent Advances in Natural Language Processing(RANLP), pages 81-87.Carbonell, J. and J. Goldstein. 1998. The use of MMR,diversity-based reranking for reordering documentsand producing summaries. In 21st annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 335-336.ACM Press New York, NY, USA.Hachey, B., G. Murray, and D. Reitter. 2005. TheEmbra System at DUC 2005: Query-oriented Multi-document Summarization with a Very Large LatentSemantic Space. In Document Understanding Con-ference (DUC).Hickl, A., K. Roberts, and F. Lacatusu. 2007. LCC'sGISTexter at DUC 2007: Machine Reading for Up-date Summarization. In Document UnderstandingConference (DUC).Lin, Z., T.S. Chua, M.Y. Kan, W.S. Lee, L. Qiu, andS. Ye. 2007. NUS at DUC 2007: Using Evolu-tionary Models of Text. In Document UnderstandingConference (DUC).Lin, C.Y. 2004. Rouge: A Package for AutomaticEvaluation of Summaries. In Workshop on Text Sum-marization Branches Out, pages 25-26.Mani, I. and M.T. Maybury. 1999. Advances in Auto-matic Text Summarization. MIT Press.Mani, I. and G. Wilson. 2000. Robust temporal pro-cessing of news. In 38th Annual Meeting on Asso-ciation for Computational Linguistics, pages 69-76.Association for Computational Linguistics Morris-town, NJ, USA.Murray, G., S. Renals, and J. Carletta. 2005. ExtractiveSummarization of Meeting Recordings. In Ninth Eu-ropean Conference on Speech Communication andTechnology. ISCA.Salton, G., A. Wong, and C. S. Yang. 1975. A vectorspace model for automatic indexing. Communica-tions of the ACM, 18(11):613-620.Swan, R. and J. Allan. 2000. Automatic generationof overview timelines. In 23rd annual internationalACM SIGIR conference on Research and develop-ment in information retrieval, pages 49-56.Winkler, W. E. 1999. The state of record linkage andcurrent research problems. In Survey Methods Sec-tion, pages 73-79.Witte, R., R. Krestel, and S. Bergler. 2007. Generat-ing Update Summaries for DUC 2007. In DocumentUnderstanding Conference (DUC).Ye, S., L. Qiu, T.S. Chua, and M.Y. Kan. 2005. NUSat DUC 2005: Understanding documents via con-cept links. In Document Understanding Conference(DUC).26