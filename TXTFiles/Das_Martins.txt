Nom du fichier :
			Das_Martins.pdf
Titre :
			A Survey on Automatic Text Summarization
Nombre d'auteur :
 			2
Auteurs :
			Dipanjan Das
			Andre F.T. Martins
Mails :
			dipanjan@cs.cmu.edu
			afm@cs.cmu.edu
Abstract :
			The increasing availability of online information has necessitated intensive research in the area of automatic text summarization within the Natural Language Processing (NLP) community. Over the past half a century, the problem has been addressed from many different perspectives, in varying domains and using various paradigms. This survey intends to investigate some of the most relevant approaches both in the areas of single-document and multipledocument summarization, giving special emphasis to empirical methods and extractive techniques. Some promising approaches that concentrate on specific details of the summarization problem are also discussed. Special attention is devoted to automatic evaluation of summarization systems, as future research on summarization is strongly dependent on progress in this area.

Intro :
			The subfield of summarization has been investigated by the NLP community fornearly the last half century. Radev et al. (2002) define a summary as "a text thatis produced from one or more texts, that conveys important information in theoriginal text(s), and that is no longer than half of the original text(s) and usuallysignificantly less than that". This simple definition captures three important aspectsthat characterize research on automatic summarization:* Summaries may be produced from a single document or multiple documents,* Summaries should preserve important information,* Summaries should be short.Even if we agree unanimously on these points, it seems from the literature thatany attempt to provide a more elaborate definition for the task would result indisagreement within the community. In fact, many approaches differ on the mannerof their problem formulations. We start by introducing some common terms in the1summarization dialect: extraction is the procedure of identifying important sectionsof the text and producing them verbatim; abstraction aims to produce importantmaterial in a new way; fusion combines extracted parts coherently; and compressionaims to throw out unimportant sections of the text (Radev et al., 2002).Earliest instances of research on summarizing scientific documents proposedparadigms for extracting salient sentences from text using features like word andphrase frequency (Luhn, 1958), position in the text (Baxendale, 1958) and keyphrases (Edmundson, 1969). Various work published since then has concentrated onother domains, mostly on newswire data. Many approaches addressed the problemby building systems depending of the type of the required summary. While extractivesummarization is mainly concerned with what the summary content should be, usu-ally relying solely on extraction of sentences, abstractive summarization puts strongemphasis on the form, aiming to produce a grammatical summary, which usuallyrequires advanced language generation techniques. In a paradigm more tuned toinformation retrieval (IR), one can also consider topic-driven summarization, thatassumes that the summary content depends on the preference of the user and canbe assessed via a query, making the final summary focused on a particular topic.A crucial issue that will certainly drive future research on summarization isevaluation. During the last fifteen years, many system evaluation competitions likeTREC,1 DUC2 and MUC3 have created sets of training material and have estab-lished baselines for performance levels. However, a universal strategy to evaluatesummarization systems is still absent.In this survey, we primarily aim to investigate how empirical methods have beenused to build summarization systems. The rest of the paper is organized as fol-lows: Section 2 describes single-document summarization, focusing on extractivetechniques. Section 3 progresses to discuss the area of multi-document summariza-tion, where a few abstractive approaches that pioneered the field are also considered.Section 4 briefly discusses some unconventional approaches that we believe can beuseful in the future of summarization research. Section 5 elaborates a few eval-uation techniques and describes some of the standards for evaluating summariesautomatically. Finally, Section 6 concludes the survey.

Corps :			2 Single-Document SummarizationUsually, the flow of information in a given document is not uniform, which meansthat some parts are more important than others. The major challenge in summa-rization lies in distinguishing the more informative parts of a document from theless ones. Though there have been instances of research describing the automaticcreation of abstracts, most work presented in the literature relies on verbatim ex-traction of sentences to address the problem of single-document summarization. In1See http://trec.nist.gov/.2See http://duc.nist.gov/.3See http://www.itl.nist.gov/iad/894.02/related projects/muc/proceedings/.muc 7 toc.html2this section, we describe some eminent extractive techniques. First, we look at earlywork from the 1950s and 60s that kicked off research on summarization. Second,we concentrate on approaches involving machine learning techniques published inthe 1990s to today. Finally, we briefly describe some techniques that use a morecomplex natural language analysis to tackle the problem.2.1 Early WorkMost early work on single-document summarization focused on technical documents.Perhaps the most cited paper on summarization is that of (Luhn, 1958), that de-scribes research done at IBM in the 1950s. In his work, Luhn proposed that thefrequency of a particular word in an article provides an useful measure of its sig-nificance. There are several key ideas put forward in this paper that have assumedimportance in later work on summarization. As a first step, words were stemmed totheir root forms, and stop words were deleted. Luhn then compiled a list of contentwords sorted by decreasing frequency, the index providing a significance measure ofthe word. On a sentence level, a significance factor was derived that reflects thenumber of occurrences of significant words within a sentence, and the linear distancebetween them due to the intervention of non-significant words. All sentences areranked in order of their significance factor, and the top ranking sentences are finallyselected to form the auto-abstract.Related work (Baxendale, 1958), also done at IBM and published in the samejournal, provides early insight on a particular feature helpful in finding salient partsof documents: the sentence position. Towards this goal, the author examined 200paragraphs to find that in 85% of the paragraphs the topic sentence came as the firstone and in 7% of the time it was the last sentence. Thus, a naive but fairly accurateway to select a topic sentence would be to choose one of these two. This positionalfeature has since been used in many complex machine learning based systems.Edmundson (1969) describes a system that produces document extracts. Hisprimary contribution was the development of a typical structure for an extractivesummarization experiment. At first, the author developed a protocol for creatingmanual extracts, that was applied in a set of 400 technical documents. The twofeatures of word frequency and positional importance were incorporated from theprevious two works. Two other features were used: the presence of cue words(presence of words like significant, or hardly), and the skeleton of the document(whether the sentence is a title or heading). Weights were attached to each of thesefeatures manually to score each sentence. During evaluation, it was found that about44% of the auto-extracts matched the manual extracts.2.2 Machine Learning MethodsIn the 1990s, with the advent of machine learning techniques in NLP, a series of semi-nal publications appeared that employed statistical techniques to produce documentextracts. While initially most systems assumed feature independence and relied onnaive-Bayes methods, others have focused on the choice of appropriate features and3on learning algorithms that make no independence assumptions. Other significantapproaches involved hidden Markov models and log-linear models to improve ex-tractive summarization. A very recent paper, in contrast, used neural networks andthird party features (like common words in search engine queries) to improve purelyextractive single document summarization. We next describe all these approachesin more detail.2.2.1 Naive-Bayes MethodsKupiec et al. (1995) describe a method derived from Edmundson (1969) that is ableto learn from data. The classification function categorizes each sentence as worthyof extraction or not, using a naive-Bayes classifier. Let s be a particular sentence,S the set of sentences that make up the summary, and F1, . . . , Fk the features.Assuming independence of the features:P(s  S | F1, F2, ..Fk) =ki=1P(Fi | s  S) * P(s  S)ki=1P(Fi)(1)The features were compliant to (Edmundson, 1969), but additionally included thesentence length and the presence of uppercase words. Each sentence was given ascore according to (1), and only the n top sentences were extracted. To evaluatethe system, a corpus of technical documents with manual abstracts was used inthe following way: for each sentence in the manual abstract, the authors manuallyanalyzed its match with the actual document sentences and created a mapping(e.g. exact match with a sentence, matching a join of two sentences, not matchable,etc.). The auto-extracts were then evaluated against this mapping. Feature analysisrevealed that a system using only the position and the cue features, along with thesentence length sentence feature, performed best.Aone et al. (1999) also incorporated a naive-Bayes classifier, but with richerfeatures. They describe a system called DimSum that made use of features liketerm frequency (tf ) and inverse document frequency (idf) to derive signature words.4The idf was computed from a large corpus of the same domain as the concerneddocuments. Statistically derived two-noun word collocations were used as units forcounting, along with single words. A named-entity tagger was used and each entitywas considered as a single token. They also employed some shallow discourse analysislike reference to same entities in the text, maintaining cohesion. The referenceswere resolved at a very shallow level by linking name aliases within a documentlike "U.S." to "United States", or "IBM" for "International Business Machines".Synonyms and morphological variants were also merged while considering lexicalterms, the former being identified by using Wordnet (Miller, 1995). The corporaused in the experiments were from newswire, some of which belonged to the TRECevaluations.4Words that indicate key concepts in a document.42.2.2 Rich Features and Decision TreesLin and Hovy (1997) studied the importance of a single feature, sentence position.Just weighing a sentence by its position in text, which the authors term as the"position method", arises from the idea that texts generally follow a predictablediscourse structure, and that the sentences of greater topic centrality tend to occur incertain specifiable locations (e.g. title, abstracts, etc). However, since the discoursestructure significantly varies over domains, the position method cannot be definedas naively as in (Baxendale, 1958). The paper makes an important contribution byinvestigating techniques of tailoring the position method towards optimality over agenre and how it can be evaluated for effectiveness. A newswire corpus was used, thecollection of Ziff-Davis texts produced from the TIPSTER5 program; it consists oftext about computer and related hardware, accompanied by a set of key topic wordsand a small abstract of six sentences. For each document in the corpus, the authorsmeasured the yield of each sentence position against the topic keywords. They thenranked the sentence positions by their average yield to produce the Optimal PositionPolicy (OPP) for topic positions for the genre.Two kinds of evaluation were performed. Previously unseen text was used fortesting whether the same procedure would work in a different domain. The firstevaluation showed contours exactly like the training documents. In the second eval-uation, word overlap of manual abstracts with the extracted sentences was measured.Windows in abstracts were compared with windows on the selected sentences andcorresponding precision and recall values were measured. A high degree of coverageindicated the effectiveness of the position method.In later work, Lin (1999) broke away from the assumption that features areindependent of each other and tried to model the problem of sentence extractionusing decision trees, instead of a naive-Bayes classifier. He examined a lot of fea-tures and their effect on sentence extraction. The data used in this work is apublicly available collection of texts, classified into various topics, provided by theTIPSTER-SUMMAC6 evaluations, targeted towards information retrieval systems.The dataset contains essential text fragments (phrases, clauses, and sentences) whichmust be included in summaries to answer some TREC topics. These fragments wereeach evaluated by a human judge. The experiments described in the paper are withthe SUMMARIST system developed at the University of Southern California. Thesystem extracted sentences from the documents and those were matched againsthuman extracts, like most early work on extractive summarization.Some novel features were the query signature (normalized score given to sen-tences depending on number of query words that they contain), IR signature (them most salient words in the corpus, similar to the signature words of (Aone et al.,1999)), numerical data (boolean value 1 given to sentences that contained a num-ber in them), proper name (boolean value 1 given to sentences that contained aproper name in them), pronoun or adjective (boolean value 1 given to sentences5See http://www.itl.nist.gov/iaui/894.02/related_projects/tipster/.6See http://www-nlpir.nist.gov/related_projects/tipster_summac/index.html.5that contained a pronoun or adjective in them), weekday or month (similar as pre-vious feature) and quotation (similar as previous feature). It is worth noting thatsome features like the query signature are question-oriented because of the settingof the evaluation, unlike a generalized summarization framework.The author experimented with various baselines, like using only the positionalfeature, or using a simple combination of all features by adding their values. Whenevaluated by matching machine extracted and human extracted sentences, the deci-sion tree classifier was clearly the winner for the whole dataset, but for three topics,a naive combination of features beat it. Lin conjectured that this happened becausesome of the features were independent of each other. Feature analysis suggestedthat the IR signature was a valuable feature, corroborating the early findings ofLuhn (1958).2.2.3 Hidden Markov ModelsIn contrast with previous approaches, that were mostly feature-based and non-sequential, Conroy and O'leary (2001) modeled the problem of extracting a sentencefrom a document using a hidden Markov model (HMM). The basic motivation forusing a sequential model is to account for local dependencies between sentences.Only three features were used: position of the sentence in the document (built intothe state structure of the HMM), number of terms in the sentence, and likeliness ofthe sentence terms given the document terms.no 321 nononoFigure 1: Markov model to extract to three summary sentences from a document(Conroy and O'leary, 2001).The HMM was structured as follows: it contained 2s + 1 states, alternating be-tween s summary states and s+1 nonsummary states. The authors allowed "hesita-tion" only in nonsummary states and "skipping next state" only in summary states.Figure 1 shows an example HMM with 7 nodes, corresponding to s = 3. Using theTREC dataset as training corpus, the authors obtained the maximum-likelihoodestimate for each transition probability, forming the transition matrix estimate M,whose element (i, j) is the empirical probability of transitioning from state i to j.Associated with each state i was an output function, bi(O) = Pr(O | state i) whereO is an observed vector of features. They made a simplifying assumption that thefeatures are multivariate normal. The output function for each state was thus esti-mated by using the training data to compute the maximum likelihood estimate ofits mean and covariance matrix. They estimated 2s+1 means, but assumed that allof the output functions shared a common covariance matrix. Evaluation was done6by comparing with human generated extracts.2.2.4 Log-Linear ModelsOsborne (2002) claims that existing approaches to summarization have always as-sumed feature independence. The author used log-linear models to obviate thisassumption and showed empirically that the system produced better extracts thana naive-Bayes model, with a prior appended to both models. Let c be a label, sthe item we are interested in labeling, fi the i-th feature, and i the correspondingfeature weight. The conditional log-linear model used by Osborne (2002) can bestated as follows:P(c | s) =1Z(s)expiifi(c, s) , (2)where Z(s) = cexp ( iifi(c, s)). In this domain, there are only two possiblelabels: either the sentence is to be extracted or it is not. The weights were trainedby conjugate gradient descent. The authors added a non-uniform prior to the model,claiming that a log-linear model tends to reject too many sentences for inclusion ina summary. The same prior was also added to a naive-Bayes model for comparison.The classification took place as follows:label(s) = arg maxcCP(c) * P(s, c) = arg maxcClog P(c) +iifi(c, s) . (3)The authors optimized the prior using the f2 score of the classifier as an objectivefunction on a part of the dataset (in the technical domain). The summaries wereevaluated using the standard f2 score where f2 = 2prp+r, where the precision and recallmeasures were measured against human generated extracts. The features includedword pairs (pairs of words with all words truncated to ten characters), sentencelength, sentence position, and naive discourse features like inside introduction orinside conclusion. With respect to f2 score, the log-linear model outperformed thenaive-Bayes classifier with the prior, exhibiting the former's effectiveness.2.2.5 Neural Networks and Third Party FeaturesIn 2001-02, DUC issued a task of creating a 100-word summary of a single newsarticle. However, the best performing systems in the evaluations could not outper-form the baseline with statistical significance. This extremely strong baseline hasbeen analyzed by Nenkova (2005) and corresponds to the selection of the first nsentences of a newswire article. This surprising result has been attributed to thejournalistic convention of putting the most important part of an article in the initialparagraphs. After 2002, the task of single-document summarization for newswirewas dropped from DUC. Svore et al. (2007) propose an algorithm based on neu-ral nets and the use of third party datasets to tackle the problem of extractivesummarization, outperforming the baseline with statistical significance.7The authors used a dataset containing 1365 documents gathered from CNN.com,each consisting of the title, timestamp, three or four human generated story high-lights and the article text. They considered the task of creating three machinehighlights. The human generated highlights were not verbatim extractions from thearticle itself. The authors evaluated their system using two metrics: the first oneconcatenated the three highlights produced by the system, concatenated the threehuman generated highlights, and compared these two blocks; the second metric con-sidered the ordering and compared the sentences on an individual level.Svore et al. (2007) trained a model from the labels and the features for eachsentence of an article, that could infer the proper ranking of sentences in a testdocument. The ranking was accomplished using RankNet (Burges et al., 2005), apair-based neural network algorithm designed to rank a set of inputs that uses thegradient descent method for training. For the training set, they used ROUGE-1(Lin, 2004) to score the similarity of a human written highlight and a sentencein the document. These similarity scores were used as soft labels during training,contrasting with other approaches where sentences are "hard-labeled", as selectedor not.Some of the used features based on position or n-grams frequencies have beenobserved in previous work. However, the novelty of the framework lay in the useof features that derived information from query logs from Microsoft's news searchengine7 and Wikipedia8 entries. The authors conjecture that if a document sentencecontained keywords used in the news search engine, or entities found in Wikipediaarticles, then there is a greater chance of having that sentence in the highlight. Theextracts were evaluated using ROUGE-1 and ROUGE-2, and showed statisticallysignificant improvements over the baseline of selecting the first three sentences in adocument.2.3 Deep Natural Language Analysis MethodsIn this subsection, we describe a set of papers that detail approaches towards single-document summarization involving complex natural language analysis techniques.None of these papers solve the problem using machine learning, but rather use a setof heuristics to create document extracts. Most of these techniques try to model thetext's discourse structure.Barzilay and Elhadad (1997) describe a work that used considerable amount oflinguistic analysis for performing the task of summarization. For a better under-standing of their method, we need to define a lexical chain: it is a sequence of relatedwords in a text, spanning short (adjacent words or sentences) or long distances (en-tire text). The authors' method progressed with the following steps: segmentationof the text, identification of lexical chains, and using strong lexical chains to identifythe sentences worthy of extraction. They tried to reach a middle ground between(McKeown and Radev, 1995) and (Luhn, 1958) where the former relied on deep7See http://search.live.com/news.8See http://en.wikipedia.org.8semantic structure of the text, while the latter relied on word statistics of the doc-uments. The authors describe the notion of cohesion in text as a means of stickingtogether different parts of the text. Lexical cohesion is a notable example wheresemantically related words are used. For example, let us take a look at the followingsentence.9John bought a Jag. He loves the car. (4)Here, the word car refers to the word Jag in the previous sentence, and exemplifieslexical cohesion. The phenomenon of cohesion occurs not only at the word level,but at word sequences too, resulting in lexical chains, which the authors used asa source representation for summarization. Semantically related words and wordsequences were identified in the document, and several chains were extracted, thatform a representation of the document. To find out lexical chains, the authors usedWordnet (Miller, 1995), applying three generic steps:1. Selecting a set of candidate words.2. For each candidate word, finding an appropriate chain relying on a relatednesscriterion among members of the chains,3. If it is found, inserting the word in the chain and updating it accordingly.The relatedness was measured in terms of Wordnet distance. Simple nouns andnoun compounds were used as starting point to find the set of candidates. In thefinal steps, strong lexical chains were used to create the summaries. The chains werescored by their length and homogeneity. Then the authors used a few heuristics toselect the significant sentences.In another paper, Ono et al. (1994) put forward a computational model of dis-course for Japanese expository writings, where they elaborate a practical procedurefor extracting the discourse rhetorical structure, a binary tree representing relationsbetween chunks of sentences (rhetorical structure trees are used more intensively in(Marcu, 1998a), as we will see below). This structure was extracted using a seriesof NLP steps: sentence analysis, rhetorical relation extraction, segmentation, can-didate generation and preference judgement. Evaluation was based on the relativeimportance of rhetorical relations. In the following step, the nodes of the rhetori-cal structure tree were pruned to reduce the sentence, keeping its important parts.Same was done for paragraphs to finally produce the summary. Evaluation was donewith respect to sentence coverage and 30 editorial articles of a Japanese newspaperwere used as the dataset. The articles had corresponding sets of key sentences andmost important key sentences judged by human subjects. The key sentence coveragewas about 51% and the most important key sentence coverage was 74%, indicatingencouraging results.Marcu (1998a) describes a unique approach towards summarization that, unlikemost other previous work, does not assume that the sentences in a document forma flat sequence. This paper used discourse based heuristics with the traditional9Example from http://www.cs.ucd.ie/staff/jcarthy/home/Lex.html.9features that have been used in the summarization literature. The discourse theoryused in this paper is the Rhetorical Structure Theory (RST) that holds betweentwo non-overlapping pieces of text spans: the nucleus and the satellite. The authormentions that the distinction between nuclei and satellites comes from the empir-ical observation that the nucleus expresses what is more essential to the writer'spurpose than the satellite; and that the nucleus of a rhetorical relation is compre-hensible independent of the satellite, but not vice versa. Marcu (1998b) describesthe details of a rhetorical parser producing a discourse tree. Figure 2 shows anexample discourse tree for a text example detailed in the paper. Once such a dis-Antithesis2ElaborationElaboration22Elaboration3Justification8Exemplification1 2 3 4 5 7 84 58 109 105 6ContrastEvidenceConcessionFigure 2: Example of a discourse tree from Marcu (1998a). The numbers in thenodes denote sentence numbers from the text example. The text below the numberin selected nodes are rhetorical relations. The dotted nodes are SATELLITES andthe normals ones are the NUCLEI.course structure is created, a partial ordering of important units can be developedfrom the tree. Each equivalence class in the partial ordering is derived from thenew sentences at a particular level of the discourse tree. In Figure 2, we observethat sentence 2 is at the root, followed by sentence 8 in the second level. In thethird level, sentence 3 and 10 are observed, and so forth. The equivalence classesare 2 > 8 > 3, 10 > 1, 4, 5, 7, 9 > 6.If it is specified that the summary should contain the top k% of the text, the firstk% of the units in the partial ordering can be selected to produce the summary. Theauthor talks about a summarization system based just on this method in (Marcu,1998b) and in one of his earlier papers. In this paper, he merged the discoursebased heuristics with traditional heuristics. The metrics used were clustering based10metric (each node in the discourse tree was assigned a cluster score; for leaves thescore was 0, for the internal nodes it was given by the similarity of the immediatechildren; discourse tree A was chosen to be better than B if its clustering scorewas higher), marker based metric (a discourse structure A was chosen to be betterthan a discourse structure B if A used more rhetorical relations than B), rhetoricalclustering based technique (measured the similarity between salient units of two textspans), shape based metric (preferred a discourse tree A over B if A was more skewedtowards the right than B), title based metric, position based metric, connectednessbased metric (cosine similarity of an unit to all other text units, a discourse structureA was chosen to be better than B if its connectedness measure was more than B).A weighted linear combination of all these scores gave the score of a discoursestructure. To find the best combination of heuristics, the author computed theweights that maximized the F-score on the training dataset, which was constitutedby newswire articles. To do this, he used a GSAT-like algorithm (Selman et al.,1992) that performed a greedy search in a seven dimensional space of the metrics.For a part of his corpus (the TREC dataset), a best F-score of 75.42% was achievedfor the 10% summaries which was 3.5% higher than a baseline lead based algorithm,which was very encouraging.3 Multi-Document SummarizationExtraction of a single summary from multiple documents has gained interest sincemid 1990s, most applications being in the domain of news articles. Several Web-based news clustering systems were inspired by research on multi-document summa-rization, for example Google News,10 Columbia NewsBlaster,11 or News In Essence.12This departs from single-document summarization since the problem involves mul-tiple sources of information that overlap and supplement each other, being contra-dictory at occasions. So the key tasks are not only identifying and coping withredundancy across documents, but also recognizing novelty and ensuring that thefinal summary is both coherent and complete.The field seems to have been pioneered by the NLP group at Columbia University(McKeown and Radev, 1995), where a summarization system called SUMMONS13was developed by extending already existing technology for template-driven messageunderstanding systems. Although in that early stage multi-document summariza-tion was mainly seen as a task requiring substantial capabilities of both languageinterpretation and generation, it later gained autonomy, as people coming from dif-ferent communities added new perspectives to the problem. Extractive techniqueshave been applied, making use of similarity measures between pairs of sentences.Approaches vary on how these similarities are used: some identify common themesthrough clustering and then select one sentence to represent each cluster (McKeown10See http://news.google.com.11See http://newsblaster.cs.columbia.edu.12See http://NewsInEssence.com.13SUMMarizing Online NewS articles.11et al., 1999; Radev et al., 2000), others generate a composite sentence from eachcluster (Barzilay et al., 1999), while some approaches work dynamically by includ-ing each candidate passage only if it is considered novel with respect to the previousincluded passages, via maximal marginal relevance (Carbonell and Goldstein, 1998).Some recent work extends multi-document summarization to multilingual environ-ments (Evans, 2005).The way the problem is posed has also varied over time. While in some pub-lications it is claimed that extractive techniques would not be effective for multi-document summarization (McKeown and Radev, 1995; McKeown et al., 1999), someyears later that claim was overturned, as extractive systems like MEAD14 (Radevet al., 2000) achieved good performance in large scale summarization of news arti-cles. This can be explained by the fact that summarization systems often distinguishamong themselves about what their goal actually is. While some systems, like SUM-MONS, are designed to work in strict domains, aiming to build a sort of briefingthat highlights differences and updates accross different news reports, putting muchemphasis on how information is presented to the user, others, like MEAD, are largescale systems that intend to work in general domains, being more concerned withinformation content rather than form. Consequently, systems of the former kind re-quire a strong effort on language generation to produce a grammatical and coherentsummary, while latter systems are probably more close to the information retrievalparadigm. Abstractive systems like SUMMONS are difficult to replicate, as theyheavily rely on the adaptation of internal tools to perform information extractionand language generation. On the other hand, extractive systems are generally easyto implement from scratch, and this makes them appealing when sophisticated NLPtools are not available.3.1 Abstraction and Information FusionAs far as we know, SUMMONS (McKeown and Radev, 1995; Radev and McKeown,1998) is the first historical example of a multi-document summarization system. Ittackles single events about a narrow domain (news articles about terrorism) andproduces a briefing merging relevant information about each event and how reportsby different news agencies have evolved over time. The whole thread of reports isthen presented, as illustrated in the following example of a "good" summary:"In the afternoon of February 26, 1993, Reuters reported that a suspectbomb killed at least five people in the World Trade Center. However,Associated Press announced that exactly five people were killed in theblast. Finally, Associated Press announced that Arab terrorists werepossibly responsible for the terrorist act."Rather than working with raw text, SUMMONS reads a database previouslybuilt by a template-based message understanding system. A full multi-document14Available for download at http://www.summarization.com/mead/.12summarizer is built by concatenating the two systems, first processing full text asinput and filling template slots, and then synthesizing a summary from the extractedinformation. The architecture of SUMMONS consists of two major components: acontent planner that selects the information to include in the summary throughcombination of the input templates, and a linguistic generator that selects the rightwords to express the information in grammatical and coherent text. The lattercomponent was devised by adapting existing language generation tools, namely theFUF/SURGE system15. Content planning, on the other hand, is made throughsummary operators, a set of heuristic rules that perform operations like "change ofperspective", "contradiction", "refinement", etc. Some of these operations requireresolving conflicts, i.e., contradictory information among different sources or timeinstants; others complete pieces of information that are included in some articlesand not in others, combining them into a single template. At the end, the linguis-tic generator gathers all the combined information and uses connective phrases tosynthesize a summary.While this framework seems promising when the domain is narrow enough so thatthe templates can be designed by hand, a generalization for broader domains wouldbe problematic. This was improved later by McKeown et al. (1999) and Barzilayet al. (1999), where the input is now a set of related documents in raw text, likethose retrieved by a standard search engine in response to a query. The system startsby identifying themes, i.e., sets of similar text units (usually paragraphs). This isformulated as a clustering problem. To compute a similarity measure between textunits, these are mapped to vectors of features, that include single words weightedby their TF-IDF scores, noun phrases, proper nouns, synsets from the Wordnetdatabase and a database of semantic classes of verbs. For each pair of paragraphs, avector is computed that represents matches on the different features. Decision rulesthat were learned from data are then used to classify each pair of text units eitheras similar or dissimilar; this in turn feeds a subsequent algorithm that places themost related paragraphs in the same theme.Once themes are identified, the system enters its second stage: information fu-sion. The goal is to decide which sentences of a theme should be included in thesummary. Rather than just picking a sentence that is a group representative, theauthors propose an algorithm which compares and intersects predicate argumentstructures of the phrases within each theme to determine which are repeated oftenenough to be included in the summary. This is done as follows: first, sentences areparsed through Collins' statistical parser (Collins, 1999) and converted into depen-dency trees, which allows capturing the predicate-argument structure and identifyfunctional roles. Determiners and auxiliaries are dropped; Fig. 3 shows a sentencerepresentation.The comparison algorithm then traverses these dependency trees recursively,adding identical nodes to the output tree. Once full phrases (a verb with at leasttwo constituents) are found, they are marked to be included in the summary. If two15FUF, SURGE, and other tools developed by the Columbia NLP group are available athttp://www1.cs.columbia.edu/nlp/tools.cgi.13and Kan 1998]. We match two verbs that share the samesemantic class in this classifi cation.In addition to the above primitive features that all com-pare single items from each text unit, we use composite fea-tures that combine pairs of primitive features. Our compos-ite features impose particular constraints on the order of thetwo elements in the pair, on the maximum distance betweenthe two elements, and on the syntactic classes that the twoelements come from. They can vary from a simple com-bination (e.g., "two text units must share two words to besimilar") to complex cases with many conditions (e.g., "twotext units must have matching noun phrases that appear inthe same order and with relative difference in position nomore than fi ve"). In this manner, we capture informationon how similarly related elements are spaced out in the twotext units, as well as syntactic information on word combi-nations. Matches on composite features indicate combinedevidence for the similarity of the two units.To determine whether the units match overall, we employa machine learning algorithm [Cohen 1996] that induces de-cision rules using the features that really make a difference.A set of pairs of units already marked as similar or not by ahuman is used for training the classifi er. We have manuallymarked a set of 8,225 paragraph comparisons from the TDTcorpus for training and evaluating our similarity classifi er.For comparison, we also use an implementation of theTF*IDF method which is standard for matching texts in in-formation retrieval. We compute the total frequency (TF) ofwords in each text unit and the number of units in our train-ing set each word appears in (DF, or document frequency).Then each text unit is represented as a vector of TF*IDFscores, calculated asTF(wordi) * log Total number of unitsDF(wordi)Similarity between text units is measured by the cosine ofthe angle between the corresponding two vectors (i.e., thenormalized inner product of the two vectors), and the opti-mal value of a threshold for judging two units as similar iscomputed from the training set.After all pairwise similarities between text units havebeen calculated, we utilize a clustering algorithm to iden-tify themes. As a paragraph may belong to multiple themes,most standard clustering algorithms, which partition theirinput set, are not suitable for our task. We use a greedy,one-pass algorithm that fi rst constructs groups from the mostsimilar paragraphs, seeding the groups with the fully con-nected subcomponents of the graph that the similarity rela-tionship induces over the set of paragraphs, and then placesadditional paragraphs within a group if the fraction of themembers of the group they are similar to exceeds a presetthreshold.Language GenerationGiven a group of similar paragraphs--a theme--the prob-lem is to create a concise and fluent fusion of information inthis theme, reflecting facts common to all paragraphs. Astraightforward method would be to pick a representativesubjectclass: noun27class: cardinalbombingclass: nounMcVeigh withclass: prepositiondefinite: yeschargeclass: verb voice :passivepolarity: +tense: pastFigure 4: Dependency grammar representation of the sen-tence "McVeigh, 27, was charged with the bombing".sentence that meets some criteria (e.g., a threshold numberof common content words). In practice, however, any repre-sentative sentence will usually include embedded phrase(s)containing information that is not common to all sentencesin the theme. Furthermore, other sentences in the theme of-ten contain additional information not presented in the rep-resentative sentence. Our approach, therefore, uses inter-section among theme sentences to identify phrases commonto most paragraphs and then generates a new sentence fromidentifi ed phrases.Intersection among Theme SentencesIntersection is carried out in the content planner, which usesa parser for interpreting the input sentences, with our newwork focusing on the comparison of phrases. Theme sen-tences are fi rst run through a statistical parser[Collins 1996]and then, in order to identify functional roles (e.g., subject,object), are converted to a dependency grammar representa-tion [Kittredge and Mel'cuk 1983], which makes predicate-argument structure explicit.We developed a rule-based component to produce func-tional roles, which transforms the phrase-structure output ofCollins' parser to dependency grammar; function words (de-terminers and auxiliaries) are eliminated from the tree andcorresponding syntactic features are updated. An exampleof a theme sentence and its dependency grammar represen-tation are shown in Figure 4. Each non-auxiliary word in thesentence has a node in the representation, and this node isconnected to its direct dependents.The comparison algorithm starts with all subtrees rootedat verbs from the input dependency structure, and traversesthem recursively: if two nodes are identical, they are addedto the output tree, and their children are compared. Oncea full phrase (verb with at least two constituents) has beenfound, it is confi rmed for inclusion in the summary.Diffi culties arise when two nodes are not identical, but aresimilar. Such phrases may be paraphrases of each other andstill convey essentially the same information. Since themesentences are a priori close semantically, this signifi cantlyFigure 3: Dependency tree representing the sentence "McVeigh, 27, was chargedwith the bombing" (extracted from (McKeown et al., 1999)).phrases, rooted at some node, are not identical but yet similar, the hypothesis thatthey are paraphrases of each other is considered; to take this into account, corpus-driven paraphrasing rules are written to allow paraphrase intersection.16 Once thesummary content (represented as predicate-argument structures) is decided, a gram-matical text is generated by translating those structures into the arguments expectedby the FUF/SURGE language generation system.3.2 Topic-driven Summarization and MMRCarbonell and Goldstein (1998) made a major contribution to topic-driven sum-marization by introducing the maximal marginal relevance (MMR) measure. Theidea is to combine query relevance with information novelty; it may be applicablein several tasks ranging from text retrieval to topic-driven summarization. MMRsimultaneously rewards relevant sentences and penalizes redundant ones by consid-ering a linear combination of two similarity measures.Let Q be a query or user profile and R a ranked list of documents retrieved bya search engine. Consider an incremental procedure that selects documents, one ata time, and adds them to a set S. So let S be the set of already selected documentsin a particular step, and R \ S the set of yet unselected documents in R. For eachcandidate document Di  R \ S, its marginal relevance MR(Di) is computed as:MR(Di) := Sim1(Di, Q) - (1 - ) maxDjSSim2(Di, Dj) (5)where  is a parameter lying in [0, 1] that controls the relative importance givento relevance versus redundancy. Sim1 and Sim2 are two similarity measures; in the16A full description of the kind of paraphrasing rules used can be found in (Barzilay et al.,1999). Examples are: ordering of sentence components, main clause vs. relative clause, realizationin different syntactic categories (e.g. classifier vs. apposition), change in grammatical features(active/passive, time, number, etc.), head omission, transformation from one POS to another,using semantically related words (e.g. synonyms), etc.14experiments both were set to the standard cosine similarity traditionally used in thevector space model, Sim1(x, y) = Sim2(x, y) = x,yx * y. The document achieving thehighest marginal relevance, DMMR = arg maxDiR\SMR(Di), is then selected, i.e.,added to S, and the procedure continues until a maximum number of documentsare selected or a minimum relevance threshold is attained. Carbonell and Goldstein(1998) found experimentally that choosing dynamically the value of  turns out to bemore effective than keeping it fixed, namely starting with small values (  0.3) togive more emphasis to novelty, and then increasing it (  0.7) to focus on the mostrelevant documents. To perform summarization, documents can be first segmentedinto sentences or paragraphs, and after a query is submitted, the MMR algorithmcan be applied followed by a selection of the top ranking passages, reordering them asthey appeared in the original documents, and presenting the result as the summary.One of the attractive points in using MMR for summarization is its topic-orientedfeature, through its dependency on the query Q, which makes it particularly ap-pealing to generate summaries according to a user profile: as the authors claim, "adifferent user with different information needs may require a totally different sum-mary of the same document." This assertion was not being taken into account byprevious multi-document summarization systems.3.3 Graph Spreading ActivationMani and Bloedorn (1997) describe an information extraction framework for sum-marization, a graph-based method to find similarities and dissimilarities in pairsof documents. Albeit no textual summary is generated, the summary content isrepresented via entities (concepts) and relations that are displayed respectively asnodes and edges of a graph. Rather than extracting sentences, they detect salientregions of the graph via a spreading activation technique.17This approach shares with the method described in Section 3.2 the propertyof being topic-driven; there is an additional input that stands for the topic withrespect to which the summary is to be generated. The topic is represented througha set of entry nodes in the graph. A document is represented as a graph as follows:each node represents the occurrence of a single word (i.e., one word together withits position in the text). Each node can have several kinds of links: adjacencylinks (ADJ) to adjacent words in the text, SAME links to other occurrences of thesame word, and ALPHA links encoding semantic relationships captured throughWordnet and NetOwl18. Besides these, PHRASE links tie together sequences ofadjacent nodes which belong to the same phrase, and NAME and COREF linksstand for co-referential name occurrences; Fig. 4 shows some of these links.Once the graph is built, topic nodes are identified by stem comparison and be-come the entry nodes. A search for semantically related text is then propagated fromthese to the other nodes of the graph, in a process called spreading activation. Salient17The name "spreading activation" is borrowed from a method used in information retrieval(Salton and Buckley, 1988) to expand the search vocabulary.18See http://www.netowl.com.151.39: Aoki, the Japanese ambassador, said in telephone calls toFujimori.Japanesebroadcaster NHK that the rebels wanted to talk directly to1.43:According to some estimates, only a couple hundred armedfollowers remain.2.19 They are freeing unot doing us any harm,"...2.27:Although the MRTAearly days in the mid-198give to the poor, it lost puturning increasingly to kibillion in damage to the csince 1980.and drug activities. 2.28:Peru have cost at least 3...close ties with Japan.1.33: Among the hostages were Japanese Ambassador Morihisa Aoki andthe ambassadors of Brazil, Bolivia, Cuba, Canada, South Korea,.........2.26:The MRTA called T"Breaking The Silence."1.32: President Alberto Fujimori, who is of Japanese ancestry, has hadGermany, Austria and Venezuela. Hood-style movement thanegotiations with the govdawn on Wednesday.......2.22:The attack was a maFujimori's government, wvirtual victory in a 16-yearebels belonging to the Mand better-known Maoist...1.28:Many leaders of the Tupac Amaru which is smaller than Peru'swas captured in June 1992 and is serving a life sentence, as is hisus: `Don't lift your heads up or you will be shot."1.19:hostages," a rebel who did not give his name told a local radio station ina telephone call from inside the compound."The guerillas stalked around the residence grounds threateninglieutenant, Peter Cardenas.1.25: "We are clear: the liberation of all our comrades, or we die with all the1.30:Other top commanders conceded defeat July 1993.and surrendered inCOREFMaoist Shining Path movement are in jail. 1.29:Its chief, Victor Polay,ADJ1.38:Fujimori whose sister was among thean emergency cabinet meeting today.hostages released, calledALPHAADJ, the rebels threatened to kill the remainingcaptives.1.24:Early WednesdayFigure 5: Texts of two related articles. The top 5 salient sentences containing commonwords in bold face; likewise, the top 5 salient sentences containing unique words have thFigure 4: Examples of nodes and links in the graph for a particular sentence (detailextracted from from a figure in (Mani and Bloedorn, 1997)).words and phrases are initialized according to their TF-IDF score. The weight ofneighboring nodes depends on the node link traveled and is an exponentially decay-ing function of the distance of the traversed path. Traveling within a sentence ismade cheaper than across sentence boundaries, which in turn is cheaper than acrossparagraph boundaries. Given a pair of document graphs, common nodes are identi-fied either by sharing the same stem or by being synonyms. Analogously, differencenodes are those that are not common. For each sentence in both documents, twoscores are computed: one score that reflects the presence of common nodes, whichis computed as the average weight of these nodes; and another score that computesinstead the average weights of difference nodes. Both scores are computed afterspreading activation. In the end, the sentences that have higher common and dif-ferent scores are highlighted, the user being able to specify the maximal number ofcommon and different sentences to control the output. In the future, the authorsexpect to use these structure to actually compose abstractive summaries, ratherthan just highlighting pieces of text.3.4 Centroid-based SummarizationAlthough clustering techniques were already being employed by McKeown et al.(1999) and Barzilay et al. (1999) for identification of themes, Radev et al. (2000)pioneered the use of cluster centroids to play a central role in summarization. A fulldescription of the centroid-based approach that underlies the MEAD system canbe found in (Radev et al., 2004); here we sketch briefly the main points. Perhapsthe most appealing feature is the fact that it does not make use of any languagegeneration module, unlike most previous systems. All documents are modeled asbags-of-words. The system is also easily scalable and domain-independent.The first stage consists of topic detection, whose goal is to group together newsarticles that describe the same event. To accomplish this task, an agglomerativeclustering algorithm is used that operates over the TF-IDF vector representationsof the documents, successively adding documents to clusters and recomputing the16centroids according tocj = dCjd|Cj|(6)where cj is the centroid of the j-th cluster, Cj is the set of documents that belongto that cluster, its cardinality being |Cj|, and d is a "truncated version" of d thatvanishes on those words whose TF-IDF scores are below a threshold. Centroidscan thus be regarded as pseudo-documents that include those words whose TF-IDF scores are above a threshold in the documents that constitute the cluster. Eachevent cluster is a collection of (typically 2 to 10) news articles from multiple sources,chronologically ordered, describing an event as it develops over time.The second stage uses the centroids to identify sentences in each cluster thatare central to the topic of the entire cluster. In (Radev et al., 2000), two metricsare defined that resemble the two summands in the MMR (see Section 3.2): cluster-based relative utility (CBRU) and cross-sentence informational subsumption (CSIS).The first accounts for how relevant a particular sentence is to the general topic ofthe entire cluster; the second is a measure of redundancy among sentences. UnlikeMMR, these metrics are not query-dependent. Given one cluster C of documentssegmented into n sentences, and a compression rate R, a sequence of nR sentencesare extracted in the same order as they appear in the original documents, which inturn are ordered chronologically. The selection of the sentences is made by approx-imating their CBRU and CSIS.19 For each sentence si, three different features areused:* Its centroid value (Ci), defined as the sum of the centroid values of all thewords in the sentence,* A positional value (Pi), that is used to make leading sentences more important.Let Cmax be the centroid value of the highest ranked sentence in the document.Then Pi = n-i+1nCmax.* The first-sentence overlap (Fi), defined as the inner product between the wordoccurrence vector of sentence i and that of the first sentence of the document.The final score of each sentence is a combination of the three scores above minus aredundancy penalty (Rs) for each sentence that overlaps highly ranked sentences.3.5 Multilingual Multi-document SummarizationEvans (2005) addresses the task of summarizing documents written in multiplelanguages; this had already been sketched by Hovy and Lin (1999). Multilingualsummarization is still at an early stage, but this framework looks quite useful fornewswire applications that need to combine information from foreign news agen-cies. Evans (2005) considered the scenario where there is a preferred language inwhich the summary is to be written, and multiple documents in the preferred and19The two metrics are used directly for evaluation (see (Radev et al., 2004) for more details).17in foreign languages are available. In their experiments, the preferred language wasEnglish and the documents are news articles in English and Arabic. The rationale isto summarize the English articles without discarding the information contained inthe Arabic documents. The IBM's statistical machine translation system is first ap-plied to translate the Arabic documents to English. Then a search is made, for eachtranslated text unit, to see whether there is a similar sentence or not in the Englishdocuments. If so, and if the sentence is found relevant enough to be included in thesummary, the similar English sentence is included instead of the Arabic-to-Englishtranslation. This way, the final summary is more likely to be grammatical, sincemachine translation is known to be far from perfect. On the other hand, the resultis also expected to have higher coverage than using just the English documents,since the information contained in the Arabic documents can help to decide aboutthe relevance of each sentence. In order to measure similarity between sentences, atool named SimFinder20 was employed: this is a tool for clustering text based onsimilarity over a variety of lexical and syntactic features using a log-linear regressionmodel.4 Other Approaches to SummarizationThis section describes briefly some unconventional approaches that, rather thanaiming to build full summarization systems, investigate some details that underliethe summarization process, and that we conjecture to have a role to play in futureresearch on this field.4.1 Short SummariesWitbrock and Mittal (1999) claim that extractive summarization is not very pow-erful in that the extracts are not concise enough when very short summaries arerequired. They present a system that generated headline style summaries. The cor-pus used in this work was newswire articles from Reuters and the Associated Press,publicly available at the LDC21. The system learned statistical models of the rela-tionship between source text units and headline units. It attempted to model boththe order and the likelihood of the appearance of tokens in the target documents.Both the models, one for content selection and the other for surface realization wereused to co-constrain each other during the search in the summary generation task.For content selection, the model learned a translation model between a docu-ment and its summary (Brown et al., 1993). This model in the simplest case can bethought as a mapping between a word in the document and the likelihood of someword appearing in the summary. To simplify the model, the authors assumed thatthe probability of a word appearing in a summary is independent of its structure.This mapping boils down to the fact that the probability of a particular summary20See http://www1.cs.columbia.edu/nlp/tools.cgi#SimFinder.21See http://ldc.upenn.edu.18candidate is the product of the probabilities of the summary content and that con-tent being expressed using a particular structure.The surface realization model used was a bigram model. Viterbi beam searchwas used to efficiently find a near-optimal summary. The Markov assumption wasviolated by using backtracking at every state to strongly discourage paths thatrepeated terms, since bigrams that start repeating often seem to pathologicallyoverwhelm the search otherwise. To evaluate the system, the authors comparedits output against the actual headlines for a set of input newswire stories. Sincephrasing could not be compared, they compared the generated headlines againstthe actual headlines, as well as the top ranked summary sentence of the story. Sincethe system did not have a mechanism to determine the optimal length of a headline,six headlines for each story were generated, ranging in length from 4 to 10 wordsand they measured the term-overlap between each of the generated headlines andthe test. For headline length 4, there was 0.89 overlap in the headline and there was0.91 overlap amongst the top scored sentence, indicating useful results.4.2 Sentence CompressionKnight and Marcu (2000) introduced a statistical approach to sentence compression.The authors believe that understanding the simpler task of compressing a sentencemay be a fruitful first step to later tackle the problems of single and multi-documentsummarization.Sentence compression is defined as follows: given a sequence of words W =w1w2 . . . wn that constitute a sentence, find a subsequence wi1wi2. . . wik, with1  i1 < i2 < . . . ik  n, that is a compressed version of W. Note that thereare 2n possibilities of output. Knight and Marcu (2000) considered two differentapproaches: one that is inspired by the noisy-channel model, and another one basedon decision trees. Due to its simplicity and elegance, we describe the first approachhere.The noisy-channel model considers that one starts with a short summary s,drawn according to the source model P(s), which is then subject to channel noise tobecome the full sentence t, in a process guided by the channel model P(t|s). Whenthe string t is observed, one wants to recover the original summary according to:s = arg maxsP(s|t) = arg maxsP(s)P(t|s). (7)This model has the advantage of decoupling the goals of producing a short text thatlooks grammatical (incorporated in the source model) and of preserving importantinformation (which is done through the channel model). In (Knight and Marcu,2000), the source and channel models are simple models inspired by probabilisticcontext-free grammars (PCFGs). The following probability mass functions are de-fined over parse trees rather than strings: Ptree(s), the probability of a parse treethat generates s, and Pexpand tree(t|s), the probability that a small parse tree thatgenerates s is expanded to a longer one that generates t.19The sentence t is first parsed by using Collins' parser (Collins, 1999). Then,rather than computing Ptree(s) over all the 2n hypotheses for s, which would beexponential in the sentence length, a shaded-forest structure is used: the parsetree of t is traversed and the grammar (learned from the Penn Treebank22) is usedto check recursively which nodes may be removed from each production in orderto achieve another valid production. This algorithm allows to compute efficientlyPtree(s) and Pexpand tree(t|s) for all possible grammatical summaries s. Conceptually,the noisy channel model works the other way around: summaries are the originalstrings that are expanded via expansion templates. Expansion operations have theeffect of decreasing the probability Pexpand tree(t|s). The probabilities Ptree(s) andPexpand tree(t|s) consist in the usual factorized expression for PCFGs times a bigramdistribution over the leaves of the tree (i.e. the words). In the end, the log probabilityis (heuristically) divided by the length of the sentence s in order not to penalizeexcessively longer sentences (this is done commonly in speech recognition).More recently, Daume III and Marcu (2002) extended this approach to documentcompression by using rhetorical structure theory as in Marcu (1998a), where theentire document is represented as a tree, hence allowing not only to compress relevantsentences, but also to drop irrelevant ones. In this framework, Daume III and Marcu(2004) employed kernel methods to decide for each node in the tree whether or notit should be kept.4.3 Sequential document representationWe conclude this section by mentioning some recent work that concerns documentrepresentation, with applications in summarization. In the bag-of-words representa-tion (Salton et al., 1975) each document is represented as a sparse vector in a verylarge Euclidean space, indexed by words in the vocabulary V . A well-known tech-nique in information retrieval to capture word correlation is latent semantic indexing(LSI), that aims to find a linear subspace of dimension k  |V | where documentsmay be approximately represented by their projections.These classical approaches assume by convenience that Euclidean geometry isa proper model for text documents. As an alternative, Gous (1999) and Hall andHofmann (2000) used the framework of information geometry (Amari and Nagaoka,2001) to generalize LSI to the multinomial manifold, which can be identified withthe probability simplexPn-1 = x  Rn |ni=1xi = 1, xi  0 for i = 1, . . . , n . (8)Instead of finding a linear subspace, as in the Euclidean case, they learn a subman-ifold of Pn-1. To illustrate this idea, Gous (1999) split a book (Machiavelli's ThePrince) into several text blocks (its numbered pages), considered each page as apoint in P|V |-1, and projected data into a 2-dimensional submanifold. The result is22See http://www.cis.upenn.edu/~treebank/.20the representation of the book as a sequential path in R2, tracking the evolution ofthe subject matter of the book over the course of its pages (see Fig. 5). Inspired byFigure 5: The 113 pages of The Prince projected onto a 2-dimensional space (ex-tracted from (Gous, 1999)). The inflection around page 85 reflects a real change inthe subject matter, where the book shifts from political theory to a more biograph-ical discourse.this framework, Lebanon et al. (2007) suggested representing a document as a sim-plicial curve (i.e. a curve in the probability simplex), yielding the locally weightedbag-of-words (lowbow) model. According to this representation, a length-normalizeddocument is a function x : [0, 1] x V  R+ such thatwjVx(t, wj) = 1, for any t  [0, 1]. (9)We can regard the document as a continuous signal, and x(t, wj) as expressingthe relevance of word wj at instant t. This generalizes both the pure sequentialrepresentation and the (global) bag-of-words model. Let y = (y1, . . . , yn)  V n bea n-length document. The pure sequential representation of y arises by definingx = xseq with:xseq(t, wj) =1, if wj = y tn0, if wj = y tn,(10)where a denotes the smallest integer greater than a. The global bag-of-wordsrepresentation of x corresponds to defining x = xbow, wherexbow(, wj) =10xseq(t, wj)dt,   [0, 1], j = 1, . . . , |V |. (11)In this case, the curve degenerates into a single point in the simplex, which isthe maximum likelihood estimate of the multinomial parameters. An intermediate21representation arises by smoothing (10) via a function f, : [0, 1]  R++, where  [0, 1] and   R++ are respectively a location and a scale parameter. Anexample of such a smoothing function is the truncated Gaussian defined in [0, 1]and normalized. This allows defining the lowbow representation at  of the n-lenghtdocument (y1, . . . , yn)  V n as the function x : [0, 1] x V  R+ such that:x(, wj) =10xseq(t, wj)f,(t)dt. (12)The scale of the smoothing function controls the amount of locality/globality inthe document representation (see Fig. 6): when    we recover the global bowrepresentation (11); when   0, we approach the pure sequential representation(10).Figure 6: The lowbow representation of a document with |V | = 3, for several valuesof the scale parameter  (extracted from (Lebanon, 2006)).Representing a document as a simplicial curve allows us to characterize geomet-rically several properties of the document. For example, the tangent vector fieldalong the curve describes sequential "topic trends" and their change; the curvaturemeasures the amount of wigglyness or deviation from a geodesic path. This prop-erties can be useful for tasks like text segmentation or summarization; for exampleplotting the velocity of the curve || x()|| along time offers a visualization of the doc-ument where local maxima tend to correspond to topic boundaries (see (Lebanonet al., 2007) for more information).225 EvaluationEvaluating a summary is a difficult task because there does not exist an ideal sum-mary for a given document or set of documents. From papers surveyed in the previ-ous sections and elsewhere in literature, it has been found that agreement betweenhuman summarizers is quite low, both for evaluating and generating summaries.More than the form of the summary, it is difficult to evaluate the summary con-tent. Another important problem in summary evaluation is the widespread use ofdisparate metrics. The absence of a standard human or automatic evaluation met-ric makes it very hard to compare different systems and establish a baseline. Thisproblem is not present in other NLP problems, like parsing. Besides this, manualevaluation is too expensive: as stated by Lin (2004), large scale manual evaluationof summaries as in the DUC conferences would require over 3000 hours of human ef-forts. Hence, an evaluation metric having high correlation with human scores wouldobviate the process of manual evaluation. In this section, we would look at some im-portant recent papers that have been able to create standards in the summarizationcommunity.5.1 Human and Automatic EvaluationLin and Hovy (2002) describe and compare various human and automatic metrics toevaluate summaries. They focus on the evaluation procedure used in the DocumentUnderstanding Conference 2001 (DUC-2001), where the Summary Evaluation En-vironment (SEE) interface was used to support the human evaluation part. NISTassessors in DUC-2001 compared manually written ideal summaries with summariesgenerated automatically by summarization systems and baseline summaries. Eachtext was decomposed into a list of units (sentences) and displayed in separate win-dows in SEE. To measure the content of summaries, assessors stepped through eachmodel unit (MU) from the ideal summaries and marked all system units (SU) shar-ing content with the current model unit, rating them with scores in the range 1 - 4to specify that the marked system units express all (4), most (3), some (2) or hardlyany (1) of the content of the current model unit. Grammaticality, cohesion, and co-herence were also rated similarly by the assessors. The weighted recall at thresholdt (where t range from 1 to 4) is then defined asRecallt =Number of MUs marked at or above tNumber of MUs in the model summary. (13)An interesting study is presented that shows how unstable the human markingsfor overlapping units are. For multiple systems, the coverage scores assigned to thesame units were different by human assessors 18% of the time for the single documenttask and 7.6% of the time for multi-document task. The authors also observe thatinter-human agreement is quite low in creating extracts from documents ( 40% forsingle-documents and  29% for multi-documents). To overcome the instability ofhuman evaluations, they proposed using automatic metrics for summary evaluation.23Inspired by the machine translation evaluation metric BLEU (Papineni et al., 2001),they outline an accumulative n-gram matching score (which they call NAMS),NAMS = a1 * NAM1 + a2 * NAM2 + a3 * NAM3 + a4 * NAM4, (14)where the NAMn n-gram hit ratio is defined as:# of matched n-grams between MU and Stotal # of n-grams in MU(15)with S denoting here the whole system summary, and where only content wordswere used in forming the n-grams. Different configurations of ai were tried; thebest correlation with human judgement (using Spearman's rank order correlationcoefficient) was achieved using a configuration giving 2/3 weight to bigram matchesand 1/3 to unigrams matches with stemming done by the Porter stemmer.5.2 ROUGELin (2004) introduced a set of metrics called Recall-Oriented Understudy for Gist-ing Evaluation (ROUGE)23 that have become standards of automatic evaluation ofsummaries.In what follows, let R = {r1, . . . , rm} be a set of reference summaries, and let s bea summary generated automatically by some system. Let n(d) be a binary vectorrepresenting the n-grams contained in a document d; the i-th component in(d) is 1if the i-th n-gram is contained in d and 0 otherwise. The metric ROUGE-N is ann-gram recall based statistic that can be computed as follows:ROUGE-N(s) = rRn(r), n(s)rRn(r), n(r), (16)where ., . denotes the usual inner product of vectors. This measure is closely relatedto BLEU which is a precision related measure. Unlike other measures previouslyconsidered, ROUGE-N can be used for multiple reference summaries, which is quiteuseful in practical situations. An alternative is taking the most similar summary inthe reference set,ROUGE-Nmulti(s) = maxrRn(r), n(s)n(r), n(r). (17)Another metric in (Lin, 2004) applies the concept of longest common subse-quences24 (LCS). The rationale is: the longer the LCS between two summary sen-tences, the more similar they are. Let r1, . . . , ru be the reference sentences of thedocuments in R, and s a candidate summary (considered as a concatenation ofsentences). The ROUGE-L is defined as an LCS based F-measure:ROUGE-L(s) =(1 + 2)RLCSPLCSRLCS + 2PLCS(18)23See http://openrouge.com/default.aspx.24A subsequence of a string s = s1. . . snis a string of the form si1. . . sinwhere 1  i1< . . . in n.24where RLCS(s) = Pui=1LCS(ri,s)Pui=1|ri|, PLCS(s) = Pui=1LCS(ri,s)|s|, |x| denotes the length ofsentence x, LCS(x, y) denotes the length of the LCS between sentences x and y,and  is a (usually large) parameter to balance precision and recall. Notice thatthe LCS function may be computed by a simple dynamic programming approach.The metric (18) is further refined by including weights that penalize subsequencematches that are not consecutive, yielding a new measure denoted ROUGE-W.Yet another measure introduced by Lin (2004) is ROUGE-S, which can be seenas a gappy version of ROUGE-N for n = 2 and is aptly called skip bigram. Let 2(d)be a binary vector indexed by ordered pairs of words; the i-th component i2(d) is1 if the i-th pair is a subsequence of d and 0 otherwise. The metric ROUGE-S iscomputed as follows:ROUGE-S(s) =(1 + 2)RSPSRS + 2PS(19)where RS(s) = Pui=12(ri),2(s)Pui=12(ri),2(ri)and PS(s) = Pui=12(ri),2(s)2(s),2(s).The various versions of ROUGE were evaluated by computing the correlationcoefficient between ROUGE scores and human judgement scores. ROUGE-2 per-formed the best among the ROUGE-N variants. ROUGE-L, ROUGE-W, andROUGE-S all performed very well on the DUC-2001 and DUC-2002 datasets. How-ever, correlation achieved with human judgement for multi-document summarizationwas not as high as single-document ones; improvement on this side of the paradigmis an open research topic.5.3 Information-theoretic Evaluation of SummariesA very recent approach (Lin et al., 2006) proposes to use an information-theoreticmethod to automatic evaluation of summaries. The central idea is to use a diver-gence measure between a pair of probability distributions, in this case the Jensen-Shannon divergence, where the first distribution is derived from an automatic sum-mary and the second from a set of reference summaries. This approach has theadvantage of suiting both the single-document and the multi-document summariza-tion scenarios.Let D = {d1, . . . , dn} be the set of documents to summarize (which is a singletonset in the case of single-document summarization). Assume that a distributionparameterized by R generates reference summaries of the documents in D. Thetask of summarization can be seen as that of estimating R. Analogously, assumethat every summarization system is governed by some distribution parameterizedby A. Then, we may define a good summarizer as one for which A is close to R.One information-theoretic measure between distributions that is adequate for thisis the KL divergence (Cover and Thomas, 1991),KL(pA ||pR ) =mi=1pAilogpAipRi. (20)However, the KL divergence is unbounded and goes to infinity whenever pAivanishes25and pRidoes not, which requires using some kind of smoothing when estimating thedistributions. Lin et al. (2006) claims that the measure used here should also besymmetric,25 another thing that the KL divergence is not. Hence, they propose touse the Jensen-Shannon divergence which is bounded and symmetric:26JS(pA ||pR ) =12KL(pA ||r) +12KL(pR ||r) == H(r) -12H(pA ) -12H(pA ), (21)where r = 12pA + 12pR is the average distribution.To evaluate a summary SA given a reference summary SR, the authors proposeto use the negative JS divergence between the estimates of pA and pR given thesummaries,score(SA|SR) = -JS(pA ||pR ) (22)The parameters are estimated via a posteriori maximization assuming a multi-nomial generation model for each summary (which means that they are modeled asbags-of-words) and using Dirichlet priors (the conjugate priors of the multinomialfamily). So:A = arg maxAp(SA|A)p(A), (23)where (m being the number of distinct words, a1, . . . , am being the word counts inthe summary, a0 = mi=1ai)p(SA|A) =(a0 + 1)mi=1(ai + 1)mi=1A,iai (24)andp(A) =(0)mi=1(i)mi=1A,ii-1 (25)where i are hyper-parameters and 0 = mi=1i. After some algebra, we getA,i =ai + i - 1a0 + 0 - m(26)which is similar to MLE with smoothing.27 R is estimated analogously using thereference summary SR. Not surprisingly, if we have more than one reference sum-mary, the MAP estimation given all summaries equals MAP estimation given theirconcatenation into a single summary.25However, the authors do not give much support for this claim. In our view, there is no reasonto require symmetry.26Although this is not mentioned in (Lin et al., 2006), the Jensen-Shannon divergence also satisfiesthe axioms to be a squared metric, as shown by Endres and Schindelin (2003). It has also a plethoraof properties that are presented elsewhere, but this is out of scope of this survey.27In particular if i= 1 it is just maximum likelihood estimation (MLE).26The authors experimented three automatic evaluation schemes (JS with smooth-ing, JS without smoothing, and KL divergence) against manual evaluation; the bestperformance was achieved by JS without smoothing. This is not surprising since, asseen above, the JS divergence is bounded, unlike the KL divergence, and so it doesnot require smoothing. Smoothing has the effect of pulling the two distributionsmore close to the uniform distribution.

Conclusion :
The rate of information growth due to the World Wide Web has called for a needto develop efficient and accurate summarization systems. Although research onsummarization started about 50 years ago, there is still a long trail to walk inthis field. Over time, attention has drifted from summarizing scientific articles tonews articles, electronic mail messages, advertisements, and blogs. Both abstractiveand extractive approaches have been attempted, depending on the application athand. Usually, abstractive summarization requires heavy machinery for languagegeneration and is difficult to replicate or extend to broader domains. In contrast,simple extraction of sentences have produced satisfactory results in large-scale ap-plications, specially in multi-document summarization. The recent popularity ofeffective newswire summarization systems confirms this claim.This survey emphasizes extractive approaches to summarization using statisti-cal methods. A distinction has been made between single document and multi-document summarization. Since a lot of interesting work is being done far fromthe mainstream research in this field, we have chosen to include a brief discussionon some methods that we found relevant to future research, even if they focus onlyon small details related to a general summarization process and not on building anentire summarization system.Finally, some recent trends in automatic evaluation of summarization systemshave been surveyed. The low inter-annotator agreement figures observed duringmanual evaluations suggest that the future of this research area heavily depends onthe ability to find efficient ways of automatically evaluating these systems and onthe development of measures that are objective enough to be commonly acceptedby the research community.

Discussion :


Reference :
Amari, S.-I. and Nagaoka, H. (2001). Methods of Information Geometry (Transla-tions of Mathematical Monographs). Oxford University Press. [20]Aone, C., Okurowski, M. E., Gorlinsky, J., and Larsen, B. (1999). A trainablesummarizer with knowledge acquired from robust nlp techniques. In Mani, I.and Maybury, M. T., editors, Advances in Automatic Text Summarization, pages71-80. MIT Press. [4, 5]Barzilay, R. and Elhadad, M. (1997). Using lexical chains for text summarization.In Proceedings ISTS'97. [8]Barzilay, R., McKeown, K., and Elhadad, M. (1999). Information fusion in thecontext of multi-document summarization. In Proceedings of ACL '99. [12, 13,14, 16]Baxendale, P. (1958). Machine-made index for technical literature - an experiment.IBM Journal of Research Development, 2(4):354-361. [2, 3, 5]Brown, F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. (1993). Themathematics of statistical machine translation: parameter estimation. Comput.Linguist., 19(2):263-311. [18]Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., andHullender, G. (2005). Learning to rank using gradient descent. In ICML '05:Proceedings of the 22nd international conference on Machine learning, pages 89-96, New York, NY, USA. ACM. [8]Carbonell, J. and Goldstein, J. (1998). The use of MMR, diversity-based rerankingfor reordering documents and producing summaries. In Proceedings of SIGIR '98,pages 335-336, New York, NY, USA. [12, 14, 15]Collins, M. (1999). Head-Driven Statistical Models for Natural Language Parsing.PhD thesis, University of Pennsylvania. [13, 20]Conroy, J. M. and O'leary, D. P. (2001). Text summarization via hidden markovmodels. In Proceedings of SIGIR '01, pages 406-407, New York, NY, USA. [6]Cover, T. and Thomas, J. (1991). Elements of Information Theory. Wiley. [25]Daume III, H. and Marcu, D. (2002). A noisy-channel model for document com-pression. In Proceedings of the Conference of the Association of ComputationalLinguistics (ACL 2002). [20]Daume III, H. and Marcu, D. (2004). A tree-position kernel for document compres-sion. In Proceedings of DUC2004. [20]Edmundson, H. P. (1969). New methods in automatic extracting. Journal of theACM, 16(2):264-285. [2, 3, 4]28Endres, D. M. and Schindelin, J. E. (2003). A new metric for probability distribu-tions. IEEE Transactions on Information Theory, 49(7):1858-1860. [26]Evans, D. K. (2005). Similarity-based multilingual multi-document summarization.Technical Report CUCS-014-05, Columbia University. [12, 17]Gous, A. (1999). Spherical subfamily models. [20, 21]Hall, K. and Hofmann, T. (2000). Learning curved multinomial subfamilies fornatural language processing and information retrieval. In Proc. 17th InternationalConf. on Machine Learning, pages 351-358. Morgan Kaufmann, San Francisco,CA. [20]Hovy, E. and Lin, C. Y. (1999). Automated text summarization in summarist. InMani, I. and Maybury, M. T., editors, Advances in Automatic Text Summariza-tion, pages 81-94. MIT Press. [17]Knight, K. and Marcu, D. (2000). Statistics-based summarization - step one: Sen-tence compression. In AAAI/IAAI, pages 703-710. [19]Kupiec, J., Pedersen, J., and Chen, F. (1995). A trainable document summarizer.In Proceedings SIGIR '95, pages 68-73, New York, NY, USA. [4]Lebanon, G. (2006). Sequential document representations and simplicial curves. InProceedings of the 22nd Conference on Uncertainty in Artificial Intelligence. [22]Lebanon, G., Mao, Y., and Dillon, J. (2007). The locally weighted bag of wordsframework for document representation. J. Mach. Learn. Res., 8:2405-2441. [21,22]Lin, C.-Y. (1999). Training a selection function for extraction. In Proceedings ofCIKM '99, pages 55-62, New York, NY, USA. [5]Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. InMarie-Francine Moens, S. S., editor, Text Summarization Branches Out: Pro-ceedings of the ACL-04 Workshop, pages 74-81, Barcelona, Spain. [8, 23, 24,25]Lin, C.-Y., Cao, G., Gao, J., and Nie, J.-Y. (2006). An information-theoretic ap-proach to automatic evaluation of summaries. In Proceedings of HLT-NAACL'06, pages 463-470, Morristown, NJ, USA. [25, 26]Lin, C.-Y. and Hovy, E. (1997). Identifying topics by position. In Proceedings ofthe Fifth conference on Applied natural language processing, pages 283-290, SanFrancisco, CA, USA. [5]Lin, C.-Y. and Hovy, E. (2002). Manual and automatic evaluation of summaries. InProceedings of the ACL-02 Workshop on Automatic Summarization, pages 45-51,Morristown, NJ, USA. [23]29Luhn, H. P. (1958). The automatic creation of literature abstracts. IBM Journal ofResearch Development, 2(2):159-165. [2, 3, 6, 8]Mani, I. and Bloedorn, E. (1997). Multi-document summarization by graph searchand matching. In AAAI/IAAI, pages 622-628. [15, 16]Marcu, D. (1998a). Improving summarization through rhetorical parsing tuning. InProceedings of The Sixth Workshop on Very Large Corpora, pages 206-215, pages206-215, Montreal, Canada. [9, 10, 20]Marcu, D. C. (1998b). The rhetorical parsing, summarization, and generation ofnatural language texts. PhD thesis, University of Toronto. Adviser-Graeme Hirst.[10]McKeown, K., Klavans, J., Hatzivassiloglou, V., Barzilay, R., and Eskin, E.(1999). Towards multidocument summarization by reformulation: Progress andprospects. In AAAI/IAAI, pages 453-460. [11, 12, 13, 14, 16]McKeown, K. R. and Radev, D. R. (1995). Generating summaries of multiple newsarticles. In Proceedings of SIGIR '95, pages 74-82, Seattle, Washington. [8, 11,12]Miller, G. A. (1995). Wordnet: a lexical database for english. Commun. ACM,38(11):39-41. [4, 9]Nenkova, A. (2005). Automatic text summarization of newswire: Lessons learnedfrom the document understanding conference. In Proceedings of AAAI 2005,Pittsburgh, USA. [7]Ono, K., Sumita, K., and Miike, S. (1994). Abstract generation based on rhetoricalstructure extraction. In Proceedings of Coling '94, pages 344-348, Morristown,NJ, USA. [9]Osborne, M. (2002). Using maximum entropy for sentence extraction. In Proceedingsof the ACL'02 Workshop on Automatic Summarization, pages 1-8, Morristown,NJ, USA. [7]Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001). Bleu: a method forautomatic evaluation of machine translation. In Proceedings of ACL '02, pages311-318, Morristown, NJ, USA. [24]Radev, D. R., Hovy, E., and McKeown, K. (2002). Introduction to the special issueon summarization. Computational Linguistics., 28(4):399-408. [1, 2]Radev, D. R., Jing, H., and Budzikowska, M. (2000). Centroid-based summarizationof multiple documents: sentence extraction, utility-based evaluation, and userstudies. In NAACL-ANLP 2000 Workshop on Automatic summarization, pages21-30, Morristown, NJ, USA. [12, 16, 17]30Radev, D. R., Jing, H., Stys, M., and Tam, D. (2004). Centroid-based summariza-tion of multiple documents. Information Processing and Management 40 (2004),40:919-938. [16, 17]Radev, D. R. and McKeown, K. (1998). Generating natural language summariesfrom multiple on-line sources. Computational Linguistics, 24(3):469-500. [12]Salton, G. and Buckley, C. (1988). On the use of spreading activation methods inautomatic information. In Proceedings of SIGIR '88, pages 147-160, New York,NY, USA. [15]Salton, G., Wong, A., and Yang, A. C. S. (1975). A vector space model for automaticindexing. Communications of the ACM, 18:229-237. [20]Selman, B., Levesque, H. J., and Mitchell, D. G. (1992). A new method for solvinghard satisfiability problems. In AAAI, pages 440-446. [11]Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single-documentsummarization by combining RankNet and third-party sources. In Proceedings ofthe EMNLP-CoNLL, pages 448-457. [7, 8]Witbrock, M. J. and Mittal, V. O. (1999). Ultra-summarization (poster abstract):a statistical approach to generating highly condensed non-extractive summaries.In Proceedings of SIGIR '99, pages 315-316, New York, NY, USA. [18]31