Nom du fichier :
			fernandez_icassp2024.pdf
Titre :
			Functional invariants to watermark large transformers
Nombre d'auteur :
 			4
Auteurs :
			Pierre Fernandez
			Guillaume Couairon
			International Conference
			Speech and Signal Processing
Mail :
			pfz@meta.com
Abstract :
			The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.

Intro :
			Large-scale transformer models are a leap forward in the fieldof machine learning, with large language models like GPT-4 [1],LLaMA [2] and others [3, 4, 5], or vision ones like ViT-22b [6] or DI-NOv2 [7]. As these models grow in complexity and size, protectingthem is important due to investments in their development. Notably,this is raised by the US "Ensuring Safe, Secure, and Trustworthy AI"announcement, European AI Act and Chinese AI governance rules.Watermarking deep neural networks [8, 9] presents a step to-wards ensuring their security, integrity and ownership. Embedding aunique identifier into the model enables tracing it to safeguard it fromunauthorized usage and distribution. However, watermarking largetransformer models poses new challenges. Current watermarkingmethods involve optimizing the weights to infuse the watermark, ei-ther during pre-training or by fine-tuning the weights with additionallosses. While these techniques have shown success for smaller mod-els, they become computationally infeasible for large-scale modelsand for the burgeoning number of potential users and applications.To address these challenges, we introduce a new approach towatermarking large transformers, when access to both the original andwatermarked model is granted, i.e. in a non-blind white-box setting.Our method capitalizes on the inherent invariance of transformers.For a given model, it generates equivalent copies that serve as carriersfor arbitrary signatures. By employing operations such as dimensionpermutation and coupled matrix multiplications, we create modelreplicas without changing the model's outputs and without training.We conduct experiments on state-of-the-art transformer architecturesCorrepondance to pfz@meta.com:Work supported by ANR / AID under Chaire SAIDA ANR-20-CHIA-0011.Invariant copiesWatermark InsertionOriginalweightsPermutationsScaling /Unscalingx InvertibleMatrixWatermark ExtractionWeightsCompareIdentifyFig. 1: Overview. We identify each model by applying invarianceoperations to the original weights.to evaluate the applicability of our approach and its robustness againstmodel processing (e.g. fine-tuning, pruning, quantization, etc.). Wealso discuss the main drawbacks of this setting.The paper is organized as follows: section 2 provides an overviewof related works on DNN watermarking and background on transform-ers; section 3 details the transformer's invariants and how to exploitthem for watermarking; section 4 presents experimental results onlarge language models.Problem statement. A provider Alice, distributes her model tovarious users Bob (either individuals or organizations). She aimsto trace the model back to a specific user, in case of unauthorizeddistribution or leaks. As a precautionary measure, Alice embeds aunique signature in the model's weights for each user. In a white-box setting, Alice has access to the models' weights and extracts thesignature from it to identify Bob. Besides, Bob may evade detectionintentionally (trying to remove the watermark) or unintentionally(fine-tuning, quantization, etc.).This setting is quite common. Indeed few entities ("Alices") havethe necessary computation resources, data and expertise to generatethe base model. For example, the training of the 65-B LLaMA modeltook around 1B GPU-hours. Therefore, there are few variants of suchlarge models in the world. Besides, when Bob gains access to the basemodel, it is common that he transforms it and that it re-emerges in apublic forum or through another channel, so that Alice can analyzeit. This can be either because Bob re-distributed it or because Alicesought the model through legal channels, as suggested by Fan et al.[10]. For example, many variants of the LLaMA models have beenfine-tuned on instruction datasets and been made available online.

Corps :			2. RELATED WORK & TECHNICAL BACKGROUND2.1. Deep Neural Network (DNN) WatermarkingDNN watermarking robustly embeds a unique identifier into themodel without affecting its performance, in order to later verify themodel's identity. Watermarking should satisfy three criteria, utility:the new model should have the same performance as the originalone; security: it should be stealthy, hard to remove and to forge;robustness: the watermark should be detected even after the modelhas been modified. Modifications may be unintentional - models arefine-tuned, pruned and quantized - or intentional - adversaries maytry to remove the watermark or embed their own [11, 12, 13]. Forinstance, some adversarial transforms employ invariance operationsin neurons and ReLU layers to evade detection [14], in a similarfashion as the techniques of this work.We distinguish between white-box and black-box settings, de-pending on whether the model weights are accessible at verificationtime, or only through a remote API. In white-box, the pioneeringwork [8] embeds watermarks into the DNN's weights. A regulariza-tion loss term during training constrains the weights to carry a specificsignal, while minimizing the impact on the model's performance. Thewatermark is then retrieved directly by analyzing the model's weights.The Deep*Signs*Marks [15, 16] extends this to target black-box set-tings and propose building collusion-resistant watermarks, RIGA [17]improves its covertness and robustness, and greedy residuals [18]improves the selection of the weights to modify.Another line of work, called trigger-set based methods, embedsthe watermark in the behavior of the model with regards to certaininputs. A recurrent idea is to use "backdoors", i.e. memorize certainsequences of input-output pairs [9, 19]. Watermarking generativemodels is also an active field of research, either by employing trig-gers [20, 21], or by watermarking their outputs [22, 23].All previous methods require training or optimization. This isexpensive computationally and requires extensive expertise in the caseof large-scale transformers. The feasibility of existing watermarkingmethods to these models therefore remains an open question.2.2. TransformersTransformer [24] neural networks have become the standard for manyapplications in the last few years. They can be trained efficiently onGPUs and scale well to large datasets and models, in both naturallanguage processing [25, 26] and computer vision [6, 7]. In thefollowing we describe the NLP architecture from [27].The input string is first tokenized into a sequence of integerspx1, . . . , xnq P Vn. An embedding layer E P R|V|d maps eachtoken xi to a continuous vector z0i " ExiP Rd, where d is theembedding dimension. The transformer is a stack of attention andfeed-forward layers, that we describe in the following.Attention layers. The self-attention mechanism enables long-rangedependencies between sequence elements. A self-attention transformsan input sequence z P Rnd into queries Q, keys K, and values V :Q " zWQ P Rndk ; K " zWK P Rndk ; V " zWV P Rndv . (1)It then computes attention weights by taking a scaled dot productbetween the queries and keys:AttentionpQ, K, V q " softmaxQKJ?dkV. (2)Where the Softmax operator is applied column-wise.This attention operator is applied h times in parallel, yielding houtput heads. The results are concatenated and projected back to theoriginal dimension:MultiHeadpQ, K, V q " Concatphead1, ., headhqWO, (3)where headi " AttentionpQWQi, KWKi, V WVi q. The projectionsWQi, WKi P Rddk , WVi P Rddv and WO P Rhdvd are learnedparameters.The feed-forward network. The output is fed to a feed-forwardnetwork (FFN), e.g. two linear layers with a ReLU activation:FFNphq " ReLUphW1  b1qW2  b2, (4)where W1 P Rddff , b1 P Rdff , W2 P Rdff d and b2 P Rd arelearned parameters (SwiGLU [28] and other variants also frequentlyreplace ReLU).A stack of residual connections. Instead of directly feeding z andh to the attention and FFN layers, residual connections are appliedand the inputs are normalized using layer normalization [29] (orvariants like RMSnorm [30]):LayerNormpzq "z  d g  b, (5)where  and  are the mean and standard deviation of z along itssecond dimension, and g P Rd and b P Rd are learned parameters.This is repeated for each layer l P t1, ..., Lu of the transformer:hl" Attl Lnlattzl zl (6)zl1" Ffnl Lnlffnhl hl. (7)The output is fed to a normalization layer Lnout and a linear layerWout P Rd|V| to generate logits, and a softmax outputs the proba-bility distribution of the next token.Positional embeddings. For many tasks, it is useful to encode theposition of tokens in the input sequence. Positional embeddings arewhat allows to encode this information. They were originally sinu-soidal functions of the position [24] added to the input embeddings.There are now several variants [25, 31, 32, 33], that may changeEq. (2). For instance, rotary embeddings [31] multiply queries andkeys depending on their relative position in the sequence. If m is theposition of the query (Qm " zmWQ) and n the position of the key,then it rewrites the product of Eq. (2) as:QmKJn " zmWQR,nmpznWKqJ. (8)R,n is a block diagonal matrix with 2  2 rotation matrix entries:pR,nqi"cos ni  sin nisin nicos ni,with rotation frequencies chosen as i " 10, 0002i{d.3. WATERMARKING THROUGH INVARIANCE3.1. Invariants in the weights of transformersWe define an invariant as a series of operation applied on the model'sweights  N 1 such that for any input x, the output f1 pxq is thesame as before the application of the invariant.Permutation invariance appears in (at least) four levels of thetransformer. We note d the set of permutations of t1, ..., du. For amatrix M P Rdd and  P d, we denote by M:, (resp. M,:) thematrix where columns (resp. rows) are permuted according to .Embedding dimension. The embedding matrix E can be permutedalong its second dimension without changing the output of the model,as long as the permutation is propagated to other matrices in themodel. More formally, for  P d, if E1 " E:,, then matricestWQ, WK, WV, W1, Wout, Lnatt, Lnffn, b2u   need to be per-muted along their first dimension by  and all matrices tWO, W2ualong their second one: pWQq1 " WQ,:, pWOq1 " WO:,, etc.FFN layer dimension. All neurons making up matrices W1 andW2 of feed-forward networks can be permuted: for  P dff , ifW11 " pW1q:, and W12 " pW2q,:, then f1 pq " fpq.Attention heads. Because of Eq. 3, heads are interchangeable, pro-vided that WO is permuted in blocks of dv rows, according to its firstdimension. In practice, the pWQiqiPt1,..,huare stored as a unique ma-trix, so permuting the heads is equivalent to permuting the columnsof this matrix in blocks of dk (same for WKiand WVi).Inside the head. Depending on the type of positional embeddings, theprevious permutations can be extended. For instance if they do notimpact Eq. 2 (this is not the case for rotary embeddings) then WQand WK can be permuted along their second dimension.Scaling/Unscaling. Whenever layer norms or variants are directlyfollowed (or preceded) by linear layers, e.g. at every attention or FFNblock, we can rescale component-wise the parameters g, b of Eq. 5by a vector  P Rd. Invariance is obtained by dividing the rows ofthe following (or preceding) linear layers by the same vector.Invertible matrices in QK products. We hereby assume the posi-tional embeddings do not impact Eq. 2. If P P Rdkdk is invertible,then choosing pWQq1 " WQP and pWKq1 " WKpPJq1 is in-variant in Eq. 2. This also applies to the case of rotary embeddingsby restricting P to be block diagonal of 2  2 matrices that apply a2D rotations and scaling by a factor  (thanks to the commutativityof 2D rotations).Combining invariants. All previous parameter transformationsmay be seen as invertible right or left matrix multiplications applied tothe model parameters. For instance,  is associated to a permutationmatrix P such that Pi,piq " 1 and Pi,j " 0 otherwise, scalings arediagonal matrix, etc. They do not interfere and may be combined in asequence of arbitrary order, yielding  N 1 N 2 N    .Combining transformations at all levels improves robustness tointentional removal attacks and to collusion (i.e. when several Bobsshare their weights to evade detection). Indeed, if Bob tries to removethe watermark by re-applying one invariant, it will still be present inthe other invariants. In the same way, if several Bobs compare theirmodels, it will be hard for them to identify which operations wereapplied to their models, since the order in which they were applied isunknown, and since the weights will differ a lot between them.3.2. From invariants to watermarksInsertion. Before starting the watermark process, for each invariantand each level of the network, we restrict the set of transformationsto 2k. For example, we randomly sample 2k possible permutationsin d for the Embedding dimension (out of the total d!). This is tosimplify the detection procedure.Therefore, we can encode k bits for each combination of aninvariant and a level. Thus, we encode a model's identifier as theconcatenation of m chunks of k bits (2mk possibilities). For instance,let k " 4 and the model have 32 layers. We choose to embed twopermutations per layer, one for the attention block and one for the FFNblock. The total number of bits is 2324 " 256, representing 1077possible models (approximately the number of atoms in the universe,an upper bound of the number of Bobs). This embedding processprovides robustness and efficiency when extracting the watermarks.Extraction. The brute-force way to extract the k-bits message froma weight matrix is to re-apply all 2k possible invariants to the origi-nal matrix. We then compute the Frobenius norm of the differencebetween the observed weight and the possible watermarked ones. Wechoose the one with lowest among the 2k and this choice is encodedas a k-bit integer. Doing that on every blocks of the network andevery invariant, we end up with a full message made of m chunks ofk bits. In the case of intertwined invariants, we do the same in theorder in which we inserted the invariants, reverting them as we go.To speed up extraction, we may select a subset of the matrices'rows before extraction. This speeds up the extraction (in the order of100), but makes the detection slightly less robust. For instance, inthe case of scaling/unscaling we may select the first 100 componentsof  from Rd to R100 and W from Rdd1to R100100.Matching. To match an extracted message (made of m chunks ofk bits) with a model's identifier, we compute the number s of chunk-wise errors with respect to all possible identifiers. We return a matchif s is bellow a fixed threshold  to ensure resilience to corruptionand to provide a confidence score. A theoretical p-value, i.e. theprobability of obtaining a number of errors lower than s for a randommodel, is given by the regularized incomplete beta function I:p-valuepsq " 1  1  I1{2k pm  s, s  1qN, (9)where N is the number of distributed models. N accounts for thefact that when multiple models are distributed, the risk of finding amatch by chance is higher.Robustness and security. Watermarking models through invari-ance is stealthy, because it does not change their outputs and doesnot leave artifacts in the weights. However, a distortion-free wa-termark is also a weakness. On the one hand, Alice can hide thewatermark without impacting the model's utility, on the other hand,an adversarial Bob may do the same at no cost. In short, most of thesewatermarks are very robust against classical model manipulations(fine-tuning, quantization, etc.) but not against a malicious user whoknows the method. In this case we would only know that the modelis an unauthorized copy, without knowing the leaker.4. EXPERIMENTSThe purpose of the experiments is to evaluate the effectiveness andthe robustness of the watermarks to transformations on transformers,in the context of large language models.4.1. SetupModel. We use LLaMA [2] models as main benchmark. The ar-chitectural differences with regards to the original transformer ar-chitecture are pre-normalization [27] with RMSnorm [30], SwiGLUactivation [28] and rotary embeddings [31]. Unless stated otherwise,we use the 7B-parameter model. To evaluate that the utility of themodel is not degraded, we show results on a next-token predictiontask. This is done on random sequences of text taken from Wikipedia,then tokenized using the default tokenizer of LLaMA.Table 1: Distortion induced on generation (measured as the pro-portion of generated tokens that differ between the original and thewatermarked model); and robustness of watermark extraction undervarious processes. Each line stands for a different invariant, "all" isthe combination of the three invariants in the following order: perm,QK and scaling. We present results of the sped-up extraction, theones for no speed-up are given as (acc).Method DistortionByte accuracy (%) on:Noise 1.0 Quant. 3b Prun. 50% Fine-tunePerm. 0.20% 51.4 (99.6) 72.0 (100.0) 100.0 100.0QK 0.18% 100.0 100.0 100.0 100.0Scaling 0.24% 100.0 98.1 (100.0) 100.0 100.0All 1.77% 60.8 (99.8) 70.0 (99.4) 100.0 100.0Attacks. We consider the following attacks. Fine-tuning. We fine-tune the model in a supervised manner with the same settings as [34],on 3 epochs with learning-rate of 2  105. Noise. We add zero-mean Gaussian noise with standard deviation  to the model weights.Quantization. We quantize the model weights into b bits. To allowflexible rates and ease the experiments, this is done by uniformlyquantizing the weights between their minimum and maximum values.Pruning. We prune the model weights by zeroing the ones withsmallest L1 norms, with sparsity given in percentage of zero weights.Watermark settings. We apply the encoding process of Sect. 3.2.For permutation invariance, we permute attention heads and FFNlayers. For scaling, we alter the layers' RMSnorms and followingmatrices. The scaling vector  is such that log10pq  Up1, 1q.For QK products, as mentioned in 3.1, the invertible matrix has tobe block diagonal of 2 by 2 rotation matrices, so we randomly sampled{2 rotation angles. We fix the number of possible choices at k=8,i.e. each choice is encoded with a byte. Therefore, we encode 2 bytesat every layer, except in QK products where we encode 1. Whencombining all invariants together, we proceed the same way for allblocks: we start with permutation, then apply invertible matrices inQK products, then scale/unscale the layer norms and matrices.4.2. Results.Robustness. We evaluate the robustness of the watermark usingthe byte accuracy, i.e. the percentage of bytes correctly recovered.Results are averaged over N=100 watermarked models except forfine-tuning where we only fine-tune one model. The model has L=32layers so the watermark is 64 bytes long except for the QK productsinvariant where it is 32. In the case of combined invariants, the totalnumber of bytes is 160. We speed-up the extraction by selecting asubset of 100 rows of the matrices (see Sect. 3.2); time needed forextraction is around 20 minutes when using the full matrix instead.Table 1 reports the byte accuracy for different processing appliedbefore extraction. We observe that the watermark is robust to allattacks with byte accuracy >50%. Errors mainly come from thespeed-up of the extraction process. We also consider the p-value ofthe associated statistical test (see Eq. 9). A byte accuracy of 50% on64-bytes messages is more than enough to reliably identify a model:the p-values are always bellow 1060, due to the very low probabilityof simultaneously observing a match between tens of pairs of randombytes. As an illustration, 8 matching bytes on 64-bytes messagesalready gives a p-value of 108.Table 2: Computational cost of watermark insertion and extractionfor different model sizes and the different invariants.Model L dInsertion (s) Extraction (s)Perm. Scaling QK Perm. Scaling QK7b 32 4096 3.5 2.7 7.4 9.2 31.7 6.013b 40 5120 7.0 4.9 15.8 14.1 30.3 7.730b 60 6656 19.3 8.7 47.3 31.7 54.7 13.570b 80 8192 37.1 17.5 106 56.3 110 21.5Model's utility. In fact, previous invariants are not perfect becauseof quantization (weights are stored as 16bits floating point numbers).Thus, we quantitatively compare watermarked and original models.We feed to both of them 1k sequences of 256 tokens. Predicted nexttokens are greedily chosen as the argmax of the 256k observed logits.Table 1 reports the proportion of predicted tokens that differ be-tween watermarked and original models. As expected this proportionis very low (<1.8%) and higher for the scaling invariant since it fur-ther affects quantization. Besides, the distortion increases when thetoken is far in the sequence e.g. for sequences of length 1024 tokens,the average distortion at the last token rises to 2.5% for the scalinginvariant. This is still very low and does not affect the utility of themodel since predicted tokens are still likely.Computational efficiency. Larger models have more layers andparameters, which increases the watermark capacity but also thecomputational cost of insertion and extraction. In Table 2, we reportresults for different model sizes. Insertion and extraction times areaveraged over 100 runs and measured on 2 Intel(R) Xeon(R) 6230 @2.10GHz cores and a total of 480GB of RAM. The low computationalcosts and requirements (no GPU needed) makes it possible to scaleto very large models.

Conclusion :
Our work presents a lightweight approach for watermarking largetransformers. We leverage invariance properties to generate equiv-alent copies for watermark embedding. It ensures that the model'soutputs are preserved while providing close-to-perfect robustnessagainst processes like fine-tuning or quantization.Yet, this approach has limitations. Namely, it is limited to white-box scenarios. Additionally, if a sophisticated attacker identifies allinvariants, they may remove the watermark by applying the sametransformation techniques. In this case, it would still be possible toidentify that the model is an unauthorized copy but without the cor-responding binary signature. Overall, this work is a starting point toexploit invariance properties that stem from the extreme redundancyof parameters of large networks, for watermarking applications.

Discussion :
N/A

Reference :
[1] OpenAI, "Gpt-4 technical report," arXiv, 2023.[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al.,"Llama: Open and efficient foundation language models," arXivpreprint arXiv:2302.13971, 2023.[3] M. AI, "Mistral 7b," arXiv preprint arXiv:2310.06825, 2023.[4] AnthropicAI, "Introducing claude," 2023.[5] S. Pichai, "An important next step on our AI journey," GoogleAI Blog, 2023.[6] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek,J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin,et al., "Scaling vision transformers to 22 billion parameters," inInternational Conference on Machine Learning, pp. 7480-7512,PMLR, 2023.[7] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec,V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby,et al., "Dinov2: Learning robust visual features without super-vision," arXiv preprint arXiv:2304.07193, 2023.[8] Y. Uchida, Y. Nagai, S. Sakazawa, and S. Satoh, "Embeddingwatermarks into deep neural networks," in Proceedings of the2017 ACM on international conference on multimedia retrieval,pp. 269-277, 2017.[9] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, "Turningyour weakness into a strength: Watermarking deep neural net-works by backdooring," in 27th USENIX Security Symposium(USENIX Security 18), pp. 1615-1631, 2018.[10] L. Fan, K. W. Ng, C. S. Chan, and Q. Yang, "Deepip: Deepneural network intellectual property protection with passports,"IEEE Transactions on Pattern Analysis & Machine Intelligence,no. 01, pp. 1-1, 2021.[11] L. Fan, K. W. Ng, and C. S. Chan, "Rethinking deep neuralnetwork ownership verification: Embedding passports to defeatambiguity attacks," Advances in neural information processingsystems, vol. 32, 2019.[12] J. Zhang, D. Chen, J. Liao, W. Zhang, G. Hua, and N. Yu,"Passport-aware normalization for deep model protection," Ad-vances in Neural Information Processing Systems, vol. 33,pp. 22619-22628, 2020.[13] K. Kallas and T. Furon, "Rose: A robust and secure dnn water-marking," in 2022 IEEE International Workshop on InformationForensics and Security (WIFS), pp. 1-6, IEEE, 2022.[14] Y. Yan, X. Pan, M. Zhang, and M. Yang, "Rethinking white-boxwatermarks on deep learning models under neural structuralobfuscation," in 32th USENIX security symposium (USENIXSecurity 23), 2023.[15] B. Darvish Rouhani, H. Chen, and F. Koushanfar, "Deepsigns:An end-to-end watermarking framework for ownership protec-tion of deep neural networks," in Proceedings of the Twenty-Fourth International Conference on Architectural Support forProgramming Languages and Operating Systems, pp. 485-497,2019.[16] H. Chen, B. D. Rouhani, C. Fu, J. Zhao, and F. Koushanfar,"Deepmarks: A secure fingerprinting framework for digitalrights management of deep learning models," in Proceedings ofthe 2019 on International Conference on Multimedia Retrieval,pp. 105-113, 2019.[17] T. Wang and F. Kerschbaum, "Riga: Covert and robust white-box watermarking of deep neural networks," in Proceedings ofthe Web Conference 2021, pp. 993-1004, 2021.[18] H. Liu, Z. Weng, and Y. Zhu, "Watermarking deep neural net-works with greedy residuals.," in ICML, pp. 6978-6988, 2021.[19] J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin, H. Huang,and I. Molloy, "Protecting intellectual property of deep neuralnetworks with watermarking," in Proceedings of the 2018 onAsia conference on computer and communications security,pp. 159-172, 2018.[20] J. H. Lim, C. S. Chan, K. W. Ng, L. Fan, and Q. Yang, "Pro-tect, show, attend and tell: Empowering image captioning mod-els with ownership protection," Pattern Recognition, vol. 122,p. 108285, 2022.[21] D. S. Ong, C. S. Chan, K. W. Ng, L. Fan, and Q. Yang, "Protect-ing intellectual property of generative adversarial networks fromambiguity attacks," in Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 3630-3639,2021.[22] P. Fernandez, G. Couairon, H. Jegou, M. Douze, and T. Furon,"The stable signature: Rooting watermarks in latent diffusionmodels," ICCV, 2023.[23] C. Kim, K. Min, M. Patel, S. Cheng, and Y. Yang, "Wouaf:Weight modulation for user attribution and fingerprinting in text-to-image diffusion models," arXiv preprint arXiv:2306.04744,2023.[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.Gomez, L. Kaiser, and I. Polosukhin, "Attention is all you need,"Advances in neural information processing systems, vol. 30,2017.[25] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,M. Matena, Y. Zhou, W. Li, and P. J. Liu, "Exploring the limitsof transfer learning with a unified text-to-text transformer," TheJournal of Machine Learning Research, vol. 21, no. 1, pp. 5485-5551, 2020.[26] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei,"Scaling laws for neural language models," arXiv preprintarXiv:2001.08361, 2020.[27] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,et al., "Language models are unsupervised multitask learners,"OpenAI blog, vol. 1, no. 8, p. 9, 2019.[28] N. Shazeer, "Glu variants improve transformer," arXiv preprintarXiv:2002.05202, 2020.[29] J. L. Ba, J. R. Kiros, and G. E. Hinton, "Layer normalization,"arXiv preprint arXiv:1607.06450, 2016.[30] B. Zhang and R. Sennrich, "Root mean square layer normal-ization," Advances in Neural Information Processing Systems,vol. 32, 2019.[31] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, and Y. Liu, "Ro-former: Enhanced transformer with rotary position embedding,"arXiv preprint arXiv:2104.09864, 2021.[32] O. Press, N. Smith, and M. Lewis, "Train short, test long: At-tention with linear biases enables input length extrapolation," inInternational Conference on Learning Representations, 2021.[33] A. Kazemnejad, I. Padhi, K. N. Ramamurthy, P. Das, andS. Reddy, "The impact of positional encoding on length gen-eralization in transformers," arXiv preprint arXiv:2305.19466,2023.[34] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin,P. Liang, and T. B. Hashimoto, "Stanford Alpaca: Aninstruction-following LLaMA model," 2023.