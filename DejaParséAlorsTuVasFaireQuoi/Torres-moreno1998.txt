Nom du fichier :
			Torres-moreno1998.pdf
Titre :
			LETTER
Nombre d'auteur :
 			1
Auteur :
			joe@next.castanet.com Joe Pickert
Abstract :
			This article presents a new incremental learning algorithm for classification tasks, called NetLines, which is well adapted for both binary and real-valued input patterns. It generates small, compact feedforward neural networks with one hidden layer of binary units and binary output units. A convergence theorem ensures that solutions with a finite number of hidden units exist for both binary and real-valued input patterns. An implementation for problems with more than two classes, valid for any binary classifier, is proposed. The generalization error and the size of the resulting networks are compared to the best published results on well-known classification benchmarks. Early stopping is shown to decrease overfitting, without improving the generalization performance.

References :			
			Alpaydin, E. A. I. (1990). Neural models of supervised and unsupervised learning. Un- published doctoral dissertation, Ecole Polytechnique F ed erale de Lausanne,
			Switzerland.
			Biehl, M., & Opper, M. (1991). Tilinglike learning in the parity machine. Physical
			Review A, 44, 6888.
			Bottou, L., & Vapnik, V. (1992). Local learning algorithms. Neural Computation,
			4(6), 888-900.
			Breiman, L. (1994). Bagging predictors (Tech. Rep. No. 421). Berkeley: Department of Statistics, University of California at Berkeley.
			Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification and regression trees. Monterey, CA: Wadsworth and Brooks/Cole.
			Denker,J.,Schwartz,D.,Wittner,B.,Solla,S.,Howard,R.,Jackel,L.,&Hopfield,J.
			(1987). Large automatic learning, rule extraction, and generalization. Complex
			Systems, 1, 877-922.
			Depenau, J. (1995). Automated design of neural network architecture for classification.
			Unpublished doctoral dissertation, Computer Science Department, Aarhus
			University.
			Drucker, H., Schapire, R., & Simard, P. (1993). Improving performance in neu- ral networks using a boosting algorithm. In S. J. Hanson, J. D. Cowan, &
			C. L. Giles (Eds.), Advances in neural information processing systems, 5 (pp. 42-
			49). San Mateo, CA: Morgan Kaufmann.
			Fahlman, S. E., & Lebiere, C. (1990). The cascade-correlation learning architec- ture. In D. S. Touretzky (Ed.), Advances in neural information processing systems,
			2 (pp. 524-532). San Mateo: Morgan Kaufmann.
			Farrell, K. R., & Mammone, R. J. (1994). Speaker recognition using neural tree networks. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in Neural
			Information Processing Systems, 6 (pp. 1035-1042). San Mateo, CA: Morgan
			Kaufmann.
			Frean, M. (1990). The Upstart algorithm: A method for constructing and training feedforward neural networks. Neural Computation, 2(2), 198-209.
			Frean, M. (1992). A "thermal" perceptron learning rule. Neural Computation, 4(6),
			946-957.
			Friedman, J. H. (1996). On bias, variance, 0/1-loss, and the curse-of-dimensionality
			(Tech. Rep.) Stanford, CA: Department of Statistics, Stanford University.
			Fritzke, B. (1994). Supervised learning with growing cell structures. In
			J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances in neural informa- tion processing systems, 6 (pp. 255-262). San Mateo, CA: Morgan Kaufmann.
			Gallant, S. I. (1986). Optimal linear discriminants. In Proc. 8th. Conf. Pattern
			Recognition, Oct. 28-31, Paris, vol. 4.
			Classification Tasks with Binary Units 1029
			Gascuel, O. (1995). Symenu. Collective Paper (Gascuel O. Coordinator) (Tech. Rep.). emes Journ ees Nationales du PRC-IA Teknea, Nancy.
			Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the bias/variance dilemma. Neural Computation, 4(1), 1-58.
			Goodman, R. M., Smyth, P., Higgins, C. M., & Miller, J. W. (1992). Rule-based neural networks for classification and probability estimation. Neural Compu- tation, 4(6), 781-804.
			Gordon, M. B. (1996). A convergence theorem for incremental learning with real- valued inputs. In IEEE International Conference on Neural Networks, pp. 381-
			386.
			Gordon, M. B., & Berchier, D. (1993). Minimerror: A perceptron learning rule that finds the optimal weights. In M. Verleysen (Ed.), European Symposium on
			Artificial Neural Networks (pp. 105-110). Brussels: D Facto.
			Gordon, M. B., & Grempel, D. (1995). Optimal learning with a temperature dependent algorithm. Europhysics Letters, 29(3), 257-262.
			Gordon, M. B., Peretto, P., & Berchier, D. (1993). Learning algorithms for percep- trons from statistical physics. Journal of Physics I (France), 3, 377-387.
			Gorman, R. P., & Sejnowski, T. J. (1988). Analysis of hidden units in a layered network trained to classify sonar targets. Neural Networks, 1, 75-89.
			Gyorgyi, G., & Tishby, N. (1990). Statistical theory of learning a rule. In
			W. K. Theumann & R. Koeberle (Eds.), Neural networks and spin glasses. Sin- gapore: World Scientific.
			Hoehfeld, M., & Fahlman, S. (1991). Learning with limited numerical precision using the cascade correlation algorithm (Tech. Rep. No. CMU-CS-91-130). Pittsburgh:
			Carnegie Mellon University.
			Knerr, S., Personnaz, L., & Dreyfus, G. (1990). Single-layer learning revisited: A stepwise procedure for building and training a neural network. In J. H erault
			& F. Fogelman (Eds.), Neurocomputing, algorithms, architectures and applications
			(pp. 41-50). Berlin: Springer-Verlag.
			Marchand, M., Golea, M., & Ruj an, P. (1990). A convergence theorem for sequen- tial learning in two-layer perceptrons. Europhysics Letters, 11, 487-492.
			Martinez, D., & Est eve, D. (1992). The offset algorithm: Building and learning method for multilayer neural networks. Europhysics Letters, 18, 95-100.
			M ezard, M., & Nadal, J.-P. (1989). Learning in feedforward layered networks:
			The Tiling algorithm. J. Phys. A: Math. and Gen., 22, 2191-2203.
			Mukhopadhyay, S., Roy, A., Kim, L. S., & Govil, S. (1993). A polynomial time al- gorithm for generating neural networks for pattern classification: Its stability properties and some test results. Neural Computation, 5(2), 317-330.
			Nadal, J.-P. (1989). Study of a growth algorithm for a feedforward neural net- work. Int. J. Neur. Syst., 1, 55-59.
			Prechelt, L. (1994). PROBEN1--A set of benchmarks and benchmarking rules for neu- ral network training algorithms (Tech. Rep. No. 21/94). University of Karlsruhe,
			Faculty of Informatics.
			Raffin, B., & Gordon, M. B. (1995). Learning and generalization with Minimerror, a temperature dependent learning algorithm. Neural Computation, 7(6), 1206-
			1224.
			1030 J. Manuel Torres Moreno and Mirta B. Gordon
			Reilly, D. E, Cooper, L. N., & Elbaum, C. (1982). A neural model for category learning. Biological Cybernetics, 45, 35-41.
			Roy, A., Kim, L., & Mukhopadhyay, S. (1993). A polynomial time algorithm for the construction and training of a class of multilayer perceptron. Neural
			Networks, 6(1), 535-545.
			Sirat, J. A., & Nadal, J.-P. (1990). Neural trees: A new tool for classification.
			Network, 1, 423-438.
			Solla, S. A. (1989). Learning and generalization in layered neural networks: The contiguity problem. In L. Personnaz & G. Dreyfus (Eds.), Neural Networks from Models to Applications. Paris: I.D.S.E.T.
			Torres Moreno, J.-M., & Gordon, M. B. (1995). An evolutive architecture coupled with optimal perceptron learning for classification. In M. Verleysen (Ed.),
			European Symposium on Artificial Neural Networks. Brussels: D Facto.
			Torres Moreno, J.-M., & Gordon, M. B. (1998). Characterization of the sonar signals benchmark. Neural Proc. Letters, 7(1), 1-4.
			Trhun, S. B., et al. (1991). The monk's problems: A performance comparison of different learning algorithms (Tech. Rep. No. CMU-CS-91-197). Pittsburgh: Carnegie
			Mellon University.
			Vapnik, V. (1992). Principles of risk minimization for learning theory. In
			J. E. Moody, S. J. Hanson, & R. P. Lippmann (Eds.), Advances in neural informa- tion processing systems, 4 (pp. 831-838). San Mateo, CA: Morgan Kaufmann.
			Verma, B. K., & Mulawka, J. J. (1995). A new algorithm for feedforward neu- ral networks. In M. Verleysen (Ed.), European Symposium on Artificial Neural
			Networks (pp. 359-364). Brussels: D Facto.
			Wolberg, W. H., & Mangasarian, O. L. (1990). Multisurface method of pattern separation for medical diagnosis applied to breast cytology. In Proceedings of the National Academy of Sciences, USA, 87, 9193-9196.
			Received February 13, 1997; accepted September 4, 1997.
			This article has been cited by:
			1. C. Citterio, A. Pelagotti, V. Piuri, L. Rocca. 1999. Function approximation-fast-convergence neural approach based on spectral analysis. IEEE Transactions on Neural Networks 10, 725-740. [CrossRef]
			2. Andrea Pelagotti, Vincenzo Piuri. 1997. Entropic Analysis and Incremental Synthesis of Multilayered Feedforward Neural
			Networks. International Journal of Neural Systems 08, 647-659. [CrossRef]

Conclusion :
			We presented an incremental learning algorithm for classification, which we
			call NetLines. It generates small feedforward neural networks with a single
			hidden layer of binary units connected to a binary output neuron. NetLines
			allows for an automatic adaptation of the neural network to the complexity
			of the particular task. This is achieved by coupling an error-correcting strat-
			egy for the successive addition of hidden neurons with Minimerror, a very
			1026 J. Manuel Torres Moreno and Mirta B. Gordon
			 
			
			
			
			
			
			
			 
			
			
			
			1HW/LQHV YRWH
			1HW/LQHV :7$
			,5,6 GDWDEDVH
			
			J
			1XPEHU RI ZHLJKWV
			Figure 7: Iris database: Generalization error g
			versus number of parameters.
			1: offset, 2: backpropagation (Martinez & Est
			eve, 1992); 4,5: backpropagation
			(Verma & Mulawka, 1995); 3,6: gradient-descent orthogonalized training (Verma
			& Mulawka, 1995).
			efficient perceptron training algorithm. Learning is fast not only because
			it reduces the problem to that of training single perceptrons, but mainly
			because there is no longer a need for the usual preliminary tests required to
			determine the correct architecture for the particular application. Theorems
			valid for binary as well as for real-valued inputs guarantee the existence of
			a solution with a bounded number of hidden neurons obeying the growth
			strategy.
			The networks are composed of binary hidden units whose states consti-
			tute a faithful encoding of the input patterns. They implement a mapping
			from the input space to a discrete H-dimensional hidden space, H being
			the number of hidden neurons. Thus, each pattern is labeled with a binary
			word of H bits. This encoding may be seen as a compression of the pattern's
			information. The hidden neurons define linear boundaries, or portions of
			boundaries, between classes in input space. The network's output may be
			given a probabilistic interpretation based on the distance of the patterns to
			these boundaries.
			Tests on several benchmarks showed that the networks generated by our
			incremental strategy are small, in spite of the fact that the hidden neurons
			are appended until error-free learning is reached. Even when the networks
			obtained with NetLines are larger than those used by other algorithms, its
			generalization error remains among the smallest values reported. In noisy
			or difficult problems, it may be useful to stop the network's growth before
			Classification Tasks with Binary Units 1027
			the condition of zero training errors is reached. This decreases overfitting, as
			smaller networks (with less parameters) are thus generated. However, the
			prediction quality (measured by the generalization error) of the classifiers
			generated with NetLines is not improved by early stopping.
			Our results were obtained without cross-validation or any data manip-
			ulation like boosting, bagging, or arcing (Breiman, 1994; Drucker, Schapire,
			& Simard, 1993). Those costly procedures combine results of very large
			numbers of classifiers, with the aim of improving the generalization perfor-
			mance through the reduction of the variance. Because NetLines is a stable
			classifier, presenting small variance, we do not expect that such techniques
			would significantly improve our results.
			Appendix
			In this appendix we exhibit a particular solution to the learning strategy of
			NetLines. This solution is built in such a way that the cardinal of a convex
			subset of well-learned patterns, Lh, grows monotonically upon the addition
			of hidden units. Because this cardinal cannot be larger than the total number
			of training patterns, the algorithm must stop with a finite number of hidden
			units.
			Suppose that h hidden units have already been included and that the
			output neuron still makes classification errors on patterns of the training set,
			called training errors. Among these wrongly learned patterns, let  be the
			one closest to the hyperplane normal to wh, called hyperplane-h hereafter.
			We define Lh as the subset of (correctly learned) patterns lying closer to
			hyperplane-h than . Patterns in Lh have 0 < h
			< | 
			h
			|. The subset Lh and
			at least pattern  are well learned if the next hidden unit, h+1, has weights:
			wh+1
			= 
			h
			wh
			- (1 - h
			)
			h
			(wh
			* )
			e0
			, (A.1)
			where 
			e0
			 (1, 0, . . . , 0). The conditions that both Lh and pattern  have
			positive stabilities (are correctly learned) impose that
			0 < h
			< min
			Lh
			| 
			h
			| -  
			h
			| 
			h
			|
			. (A.2)
			The following weights between the hidden units and the output will give
			the correct output to pattern  and to the patterns of Lh:
			W0
			(h + 1) = W0
			(h) +  (A.3)
			Wi
			(h + 1) = Wi
			(h) for 1  i  h (A.4)
			Wh+1
			(h + 1) = -. (A.5)
			Thus, card(Lh+1
			)  card(Lh
			) + 1. As the number of patterns in Lh increases
			monotonically with h, convergence is guaranteed with less than P hidden
			units.
			1028 J. Manuel Torres Moreno and Mirta B. Gordon


Discussion :

