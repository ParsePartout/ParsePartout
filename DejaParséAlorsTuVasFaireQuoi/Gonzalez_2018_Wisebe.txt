Nom du fichier :
			Gonzalez_2018_Wisebe.pdf
Titre :
			WiSeBE: Window-Based Sentence Boundary Evaluation
Nombre d'auteur :
 			2
Auteurs :
			Carlos-Emiliano Gonzalez-Gallardo
			Juan-Manuel Torres-Moreno
Abstract :
			Sentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the transcript? In this paper we propose Window-based Sentence Boundary Evaluation (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable metric for the SBD task.

References :			
			1. Bohac, M., Blavka, K., Kucharova, M., Skodova, S.: Post-processing of the recog- nized speech for web presentation of large audio archive. In: 2012 35th International
			Conference on Telecommunications and Signal Processing (TSP), pp. 441-445.
			IEEE (2012)
			2. Brum, H., Araujo, F., Kepler, F.: Sentiment analysis for Brazilian portuguese over a skewed class corpora. In: Silva, J., Ribeiro, R., Quaresma, P., Adami, A., Branco,
			A. (eds.) PROPOR 2016. LNCS (LNAI), vol. 9727, pp. 134-138. Springer, Cham
			(2016). https://doi.org/10.1007/978-3-319-41552-9 14
			3. Che, X., Wang, C., Yang, H., Meinel, C.: Punctuation prediction for unsegmented transcript based on word vector. In: LREC (2016)
			4. Fleiss, J.L.: Measuring nominal scale agreement among many raters. Psychol. Bull.
			76(5), 378 (1971)
			5. Fohr, D., Mella, O., Illina, I.: New paradigm in speech recognition: deep neural net- works. In: IEEE International Conference on Information Systems and Economic
			Intelligence (2017)
			6. Gonz alez-Gallardo, C.E., Hajjem, M., SanJuan, E., Torres-Moreno, J.M.: Tran- scripts informativeness study: an approach based on automatic summarization. In:
			Conf erence en Recherche d'Information et Applications (CORIA), Rennes, France,
			May (2018)
			7. Gonz alez-Gallardo, C.E., Torres-Moreno, J.M.: Sentence boundary detection for
			French with subword-level information vectors and convolutional neural networks. arXiv preprint arXiv:1802.04559 (2018)
			8. Gotoh, Y., Renals, S.: Sentence boundary detection in broadcast speech transcripts.
			In: ASR2000-Automatic Speech Recognition: Challenges for the new Millenium
			ISCA Tutorial and Research Workshop (ITRW) (2000)
			130 C.-E. Gonz alez-Gallardo and J.-M. Torres-Moreno
			9. Hinton, G., et al.: Deep neural networks for acoustic modeling in speech recogni- tion: the shared views of four research groups. IEEE Signal Process. Mag. 29(6),
			82-97 (2012)
			10. Jamil, N., Ramli, M.I., Seman, N.: Sentence boundary detection without speech recognition: a case of an under-resourced language. J. Electr. Syst. 11(3), 308-318
			(2015)
			11. Kiss, T., Strunk, J.: Unsupervised multilingual sentence boundary detection. Com- put. Linguist. 32(4), 485-525 (2006)
			12. Klejch, O., Bell, P., Renals, S.: Punctuated transcription of multi-genre broadcasts using acoustic and lexical approaches. In: 2016 IEEE Spoken Language Technology
			Workshop (SLT), pp. 433-440. IEEE (2016)
			13. Kol a r, J., Lamel, L.: Development and evaluation of automatic punctuation for
			French and english speech-to-text. In: Thirteenth Annual Conference of the Inter- national Speech Communication Association (2012)
			14. Kol a r, J., 
			Svec, J., Psutka, J.: Automatic punctuation annotation in Czech broad- cast news speech. In: SPECOM 2004 (2004)
			15. Liu, Y., Chawla, N.V., Harper, M.P., Shriberg, E., Stolcke, A.: A study in machine learning from imbalanced data for sentence boundary detection in speech. Comput.
			Speech Lang. 20(4), 468-494 (2006)
			16. Lu, W., Ng, H.T.: Better punctuation prediction with dynamic conditional ran- dom fields. In: Proceedings of the 2010 Conference on Empirical Methods in Natu- ral Language Processing. pp. 177-186. Association for Computational Linguistics
			(2010)
			17. Meteer, M., Iyer, R.: Modeling conversational speech for speech recognition. In:
			Conference on Empirical Methods in Natural Language Processing (1996)
			18. Mrozinski, J., Whittaker, E.W., Chatain, P., Furui, S.: Automatic sentence seg- mentation of speech for automatic summarization. In: 2006 IEEE International
			Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1, p. I.
			IEEE (2006)
			19. Palmer, D.D., Hearst, M.A.: Adaptive sentence boundary disambiguation. In: Pro- ceedings of the Fourth Conference on Applied Natural Language Processing, pp.
			78-83. ANLC 1994. Association for Computational Linguistics, Stroudsburg, PA,
			USA (1994)
			20. Palmer, D.D., Hearst, M.A.: Adaptive multilingual sentence boundary disambigua- tion. Comput. Linguist. 23(2), 241-267 (1997)
			21. Pearson, K.: Note on regression and inheritance in the case of two parents. Proc.
			R. Soc. Lond. 58, 240-242 (1895)
			22. Peitz, S., Freitag, M., Ney, H.: Better punctuation prediction with hierarchical phrase-based translation. In: Proceedings of the International Workshop on Spoken
			Language Translation (IWSLT), South Lake Tahoe, CA, USA (2014)
			23. Rott, M., 
			Cerva, P.: Speech-to-text summarization using automatic phrase extrac- tion from recognized text. In: Sojka, P., Hor ak, A., Kope cek, I., Pala, K. (eds.) TSD
			2016. LNCS (LNAI), vol. 9924, pp. 101-108. Springer, Cham (2016). https://doi. org/10.1007/978-3-319-45510-5 12
			24. Shriberg, E., Stolcke, A.: Word predictability after hesitations: a corpus-based study. In: Proceedings of the Fourth International Conference on Spoken Language,
			1996. ICSLP 1996, vol. 3, pp. 1868-1871. IEEE (1996)
			25. Stevenson, M., Gaizauskas, R.: Experiments on sentence boundary detection. In:
			Proceedings of the sixth conference on Applied natural language processing, pp.
			84-89. Association for Computational Linguistics (2000)
			WiSeBE: Window-Based Sentence Boundary Evaluation 131
			26. Stolcke, A., Shriberg, E.: Automatic linguistic segmentation of conversational speech. In: Proceedings of the Fourth International Conference on Spoken Lan- guage, 1996. ICSLP 1996, vol. 2, pp. 1005-1008. IEEE (1996)
			27. Strassel, S.: Simple metadata annotation specification v5. 0, linguistic data consor- tium (2003). http://www.ldc.upenn.edu/projects/MDE/Guidelines/SimpleMDE
			V5.0.pdf
			28. Tilk, O., Alum ae, T.: Bidirectional recurrent neural network with attention mech- anism for punctuation restoration. In: Interspeech 2016 (2016)
			29. Treviso, M.V., Shulby, C.D., Aluisio, S.M.: Evaluating word embeddings for sen- tence boundary detection in speech transcripts. arXiv preprint arXiv:1708.04704
			(2017)
			30. Ueffing, N., Bisani, M., Vozila, P.: Improved models for automatic punctuation prediction for spoken and written text. In: Interspeech, pp. 3097-3101 (2013)
			31. Wang, W., Tur, G., Zheng, J., Ayan, N.F.: Automatic disfluency removal for improving spoken language translation. In: 2010 IEEE International Conference on
			Acoustics Speech and Signal Processing (ICASSP), pp. 5214-5217. IEEE (2010)
			32. Xu, C., Xie, L., Huang, G., Xiao, X., Chng, E.S., Li, H.: A deep neural network approach for sentence boundary detection in broadcast news. In: Fifteenth Annual
			Conference of the International Speech Communication Association (2014)
			33. Yu, D., Deng, L.: Automatic Speech Recognition. Springer, London (2015). https:// doi.org/10.1007/978-1-4471-5779-3

Conclusion :
			In this paper we presented WiSeBE, a semi-automatic multi-reference sentence
			boundary evaluation protocol based on the necessity of having a more reliable
			way for evaluating the SBD task. We showed how WiSeBE is an inclusive metric
			which not only evaluates the performance of a system against all references, but
			also takes into account the agreement between them. According to your point
			of view, this inclusivity is very important given the difficulties that are present
			when working with spoken language and the possible disagreements that a task
			like SBD could provoke.
			WiSeBE shows to be correlated with standard SBD metrics, however we
			want to measure its correlation with extrinsic evaluations techniques like auto-
			matic summarization and machine translation.


Discussion :
			5.1 RGA R
			and Fleiss' Kappa correlation
			In Sect. 3 we described the WiSeBE score and how it relies on the RGAR
			value
			to scale the performance of CT
			over RW
			. RGAR
			can intuitively be consider an
			agreement value over all elements of R. To test this hypothesis, we computed
			the Pearson correlation coefficient (PCC) [21] between RGAR
			and the Fleiss'
			Kappa [4] of each video in the dataset (R
			).
			A linear correlation between RGAR
			and R
			can be observed in Table 6. This
			is confirmed by a PCC value equal to 0.890, which means a very strong positive
			linear correlation between them.
			5.2 F 1mean vs. W iSeBE
			Results form Table 5 may give an idea that WiSeBE is just an scaled F1mean
			.
			While it is true that they show a linear correlation, WiSeBE may produce a
			128 C.-E. Gonz
			alez-Gallardo and J.-M. Torres-Moreno
			Table 5. WiSeBE evaluation
			Transcript System F1mean
			F1RW
			RGAR
			WiSeBE
			v1 S1 0.432 0.495 0.691 0.342
			S2 0.480 0.513 0.354
			v2 S1 0.578 0.659 0.688 0.453
			S2 0.549 0.595 0.409
			v3 S1 0.270 0.303 0.684 0.207
			S2 0.325 0.400 0.274
			v4 S1 0.505 0.593 0.578 0.342
			S2 0.735 0.800 0.462
			v5 S1 0.592 0.614 0.767 0.471
			S2 0.499 0.500 0.383
			v6 S1 0.443 0.550 0.541 0.298
			S2 0.457 0.535 0.289
			v7 S1 0.518 0.592 0.617 0.366
			S2 0.539 0.606 0.374
			v8 S1 0.429 0.494 0.525 0.259
			S2 0.487 0.508 0.267
			v9 S1 0.459 0.569 0.604 0.344
			S2 0.541 0.667 0.403
			v10 S1 0.582 0.581 0.619 0.359
			S2 0.487 0.545 0.338
			Mean scores S1 0.481 0.545 0.631 0.344
			S2 0.510 0.567 0.355
			Table 6. Agreement within dataset
			Agreement metric v1 v2 v3 v4 v5 v6 v7 v8 v9 v10
			RGAR
			0.691 0.688 0.684 0.578 0.767 0.541 0.617 0.525 0.604 0.619
			R 0.776 0.697 0.757 0.696 0.839 0.630 0.743 0.655 0.704 0.718
			different system ranking than F1mean
			given the integral multi-reference principle
			it follows. However, what we consider the most profitable about WiSeBE is the
			twofold inclusion of all available references it performs. First, the construction of
			RW
			to provide a more inclusive reference against to whom be evaluated and then,
			the computation of RGAR
			, which scales the result depending of the agreement
			between references.
			WiSeBE: Window-Based Sentence Boundary Evaluation 129

