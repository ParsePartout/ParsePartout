<article>
	<preamble>Torres.pdf</preamble>
	<title>Summary Evaluation with and without References</title>
	<auteurs>
		<auteur>
			<name>Juan-Manuel Torres-Moreno</name>
			<mail>juan-manuel.torres@univ-avignon.fr.</mail>
            <affiliation>is with LIA/Universite d'Avignon, France and E cole Polytechnique de Montreal, Canada (.</affiliation>
      </auteur>
		<auteur>
			<name>Iria da Cunha</name>
			<mail>eric.sanjuan@univ-avignon.fr.</mail>
            <affiliation>is with LIA/Universite d'Avignon, France (.</affiliation>
      </auteur>
		<auteur>
			<name>Eric SanJuan</name>
			<mail>horacio.saggion@upf.edu.</mail>
            <affiliation>is with DTIC/Universitat Pompeu Fabra, Spain (.</affiliation>
      </auteur>
		<auteur>
			<name>Horacio Saggion</name>
			<mail>iria.dacunha@upf.edu.</mail>
            <affiliation>is with IULA/Universitat Pompeu Fabra, Spain; LIA/Universite d'Avignon, France and Instituto de Ingenieria/UNAM, Mexico (.</affiliation>
      </auteur>
		<auteur>
			<name>Patricia Velazquez-Morales</name>
			<mail>velazquez@yahoo.com.</mail>
            <affiliation>is with VM Labs, France (patricia .</affiliation>
      </auteur>
	</auteurs>
	<abstract>
	Abstract--We study a new content-based method for the evaluation of text summarization systems without human models which is used to produce system rankings. The research is carried out using a new content-based evaluation framework called FRESA to compute a variety of divergences among probability distributions. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as COVERAGE, RESPONSIVENESS, PYRAMIDS and ROUGE studying their associations in various text summarization tasks including generic multi-document summarization in English and French, focus-based multi-document summarization in English and generic single-document summarization in French and Spanish.
	</abstract>
	<introduction>
	TEXT summarization evaluation has always been acomplex and controversial issue in computationallinguistics. In the last decade, significant advances have beenmade in this field as well as various evaluation measures havebeen designed. Two evaluation campaigns have been led bythe U.S. agence DARPA. The first one, SUMMAC, ran from1996 to 1998 under the auspices of the Tipster program [1],and the second one, entitled DUC (Document UnderstandingConference) [2], was the main evaluation forum from 2000until 2007. Nowadays, the Text Analysis Conference (TAC)[3] provides a forum for assessment of different informationaccess technologies including text summarization.Evaluation in text summarization can be extrinsic orintrinsic [4]. In an extrinsic evaluation, the summaries areassessed in the context of an specific task carried out by ahuman or a machine. In an intrinsic evaluation, the summariesare evaluated in reference to some ideal model. SUMMACwas mainly extrinsic while DUC and TAC followed anintrinsic evaluation paradigm. In an intrinsic evaluation, anManuscript received June 8, 2010. Manuscript accepted for publication July25, 2010.Juan-Manuel Torres-Moreno is with LIA/Universite d'Avignon,France and Ecole Polytechnique de Montreal, Canada(juan-manuel.torres@univ-avignon.fr).Eric SanJuan is with LIA/Universite d'Avignon, France(eric.sanjuan@univ-avignon.fr).Horacio Saggion is with DTIC/Universitat Pompeu Fabra, Spain(horacio.saggion@upf.edu).Iria da Cunha is with IULA/Universitat Pompeu Fabra, Spain;LIA/Universite d'Avignon, France and Instituto de Ingenieria/UNAM, Mexico(iria.dacunha@upf.edu).Patricia Velazquez-Morales is with VM Labs, France(patricia velazquez@yahoo.com).automatically generated summary (peer) has to be comparedwith one or more reference summaries (models). DUC usedan interface called SEE to allow human judges to comparea peer with a model. Thus, judges give a COVERAGE scoreto each peer produced by a system and the final systemCOVERAGE score is the average of the COVERAGE's scoresasigned. These system's COVERAGE scores can then be usedto rank summarization systems. In the case of query-focusedsummarization (e.g. when the summary should answer aquestion or series of questions) a RESPONSIVENESS scoreis also assigned to each summary, which indicates howresponsive the summary is to the question(s).Because manual comparison of peer summaries with modelsummaries is an arduous and costly process, a body ofresearch has been produced in the last decade on automaticcontent-based evaluation procedures. Early studies used textsimilarity measures such as cosine similarity (with or withoutweighting schema) to compare peer and model summaries[5]. Various vocabulary overlap measures such as n-gramsoverlap or longest common subsequence between peer andmodel have also been proposed [6], [7]. The BLEU machinetranslation evaluation measure [8] has also been tested insummarization [9]. The DUC conferences adopted the ROUGEpackage for content-based evaluation [10]. ROUGE implementsa series of recall measures based on n-gram co-occurrencebetween a peer summary and a set of model summaries. Thesemeasures are used to produce systems' rank. It has been shownthat system rankings, produced by some ROUGE measures(e.g., ROUGE-2, which uses 2-grams), have a correlation withrankings produced using COVERAGE.In recent years the PYRAMIDS evaluation method [11] hasbeen introduced. It is based on the distribution of "content"of a set of model summaries. Summary Content Units (SCUs)are first identified in the model summaries, then each SCUreceives a weight which is the number of models containingor expressing the same unit. Peer SCUs are identified in thepeer, matched against model SCUs, and weighted accordingly.The PYRAMIDS score given to a peer is the ratio of the sumof the weights of its units and the sum of the weights of thebest possible ideal summary with the same number of SCUs asthe peer. The PYRAMIDS scores can be also used for rankingsummarization systems. [11] showed that PYRAMIDS scoresproduced reliable system rankings when multiple (4 or more)models were used and that PYRAMIDS rankings correlate withrankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGEwith skip 2-grams). However, this method requires the creation13 Polibits (42) 2010of models and the identification, matching, and weighting ofSCUs in both: models and peers.[12] evaluated the effectiveness of the Jensen-Shannon(J S) [13] theoretic measure in predicting systems ranksin two summarization tasks: query-focused and updatesummarization. They have shown that ranks producedby PYRAMIDS and those produced by J S measurecorrelate. However, they did not investigate the effectof the measure in summarization tasks such as genericmulti-document summarization (DUC 2004 Task 2),biographical summarization (DUC 2004 Task 5), opinionsummarization (TAC 2008 OS), and summarization inlanguages other than English.In this paper we present a series of experiments aimed ata better understanding of the value of the J S divergencefor ranking summarization systems. We have carried outexperimentation with the proposed measure and we haveverified that in certain tasks (such as those studied by[12]) there is a strong correlation among PYRAMIDS,RESPONSIVENESS and the J S divergence, but as we willshow in this paper, there are datasets in which the correlationis not so strong. We also present experiments in Spanishand French showing positive correlation between the J Sand ROUGE which is the de facto evaluation measure usedin evaluation of non-English summarization. To the best ofour knowledge this is the more extensive set of experimentsinterpreting the value of evaluation without human models.The rest of the paper is organized in the following way:First in Section II we introduce related work in the area ofcontent-based evaluation identifying the departing point forour inquiry; then in Section III we explain the methodologyadopted in our work and the tools and resources used forexperimentation. In Section IV we present the experimentscarried out together with the results. Section V discusses theresults and Section VI concludes the paper and identifies futurework.
	</introduction>
	<corps>
	II. RELATED WORKOne of the first works to use content-based measures intext summarization evaluation is due to [5], who presented anevaluation framework to compare rankings of summarizationsystems produced by recall and cosine-based measures. Theyshowed that there was weak correlation among rankingsproduced by recall, but that content-based measures producerankings which were strongly correlated. This put forwardthe idea of using directly the full document for comparisonpurposes in text summarization evaluation. [6] presented aset of evaluation measures based on the notion of vocabularyoverlap including n-gram overlap, cosine similarity, andlongest common subsequence, and they applied them tomulti-document summarization in English and Chinese.However, they did not evaluate the performance of themeasures in different summarization tasks. [7] also comparedvarious evaluation measures based on vocabulary overlap.Although these measures were able to separate random fromnon-random systems, no clear conclusion was reached on thevalue of each of the studied measures.Nowadays, a widespread summarization evaluationframework is ROUGE [14], which offers a set of statisticsthat compare peer summaries with models. It countsco-occurrences of n-grams in peer and models to derive ascore. There are several statistics depending on the usedn-grams and the text processing applied to the input texts(e.g., lemmatization, stop-word removal).[15] proposed a method of evaluation based on theuse of "distances" or divergences between two probabilitydistributions (the distribution of units in the automaticsummary and the distribution of units in the modelsummary). They studied two different Information Theoreticmeasures of divergence: the Kullback-Leibler (KL) [16] andJensen-Shannon (J S) [13] divergences. KL computes thedivergence between probability distributions P and Q in thefollowing way:DKL(P||Q) =12wPwlog2PwQw(1)While J S divergence is defined as follows:DJ S(P||Q) =12wPwlog22PwPw+ Qw+ Qwlog22QwPw+ Qw(2)These measures can be applied to the distribution of units insystem summaries P and reference summaries Q. The valueobtained may be used as a score for the system summary. Themethod has been tested by [15] over the DUC 2002 corpus forsingle and multi-document summarization tasks showing goodcorrelation among divergence measures and both coverage andROUGE rankings.[12] went even further and, as in [5], they proposed tocompare directly the distribution of words in full documentswith the distribution of words in automatic summaries toderive a content-based evaluation measure. They found ahigh correlation between rankings produced using modelsand rankings produced without models. This last work is thedeparting point for our inquiry into the value of measures thatdo not rely on human models.III. METHODOLOGYThe followed methodology in this paper mirrors the oneadopted in past work (e.g. [5], [7], [12]). Given a particularsummarization task T, p data points to be summarizedwith input material {Ii}p-1i=0(e.g. document(s), question(s),topic(s)), s peer summaries {SUMi,k}s-1k=0for input i, andm model summaries {MODELi,j}m-1j=0for input i, we willcompare rankings of the s peer summaries produced by variousevaluation measures. Some measures that we use comparesummaries with n of the m models:MEASUREM(SUMi,k, {MODELi,j}n-1j=0) (3)Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velazquez-Morales14Polibits (42) 2010while other measures compare peers with all or some of theinput material:MEASUREM(SUMi,k, Ii) (4)where Iiis some subset of input Ii. The values producedby the measures for each summary SUMi,k are averagedfor each system k = 0, . . . , s - 1 and these averages areused to produce a ranking. Rankings are then comparedusing Spearman Rank correlation [17] which is used tomeasure the degree of association between two variableswhose values are used to rank objects. We have chosento use this correlation to compare directly results to thosepresented in [12]. Computation of correlations is done usingthe Statistics-RankCorrelation-0.12 package1, which computesthe rank correlation between two vectors. We also verifiedthe good conformity of the results with the correlation testof Kendall  calculated with the statistical software R. Thetwo nonparametric tests of Spearman and Kendall do notreally stand out as the treatment of ex-aequo. The goodcorrespondence between the two tests shows that they do notintroduce bias in our analysis. Subsequently will mention onlythe  of Sperman more widely used in this field.A. ToolsWe carry out experimentation using a new summarizationevaluation framework: FRESA -FRamework for EvaluatingSummaries Automatically-, which includes document-basedsummary evaluation measures based on probabilitiesdistribution2. As in the ROUGE package, FRESA supportsdifferent n-grams and skip n-grams probability distributions.The FRESA environment can be used in the evaluation ofsummaries in English, French, Spanish and Catalan, and itintegrates filtering and lemmatization in the treatment ofsummaries and documents. It is developed in Perl and willbe made publicly available. We also use the ROUGE package[10] to compute various ROUGE statistics in new datasets.B. Summarization Tasks and Data SetsWe have conducted our experimentation with the followingsummarization tasks and data sets:1) Generic multi-document-summarization in English(production of a short summary of a cluster of relateddocuments) using data from DUC'043, task 2: 50clusters, 10 documents each - 294,636 words.2) Focused-based summarization in English (production ofa short focused multi-document summary focused on thequestion "who is X?", where X is a person's name) usingdata from the DUC'04 task 5: 50 clusters, 10 documentseach plus a target person name - 284,440 words.1http://search.cpan.org/gene/Statistics-RankCorrelation-0.12/2FRESA is available at: http://lia.univavignon.fr/fileadmin/axes/TALNE/Ressources.html3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.html3) Update-summarization task that consists of creating asummary out of a cluster of documents and a topic. Twosub-tasks are considered here: A) an initial summary hasto be produced based on an initial set of documents andtopic; B) an update summary has to be produced froma different (but related) cluster assuming documentsused in A) are known. The English TAC'08 UpdateSummarization dataset is used, which consists of 48topics with 20 documents each - 36,911 words.4) Opinion summarization where systems have to analyzea set of blog articles and summarize the opinionsabout a target in the articles. The TAC'08 OpinionSummarization in English4 data set (taken from theBlogs06 Text Collection) is used: 25 clusters and targets(i.e., target entity and questions) were used - 1,167,735words.5) Generic single-document summarization in Spanishusing the Medicina Clinica5 corpus, which is composedof 50 medical articles in Spanish, each one with itscorresponding author abstract - 124,929 words.6) Generic single document summarization in French usingthe "Canadien French Sociological Articles" corpusfrom the journal Perspectives interdisciplinaires sur letravail et la sante (PISTES)6. It contains 50 sociologicalarticles in French, each one with its correspondingauthor abstract - 381,039 words.7) Generic multi-document-summarization in French usingdata from the RPM27 corpus [18], 20 different themesconsisting of 10 articles and 4 abstracts by referencethematic - 185,223 words.For experimentation in the TAC and the DUC datasets we usedirectly the peer summaries produced by systems participatingin the evaluations. For experimentation in Spanish and French(single and multi-document summarization) we have createdsummaries at a similar ratio to those of reference using thefollowing systems:- ENERTEX [19], a summarizer based on a theory oftextual energy;- CORTEX [20], a single-document sentence extractionsystem for Spanish and French that combines variousstatistical measures of relevance (angle between sentenceand topic, various Hamming weights for sentences, etc.)and applies an optimal decision algorithm for sentenceselection;- SUMMTERM [21], a terminology-based summarizer thatis used for summarization of medical articles anduses specialized terminology for scoring and rankingsentences;- REG [22], summarization system based on an greedyalgorithm;4http://www.nist.gov/tac/data/index.html5http://www.elsevier.es/revistas/ctl servlet? f=7032&revistaid=26http://www.pistes.uqam.ca/7http://www-labs.sinequa.com/rpm2Summary Evaluation with and without References15 Polibits (42) 2010- J S summarizer, a summarization system that scoresand ranks sentences according to their Jensen-Shannondivergence to the source document;- a lead-based summarization system that selects the leadsentences of the document;- a random-based summarization system that selectssentences at random;- Open Text Summarizer [23], a multi-lingual summarizerbased on the frequency and- commercial systems: Word, SSSummarizer8, Pertinence9and Copernic10.C. Evaluation MeasuresThe following measures derived from human assessment ofthe content of the summaries are used in our experiments:- COVERAGE is understood as the degree to which onepeer summary conveys the same information as a modelsummary [2]. COVERAGE was used in DUC evaluations.This measure is used as indicated in equation 3 usinghuman references or models.- RESPONSIVENESS ranks summaries in a 5-point scaleindicating how well the summary satisfied a giveninformation need [2]. It is used in focused-basedsummarization tasks. This measure is used as indicatedin equation 4 since a human judges the summarywith respect to a given input "user need" (e.g., aquestion). RESPONSIVENESS was used in DUC and TACevaluations.- PYRAMIDS [11] is a content assessment measure whichcompares content units in a peer summary to weightedcontent units in a set of model summaries. Thismeasure is used as indicated in equation 3 using humanreferences or models. PYRAMIDS is the adopted metricfor content-based evaluation in the TAC evaluations.For DUC and TAC datasets the values of these measures areavailable and we used them directly. We used the followingautomatic evaluation measures in our experiments:- ROUGE [14], which is a recall metric that takes intoaccount n-grams as units of content for comparing peerand model summaries. The ROUGE formula specified in[10] is as follows:ROUGE-n(R, M) =m Mn-gramPcountmatch(n - gram)m M count(n-gram)(5)where R is the summary to be evaluated, M is the set ofmodel (human) summaries, countmatch is the number ofcommon n-grams in m and P, and count is the numberof n-grams in the model summaries. For the experiments8http://www.kryltech.com/summarizer.htm9http://www.pertinence.net10http://www.copernic.com/en/products/summarizerpresented here we used uni-grams, 2-grams, and the skip2-grams with maximum skip distance of 4 (ROUGE-1,ROUGE-2 and ROUGE-SU4). ROUGE is used to comparea peer summary to a set of model summaries in ourframework (as indicated in equation 3).- Jensen-Shannon divergence formula given in Equation 2is implemented in our FRESA package with the followingspecification (Equation 6) for the probability distributionof words w.Pw=CTwNQw=CSwNSif w  SCTw+N+Botherwise(6)Where P is the probability distribution of words w intext T and Q is the probability distribution of words win summary S; N is the number of words in text andsummary N = NT+NS, B = 1.5|V |, CTwis the numberof words in the text and CSwis the number of words inthe summary. For smoothing the summary's probabilitieswe have used  = 0.005. We have also implementedother smoothing approaches (e.g. Good-Turing [24], thatuses the CPAN Perl's Statistics-Smoothing-SGT-2.1.2package11) in FRESA, but we do not use them inthe experiments reported here. Following the ROUGEapproach, in addition to word uni-grams we use 2-gramsand skip n-grams computing divergences such as J S(using uni-grams) J S2 (using 2-grams), J S4 (using theskip n-grams of ROUGE-SU4), and J SM which is anaverage of the J Si. J Ss measures are used to compare apeer summary to its source document(s) in our framework(as indicated in equation 4). In the case of summarizationof multiple documents, these are concatenated (in thegiven input order) to form a single input from whichprobabilities are computed.IV. EXPERIMENTS AND RESULTSWe first replicated the experiments presented in [12] toverify that our implementation of J S produced correlationresults compatible with that work. We used the TAC'08Update Summarization data set and computed J S andROUGE measures for each peer summary. We producedtwo system rankings (one for each measure), which werecompared to rankings produced using the manual PYRAMIDSand RESPONSIVENESS scores. Spearman correlations werecomputed among the different rankings. The results arepresented in Table I. These results confirm a high correlationamong PYRAMIDS, RESPONSIVENESS and J S. We alsoverified high correlation between J S and ROUGE-2 (0.83Spearman correlation, not shown in the table) in this task anddataset.Then, we experimented with data from DUC'04, TAC'08Opinion Summarization pilot task as well as single and11http://search.cpan.org/bjoernw/Statistics-Smoothing-SGT-2.1.2/Juan-Manuel Torres-Moreno, Horacio Saggion, Iria da Cunha, Eric SanJuan, and Patricia Velazquez-Morales16Polibits (42) 2010TABLE ISPEARMAN CORRELATION OF CONTENT-BASED MEASURES IN TAC'08UPDATE SUMMARIZATION TASKMesure PYRAMIDS p-value RESPONSIVENESS p-valueROUGE-2 0.96 p &lt; 0.005 0.92 p &lt; 0.005J S 0.85 p &lt; 0.005 0.74 p &lt; 0.005multi-document summarization in Spanish and French. In spiteof the fact that the experiments for French and Spanish corporause less data points (i.e., less summarizers per task) thanfor English, results are still quite significant. For DUC'04,we computed the J S measure for each peer summary intasks 2 and 5 and we used J S, ROUGE, COVERAGE andRESPONSIVENESS scores to produce systems' rankings. Thevarious Spearman's rank correlation values for DUC'04 arepresented in Tables II (for task 2) and III (for task 5).For task 2, we have verified a strong correlation betweenJ S and COVERAGE. For task 5, the correlation betweenJ S and COVERAGE is weak, and that between J S andRESPONSIVENESS is weak and negative.Although the Opinion Summarization (OS) task is a newtype of summarization task and its evaluation is a complicatedissue, we have decided to compare J S rankings with thoseobtained using PYRAMIDS and RESPONSIVENESS in TAC'08.Spearman's correlation values are listed in Table IV. As it canbe seen, there is weak and negative correlation of J S withboth PYRAMIDS and RESPONSIVENESS. Correlation betweenPYRAMIDS and RESPONSIVENESS rankings is high for thistask (0.71 Spearman's correlation value).For experimentation in mono-document summarizationin Spanish and French, we have run 11 multi-lingualsummarization systems; for experimentation in French, wehave run 12 systems. In both cases, we have producedsummaries at a compression rate close to the compression rateof the authors' provided abstracts. We have then computed J Sand ROUGE measures for each summary and we have averagedthe measure's values for each system. These averages wereused to produce rankings per each measure. We computedSpearman's correlations for all pairs of rankings.Results are presented in Tables V, VI and VII. All resultsshow medium to strong correlation between the J S measuresand ROUGE measures. However the J S measure based onuni-grams has lower correlation than J Ss which use n-gramsof higher order. Note that table VII presents results forgeneric multi-document summarization in French, in thiscase correlation scores are lower than correlation scores forsingle-document summarization in French, a result which maybe expected given the diversity of input in multi-documentsummarization.</corps>
	<conclusion>
	This paper has presented a series of experiments incontent-based measures that do not rely on the use of modelsummaries for comparison purposes. We have carried outextensive experimentation with different summarization tasksdrawing a clearer picture of tasks where the measures couldbe applied. This paper makes the following contributions:- We have shown that if we are only interested in rankingsummarization systems according to the content of theirautomatic summaries, there are tasks were models couldbe subtituted by the full document in the computation ofthe J S measure obtaining reliable rankings. However,we have also found that the substitution of modelsby full-documents is not always advisable. We haveSummary Evaluation with and without References17 Polibits (42) 2010TABLE IISPEARMAN  OF CONTENT-BASED MEASURES WITH COVERAGE IN DUC'04 TASK 2Mesure COVERAGE p-valueROUGE-2 0.79 p &lt; 0.0050J S 0.68 p &lt; 0.0025TABLE IIISPEARMAN  OF CONTENT-BASED MEASURES IN DUC'04 TASK 5Mesure COVERAGE p-value RESPONSIVENESS p-valueROUGE-2 0.78 p &lt; 0.001 0.44 p &lt; 0.05J S 0.40 p &lt; 0.050 -0.18 p &lt; 0.25TABLE IVSPEARMAN  OF CONTENT-BASED MEASURES IN TAC'08 OS TASKMesure PYRAMIDS p-value RESPONSIVENESS p-valueJ S -0.13 p &lt; 0.25 -0.14 p &lt; 0.25TABLE VSPEARMAN  OF CONTENT-BASED MEASURES WITH ROUGE IN THE Medicina Clinica CORPUS (SPANISH)Mesure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-valueJ S 0.56 p &lt; 0.100 0.46 p &lt; 0.100 0.45 p &lt; 0.200J S2 0.88 p &lt; 0.001 0.80 p &lt; 0.002 0.81 p &lt; 0.005J S4 0.88 p &lt; 0.001 0.80 p &lt; 0.002 0.81 p &lt; 0.005J SM 0.82 p &lt; 0.005 0.71 p &lt; 0.020 0.71 p &lt; 0.010found weak correlation among different rankings incomplex summarization tasks such as the summarizationof biographical information and the summarization ofopinions.- We have also carried out large-scale experiments inSpanish and French which show positive medium tostrong correlation among system's ranks produced byROUGE and divergence measures that do not use themodel summaries.- We have also presented a new framework, FRESA, forthe computation of measures based on J S divergence.Following the ROUGE approach, FRESA package useword uni-grams, 2-grams and skip n-grams computingdivergences. This framework will be available to thecommunity for research purposes.Although we have made a number of contributions, this paperleaves many open questions than need to be addressed. Inorder to verify correlation between ROUGE and J S, in theshort term we intend to extend our investigation to otherlanguages such as Portuguese and Chinesse for which wehave access to data and summarization technology. We alsoplan to apply FRESA to the rest of the DUC and TACsummarization tasks, by using several smoothing techniques.As a novel idea, we contemplate the possibility of adaptingthe evaluation framework for the phrase compression task[29], which, to our knowledge, does not have an efficientevaluation measure. The main idea is to calculate J S froman automatically-compressed sentence taking the completesentence by reference. In the long term, we plan to incorporatea representation of the task/topic in the calculation ofmeasures. To carry out these comparisons, however, we aredependent on the existence of references.FRESA will also be used in the new question-answer taskcampaign INEX'2010 (http://www.inex.otago.ac.nz/tracks/qa/qa.asp) for the evaluation of long answers. This task aimsto answer a question by extraction and agglomeration ofsentences in Wikipedia. This kind of task correspondsto those for which we have found a high correlationamong the measures J S and evaluation methods withhuman intervention. Moreover, the J S calculation will beamong the summaries produced and a representative set ofrelevant passages from Wikipedia. FRESA will be used tocompare three types of systems, although different tasks: themulti-document summarizer guided by a query, the searchsystems targeted information (focused IR) and the questionanswering systems.</conclusion>
	<discussion>
	The departing point for our inquiry into text summarizationevaluation has been recent work on the use of content-basedevaluation metrics that do not rely on human models but thatcompare summary content to input content directly [12]. Wehave some positive and some negative results regarding thedirect use of the full document in content-based evaluation.We have verified that in both generic muti-documentsummarization and in topic-based multi-documentsummarization in English correlation among measuresthat use human models (PYRAMIDS, RESPONSIVENESSand ROUGE) and a measure that does not use models(J S divergence) is strong. We have found that correlationamong the same measures is weak for summarization ofbiographical information and summarization of opinions inblogs. We believe that in these cases content-based measuresshould be considered, in addition to the input document, thesummarization task (i.e. text-based representation, description)to better assess the content of the peers [25], the task being adeterminant factor in the selection of content for the summary.Our multi-lingual experiments in generic single-documentsummarization confirm a strong correlation among theJ S divergence and ROUGE measures. It is worth notingthat ROUGE is in general the chosen framework forpresenting content-based evaluation results in non-Englishsummarization.For the experiments in Spanish, we are conscious that weonly have one model summary to compare with the peers.Nevertheless, these models are the corresponding abstractswritten by the authors. As the experiments in [26] show, theprofessionals of a specialized domain (as, for example, themedical domain) adopt similar strategies to summarize theirtexts and they tend to choose roughly the same content chunksfor their summaries. Previous studies have shown that authorabstracts are able to reformulate content with fidelity [27] andthese abstracts are ideal candidates for comparison purposes.Because of this, the summary of the author of a medical articlecan be taken as reference for summaries evaluation. It is worthnoting that there is still debate on the number of models to beused in summarization evaluation [28]. In the French corpusPISTES, we suspect the situation is similar to the Spanishcase.</discussion>
	<biblio>
	[1] I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, andB. Sundheim, "Summac: a text summarization evaluation," NaturalLanguage Engineering, vol. 8, no. 1, pp. 43-68, 2002.[2] P. Over, H. Dang, and D. Harman, "DUC in context," IPM, vol. 43,no. 6, pp. 1506-1520, 2007.[3] Proceedings of the Text Analysis Conference. Gaithesburg, Maryland,USA: NIST, November 17-19 2008.[4] K. Sparck Jones and J. Galliers, Evaluating Natural LanguageProcessing Systems, An Analysis and Review, ser. Lecture Notes inComputer Science. Springer, 1996, vol. 1083.[5] R. L. Donaway, K. W. Drummey, and L. A. Mather, "A comparison ofrankings produced by summarization evaluation measures," in NAACLWorkshop on Automatic Summarization, 2000, pp. 69-78.[6] H. Saggion, D. Radev, S. Teufel, and W. Lam, "Meta-evaluationof Summaries in a Cross-lingual Environment using Content-basedMetrics," in COLING 2002, Taipei, Taiwan, August 2002, pp. 849-855.[7] D. R. Radev, S. Teufel, H. Saggion, W. Lam, J. Blitzer, H. Qi, A. C elebi,D. Liu, and E. Drabek, "Evaluation challenges in large-scale documentsummarization," in ACL'03, 2003, pp. 375-382.[8] K. Papineni, S. Roukos, T. Ward, , and W. J. Zhu, "BLEU: a methodfor automatic evaluation of machine translation," in ACL'02, 2002, pp.311-318.[9] K. Pastra and H. Saggion, "Colouring summaries BLEU," in EvaluationInitiatives in Natural Language Processing. Budapest, Hungary: EACL,14 April 2003.[10] C.-Y. Lin, "ROUGE: A Package for Automatic Evaluation ofSummaries," in Text Summarization Branches Out: ACL-04 Workshop,M.-F. Moens and S. Szpakowicz, Eds., Barcelona, July 2004, pp. 74-81.[11] A. Nenkova and R. J. Passonneau, "Evaluating Content Selection inSummarization: The Pyramid Method," in HLT-NAACL, 2004, pp.145-152.[12] A. Louis and A. Nenkova, "Automatically Evaluating Content Selectionin Summarization without Human Models," in Empirical Methods inNatural Language Processing, Singapore, August 2009, pp. 306-314.[Online]. Available: http://www.aclweb.org/anthology/D/D09/D09-1032[13] J. Lin, "Divergence Measures based on the Shannon Entropy," IEEETransactions on Information Theory, vol. 37, no. 145-151, 1991.[14] C.-Y. Lin and E. Hovy, "Automatic Evaluation of Summaries UsingN-gram Co-occurrence Statistics," in HLT-NAACL. Morristown, NJ,USA: Association for Computational Linguistics, 2003, pp. 71-78.[15] C.-Y. Lin, G. Cao, J. Gao, and J.-Y. Nie, "An information-theoreticapproach to automatic evaluation of summaries," in HLT-NAACL,Morristown, USA, 2006, pp. 463-470.[16] S. Kullback and R. Leibler, "On information and sufficiency," Ann. ofMath. Stat., vol. 22, no. 1, pp. 79-86, 1951.[17] S. Siegel and N. Castellan, Nonparametric Statistics for the BehavioralSciences. McGraw-Hill, 1998.[18] C. de Loupy, M. Guegan, C. Ayache, S. Seng, and J.-M. Torres-Moreno,"A French Human Reference Corpus for multi-documentssummarization and sentence compression," in LREC'10, vol. 2,Malta, 2010, p. In press.[19] S. Fernandez, E. SanJuan, and J.-M. Torres-Moreno, "Textual Energyof Associative Memories: performants applications of Enertex algorithmin text summarization and topic segmentation," in MICAI'07, 2007, pp.861-871.[20] J.-M. Torres-Moreno, P. Velazquez-Morales, and J.-G. Meunier,"Condenses de textes par des methodes numeriques," in JADT'02, vol. 2,St Malo, France, 2002, pp. 723-734.[21] J. Vivaldi, I. da Cunha, J.-M. Torres-Moreno, and P. Velazquez-Morales,"Automatic summarization using terminological and semanticresources," in LREC'10, vol. 2, Malta, 2010, p. In press.[22] J.-M. Torres-Moreno and J. Ramirez, "REG : un algorithme gloutonapplique au resume automatique de texte," in JADT'10. Rome, 2010,p. In press.[23] V. Yatsko and T. Vishnyakov, "A method for evaluating modernsystems of automatic text summarization," Automatic Documentationand Mathematical Linguistics, vol. 41, no. 3, pp. 93-103, 2007.[24] C. D. Manning and H. Schutze, Foundations of Statistical NaturalLanguage Processing. Cambridge, Massachusetts: The MIT Press,1999.[25] K. Sparck Jones, "Automatic summarising: The state of the art," IPM,vol. 43, no. 6, pp. 1449-1481, 2007.[26] I. da Cunha, L. Wanner, and M. T. Cabre, "Summarization of specializeddiscourse: The case of medical articles in spanish," Terminology, vol. 13,no. 2, pp. 249-286, 2007.[27] C.-K. Chuah, "Types of lexical substitution in abstracting," in ACLStudent Research Workshop. Toulouse, France: Association forComputational Linguistics, 9-11 July 2001 2001, pp. 49-54.[28] K. Owkzarzak and H. T. Dang, "Evaluation of automatic summaries:Metrics under varying data conditions," in UCNLG+Sum'09, Suntec,Singapore, August 2009, pp. 23-30.[29] K. Knight and D. Marcu, "Statistics-based summarization-step one:Sentence compression," in Proceedings of the National Conference onArtificial Intelligence. Menlo Park, CA; Cambridge, MA; London;AAAI Press; MIT Press; 1999, 2000, pp. 703-710.Summary Evaluation with and without References19 Polibits (42) 2010
	</biblio>
</article>