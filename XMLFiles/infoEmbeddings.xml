<article>
	<preamble>infoEmbeddings.pdf</preamble>
	<title>arXiv:1904.08770v1 [cs.IR] 16 Apr 2019</title>
	<auteurs>
		<auteur>
			<name>An Empirical</name>
            <affiliation>aDA-IICT Gandhinagar,India;</affiliation>
      </auteur>
		<auteur>
			<name>Representation Schemes</name>
            <affiliation>aDA-IICT Gandhinagar,India;</affiliation>
      </auteur>
		<auteur>
			<name>Web to Filter</name>
      </auteur>
		<auteur>
			<name>Textual Aggression</name>
      </auteur>
		<auteur>
			<name>Compiled April</name>
      </auteur>
	</auteurs>
	<abstract>
	BSTRACT Due to an exponential rise in the social media user base, incidents like hate speech, trolling, cyberbullying are also increasing and that lead hate speech detection problem reshaped into different tasks like Aggression detection, Fact detection. This paper attempt to study the effectiveness of text representation schemes on two tasks namely: User Aggression and Fact Detection from the social media contents. In User Aggression detection, The aim is to identify the level of aggression from the contents generated in the Social media and written in the English, Devanagari Hindi and Romanized Hindi. Aggression levels are categorized into three predefined classes namely: `Non-aggressive`, `Overtly Aggressive`, and `Covertly Aggressive`. During the disaster-related incident, Social media like, Twitter is flooded with millions of posts. In such emergency situations, identification of factual posts is important for organizations involved in the relief operation. We anticipated this problem as a combination of classification and Ranking problem. This paper presents a comparison of various text representation scheme based on BoW techniques, distributed word/sentence representation, transfer learning on classifiers. Weighted F1 score is used as a primary evaluation metric. Results show that text representation using BoW performs better than word embedding on machine learning classifiers. While pre-trained Word embedding techniques perform better on classifiers based on deep neural net. Recent transfer learning model like ELMO, ULMFiT are fine-tuned for the Aggression classification task. However, results are not at par with pretrained word embedding model. Overall, word embedding using fastText produce best weighted F1-score than Word2Vec and Glove. Results are further improved using pre-trained vector model. Statistical significance tests are employed to ensure the significance of the classification results. In the case of lexically different test Dataset, other than training Dataset, deep neural models are more robust and perform substantially better than machine learning classifiers.
	</abstract>
	<introduction>
	The Social Web is a great source for studying human interaction and behavior. Inthe last few years, there is an exponential growth in Social Media user base. Sensingcontent of Social Media like Facebook, Twitter, by the smart autonomous applicationempower its user community with real-time information which is unfolded across thedifferent part of the world. Social media provide the easiest and anonymous platformfor common people to voice their opinion or view on a various entity like celebrity,politician, product, stock market etc or any social movement. Sometime such opinionsmight be aggressive in nature and propagate hate in the social media community.With the unprecedented increase in the user base of the social media and its avail-ability on the Smartphones, incidents like Hate speech, trolling, Cyberbullying, andAggressive posts are increasing exponentially. A smart autonomous system is requiredwhich enable surveillance on the social media platform and detect such incidents.Some of the researchers look posts from the aspect like aggression Kumar Ritesh etal. (2018) to filter the contents. some of the posts contain words which might be qual-ified as either highly or overly aggressive or have hidden aggression. Sometimes postsdo not have any aggression. Based on these, posts or comments are categorized intothree classes namely: `Overtly Aggressive`, `Covertly Aggressive` and `Non-aggressive`Kumar Ritesh et al. (2018). Henceforth, in the rest of the paper, we will denote theseclasses by these abbreviations namely: OAG, CAG, NAG respectively.Table 1 showsthe sample posts belonging to these classes.Social Media, specifically Microblog has proved its importance during the disaster-related incidents like an earthquake, Hurricane and floods 1. Organizations involvedin relief operation actively track posts related to situational information posted onFacebook and Twitter during the disaster. However, At the same time, social mediais flooded with lots of prayer and condolence messages. Posts which contain factualinformation are extremely important for the organization involved in post-disasterrelief operations for coordination. Filtering and Ranking of the posts containing factualinformation will be very useful to them. We believe that this is the special problemof the Sentiment Analysis task. We consider this problem as a combination of two-class classification problem: factual posts and nob-factual posts plus Ranking. Table
	</introduction>
	<corps>
	2 shows the example of the posts of belong to these class.The Text representation of social web content plays a pivotal role in any NLP task.Bag-of-word is the oldest and simple technique to represent the document or post intoa fixed length vector. The BoW techniques generate very sparse and high dimensionalspace vector. Text representation using distributed word/sentence representation orword embedding is gain rapid momentum recently. In this paper, one of the objectives1https://phys.org/news/2018-08-social-media-bad-disaster-zones.html2Table 2. sample post for the each class.Post text Class Label1 #Nepal #Earthquake day four. Slowly in the capital valleyInternet and electricity beeing restored . A relief for at leastsome onesFactual2 PMOIndia Indian Government is doing every possible help tothe earthquake victims and they need money so plz contributeNon-factualis to find the best text representation scheme to model social web content for themachine learning classifier and deep neural net. Various Text representation schemebased on BoW, word embedding and are studied empirically. We have reported resulton popular word embedding technique like Word2vec, Glove and fastText on stan-dard machine learning classifier like Multinomial Naive Bayes (MNB), Logistic Re-gression(LR), K-Nearest neighbors KNN, Support Vector Classifier (SVC), DecisionTree (DT),Stochastic Gradient Descent(SGD), Random forest (RF), Ridge, AdaBoost,Perceptron, Deep neural net based on LSTM, CNN and Bidirectional LSTM. Resultsare also reported on Doc2vec embedding, a popular sentence or paragraph embeddingtechnique for the above classifiers.Transfer Learning is well practiced in the area of computer vision. However, in theNLP, transfer learning has limited application in the form of pre-trained word vectorwhich is used to initialize the weights of the embedding layer of the deep neural net-work. With the advent of transfer learning method like ELMO (Peters et al. , 2018),ULMFiT (Howaard et al., 2018) claimed substantial improvement in the performanceof various NLP tasks like Sentiment Analysis, Question/Answering, Textual Entail-ment empirically. The main idea behind these methods is to train language modelon the large corpus and fine tune on the task-specific corpus. In this paper, We haveevaluated the performance of these methods in the Aggression classification tasks.1.1. Research QuestionsIn this study, experiments are performed on the benchmark dataset with to answerthe following questions* Which is the best Text Representation scheme to model text from the SocialWeb?* Does pre-trained language model based on transfer learning better than pre-trained word embedding based on shallow transfer learning on Social mediadata?* Does Making too Deep Neural net make sense?To answer all research question listed above, experiments are performed on twotasks namely: Aggression detection (Trolling Aggression and Cyberbullying (TRAC)dataset) Kumar et al. (2018) and Fact detection (FIRE iRMDI Dataset)Basu et al.(2018). In this paper, we present exhaustive benchmarking of text representationschemes on these datasets. Our results reveal that fastText with pre-trained vectoralong with CNN outperform standard machine learning classifiers based on BoWModel and marginally perform better than Word2vec and Glove. Paragraph vectoror Doc2vec Le, Quoc et al. (2014) perform very poor on our dataset and turn out tobe the worst text representation scheme among all. We also found that model basedon the deep neural net is more robust than machine learning classifier when testedon lexically different dataset than training Dataset. i.e. deep neural model substan-3tially outperforms machine learning classifier on Twitter test Dataset while trained onFacebook Dataset in this evaluation.To validate our claims, statistical significance tests are performed on weighted F1-score of the classifier for each text representing scheme. Statistical inference is usedto check evidence to support or reject these claims. Significance tests like Wilcoxonsigned-rank and Student t-test were carried out by comparing weighted F1 score allthe text representation scheme with the fastText pre-trained vector. In most of thecases, p-values are less than 0.05.The rest of the paper is organized as follows: In section 2, we review the relevantworks in the area of Sentiment analysis and hate speech detection. Section 3 containsthe detail information about the various benchmark Datasets used in the experiments.Various Text Representation schemes are described in section 4. We formally describethe evaluation task and models in section 5. We report results in section 6 and presentdetail result analysis in section 7. We conclude the discussion and provide insight forthe future work in section 8.2. Related WorkBag-of-Words (BoW) (Harris et al. , 1954) is the oldest technique to represent thetext of the documents in fixed-length vectors with high dimensionality. Mikolov et.al(2013) proposed two architecture namely: skip-gram(SG) and continuous-bag-of word(CBOW) to learn high quality low dimensional word embedding. However, to gener-ate sentence vector often, average or mean of word vector are considered. Doc2vec orparagraph vector Le, Quoc et al. (2014) proposed Paragraph2vec (Doc2vec) which isthe extension of the Word2vec to learning document level embedding. It is an unsuper-vised method which learns document vector from paragraph, sentence or document.Pennington et.al. (2014) proposed word embedding based on the co-occurrence ma-trix. Lau et al. (2016) have performed a comprehensive evaluation of Doc2Vec on twotasks namely: Forum Question Duplication and Semantic Textual Similarity (STS)task. Authors claimed that Doc2Vec performs better than Word2vec provided thatmodels trained on large external corpora, and can be further improved by using pre-trained word embedding. They have published the hyper-parameter for the Doc2Vecembedding. Our work is similar to this but we have reported the evaluation of all thetext representation scheme including doc2vec on TRAC dataset(Kumar et al. , 2018)on each classifier.Hate speech is a type of language which is used to incite or spread violence to-wards the group of people based on the gender, community, race, religion. Sentimentanalysis and hate-speech are closely related in fact sentiment analysis techniques areused in hate speech detection. Initially, Sentiment Analysis problem is formulated asa binary classification problem for predicting the election results or detecting politicalopinion (Conover et al. , 2011; Conover Micheal et al. , 2011; Maynard et al., 2011;Tumasjan et al. , 2010) on Twitter. Then after, It turned into the multi-class classi-fication problem with the introduction of the neutral label. Soon, Researchers comewith different notion like aggression (Kumar Ritesh et al., 2018), cyberbullying(xu etal. , 2012), sarcasm, trolling. Semeval (International workshop on semantic evalua-tion)(Rosenthal et al. , 2017) is one of the popular competition on sentiment analysiswhich is started since 2013. TRAC 2(Trolling, aggression, cyberbullying) workshop2https://sites.google.com/view/trac14(Kumar Ritesh et al., 2018) co-located with the International Conference of Com-putational Linguistics (COLING 2018) redefine hate speech detection task in termsof three type of aggression namely: Non-Aggression (NAG), Overly-Aggression(OAG)and Covertly Aggression (CAG).2.1. Sentiment AnalysisDuring the initial year, there is a lack of standard dataset for comparative perfor-mance analysis. International Workshop on Semantic Evaluation 2013 (SemEval-2013)(Hltcoe et.al , 2013) was the first forum who developed standard tweet dataset for thebenchmarking of the various sentiment analysis system. Most of the team who had par-ticipated in the competition used supervised approaches based on SVM, Naive Bayes,and Maximum Entropy. some of the team had used ensemble classifier and rule-basedclassifier. Mohammad et al. (2013) was the top team of the Semeval-2013 challenge.They have incorporated various semantic and lexicon based sentiment features for theexperiment and SVM was used for the classification. Deep learning and word embed-ding had shown its footprints in SemEval-2015 (Rosenthal et al. , 2015). Team UNITN(Severyn et al. , 2015) was the second team in the message polarity task. They havebuild convolution neural network for the sentiment classification. They have used anunsupervised neural language model to initialize word embeddings that are furthertuned by deep learning model on a distant supervised corpus (Severyn et al. , 2015).In fourth edition SemEval-2016 (Nakov et al. , 2016),Team SwissCheese (Deriu et al., 2016) was the first ranked team with F1 score around 63.3 %. Their approach wasbased on 2-layer convolution neural networks whose predictions are combined using arandom forest classifier. SemEval-2017 (Rosenthal et al. , 2017) was the fifth edition,Team DataStories (Baziotis et al. , 2017) was the top-ranked team with AvgRec= 68.1and F1 around=67.7 %. They use Long Short-Term Memory (LSTM) networks aug-mented with two kinds of attention mechanisms, on top of word embedding pre-trainedon a big collection of Twitter messages without using any hand-crafted features.2.2. Hate Speech/Cyberbullying/Aggression DetectionHate Speech Detection research attracts researchers from the diverse backgroundlike Computational linguistic, computer science, social science. The actual term hatespeech was coined by Warner et al. (2012). Various Authors used different notionlike offensive language (Razavi et.al , 2010), Cyberbullying (xu et al. , 2012), Aggres-sion (Kumar et al. , 2018). Davidson et al. (2017) studied tweet classification of hatespeech and offensive language and defined hate speech as following: language that isused to expresses hatred towards a targeted group or is intended to be derogatory,to humiliate, or to insult the members of the group. Authors observed that offensivelanguage often miss-classified as hate speech. They have trained a multi-class classi-fier on N-gram features weighted by its TF-IDF weights and PoS tags. In additionto these, features like sentiment score of each tweet, no of hashtags, URLS, mentionsare considered. Authors concluded that Logistic regression and Linear SVM performbetter than NB, Decision Tree, Random Forests. Schmidt et al. (2017) perform com-prehensive survey on hate speech. They have identified features like Surface features,sentiment, word generalization,lexical, linguistics etc. can be used by classifier.Cyberbullying is the type bullying that occurs on social media platform or app viacellphone or any internet enabled device. xu et al. (2012) introduces Cyberbullying to5the NLP community. They have performed various binary classification on tweets textwith bullying perspective to determine whether the user is cyberbully or not. Theyreported binary classification accuracy around 81%. Kwok et al. (2013(@), authorsperformed classification using NB classifier on tweets based on two classes :racist andnon-racists and achieved accuracy around 76 %. Burnap et al. (2015), authors studiedcyber hate on Twitter. They have used various classifier like SVM, BLR, RFDT, Votingbase ensemble for the binary classification achieved best F1-score of 0.77 in the votedensemble.Malmasi, et al. (2017), authors have used NLP based lexical approach toaddress the multi-class classification problem. They have used character N-gram, wordN-gram and word skip-gram feature for the classification.Schmidt et al. (2017), have described the key areas that have been explored to detecthate speech. They have surveyed different types of features used for hate speech classi-fication. They have categorized features in Simple surface features, word generalizationfeatures, sentiment features, linguistic features, lexical resources features, Knowledge-based features, and Meta-Information features Simple surface features include featureslike character level unigram/n-gram, word generalization features include features likethe bag-of-words, clustering, word embedding, paragraph embedding. Linguistic fea-tures include PoS tag of tokens. list of bad words or hate words can be considered asa lexical resource. Malmasi et al. (2018), tried to address the problem of discrimi-nating profanity from the hate speech in the social media posts. n-grams, skip-gramand clustering based word representation features are considered for the 3-class classi-fication.The Author use SVM and advance ensemble based classifier for this task andachieved 80 % accuracy.Aroyehun et al. (2018) performed translation as data augmentation strategy. TRACDataset (Kumar et al. , 2018) was also augmented using translation and pseudo labeledusing an external dataset on hate speech. they have reported best performance withLSTM and F1 score around 0.6415 on TRAC English dataset (Kumar et al. , 2018).Arroyo et al. (2018) implement ensemble of the Passive-Aggressive (PA) and SVMclassifiers with character n-grams. TF-IDF weighting used for feature representation.FIRE initiative also gave importance text representation in Indian language since itsinception. (Majumder et al. , 2008)(Majumder et al. , 2007).3. DatasetExperiments are performed on standard benchmarked Datasets to evaluate the perfor-mance of various text representation scheme. For User Aggression detection problem,Trolling, Aggression and Cyberbullying TRAC (Kumar et al. , 2018) is considered forthe experiments which contain post in English and code-mixed Hindi. For the FactualDetection task, experiments are performed on FIRE IRMiDis Dataset.3.1. TRAC DatasetTRAC (Trolling, Aggresion and Cyberbullying) consist of 15,001 aggression-annotatedFacebook Posts and Comments each in Hindi (Romanized and Devanagari script) andEnglish for training and validation Kumar et al. (2018).6Table 3. Class distribution in the Training DatasetEnglish Corpus Hindi Corpus# Training # Validation # Training # ValidationNAG 5,052 1, 233 2, 275 538CAG 4240 1, 057 4, 869 1, 246OAG 2, 708 711 4, 856 1217Total 12, 000 3, 001 12, 000 3, 001Table 4. Test Data Corpus statisticsTest Dataset # of postsFacebook English Corpus 916Twitter English Corpus 1, 257Facebook mixed script Hindi Corpus 970Twitter mixed script Hindi Corpus 1, 1943.2. FIRE IRMiDis DatasetForum for Information Retrieval Evaluation, have introduced Microblog track since2016 as Information Retrieval from Microblogs during Disasters (IRMiDis). IRMiDistrack (Basu et al. , 2018) of FIRE is organized with the objective to extract factualor fact-checkable tweets during the disaster which might be helpful to the victimsor the people who are involved in the relief operation. Dataset contain tweets whichare downloaded from the Twitter during Nepal earthquake 2015. Following are theexample of factual or fact-checkable and non-fact-checkable tweet.Table 5 shows adetail statistics of FIRE IRMiDis Dataset. As we look at the table, There are only83 tweets is annotated with objective class. not a single tweet is annotated from thesubjective class.4. Text Representation SchemesThe main objective of this paper is an identification of the best text representationscheme for the Social media text which is very sparse and noisy in nature. Text rep-resentation is about representing documents in a numerical way so that they can befeed as an input to the classifier. This numerical representation is in the form of thevectors which together form matrices. Essentially, There are two types of text repre-sentation scheme :(i) Bag-of-words(BoW) (ii) Distributed Word/sentence representa-tion. BoW with count vector and TF/IDF weighting , various word embedding tech-niques(Word2Vec, Glove, fastText), and sentence or paragraph embedding (Doc2Vec)are studied.Table 5. FIRE IRMiDis Dataset statisticsParticulars # tweets RemarkNumber of Tweets 50000+Labelled Tweets 83 only tweets belong to Factual classClasses 274.1. Bag-of-Word Model for Text RepresentationThe Bag-of-words is the simple technique to represent the document or social mediaposts in the vector form and also a very common feature extraction method from thetext. Word count or TF/IDF weight of each n-gram word can be used as a features.The dimension of the vector is equal to the size of vocabulary of the text corpusor dataset which results in very high dimensional sparse document vector. It is thecommon method used for the text representation in order to perform various NLP tasklike text classification, clustering. However,the BoW methods ignore the word orderwhich may lead to loss of the context.4.2. Word Embedding for Text representationWord Embedding is the text representation technique to represent the word in the lowdimensional space so that semantically similar word have similar representation. Majorword embedding techniques like Word2vec learn word embedding using shallow neuralnetwork. The fastText, extension of Word2vec, consider the morphological structureof the word.4.2.1. Word2VecWord2vec (Mikolov et.al , 2013) is the unsupervised and predictive neural word em-bedding technique to learn the word representation in the low dimensional space.Word2vec is a two-layer neural net that take text corpus as an input and output is aset of vectors. two novel model architectures: Skipgram and CBOW(Continuous bagof words) are proposed for computing continuous vector representations of words fromvery large data sets.4.2.2. GloveGloVe stands for Global vector for [Word Representation] (Pennington et.al. , 2014)isan unsupervised method for learning word embedding. A Co-occurrence word matrixis created from the text corpus for the training and is reduced in low dimensional spacewhich explain the variance of high dimensional data and provide word vector for eachword.4.2.3. fastTextfastText (Bojanowski et al. , 2017) is the neural word embedding technique whichlearn distributed low dimensional word embedding. Word2vec, Glove consider eachword as single unit and ignore the morphological structure of the word. They are notable generate word embedding for the unseen or out of vocabulary word during thetraining. fastText overcome this limitation of Word2vec and GLOVE by consideringeach word as N-gram of characters. A word vector for a word is computed from thesum of the n-gram characters. The range of N is typically 3 to 6. Since user on socialmedia often make spelling error, typos, fastText will be more effective then rest of two.4.3. Paragraph vector/Doc2vecParagraph Vector is an unsupervised algorithm that learns fixed-length feature rep-resentations from variable-length pieces of texts, such as sentences, paragraphs, and8documents (Le, Quoc et al. , 2014). Paragraph vector represents each document bya dense vector which is trained to predict words in the document. Authors believethat Paragraph vector have the potential to overcome the weaknesses of bag-of-wordsmodels and claimed that Paragraph Vectors outperform bag-of-words models as wellas other techniques for text representations. Paragraph vector model is also referredas doc2vec model. Henceforth, we will refer paragraph and Doc2vec interchangeably.Doc2vec model have two architecture namely : (i) DM: This is the Doc2Vec modelanalogous to CBOW model in Word2vec. The paragraph vectors are obtained by train-ing a neural network on the task of inferring a center word based on context words anda context paragraph. (ii) DBOW: This is the Doc2Vec model analogous to Skip-grammodel in Word2Vec. The paragraph vectors are obtained by training a neural networkon the task of predicting a probability distribution of words in a paragraph given arandomly-sampled word from the paragraph.4.4. Transfer LearningTransfer Learning in NLP is not as matured as compare to in Computer Vision. Trans-fer learning is a method in which model is trained on large corpus for a particular taskand use this pre-trained model for the similar task. There are two way to use transferlearning in NLP (i) Use of Pre-trained word embedding to initialize first layer of neuralnetwork model which can be called as shallow representation. (ii) Use the full modeland fine tune for the task specific in supervise learning way.Word2vec, Glove and fastText provide pre-trained word vector trained on the largecorpus. Google Word2vec pre-trained model have word vector for 3 million words withsize 300 and trained on Google news. Glove pre-trained model available with differentembed size and trained on common crawl, Twitter. We have use Glove pre-trainedmodel with vocabulary size 2.2 million and trained on common crawl. fastText pre-trained models are available in 157 language. We have use fasttext pre-trained vectorfor Englsih and Hindi language trained on commnon crawl and wikipedia.Recently, transfer learning in NLP done in new way; First language model is trainedon large text corpus in unsupervise way and fine tune on specific task like text classifi-cation on labeled data. Peters et al. (2018) author argued that word representation isdepend upon the context. So each word has different word vector depending upon theposition of the word in the sentence. Essentially Each word has dynamic word vectorwith respect to the context as opposed to the traditional word embedding techniqueswhich always give same word vector ignoring the context. Embedding from LanguageModels (ELMos) use languge model for the word embedding. Howaard et al. (2018)author propose Universal Language Model Fine-Tuning for Text Classification (ULM-FiT) which is bi-LSTM model that is trained on a general language modeling (LM)task and then fine tuned on text classification. Results are reported on both transferlearning model on TRAC dataset (Kumar et al. , 2018).5. Evaluation TasksWe have benchmarked various text representation scheme on two specialized NLPtask namely: aggression detection and fact detection. Text Representation scheme areevaluated on machine learning and deep neural model.95.1. Aggression Detection taskThe objective of this task is to identify type of aggression present in the text inboth Englsih and code-mixed Hindi language. Aggression are classified into threelevel namely: `Overtly Aggressive` (OAG), `Covertly Aggressive` (CAG) and `Non-aggressive` (NAG). We have implemented all standard machine learning classifierslike Multinomial Naive Bayes (MNB), Logistic Regression(LR), K-Nearest neighbors(KNN), Support Vector Classifier (SVC), Decision Tree (DT),Stochastic Gradient De-scent(SGD), Random forest (RF), Ridge, AdaBoost, Perceptron, and various votingbased ensemble with different text representation schemes like count based, TF/IDFand word embedding to prepare baseline results. Various word embedding techniqueslike Word2Vec, Glove, fastText, Paragraph2Vec are studied.5.1.1. Problem statementBasically Aggression detection is a Text classification problem. Formally, the task ofText Classification is stated as follows. Given a set of social media feed and a set ofclasses, We need to compute a function of the form:C = f(T, )where f is the multi-class classifier that is computed using training data, T is thenumeric representation of the text of the dataset,  is the set of parameters of theclassifier and C is the pre-define class-labels.5.1.2. Model Architectures and HyperparametersIn this subsection, we will discuss the architecture and hyperparameters of our deepneural model used for the classification. Model learns feature from the input textsTher is no need to design hand-crafted features which used to encode text into featurevector.5.1.2.1. Bidirectional LSTM. The first model is based on the Bidirectional LSTMinclude embedding layer with embed size 300, convert each word from the post intoa fixed length vector. short posts are padded with zero values. Subsequent layersincludes Bidirectional LSTM layer with 50 memory units followed by one-dimensionalglobal max pooling layer, a hidden layer with size 50 and output layer with softmaxactivations. ReLU activation function is used for the hidden layer activation. A dropout layer is added between the last two layers to counter the overfitting with parameter0.1. Hyperparameters are as follows: Sequence length is fixed at 1073 word; maximumlength of posts in the dataset. No of features is equal to half of total vocabulary size.Models are trained for 10 epoch with batch size 128. Adam optimization algorithm isused to update network weights.5.1.2.2. Single LSTM with higher dropout. This model is based on the LongShort Term Memory, a type of recurrent neural network with higher dropout. Thismodel is having one embedding layer, one LSMT layer with a size 64 memory unit,and one fully connected hidden layer with Relu activation and size 256 and an outputlayer with softmax activation. Hyperparameters are same as discussed in the previousmodel. A dropout layer is added between the hidden layer and an output layer with10drop out rate 0.2 to address the overfitting issue.5.1.2.3. CNN Model. This model includes one embedding layer whose weightsare initialized with fastText pre-trained vector with embed size is 300, followed byone-dimensional convolution layer with 100 filters of height 2 and stride 1 to targetbiagrams. In addition to this, Global Max Pooling layer added to fetch the maximumvalue from the filters which are feed to the fully connected hidden layer with size256, followed by output layer. ReLU and softmax activation function are used for thehidden layer and output layer respectively.5.1.2.4. CNN model with Multiple Convolution layer. This model includesembedding layer with embed size 300. Three one dimensional convolution layers withsize 100 and different filters with height 2,3,4 to target bigrams, trigrams, and four-grams features, followed by max pooling layer which concatenate max pooled resultfrom each of one-dimensional convolution layer. The final two layers include a fully con-nected hidden layer with size 250 and output layer with ReLu and softmax activation.A Drop out layer is added between the last two layer with rate 0.2. Hyperparametersare same as discussed in the first model. This model is similar to proposed by (Zhanget al. , 2015).5.2. Factual Post/Tweet Detection from Social MediaDuring the emergency situation like earthquake or floods, Microblog plays a veryimportant role as an anonymous communication medium. The various entity like,Volunteers, NGOs involved in relief operation always look for real-time informationwhich contains facts instead of prayer and condolence messages. In more technicalterm, these agencies are looking for factual information from Microblog instead of thesubjective information. In addition to this, the system should generate rank-list of thetweets based upon the worthiness of facts. we considered this problem as a binaryclassification problem plus pure IR Ranking problem. two classes can be labeled asfactual and non-factual.the IRMiDis dataset 3,which was prepared from the tweet posted during Nepalearthquake 2015 Basu et al. (2018) is considered for the experiment. There are only83 fact checkable tweets in the dataset. Non-factual tweets are not available. Total noof tweets in the dataset is more than 50000.5.2.1. Preparation of Training DataDue to the unavailability of adequate training data, The first task is to preparetraining data to train the deep neural model. We randomly choose 100 tweets fromthe dataset and labeled as a non-fact-checkable tweet and 83 fact-checkable tweetspresent in the dataset labeled as fact-checkable. We have trained our Convolutionneural network on these training data and tested the model on the remaining 50000tweets. At this stage we are not interested in the class but, we have sorted all thetweets based upon the predicted probability of the fact-checkable class and selectedtop 2000 tweets. We have randomly selected tweets and gave relevance judgmentbased upon availability of factual information in first 1000 tweets and manually3https://sites.google.com/site/irmidisfire2018/11extracted 300 tweets as non-fact-checkable tweets to minimize the false positives.Remaining 1700 tweets labeled as fact-checkable tweets. We selected the last 1700tweets with the least probability of the class fact-checkable and labeled them asnon-fact-checkable tweets. So our Training corpus has 1783 fact-checkable and 2000non-fact-checkable tweets.5.2.2. Proposed ApproachWe have used word embedding to represent the text instead of bags-of-words. fastText(Mikolov et al. , 2018) pre-trained vector with 300 dimensions is used to initialize theweight matrix of the embedding layer of the network. We trained our CNN model onthis training corpus with 10-fold cross-validation.The Model gives validation accuracyaround 94%. Finally, we run the model on the entire corpus and sorted the tweet basedupon the predicted probability of the Fact-checkable class. Essentially this approachtermed as weakly-supervise classification.6. ResultsIn this section, we first present results of classifiers TRAC dataset Kumar et al.(2018) with different text representation scheme. Latter we present result on FIREIRMiDis 2018 Dataset. Tweets are very noisy in nature contains user mentions, Hash-tags, Emojis, and URLs. We do not perform any kind of text pre-processing on tweetsin experiments with deep neural models. In experiments with machine learning classi-fier, before classification, Hashtag symbol # and User mentions are dropped from thetweets. Non-ASCII characters and stop-words are removed from tweet text (Modha etal. , 2016).6.1. Results On TRAC DatasetPrecision, Recall, and F1-score are the standard metrics which are used to evaluate theclassifier performance. We have evaluated 16 classifiers performance on 4 Datasets (2English+2 Hindi) with 10 Text Representation scheme(8 in the case of Hindi Dataset).Looking at such massive experiment, it is difficult to report results in all the abovemetrics. Therefore, Results are reported in terms of weighted F1-score only which isthe function of Precision and Recall. Classifiers results based on LSTM and CNNon BoW text representation schemes are not possible due to the high dimensionality.Bernoulli classifier is used instead of Naive Bayes Classifier in case of text represen-tation schemes other than BoW. Since word vectors might have negative weights, itis impossible to calculate probabilities with negative weights. Skip-gram variant ofWord2Vec and fastText is used in this experiment instead of continuous bag-of-word.Table 6 and 7 shows results on Facebook and Twitter English Dataset with BoW andword embedding while Table 8 present result with pre-trained word embedding withsame dataset. Table 9 and 10 shows results on Facebook and Twitter code-mixed HindiDataset. Only fastText provide pre-trained word vector (Mikolov et al. , 2018) for theHindi language. Exhaustive evaluation is performed with all classifiers with respectto each text representation schemes. Experiments are also performed with the newtransfer learning model like ELMO and ULMFIT. Table 11 presents results on bothFacebook and Twitter English Datasets. Figure 1 and figure 2 display the heatmap of12Table 6. F1-score on TRAC Facebook English DatasetClassifier Count-vectorTF/IDF W2Vec Glove Fasttext doc2vec-dmcdoc2vec-dbowNB 0.5571 0.5596 0.4870 0.3873 0.5035 0.4585 0.4634LR 0.5953 0.6046 0.5675 0.5358 0.5400 0.5266 0.5139KNN 0.5466 0.5428 0.5061 0.5130 0.5113 0.5114 0.5095SVC 0.5801 0.5902 0.5369 0.5037 0.5137 0.5388 0.5033DT 0.5269 0.5055 0.4468 0.4067 0.5002 0.4198 0.4198SGD 0.5706 0.5938 0.4647 0.3571 0.5167 0.5060 0.3521RF 0.5621 0.5582 0.5199 0.4752 0.5513 0.4230 0.4210Ridge 0.6009 0.5999 0.5347 0.5336 0.5225 0.5385 0.5083AdaB 0.6210 0.6141 0.5491 0.4932 0.5644 0.4689 0.4852Perce. 0.5387 0.5491 0.5230 0.4020 0.4848 0.3800 0.3253ANN 0.5703 0.5350 0.5350 0.5037 0.5380 0.4980 0.4401Ensemble 0.58 0.5900 0.5558 0.4067 0.5617 0.4980 0.4401LSTM 0.5649 0.5454 0.5062BLSTM 0.5759 0.4760 0.5641CNN 0.5515 0.5365 0.5638NCNN 0.5919 0.5488 0.4849Table 7. F1-score on TRAC Twitter English DatasetClassifier Count-vectorTF/IDF W2Vec Glove Fasttext doc2vec-dmcdoc2vec-dbowNB 0.5102 0.4528 0.5551 0.3936 0.5495 0.3254 0.3536LR 0.4849 0.4890 0.3457 0.3959 0.3871 0.3041 0.3274KNN 0.3539 0.2891 0.3843 0.3607 0.3997 0.3225 0.3191SVC 0.4642 0.4853 0.3078 0.3627 0.2858 0.3019 0.3274DT 0.4229 0.4111 0.3884 0.3673 0.3948 0.3326 0.3326SGD 0.4682 0.5020 0.4512 0.4182 0.3838 0.3251 0.3350RF 0.4199 0.3917 0.4333 0.3634 0.4069 0.3301 0.3293Ridge 0.4703 0.5003 0.3352 0.3877 0.3180 0.2994 0.3243AdaB 0.3343 0.3696 0.4485 0.3552 0.4215 0.3288 0.3223Perce. 0.4930 0.4778 0.3521 0.3938 0.3340 0.3015 0.2990ANN 0.4912 0.5164 0.5111 0.3552 0.4532 0.3230 0.3281Ensemble 0.495 0.4842 0.4500 0.3938 0.4471 0.3230 0.3281LSTM 0.5385 0.5156 0.5335BLSTM 0.5314 0.3860 0.4985CNN 0.5012 0.5377 0.4849NCNN 0.5120 0.4984 0.5179the results achieved by classifiers on each text representation scheme.6.2. Information Retrieval from Microblogs during Disasters (IRMiDis)DatasetAs discussed in the previous section 1, This task is classification plus Ranking task.Table 13 shows our system results on IRMiDis dataset (Basu et al. , 2018) along withthe rest of teams. nDCG overall is the primary metric for the evaluation. Our systemsubstantially outperforms rest of team in the most of the metrics which justifies ourclaim established on TRAC dataset (Kumar et al. , 2018)7. Result AnalysisIn this section, we will present the comprehensive result analysis and try to answerthe research questions which framed before the experiments were performed. As welook at the table 6 7, and 8, Overall, LSTM and CNN with pre-trained fastText wordembedding marginally outperform (around 2 % to 4%) standard machine learning13Table 8. F1-score on TRAC Facebook and Twitter English Dataset:Using Pre-trained word vectors.Facebook Test Dataset Twitter Test DatasetClassifier p-Word2vec p-Glove p-Fasttext p-Word2vec p-Glove p-FasttextNB 0.5342 0.5373 0.5519 0.4152 0.4527 0.4276LR 0.5799 0.6050 0.6045 0.4197 0.4527 0.4441KNN 0.4981 0.5103 0.4819 0.3405 0.3959 0.3912SVC 0.5832 0.5678 0.6120 0.4446 0.4581 0.4350DT 0.4700 0.4515 0.4900 0.3640 0.3949 0.3632SGD 0.5019 0.5521 0.5360 0.3692 0.3793 0.3852RF 0.5402 0.5338 0.5505 0.3394 0.3716 0.3687Ridge 0.5829 0.5952 0.6140 0.4092 0.4530 0.4461AdaB 0.5713 0.5781 0.5907 0.4241 0.4261 0.4033Perce. 0.5114 0.5201 0.5660 0.4224 0.4118 0.4049ANN 0.5025 0.5498 0.5722 0.3728 0.3722 0.4842Ensemble 0.5300 0.5500 0.5558 0.3728 0.3722 0.4500LSTM 0.4979 0.4979 0.6178 0.5537 0.5518 0.5541BLSTM 0.5501 0.6062 0.6000 0.5359 0.5466 0.5423CNN 0.4749 0.5405 0.6407 0.5226 0.5667 0.5520NCNN 0.5169 0.5883 0.5600 0.5384 0.5067 0.5407Table 9. F1-score on TRAC Facebook Code-mixed Hindi DatasetClassifier Count-vectorTF/IDF W2Vec Glove Fasttext p-fastTextdoc2vec-dmcdoc2vec-dbowNB 0.5535 0.6031 0.3001 0.372 0.2959 0.3176 0.3459 0.4736LR 0.5855 0.6134 0.5779 0.464 0.5457 0.5518 0.3894 0.4380KNN 0.3340 0.1721 0.4998 0.425 0.5106 0.4909 0.3768 0.4038SVC 0.5556 0.5862 0.4806 0.373 0.5186 0.5442 0.3879 0.4344DT 0.5307 0.5025 0.4629 0.388 0.4392 0.4288 0.3485 0.3485SGD 0.5533 0.5922 0.3912 0.393 0.3670 0.4746 0.3331 0.4134RF 0.5473 0.5473 0.5374 0.440 0.5047 0.4788 0.3512 0.3477Ridge 0.5780 0.5850 0.5293 0.381 0.5092 0.5544 0.3866 0.4292AdaB 0.5373 0.5233 0.5342 0.479 0.5336 0.4913 0.3751 0.4214Perce. 0.5213 0.5598 0.4232 0.364 0.3763 0.4873 0.2661 0.3282ANN 0.5703 0.5350 0.5455 0.5037 0.5842 0.5190 0.4091 0.4440Ensemble 0.5700 0.6087 0.5558 0.4067 0.534 0.5612 0.4980 0.4401LSTM 0.5649 0.590 0.6021 0.5916BLSTM 0.5759 0.527 0.5770 0.5900CNN 0.5515 0.566 0.5950 0.6081NCNN 0.5919 0.573 0.5912 0.5965Table 10. F1-score on TRAC Twitter Code-mixed Hindi DatasetClassifier Count-vectorTF/IDF W2Vec Glove Fasttext p-fastText doc2vec-dmcdoc2vec-dbowNB 0.2970 0.2902 0.3215 0.273 0.3359 0.2897 0.3270 0.3205LR 0.3787 0.3724 0.2819 0.279 0.3184 0.3524 0.2438 0.2833KNN 0.2527 0.2553 0.3704 0.334 0.3381 0.2917 0.3051 0.3299SVC 0.3781 0.3886 0.2821 0.261 0.3087 0.3472 0.2580 0.2905DT 0.3685 0.3936 0.3572 0.326 0.3475 0.3473 0.2988 0.2988SGD 0.3996 0.3993 0.2822 0.287 0.2739 0.3163 0.2605 0.2588RF 0.3585 0.3737 0.3286 0.344 0.3449 0.3288 0.2981 0.2988Ridge 0.3616 0.3872 0.2811 0.242 0.3346 0.3361 0.2549 0.2875AdaB 0.1886 0.1903 0.3256 0.362 0.3261 0.3441 0.2614 0.2933Perce. 0.3931 0.3868 0.2802 0.329 0.2787 0.3835 0.2616 0.2752ANN 0.430 0.44 0.3163 0.3552 0.2399 0.3593 0.2419 0.3132Ensemble 0.4400 0.4600 0.4500 0.3938 0.3426 0.3555 0.3230 0.3281LSTM 0.3840 0.376 0.3667 0.4600BLSTM 0.2846 0.318 0.3005 0.4600CNN 0.3323 0.317 0.2669 0.4992NCNN 0.3338 0.380 0.3494 0.460014Table 11. F1 score on TRAC Facebook English Test Dataset usingTransfer Learning methodsTransfer learning Model EnglishFacebook Dataset Twitter DatasetELMO 0.3699 0.3854ULMFiT 0.4725 0.4664Table 12. weighted F1-score TRAC Test Dataset: comparison with peersSystem English HindiFacebookDatasetTwitterDatasetFacebookDatasetTwitterDatasetOur system result 0.6407 0.5541 0.6081 0.4992DA-LD-hildesheim 0.6178 0.552 0.6081 0.4992saroyehun 0.6425 0.5920 NA NAEBSI-LIA-UNAM 0.6315 0.5715 NA NATakeLab 0.5920 0.5651 NA NAtaraka rama 0.6008 0.5656 0.6420 0.40vista.ue 0.5812 0.6008 0.5951 0.4829na14 0.5920 0.5663 0.6450 0.4853Figure 1. Heatmap on English Facebook Test Dataset Results.Table 13. Results Comparison with rest of team on FIRE 2018 IRMiDis Dataset.System p@100 R@1000 MAP@100 MAP nDCG @100 nDCGOur System 0.4 0.2002 0.0129 0.1471 0.4021 0.7492MIDAS-semiauto 0.9600 0.1148 0.0740 0.1345 0.6007 0.6899MIDAS-1 0.8800 0.1292 0.0581 0.1329 0.5649 0.6835FAST NU Run2 0.7000 0.0885 0.0396 0.0801 0.5723 0.6676UEM DataMining 0.6800 0.1427 0.0378 0.1178 0.5332 0.6396iitbhu irlab2 0.3900 0.0447 0.0144 0.0401 0.3272 0.620015Figure 2. Heatmap on English Twitter Test Dataset Results.Table 14. Results Comparison with CNN model and Logistic Regression TRACFacebook English Test DatasetClass CNN model Logistic Regression #PostsP R Weighted F1 P R Weighted F1NAG 0.86 0.64 0.73 0.83 0.60 0.70 630CAG 0.28 0.46 0.35 0.23 0.54 0.32 142OAG 0.42 0.61 0.50 0.46 0.39 0.42 144overall 0.70 0.61 0.64 0.68 0.56 0.60 916classifiers and ensemble of classifier with respect to weighted F1- score on FacebookEnglish corpus and substantially outperforms on Twitter English corpus. By and largesimilar results observed on code-mixed Hindi corpus as shown in table 9 and 10.Table 14 present the detailed comparative results of two classifiers: CNN modelwith fastText pre-trained vector and the logistic regression with TF/IDF weightingon TRAC Facebook English dataset. The CNN Model classify Facebook posts betterthan logistic regression at the individual class level and overall. It has been quiteevident that posts belong to CAG class are hard to classify and Malmasi, et al. (2017)reported that the same observation. Table 15 show posts which are miss-classified bylogistic regression however, CNN model correctly classified them into the CAG class.7.1. Significance TestTo support our claim drawn in the previous section, significance tests, like Wilcoxonsigned-rank test and Student t-test were carried out by comparing Weighted F1 scoreof each classifier for each text representation scheme with fastText pre-trained vectorscheme. Table 16 and 17 summarizes the p-values of statistical significance tests on16Table 15. sample post for the CAG class.no Post text Gold Label CNN LR1 Mauni singh trying very hard to convince himself whatis written in script... body language says it allCAG CAG NAG2 Indian govt is all Abt giving money to Bangladesh onthe terms of Bangladesh ll give that all projects toamabani n adani for thier benefits lol who cares Abtsoldiers or India they r just puppets of thier ownersfeku or pappuCAG CAG NAG3 When asked to speak in Parliament ran away. Speaksonly in TV,radio or in election rally. Can we expectAnother crying drama after Demonetisation disaster ?#cryBaby"CAG CAG NAGTable 16. p-values of Significance test on F1-score on TRAC Facebook English DatasetText Rep. scheme Facebook English Twitter EnglishWilcoxon T-test Wilcoxon T-testCount Vector 0.001 0.12 0.004 0.016TF/IDF 0.0001 0.1465 0.0061 0.0391Word2vec 0.00002 0.0001 0.69 0.30Glove 0.00001 0.000002 0.01 0.0009fastText 0.00003 0.0004 0.12 0.08doc2vec-dmc 0.000009 0.0001 0.0004 0.000004doc2vec-dbow 0.000009 0.00003 0.0004 0.000006p-Word2vec 0.00001 0.0006 0.02 0.01P-Glove 0.0002 0.02 0.08 0.30English and Hindi Dataset respectively. In Wilcoxon signed-rank test, p-values of theresults is less than 0.05 for Facebook English dataset and Twitter Hindi Dataset.However, On Twitter English dataset and Facebook Hindi Dataset, some of the p-values are higher than 0.05. In student t-test, we get mixed bag results. By and large,our results are statistically significant.In the following subsection, we will try to answer all the research questions framedduring the experiments were planned.7.2. Best Text Representation scheme to model the text from Social webText Representation is the primary task for to address any NLP task like Ques-tion/answering, classification etc. As dicusses in section 4, There are basically twotext representing scheme:Bag-of-Word(BoW) with countvector, TF/IDF weightingand word embedding. Word2Vec (Mikolov et.al , 2013), Glove (Pennington et.al. ,2014), and fastText (Mikolov et al. , 2018), an extension of Word2vec are popularword embedding techniques.Table 17. p-values of Significance test on F1-score on TRAC code-mixed Hindi Test DatasetText Rep. scheme Facebook Code-mixed Hindi Twitter Code-mixed HindiWilcoxon T-test Wilcoxon T-testCount Vector 0.003 0.05 0.01 0.20TF/IDF 0.003 0.14 0.007 0.12Word2vec 0.40 0.17 0.043 0.017Glove 0.001 0.0005 0.015 0.004fastText 0.35 0.15 0.022 0.006doc2vec-dmc 0.005 0.00001 0.001 0.0008doc2vec-dbow 0.003 0.002 0.001 0.00217Results clearly show that models with fastText pre-trained vector outperform Glovepre-trained vector on Facebook test dataset as well as the Twitter test dataset. themain reason behind the outperformance of fastText over Glove and Word2vec is thatThe fastText consider each word as N-gram characters. A word vector for a wordis computed from the sum of the n-gram characters. Glove and Word2vec considereach word as a single unit and provide a word vector for each word. Since Facebookusers make a lot of mistakes in spelling, typos, fastText is more convenient than Glove(Majumder et al., 2018). from Figure 1 and 2 shows that BoW is still effective textrepresentation scheme for the standard machine learning classier which takes hand-crafted feature and n-grams as inputs. Logistic Regression and Support Vector performbetter than other classifiers in English as well as Hindi Dataset. Adaboost performsbetter than LR and SVC on Facebook English Dataset but substantially underperformthem on rest of three Datsets. Our participation (Majumder et al., 2018) in TRACcompetition (Kumar Ritesh et al., 2018) FIRE Information Retrieval from Microblogsduring Disasters (Basu et al. , 2018) track where our team performed well and securedtop position.7.3. Transfer Learning Model vs Pre-trained Word Embedding ModelTransfer learning is focused on storing knowledge gained while solving one problem andapplying it to a different but related problem. On many occasion, NLP researchers facethe problem of unavailability of sufficient labeled data to train the model. With theadvent of new transfer learning method like ELMO (Peters et al. , 2018) and Universallanguage model fine-tuning for Text Classification (ULMFiT) (Howaard et al., 2018)attract interest among NLP Researchers. These models are trained or large text corpus.Howaard et al. (2018) claimed that these model can be fine-tuned on the task-specificcorpus. We have used these transfer learning model on TRAC English dataset Kumaret al. (2018) and results are presented in table 11. one can observe that results aresubstantially lower than the results reported in Table 6, 7, and 8 where pre-trainedword vectors are used to initialize the first layer of deep neural model and rest of thenetwork is trained from scratch achieve better results than transfer learning model.Howaard et al. (2018) termed use of pre-trained vector as shallow representation.In these experiments, we trained different classifier models on Facebook posts. Table7 10 shows the results on Twitter dataset Kumar et al. (2018). There is lexicaldifference between Facebook and Twitter posts. From the results shown in Table 7 8and table 10, one can conclude that weighted F1 score of standard machine learningclassifiers are substantially lower in Twitter Dataset as compare to Facebook Dataset.While deep learning models perform better than machine learning classifiers for theTwitter Dataset. Thus, Deep learning models are more robust than machine learningclassifier across diverse datasets.7.4. Does Deeper Neural Net make SenseTo answer this question, we designed first CNN model with one convolution layer andother CNN model with 3 convolution layer with different filters height. As we lookat results shown in Table 6,7,8, and 9, one can conclude that by and large weightedF1 score lower for CNN model with multiple convolution layer than CNN model withsingle convolution layer.18</corps>
	<conclusion>
	In this Paper, Multilingual Social media stream is studied with special kind of textfeatures: Aggression and fact perspective. Exhaustive experiments are performed tobenchmark the text representation scheme on machine learning classifiers and deepneural nets. From the results, we conclude that deep Neural model with pre-trainedword embedding is the better choice than machine earning classifier and transfer learn-ing model. Word embedding is the better text representative scheme than Bag-of-wordsfor the deep neural models. In fact, performance can be improved with the help of fast-Text pre-trained vector. However, machine learning classifiers perform better in BoWwith TF/IDF weighting than word embedding. We also concluded that higher dropout will help to counter model overfitting and improvise a standard evaluation metrics.CNN and LSTM are the better models for these datasets. On the English test corpus,we obtained a better weighted F1 score for NAG class and poor weighted F1 scorefor CAG class which supports the previous (Malmasi, et al. , 2017) findings. For theFacebook Hindi test corpus, the same seems not to be true. We obtained a better F1score for CAG class than NAG class. It is also to be noted that the model leads topoor result on Twitter test data since the training corpus was created from Facebook.In such cases, deep neural models substantially outperform machine learning classi-fiers. Significance test confirms these claims with 95 % confidence interval in most thecases. Our work shows what kind of problems are moving into the center of attentionfor research in machine learning. Using deep learning models, there is great potentialto solve some of these problems, yet still, the performance is far from perfect. Modeltransfer between problems and the application of derived knowledge in user interfacesare areas directions for future work.</conclusion>
	<discussion>
	</discussion>
	<biblio>
	Aroyehun, Segun Taofeek and Gelbukh, Alexander (2018). Aggression detection in social media:Using deep neural networks, data augmentation, and pseudolabeling.Proceedings of the FirstWorkshop on Trolling, Aggression and Cyberbullying (TRAC-2018), pp.90-97.Arroyo-Fernandez, Ignacio and Forest, Dominic and Torres-Moreno, Juan-Manuel andCarrasco-Ruiz, Mauricio and Legeleux, Thomas and Joannette,Karen (2018). CyberbullyingDetection Task: the EBSI-LIA-UNAM System (ELU) at COLING'18 TRAC-1.Proceedingsof the First Workshop on Trolling,Aggression and Cyberbullying (TRAC-2018), pp 140-149.Basu, Moumitaand Ghosh, Saptarshi and Ghosh, Kripabandhu (2018). Overview of the FIRE2018 track: Information Retrieval from Microblogs during Disasters (IRMiDis). Proceedingsof FIRE 2018 - Forum for Information Retrieval Evaluation, Gujrat, India, December .Baziotis, Christos and Pelekis, Nikos and Doulkeridis, Christos (2017). Datastories at semeval-2017 task 4: Deep lstm with attention for message-level and topic-based sentiment analysis.Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017),pp.747-754Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas (2017) En-riching word vectors with subword information, Transactions of the Association for Compu-tational Linguistics, vol-5,pp.135-146, MIT Press.Burnap, Pete and Williams, Matthew Ltitle (2015). Cyber hate speech on twitter: An applica-tion of machine classification and statistical modeling for policy and decision makingPolicy& Internet, vol-7 number 2, pp.223-242, Wiley Online Library.Conover, Michael and Ratkiewicz, Jacob and Francisco, Matthew R and Goncalves , Bruno andMenczer, Filippo and Flammini, Alessandro (2011). Political polarization on twitter.,Icwsm,vol-133, pp.89-96.19Conover, Michael D and Goncalves, Bruno and Ratkiewicz, Jacob and Flammini, Alessandroand Menczer, Filippo (2011). Predicting the political alignment of twitter users, Privacy,Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on SocialComputing (SocialCom), 2011 IEEE Third International Conference on, pp.192-199.Davidson, Thomas and Warmsley, Dana and Macy, Michael and Weber, Ingmar (2017). Au-tomated Hate Speech Detection and the Problem of Offensive Langemuage. Proceedings ofICWSM.Deriu, Jan and Gonzenbach, Maurice and Uzdilli, Fatih and Lucchi, Aurelien and Luca, ValeriaDe and Jaggi, Martin (2016). Swisscheese at semeval-2016 task 4: Sentiment classificationusing an ensemble of convolutional neural networks with distant supervision. Proceedings ofthe 10th international workshop on semantic evaluation,pp.1124-1128.Hltcoe, J (2013).Semeval-2013 task 2: Sentiment analysis in Twitter,vol-312 Atlanta, Georgia,USA.Howard, Jeremy & Ruder, Sebastian 2018. Universal language model fine-tuning for text clas-sification",arXiv preprint arXiv:1801.06146,Harris, Zellig S (1954),Distributional structure Word,10, number=2-3,pp.146-162, 1954, Taylor& Francis.Kumar, Ritesh and Reganti, Aishwarya N. and Bhatia, Akshit and Maheshwari,Tushar (2018),Aggression-annotated Corpus of Hindi-English Code-mixed Data, Proceedings of the 11thLanguage Resources and Evaluation Conference (LREC), Miyazaki, Japan.Kumar, Ritesh and Ojha, Atul Kr. and Malmasi, Shervin and Zampieri Marcos (2018). Bench-marking Aggression Identification in Social Media, Proceedings of the First Workshop onTrolling, Aggression and Cyberbulling (TRAC), Santa Fe, USAKwok, Irene and Wang, Yuzhou (2013),Locate the hate: Detecting Tweets Against Blacks,Twenty-Seventh AAAI Conference on Artificial Intelligence.Lau, Jey Han and Baldwin, Timothy (2016). An empirical evaluation of doc2vec with practicalinsights into document embedding generation. arXiv preprint arXiv:1607.05368.Le, Quoc and Mikolov, Tomas (2014) Distributed representations of sentences and docu-ments.International Conference on Machine Learning, pp.1188-1196Majumder, Prasenjit and Mandl, Thomas and Modha Sandip (2018)Filtering Aggression fromthe Multilingual Social Media Feed Proceedings of the First Workshop on Trolling, Aggres-sion and Cyberbullying (TRAC-2018), pp. 199-207Malmasi, Shervin and Zampieri, Marcos (2017)Detecting Hate Speech in Social Media Pro-ceedings of the International Conference Recent Advances in Natural Language Processing(RANLP), pp.467-472.Malmasi, Shervin and Zampieri, Marcos (2018).Challenges in Discriminating Profanity fromHate Speech Journal of Experimental & Theoretical Artificial Intelligence pp.1-16, vol-30,issue-2,Taylor & Francis.Maynard, Diana and Funk, Adam (2011) Automatic detection of political opinionsin tweets,Extended Semantic Web Conference,pp. 88-99, Springer.Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey (2013). Efficient estima-tion of word representations in vector space,arXiv preprint arXiv:1301.3781.Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin,Armand (2018). Advances in Pre-Training Distributed Word Representations,Proceedings ofthe International Conference on Language Resources and Evaluation (LREC 2018).Modha, Sandip and Agrawal, Krati and Verma, Deepali and Majumder, Prasenjit and Man-dalia, Chintak 2016. DAIICT at TREC RTS 2016: Live Push Notification and Email Di-gest.,TREC.Mohammad, Saif M and Kiritchenko, Svetlana and Zhu, Xiaodan (2013). NRC-Canada: Build-ing the state-of-the-art in sentiment analysis of tweets.arXiv preprint arXiv:1308.6242.Nakov, Preslav and Ritter, Alan and Rosenthal, Sara and Sebastiani, Fabrizio and Stoyanov,Veselin (2016). SemEval-2016 task 4: Sentiment analysis in Twitter,Proceedings of the 10thinternational workshop on semantic evaluation (semeval-2016), pp. 1-18.Pennington, Jeffrey and Socher, Richard and Manning, Christopher Glove: Global vectors for20word representation Proceedings of the 2014 conference on empirical methods in naturallanguage processing (EMNLP), pp.1532-1543.Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark,Christopher and Lee, Kenton and Zettlemoyer, Luke (2018). Deep contextualized word rep-resentations arXiv preprint arXiv:1802.05365.Razavi, Amir H and Inkpen, Diana and Uritsky, Sasha and Matwin, Stan (2010) Offensivelanguage detection using multi-level classification, Canadian Conference on Artificial Intel-ligence pp.16-27, SpringerRosenthal, Sara and Nakov, Preslav and Kiritchenko, Svetlana and Mohammad, Saif and Rit-ter, Alan and Stoyanov, Veselin (2015). Semeval-2015 task 10: Sentiment analysis in twit-ter Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015),pp.451-463.Rosenthal, Sara and Farra, Noura and Nakov, Preslav (2017). SemEval-2017 task 4: Sentimentanalysis in Twitter Proceedings of the 11th International Workshop on Semantic Evaluation(SemEval-2017), pp.502-518.Schmidt, Anna and Wiegand, Michael (2017).A Survey on Hate Speech Detection Using NaturalLanguage Processing,Proceedings of the Fifth International Workshop on Natural LanguageProcessing for Social Media. Association for Computational Linguistics Valencia, Spain,pp.1-10,Severyn, Aliaksei and Moschitti, Alessandro (2015) Unitn: Training deep convolutional neuralnetwork for twitter sentiment classification, Proceedings of the 9th international workshopon semantic evaluation (SemEval 2015), pp. 464-469.Tumasjan, Andranik and Sprenger, Timm Oliver and Sandner, Philipp G and Welpe, IsabellM (2010). Predicting elections with twitter: What 140 characters reveal about political sen-timent.,Icwsm, vol-10, number-1, pp. 178-185.Warner, William and Hirschberg, Julia (2012) Detecting hate speech on the world wide web,Proceedings of the Second Workshop on Language in Social Media,pp 19-26,Association forComputational Linguistics.Xu, Jun-Ming and Jun, Kwang-Sung and Zhu, Xiaojin and Bellmore, Amy (2012). Learningfrom bullying traces in social media, Proceedings of the 2012 conference of the North Amer-ican chapter of the association for computational linguistics: Human language technologies,pp.656-666, Association for Computational LinguisticsZhang, Ye and Wallace, Byron (2015). A Sensitivity Analysis of (and Practitioners' Guide to)Convolutional Neural Networks for Sentence Classification,arXiv preprint arXiv:1510.03820.Majumder, Prasenjit and Mitra, Mandar and Pal, Dipasree and Bandyopadhyay, Ayan andMaiti, Samaresh and Mitra, Sukanya and Sen, Aparajita and Pal, Sukomal.Text collectionsfor FIRE, Proceedings of the 31st annual international ACM SIGIR conference on researchand development in information retrieval,pp.699-700,ACMMajumdar, P and Mitra, Mandar and Parui, Swapan K and Bhattacharya, Initiative for indianlanguage ir evaluation, The First International Workshop on Evaluating Information Access(EVIA)21
	</biblio>
</article>