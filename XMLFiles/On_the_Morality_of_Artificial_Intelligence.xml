<article>
	<preamble>On_the_Morality_of_Artificial_Intelligence.pdf</preamble>
	<titre>On the Morality of Artificial Intelligence</titre>
	<auteurs>
		<auteur>
			<name></name>
			<mail>@uantifying</mail>
            <affiliation>Alexandra Luccioni and Yoshua Bengio Universite de Montreal, Mila</affiliation>
      </auteur>
	</auteurs>
	<abstract>Much of the existing research on the social and ethical impact of Artificial Intelligence has been focused on defining ethical principles and guidelines surrounding Machine Learning (ML) and other Artificial Intelligence (AI) algorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for helping define the appropriate social norms of AI, we believe that it is equally important to discuss both the potential and risks of ML and to inspire the community to use ML for beneficial objectives. In the present article, which is specifically aimed at ML practitioners, we thus focus more on the latter, carrying out an overview of existing high-level ethical frameworks and guidelines, but above all proposing both conceptual and practical principles and guidelines for ML research and deployment, insisting on concrete actions that can be taken by practitioners to pursue a more ethical and moral practice of ML aimed at using AI for social good.</abstract>
	<introduction>N/A</introduction>
	<body>2. Who will benefit or suffer from it?3. How much and what social impact will it have?4. How does my job fit with my values?We are conscious that the questions listed above are subjective and the answers will depend highly on thevalues and ethics of the individual answering them. Nonetheless, we hope that work on some applications,such as the design and deployment of lethal autonomous weapons and automatic surveillance, will clearlybe seen to contradict fundamental rights and dignity, as defined in, among other places, the UN Declarationof Human Rights [1948]. Other applications of ML, such as those increasing the efficiency of advertising orbeating the stock market, are less clear-cut in their moral value, and merit informed debate and discussionwithin the scientific community and society at large. As some of us become more conscious of the potentialor definite social impact of ML, we have the opportunity, if not the duty, to make our voices heard. A goodexample of this is a recent letter signed by numerous scientists calling for an international treaty banninglethal autonomous weapon systems, e.g., killer drones which can decide to shoot at a person without humaninvolvement, which would make it possible to take the broad social, moral and psychological context intoaccount and potentially decide to abort the mission (for instance, when the target is in a school or at a familydinner surrounded by women and children).Finally, while the legal frameworks to oversee and limit research and development violating these prin-ciples are often and unfortunately updated in a reactive rather than a proactive manner, we believe thatwe should not wait until all of the dots between ML and ethics are formally connected by legislation andregulation. We believe that we have a responsibility to educate ourselves, to think ahead about potentialconsequences, to use our internal moral compasses and to consciously choose the direction of the researchor engineering that we practice. This is important because we believe that we are faced with a wisdomrace: as technology becomes more powerful, its impact can be proportionally greater, either positively ornegatively.To curb the negative impact, we need to become wiser individually (as reflected in our personal deci-sions) and collectively (through social norms, laws and regulations). Unfortunately, technological progressin AI has accelerated faster than the current rate of progress of personal and social wisdom, ultimately mak-ing it possible for unwise humans or organizations, even those with good intentions and acting legally, tohave large-scale, major destructive effects. This is comparable to a world in which nuclear bombs (i.e. verypowerful technology) were accessible and usable by children (i.e., persons with insufficient maturity andwisdom), which could easily result in global nuclear war. This highlights the importance of the discussionsstill to be had by large numbers of ML practitioners about ethics and social impact, as well as the safeguardsthat need to be put in place to protect especially the most vulnerable members of our society. We will dis-cuss some of the most advanced efforts to introduce these safeguards in the next section, followed by someexamples of socially beneficial applications of ML.2 Ethics and AI - Existing InitiativesIn recent years, there have been numerous initiatives which have taken one of two major approaches tofostering the ethical practice of AI: (1) Proposing principles guiding the socially responsible developmentof AI or (2) Raising concerns about the social impact of AI. We will describe both approaches in the currentsection, as well as giving examples of notable initiatives and projects which have adopted either of theapproaches.11For a more complete overview of different global ML ethics initiatives, see a recent review in Jobin et al. [2019]22.1 Defining Principles for Practicing AI ResponsiblyThe topic of ethical research and practice in technology has been gaining momentum in different corners ofthe computing community in recent years, and the various initiatives that have been proposed are indicativeof the interest and the concern that many members share. For instance, in the United States, the Associationfor Computing Machinery (ACM) has proposed a Code of Ethics and Professional Conduct, to be followedby all members of the association and to guide them in their usage of computer science [Gotterbarn et al.,2018]. A similar initiative was undertaken by the Royal Statistical Society (RSS) in the United Kingdom,which has created a practical guide for practitioners regarding the ethical use of mathematics [RSS, 2019].In the present section, we will address the two most relevant and extensive initiatives to establish ethicalguidelines for AI research and practice: the Montreal Declaration for Responsible Development of AI andthe IEEE report for Ethically Aligned Design.2.1.1 The Montreal Declaration for a Responsible Development of Artificial IntelligenceOne of the most notable approaches to establishing guidelines for AI deployment is the Montreal Declara-tion for a Responsible Development of Artificial Intelligence, developed in 2017 and revised in 2018 basedon public feedback2. It was elaborated under the premise that given the assumption that since AI will even-tually affect all sectors of society, it requires principles to guide its development to ensure its adherence tohuman values and social progress. The resulting Declaration has ten principles, ranging from protectionof privacy to equal representation, with some principles touching responsibility and ethics directly; for in-stance, the principle of Prudence stipulates that "Every person involved in AI development must exercisecaution by anticipating, as far as possible, the adverse consequences of AIS [Artificial Intelligent Systems]use and by taking the appropriate measures to avoid them." These principles were defined after extensivedebates and dialogue between both specialists and non-specialists from different domains and parts of theworld to ensure representability and cohesion. The overall aim of the declaration was to spark public debateand to encourage a progressive and inclusive orientation to the development of AI.However, the Montreal declaration goes further than theoretical ethical principles, proposing recom-mendations to accomplish an ethical digital transition that includes all of the different levels of society,from researchers to policy-makers. For instance, it includes a proposition for auditing and validating theuse of AIS using concrete frameworks and certifications in order to prevent biases and discrimination. Spe-cific steps were also proposed for ensuring the protection of democracy and reducing the environmentalfootprint of AI, all within the framework of a democratic and citizen-led process. This is important giventhat the effects of AI will permeate all levels of society, from the programmers and engineers who writethe code, to the leaders who will legislate it, and the businesses who will make products with it that will beused by all. The process of creation of the Montreal declaration was consequently the keystone to buildinga way of including all of these different stakeholders in the elaboration of an ethical AI, and paves the wayfor subsequent work on the topic.2.1.2 IEEE Ethically Aligned DesignA more recent effort, initiated by the IEEE Global Initiative on Ethics of Autonomous and Intelligent Sys-tems, carried out an in-depth study on the issue of the ethics surrounding the design of AI systems [IEEE,2017]. In particular, aspects that are relevant to the topics covered in the present paper include: the usage ofA/IS [autonomous and intelligent systems] in service to sustainable development for all, and more specif-ically for the attainment of the United Nations Sustainable Development Goals (SDGs) [UNHCR, 2017].The authors of the study specifically underline the potential of AI to contribute to resolving some of theworld's most urgent problems, such as climate change and poverty, given the necessary will and orientationtowards these problems. Furthermore, they highlight the fact that despite their great potential, current AIdeployment and development is currently not aligned with these goals and impacts [IEEE, 2017, p. 144],which is unsettling given the myriad of ML project and initiatives worldwide.The IEEE report also lays down principles to guide "the ethical and values-based design, development,and implementation of autonomous and intelligent systems", many of which are similar to those defined2 https://www.montrealdeclaration-responsibleai.com/3by the Montreal Declaration: respect of human rights, data agency, transparency, accountability, etc. Theygo further in proposing that "A/IS creators shall adopt increased human well-being as a primary successcriterion for development" instead of focusing on isolated metrics such as accuracy, and, from a deploymentperspective, offering alternative metrics to quantify meaningful progress, for instance by evaluating social,economic and environmental factors instead of profit and other common success metrics. The report alsoincludes propositions for policymakers, legislators and other stakeholders from the extended AI communityand, as such, represents the most extensive effort of establishing ethical boundaries and guidelines for AIresearch to date.In a recent survey of the various global ethics guidelines proposed around AI, the authors observed thatdespite a conceptual overlap between the many existing guidelines, including the two mentioned above,there are major differences regarding how the principles are interpreted [Jobin et al., 2019]. This underlinesthe complexity and nuance of applying theoretical, philosophical principles in practice, and raises questionssuch as: what aspects of the AI research and deployment pipeline do ethics principles affect? how would itbe possible to resolve conflicts between, for instance, fairness and sustainability (i.e. training an algorithmlonger and with more data - thus potentially leading to more greenhouse gas emissions - to ensure that itis not discriminatory and covers all demographic groups equally well)? And, above all, how is it possibleto translate ethical principles into a programming language? In any case, the bridge between theory andpractice has yet to be built and there are different ways in which that can happen. This underlines thenecessity of involving actors from different levels of the AI ecosystem (and neighboring ones) in order toensure that experts in policy-making work in tandem with experts in coding and engineering to create toolsand frameworks that are coherent and usable by all.2.2 Identifying Ethical Concerns of AI ApplicationsThere are several types of ethical concerns regarding AI applications and, in this paper, we will focus moreconcretely on bias leading to potential discrimination. While it is true that on the one hand, AI-infusedtechnology such as computer vision can enhance public security, for instance by identifying crime in real-time based on CCTV cameras, but the trade-off is that can also be abused to track individuals and to establisha surveillance state where privacy is greatly threatened by those who control the technology. On the militaryside, similar technology can be used to design autonomous drones which use computer vision to identitytheir target, representing a grave threat to global security and democracy due to the lack of human oversight.In addition to the security risk, such weapons would be moral and legal hazard: AI technology is not yetcapable to comprehend and represent the social and psychological context in which such a targeted attackcould take place in a manner that is coherent with international laws regarding war as well as with humanmorality.Unfortunately, the most common argument brought in favour of developing lethal autonomous weaponsis that they are needed as a precautionary measure (i.e. since other countries are undeniably working onthem, each country needs to do the same). In reality, the weapons needed to defend against killer droneswould be very different from the drones themselves, and do not need to be lethal autonomous weapons sincethey would be designed to destroy weapons rather than to target people, similar to the Iron Dome used byIsrael. Another common argument is that an international treaty would be useless since some countries willrefuse to sign it. But we have seen in the past that even when major powers do not sign a treaty (such asthe one on anti-personnel mines, signed by 133 countries, excluding the US, in 1997), the treaty can still beused to create a moral stigma, as well as a decline in demand; in the case of anti-personnel mines, the resulthas been that U.S. companies have stopped building them, even though their government never signed thetreaty. Another flawed argument is that regulating lethal autonomous weapons could threaten the innovationin AI, whereas in fact AI has been developed very successfully in a civilian setting (mostly in academia andmajor technology companies) and its continued development does not require neither data nor engineeringwhich would come from AI military development.Another potential threat to democracy stemming from AI could come not simply from the increasedability to monitor and to target individuals, but also from the more subtle power to influence them, e.g. viaAI-driven advertising, automated online trolls and other psychological manipulations via the internet andsocial media. The recent use of AI to influence political campaigns such as the 2016 US election or Brexitis just the beginning of what can be done when machines learn how to "press our buttons" in a personalizedway. This is due to the fact that micro-targeting makes it possible for ads to be truly bespoke depending4on your political views, network of friends and personal history. While we may not mind being influencedwhen it comes to choosing a brand of soft drinks, when the profit or power motives of a corporation orpolitical organization go against our individual and collective interests, it becomes important to establishsocial norms, laws and regulations to protect us from such psychological manipulation. But where shouldthe line be drawn between, for example, manipulation and education? These are difficult questions butthere are clues which can be used (like whether the organization that stands to profit is paying for theadvertisement or social network influence), so human judgement remains key for judging the ethical aspect,e.g. in balancing different values (like autonomy vs well-being, when considering an ad campaign againstcigarettes, for example). In the case of advertising, what is interesting is that in addition to the moral hazardassociated with psychological manipulation, it is not even clear that advertising is beneficial to society froma purely economic perspective, as it tends to favour established brands and thus slow down innovation.Closely related to the political misuse and manipulation with AI is also increasing concern about AI-generated false images, videos and news. Thanks to rapid progress in generative neural networks such asthe GANs [Goodfellow et al., 2014], it is becoming possible to synthesize images and sounds in a controlledway, e.g., using "deep fakes" for making a video of a president declaring war, or with the face of a celebrityseamlessly integrated on the body and behavior of a pornography actor. Other commonly discussed concernsof AI deployment include the effect on the job market [Perisic, 2018], which means that governments andcommunities must prepare, e.g. by adapting the education system and the social safety net, which cantake decades, as well as the potential concentration of power which it may lead to in specific individuals,corporations and countries, and the bias and discrimination it may contribute to increase, as we discuss next.2.2.1 Identifying and Mitigating BiasIn recent years, we have been confronted numerous times with the fact that biased algorithmic systems canperpetuate injustice and discrimination, whether we are aware of it or not. There are many different waysthat this kind of bias can creep into algorithms: it can be from the data itself, or the implicit bias that thecreator programmed into the system, and even the way the problem is framed3. Therefore, in order to ensurethat the models that we develop and the systems that they are later used in are as fair and ethical as possible,there are steps to take to identify bias and to reduce it as much as possible.Numerical BiasA major challenge in designing ML systems is understanding how they work during training and deploy-ment, and what factors and features they use to make decisions. However, diagnosing the presence of biasin these systems is not a straightforward task, since it is not always obvious during a model's constructionwhat the downstream impacts of design choices may be; therefore, upstream efforts are needed to reducethis risk as much as possible. To this end, there have been several proposals to help practitioners identifyand mitigate bias in ML models, some of which we will describe in the current section.More concretely, exploring, analyzing and visualizing the data used for training a model is a key partof the ML process. But it is not straightforward to identify bias simply by looking at the data; often, morein-depth probing is needed to figure out what features and implicit information is present and, once a modelis developed, how this will influence the model's behavior. For instance, it was recently found that theCOMPAS system, a criminal risk assessment tool developed widely used in the United States, is oftenbiased with respect to race [Angwin et al., 2016]. Whereas the bias in the COMPAS system was identifiedafter its deployment, once the data was made public, this bias is an aspect of the model that should have beenidentified much earlier, during development and certainly before deployment. Similarly, off-the-shelf facialrecognition technology used by police forces has been shown to perform much worse on racial and genderminorities, with a difference of up to 34.4% in error rate between lighter-skinned males and darker-skinnedfemales, mostly due to the lack of reliable training data [Buolamwini and Gebru, 2018].To address these types of issues, several approaches exist: for instance, researchers have recently re-leased a tool called `What-If', an open-source application that lets practitioners not only visualize theirdata, but also test the performance of their ML model in hypothetical situations, for instance modifying3For a more hands-on presentation of bias and fairness in AI, we suggest Google's Online course designed specifically for MLpractitioners5some characteristics of data points and analyzing subsequent model behavior, by measuring fairness met-rics such as Equal Opportunity and Demographic Parity [Wexler et al., 2019]. Other approaches addressbias by changing the training procedure or the structure of ML models themselves, for instance by trans-forming the raw data in a space in which discriminatory information cannot be found [Zemel et al., 2013]or using a variational autoencoder to learn the latent structure from the dataset and using this structure tore-weight the importance of specific data points during model training [Ribeiro et al., 2016]. Whatever theapproach chosen, using these kinds of tools during ML model development and deployment can change thelife of individual people, who could go from unfairly spending decades in prison to having the chance of abetter life - an immensely important difference when multiplied by the thousands of people whose lives canbe affected by the deployment of these tools. This multiplication of bias is especially important to considersince ML is being used more and more, and therefore even edge cases and small minorities can be amplifiedin real-world applications.Textual BiasBias is not always in numbers, it can also manifest itself in the words that we use to describe theworld around us. For instance, in 2018, Reuters reported that Amazon was forced to decommission anML-powered recruiting engine when it was discovered that it penalized any mention of female-relatedvocabulary, including applicants who attended all-women colleges [Dastin, 2018]. This is not surprisinggiven the gender disparity that exists in the technology sector and since the data used to develop this toolwas comprised of resumes submitted (and accepted) to Amazon over a 10-year period. It is nonethelessdisturbing in terms of algorithmic fairness, especially if algorithms such as this one make filtering or hiringdecisions that can ultimately affect an entire gender's lives and careers. This can potentially create a negativefeedback loop, as such a system would reduce the number of female workers and thus the number ofpositive role models for girls interested in technology. A similar type of gender bias was also found inpretrained word embedding models, which were found to exhibit gender stereotypes in terms of highercosine similarity between, for instance, `woman' and `homemaker' or `receptionist' as opposed to `woman'and `doctor' or `lawyer', notably due to these biases existing in the corpus that they were trained on, whichconsisted of mainstream news articles [Bolukbasi et al., 2016].In order to reduce and eventually remove gender bias in written text, researchers have proposed ap-proaches such as identifying the gender subspace of vectors and adjusting the dimensions in a way thateither neutralizes or entirely removes gender bias [Bolukbasi et al., 2016]. Others have defined a formalgender bias taxonomy in order to capture gender bias and to train ML models to later identify this bias intexts [Hitti et al., 2019]. Debiasing the computational representation of language, notably word embeddingmodels, is especially important because of the extent of their usage; pretrained embedding models trainedon corpora such as Google News and the Common Crawl are used in a variety of applications and systems,and can therefore continue perpetuating gender bias in downstream usages in Natural Language Processing(NLP) applications such as dialogue systems. This is a challenge given the complex and sub-symbolic na-ture of modern NLP, which makes it difficult to analyze specific features and aspects of data and identifylatent connections and bias between words and concepts. Therefore, more work is needed to explore andanalyse these issues, which constitutes an interesting research direction in itself, and one that is importantto pursue and to integrate into mainstream ML research.Despite the research initiatives described above to carve appropriate social norms about AI, there re-mains a noticeable gap between the recommendations they make and ways to ensure that these are respected.Legislation of AI is still catching up to the progress made in research and practice, and there have not yetbeen any country-level laws governing AI research specifically. However, there have been, on the one hand,more high-level legislative frameworks such as the European Union (EU) General Data Protection Regula-tion (GDPR), which aims to ensure data privacy and protection and, on the other hand, more local initiativessuch as San Francisco's Facial Recognition Software Ban. Nonetheless, more complete legal frameworksare needed to control nefarious use of AI and to ensure that the principles defined in theory are applied andenforced in practice.63 AI for Good InitiativesWhereas the profit motive is the main driver behind much of the commercial deployment of AI today, thereare nonetheless many projects going on in academia, government organizations, civil society and industrylabs motivated by more noble objectives, often called AI for Social Good (AISG) projects. In additionto the specific projects being undertaken in areas such as healthcare, education or the environment, it isinteresting to highlight higher-level efforts which aim to foster and facilitate these projects. For example,the AI Commons projec aims to construct a hub where different kinds of actors can connect and collaborateon AISG projects, e.g., ML graduate students or engineers, problem owners in NGOs or local governments,philanthropy organizations, or startups which could deploy the ML solutions. Their interaction is to befacilitated by online tools and datasets as well as a standardized description of the status, progress andexpected impact of each project. We hope that initiatives like this will help solidify and amplify the impactof AISG; in the meantime, there are also many profoundly positive uses of AI that are emerging and wewould like to highlight and applaud such efforts in the present section.3.1 AI in HealthcareAchieving universal health coverage is one of the 17 UN Sustainable Development Goals [UNHCR, 2017]and although major progress has been made in numerous domains, such as maternal health as well asHIV/AIDS reduction, there are still many problems that are far from being solved. While ML is not a cure-all, there are many challenges that it can help with such as personalized medicine, diagnosis of medicalimagery, and improved drug discovery [Ghassemi et al., 2018]. ML in the health sector is in fact a thrivingdomain of research, with its own workshops at major ML conferences and research published in majormedical journals read by practitioners worldwide. In the last five years alone, groundbreaking work hasbeen done in improving the diagnosis of diabetic retinopathy from a single visit [Arcadu et al., 2019],detecting breast cancer in lymph nodes [Golden, 2017] and large-scale discovery of diseases based on healthrecords [Pivovarov et al., 2015]. There is also an increasing number of startups and companies working inthe space, either by commercializing research done in academia or by developing products specificallycatered to the medical sector, with the most advanced applications harnessing the power of deep learningfor analyzing and classifying medical imagery.Despite the many exciting advances that are being made, there are many hurdles in ML research inhealthcare, starting from data privacy and control (who owns the data? Can patients share their own data, orshould the process be centralized? How to find the right balance between privacy and the lives which willbe saved by applying ML on the aggregated health records from many different sources?), to the mannerin which medical data should be processed (Should it contain information such as race and postal/zip code,which can impact diagnoses, be included in electronic heath records, or does that open the door to discrim-ination and bias?) and how should such systems be deployed (human-in-the-loop or fully automated?)4.There are also often questions of responsibility and interpretability that arise, given the high stakes of de-ploying ML systems in situations of life and death. In order to make meaningful progress in this sector, it istherefore important to continue existing research on fair and ethical usage of ML in healthcare [Wiens et al.,2019] and to ensure that Hippocratic principles are a solid part of the research and development process,as well as working with stakeholders of the domain (e.g. radiologists, clinicians, patient organizations andhospital administrators) to propose solutions to the hurdles proposed above.3.2 AI for EducationThe promise of using adaptive intelligent systems and agents for education has been around since the1960s [Suppes and Morningstar, 1969], but access to personalized digital education tools has yet to be-come a reality in most countries, especially in the developing world, where it could have the most impactto democratize education and knowledge [Nye, 2015]. In recent years, given the increasing global shortageof qualified teachers along with the increasing number of students, the issue of access to education hasbecome a global one, a fact highlighted by its presence among the UN SDGs. And yet, the usage of MLin the education sector has been limited to specific, narrow applications such as predicting the probability4For a more extensive overview of the opportunities and challenges of using ML in healthcare, see Ghassemi et al. [2018]7of learner attrition [Chaplot et al., 2015] or improving learner evaluation [Abbott, 2006]. There are manyreasons for this, starting from the difficulty in representing learning content in a domain-agnostic way tofacilitate scalability, to overcoming cultural and linguistic barriers to deploying tutors worldwide, but alsomore fundamental issues such as the lack of large-scale educational datasets and the inherent technologicalconstraints in developing countries.Despite these hurdles, there are many new and longstanding efforts to create intelligent tutors, be it usingsymbolic AI approaches such as ontologies and knowledge modeling [Nkambou et al., 2010], educationaldata mining [Dutt et al., 2017] or, more recently, ML-driven approaches [Conati et al., 2018]. However,there are very high stakes in the field, since technological interventions have the potential to make consider-able, long-term impact on human livelihoods, for example lifting people out of poverty by endowing themwith linguistic and numerical literacy, but these can be hindered by bias and technological constraints. Wetherefore agree with recent proposals to improve and support human learning at scale and believe that MLhas a key role to play in this endeavour. This can be done, for instance, by partnering up with existingeducation initiatives and organizations in order to learn what their specific needs are and how ML can beused to meet them, or else by collaborating with Massive Open Online Course (MOOC) creators in orderto gather data and make it available to the ML community, and finally by sharing learning materials andactivities used in local education initiatives (e.g. university courses in Machine Learning) so that they profitlearners in places where access to high-quality technical education is limited.3.3 AI for the EnvironmentClimate change is, without a doubt, one of the biggest challenges that humanity has faced, and we are atan important point in history when we are both aware of the issue and still have the possibility to changeits course. Climate change has been described as a `wicked' problem, due to features such as the difficultyin defining the problem itself and in developing and deploying solutions to it, the lack of central authoritythat can solve it, the incentives for individual countries or companies to not do their share, and the cognitivebiases that discount the future impacts of our actions [Head et al., 2008, Levin et al., 2012]. Furthermore,while we do not know of any single technological silver bullet as solution to climate change, there arenonetheless numerous technical challenges for which ML can be helpful, and which can be combined tomake a significant impact on the overall issue. These challenges and the ongoing ML approaches to tacklethem were presented in a recent survey paper [Rolnick et al., 2019]. We will not go into all of these atlength, but we will focus on a few examples that are particularly salient and that we hope will give an ideaof both the relevance of deploying ML in environmental applications and the opportunities that this cangenerate.Energy and TransportationTogether, electricity and transportation systems are estimated to produce close to half of anthropogenicgreenhouse gas (GHG) emissions [Allen et al., 2019] and both sectors have their own unique challenges fordecarbonization. For instance, one of the major obstacles to building and using renewable energy sourcessuch as solar and wind is the variability of their output, which is inherently problematic since the powergenerated by an energy grid must equal the power used by its consumers at any given moment. Currently,this means that despite the existence of solar panels and wind turbines, these must be complemented bycontrollable but highly polluting energy sources such as coal and natural gas plants. ML methods that areappropriate for time-series predictions, such as Recurrent Neural Networks are particularly suited for thesetypes of tasks [Voyant et al., 2017] and can dramatically lower the barrier to entry for renewable energyglobally. Furthermore, even in cases where controllable energy sources are used, demand on the energygrid will still fluctuate based on usage; in this case, ML techniques such as Reinforcement Learning andDynamic Scheduling can be used to balance the grid in real time [Vazquez-Canteli and Nagy, 2019].In transportation, reducing activity is a key part in reducing GHG emissions; however, given the highlyregional nature of transportation methods (i.e. high-speed trains are only an option in Europe, whereasmany major US cities have limited public transportation), custom solutions are needed to make a significantimpact. ML can be of particular help in estimating and predicting vehicle flow to minimize it, for exampleby helping to optimize the design of new roads and hubs [Sommer et al., 2017] and monitoring traffic [Kaack8et al., 2019], as well as estimating carbon emissions in real-time [Nocera et al., 2018]. ML can also be usedfor designing more energy-efficient batteries [Hoffmann et al., 2019] which will become an increasinglyimportant concern as more people switch to electric vehicles. In both cases of energy and transportation,ML can be used to make systems more efficient and to improve predictions of complex phenomena basedon large amounts of data; nonetheless, it remains only one part of the solution, and as tempting as it is to haltresearch projects once a theoretically plausible solution has been found (and a research paper published),what is key here is working with domain experts to bring projects towards deployment, where concreteimpact can be made. Transversal connections between disciplines are therefore key, and must be establishedand fostered for projects to flourish.Individuals and SocietiesWhile changes in our climate can be abstract, quantified in degrees of warming or tons of CO2, climatechange will also have very concrete impacts on society, for instance by decreasing crop yield, increasingthe frequency of extreme weather events such as hurricanes and storms, and impacting biodiversity. Thereare a myriad of ways in which ML can help face these, whether it be by analyzing real-time images andrecordings of ecosystems to detect species [Duhart et al., 2018] and deforestation [McDowell et al., 2015],improving disaster preparation and response by generating real-time maps from satellite imagery [Voigtet al., 2007] and even setting an optimal price on carbon to accelerate the transition to a low-carbon energyeconomy [Wei et al., 2018]. Finally, while we are far from being able to predict the exact impact thatincreasing the carbon tax will have on the different levels of society and industry (i.e. federal and regionalgovernments, local and international companies, and individuals), this is a worthwhile area of research andexploration, with potentially huge consequences in helping political leaders make more informed choicesin addressing the climate crisis. It is therefore useful to continue gathering data and building trust betweenmembers of the political ecosystem and ML practitioners to learn from each other and to facilitate thedeployment of technological solutions in setting government policies.On an individual level, there are many reasons why individuals cannot, or will not, act on climate change,either common misconceptions regarding the fact that individuals cannot make meaningful impact on aglobal problem, or cognitive biases that increase an individual's psychological distance to climate change.In the first case, ML-infused tools to estimate the carbon footprint of individuals and households [Jones andKammen, 2011] and to model individual behavior with regards to sustainable lifestyle choices and technolo-gies [Carr-Cornish et al., 2011] can be very useful if they are sufficiently accurate and deployed on a largescale. Finally, minimizing psychological distance to the future effects of climate change is a promising wayto reduce cognitive bias - in this regard, it is possible to use images generated using Generative AdversarialNetworks (GANs) which represent the impacts of extreme events on locations that have personal value tothe viewer [Schmidt et al., 2019]. A crucial part of developing ML tools for individuals is, once again,working with multidisciplinary experts in psychology, scientific communication, and user design to ensurethat the tools created reach the largest possible audience and maximize their positive impact.</body>
	<conclusion>Technology in general, and ML more specifically, carries a great potential for change and disruption. Whileneither of these is guaranteed to make the world a better place, this potential can most definitely be usedto have a positive impact on the world. In the present article, we have illustrated some inspiring projectsthat aim to make the world a better place and by using the powerful techniques and approaches that ML hasbrought forward. We believe that as ML researchers and practitioners, we have the responsibility to leverageour (super)powers to contribute to these efforts. This can be done by connecting with established actors fromindustry and policy or experts from other relevant disciplines, by learning from their past experiences, andby working together to propose innovative solutions to major problems, deployed in places where they willhave a positive impact.We live in a world with many global and local challenges and issues that are in constant evolution, andit is easy to be overwhelmed by this flux of information and focus on a small sandbox in which we feelsafe and in control, in order to develop and study the aspects of ML that interest us most. But it is naive9to believe that our sandbox is an isolated isle that is not connected to the rest of the world - since even inthe case of theoretical work, communication and cross-pollination are unavoidable - and each of us is alsoa citizen who is concerned collective debates, while many of us could worry about the world in which ourdescendants will live. We believe that there are thought processes that should take place in the head of everyML practitioner regarding the nature of the work they are doing and the potential pitfalls and impacts ofthis work in the world around them, some of which we have listed in the first part of the current paper. Andwhile we do not claim to have all the answers to all of these tough questions, we hope that we can start aconversation that will accompany ML research and practice throughout its infancy towards its tumultuousteenage years in the coming decades, and eventually towards mature adulthood beyond that.</conclusion>
	<discussion>N/A</discussion>
	<biblio>Robert G Abbott. Automated expert modeling for automated student evaluation. In International Confer-ence on Intelligent Tutoring Systems, pages 1-10. Springer, 2006.M Allen, P Antwi-Agyei, F Aragon-Durand, M Babiker, P Bertoldi, M Bind, S Brown, M Buckeridge,I Camilloni, A Cartwright, et al. Technical summary: Global warming of 1.5c. an ipcc special reporton the impacts of global warming of 1.5c above pre-industrial levels and related global greenhouse gasemission pathways, in the context of strengthening the global response to the threat of climate change,sustainable development, and efforts to eradicate poverty, 2019.Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias, propub-lica. https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, 2016. Accessed: 2019-11-25.Filippo Arcadu, Fethallah Benmansour, Andreas Maunz, Jeff Willis, Zdenka Haskova, and Marco Prunotto.Deep learning algorithm predicts diabetic retinopathy progression in individual patients. NPJ digitalmedicine, 2(1):1-9, 2019.UN General Assembly. Universal declaration of human rights. UN General Assembly, 302(2), 1948.Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is tocomputer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neuralinformation processing systems, pages 4349-4357, 2016.Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gen-der classification. In Conference on fairness, accountability and transparency, pages 77-91, 2018.Simone Carr-Cornish, Peta Ashworth, John Gardner, and Stephen J Fraser. Exploring the orientationswhich characterise the likely public acceptance of low emission energy technologies. Climatic change,107(3-4):549-565, 2011.Devendra Singh Chaplot, Eunhee Rhim, and Jihie Kim. Predicting student attrition in moocs using senti-ment analysis and neural networks. In AIED Workshops, volume 53, pages 54-57, 2015.Cristina Conati, Kaska Porayska-Pomsta, and Manolis Mavrikis. Ai in education needs interpretable ma-chine learning: Lessons from open learner modelling. arXiv preprint arXiv:1807.00154, 2018.Jeffrey Dastin. Amazon scraps secret ai recruiting tool that showed bias against women, reuters businessnews. https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G, 2018. Accessed: 2019-11-25.Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255.Ieee, 2009.Clement Duhart, Gershon Dublon, Brian Mayton, and Joseph Paradiso. Deep learning locally trainedwildlife sensing in real acoustic wetland environment. In International Symposium on Signal Processingand Intelligent Recognition Systems, pages 3-14. Springer, 2018.10Ashish Dutt, Maizatul Akmar Ismail, and Tutut Herawan. A systematic review on educational data mining.IEEE Access, 5:15991-16005, 2017.Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew L Beam, and Rajesh Ranganath. Opportu-nities in machine learning for healthcare. arXiv preprint arXiv:1806.00388, 2018.Jeffrey Alan Golden. Deep learning algorithms for detection of lymph node metastases from breast cancer:helping artificial intelligence be seen. Jama, 318(22):2184-2186, 2017.Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processingsystems, pages 2672-2680, 2014.Don W Gotterbarn, Amy Bruckman, Catherine Flick, Keith Miller, and Marty J Wolf. Acm code of ethics:a guide for positive action, 2018.Brian W Head et al. Wicked problems in public policy. Public policy, 3(2):101, 2008.Yasmeen Hitti, Eunbee Jang, Ines Moreno, and Carolyne Pelletier. Proposed taxonomy for gender bias intext; a filtering methodology for the gender generalization subtype. In Proceedings of the First Workshopon Gender Bias in Natural Language Processing, pages 8-17, 2019.Jordan Hoffmann, Louis Maestrati, Yoshihide Sawada, Jian Tang, Jean Michel Sellier, and Yoshua Bengio.Data-driven approach to encoding and decoding 3-d crystal structures. arXiv preprint arXiv:1909.00949,2019.IEEE. Ieee standard review -- ethically aligned design: A vision for prioritizing human wellbeing withartificial intelligence and autonomous systems. In 2017 IEEE Canada International Humanitarian Tech-nology Conference (IHTC), pages 197-201. IEEE, 2017.Anna Jobin, Marcello Ienca, and Effy Vayena. Artificial intelligence: the global landscape of ethics guide-lines. arXiv preprint arXiv:1906.11668, 2019.Christopher M Jones and Daniel M Kammen. Quantifying carbon footprint reduction opportunities for ushouseholds and communities. Environmental science &amp; technology, 45(9):4088-4095, 2011.Lynn H Kaack, George H Chen, and M Granger Morgan. Truck traffic monitoring with satellite images. InProceedings of the Conference on Computing &amp; Sustainable Societies, pages 155-164. ACM, 2019.Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied todocument recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.Kelly Levin, Benjamin Cashore, Steven Bernstein, and Graeme Auld. Overcoming the tragedy of superwicked problems: constraining our future selves to ameliorate global climate change. Policy sciences, 45(2):123-152, 2012.Nate G McDowell, Nicholas C Coops, Pieter SA Beck, Jeffrey Q Chambers, Chandana Gangodagamage,Jeffrey A Hicke, Cho-ying Huang, Robert Kennedy, Dan J Krofcheck, Marcy Litvak, et al. Globalsatellite monitoring of climate-induced vegetation disturbances. Trends in plant science, 20(2):114-123,2015.Roger Nkambou, Riichiro Mizoguchi, and Jacqueline Bourdeau. Advances in intelligent tutoring systems,volume 308. Springer Science &amp; Business Media, 2010.Silvio Nocera, Cayetano Ruiz-Alarcon-Quintero, and Federico Cavallaro. Assessing carbon emissions fromroad transport through traffic flow estimators. Transportation Research Part C: Emerging Technologies,95:125-148, 2018.Benjamin D Nye. Intelligent tutoring systems by and for the developing world: A review of trends andapproaches for educational technology in a global context. International Journal of Artificial Intelligencein Education, 25(2):177-203, 2015.11I Perisic. How artificial intelligence is shaking up the job market. https://www.weforum.org/agenda/2018/09/artificial-intelligence-shaking-up-job-market/, 2018. Ac-cessed: 2019-11-25.Rimma Pivovarov, Adler J Perotte, Edouard Grave, John Angiolillo, Chris H Wiggins, and Noemie Elhadad.Learning probabilistic phenotypes from heterogeneous ehr data. Journal of biomedical informatics, 58:156-165, 2015.Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the predic-tions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledgediscovery and data mining, pages 1135-1144. ACM, 2016.David Rolnick, Priya L Donti, Lynn H Kaack, Kelly Kochanski, Alexandre Lacoste, Kris Sankaran, An-drew Slavin Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, Alexandra Luc-cioni, et al. Tackling climate change with machine learning. arXiv preprint arXiv:1906.05433, 2019.RSS. A guide for ethical data science. https://www.actuaries.org.uk/system/files/field/document/An%20Ethical%20Charter%20for%20Date%20Science%20WEB%20FINAL.PDF., 2019. Accessed: 2019-11-25.Victor Schmidt, Alexandra Luccioni, S Karthik Mukkavilli, Narmada Balasooriya, Kris Sankaran, JenniferChayes, and Yoshua Bengio. Visualizing the consequences of climate change using cycle-consistentadversarial networks. arXiv preprint arXiv:1905.03709, 2019.Lars Wilko Sommer, Tobias Schuchert, and Jurgen Beyerer. Fast deep vehicle detection in aerial images.In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 311-319. IEEE,2017.Patrick Suppes and Mona Morningstar. Computer-assisted instruction. Science, 166(3903):343-350, 1969.M Szczepanski. Economic impacts of artificial intelligence (ai), european parliamentary research ser-vice, pe 637.967. http://europarl.europa.eu/RegData/etudes/BRIE/2019/637967/EPRS_BRI(2019)637967_EN.pdf, 2019. Accessed: 2019-11-25.UNHCR. The sustainable development goals and addressing statelessness. https://www.refworld.org/docid/58b6e3364.html, 2017. Accessed: 2019-11-25.Jose R Vazquez-Canteli and Zoltan Nagy. Reinforcement learning for demand response: A review ofalgorithms and modeling techniques. Applied energy, 235:1072-1089, 2019.Stefan Voigt, Thomas Kemper, Torsten Riedlinger, Ralph Kiefl, Klaas Scholte, and Harald Mehl. Satelliteimage analysis for disaster and crisis-management support. IEEE transactions on geoscience and remotesensing, 45(6):1520-1528, 2007.Cyril Voyant, Gilles Notton, Soteris Kalogirou, Marie-Laure Nivet, Christophe Paoli, Fabrice Motte, andAlexis Fouilloy. Machine learning methods for solar radiation forecasting: A review. Renewable Energy,105:569-582, 2017.Sun Wei, Zhang Chongchong, and Sun Cuiping. Carbon pricing prediction based on wavelet transformand k-elm optimized by bat optimization algorithm in china ets: the case of shanghai and hubei carbonmarkets. Carbon Management, 9(6):605-617, 2018.James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viegas, and Jimbo Wil-son. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualiza-tion and computer graphics, 26(1):56-65, 2019.Jenna Wiens, Suchi Saria, Mark Sendak, Marzyeh Ghassemi, Vincent X Liu, Finale Doshi-Velez, KennethJung, Katherine Heller, David Kale, Mohammed Saeed, et al. Do no harm: a roadmap for responsiblemachine learning for health care. Nature medicine, 25(9):1337-1340, 2019.Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations. InInternational Conference on Machine Learning, pages 325-333, 2013.12</biblio>
</article>