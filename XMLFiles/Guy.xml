<article>
	<preamble>Guy.pdf</preamble>
	<title>Skill2vec: Machine Learning Approach for Determining the Relevant Skills from Job Description</title>
	<auteurs>
		<auteur>
			<name>Le Van-Duyet</name>
			<mail>me@duyet.net</mail>
      </auteur>
		<auteur>
			<name>Vo Minh Quan</name>
			<mail>95.vominhquan@gmail.com</mail>
      </auteur>
		<auteur>
			<name>Dang Quang An</name>
			<mail>an.dang1390@gmail.com</mail>
      </auteur>
	</auteurs>
	<abstract>
	Abstract-- Unsupervise learned word embeddings have seen tremendous success in numerous Natural Language Processing (NLP) tasks in recent years. The main contribution of this paper is to develop a technique called Skill2vec, which applies machine learning techniques in recruitment to enhance the search strategy to find candidates possessing the appropriate skills. Skill2vec is a neural network architecture inspired by Word2vec, developed by Mikolov et al. in 2013. It transforms skills to new vector space, which has the characteristics of calculation and presents skills relationships. We conducted an experiment evaluation manually by a recruitment company's domain experts to demonstrate the effectiveness of our approach.
	</abstract>
	<introduction>
	Recruiters in information technology domain have met theproblem finding appropriate candidates by their skills. In theresume, the candidate may describe one skill in differentways or skills could be replaced by others. The recruitersmay not have the domain knowledge to know if one's skillsare fit or not, so they can only find ones with matched skills.In order to cope with the problem, one should try tofind the relatedness of skills. There are some approaches:building a dictionary manually, ontology approach, naturallanguage processing methods, etc. In this study, we apply aword embedding method Word2Vec, using skills from onlinejob post descriptions. We treat skills as terms, job posts asdocuments and find the relatedness of these skills.
	</introduction>
	<corps>
	II. RELATED WORKTo find relatedness of skills, Simon Hughes [4] from Diceproposed an approach using Latent Semantic Analysis withan assumption that skills are related to skills which occurin the same context, and here contexts are job posts. Thisapproach will build a term-document matrix and use SingularValue Decomposition to reduce the dimensionality. The consof this approach is that when we have new data coming, wecan not update the old term-document matrix, this leads todifficulties in maintaining the model, as the change of trendin this domain is high.Google's Data Scientists also face the same problemsin Cloud Jobs API [7]. Their solution is to build a skillontology defining around 50,000 skills in 30 job domain withrelationships such as is a, related to, etc. This approach canrepresent complicate relationships between skills and jobs,but building such an ontology costs so much time and effort.To overcome the problem of relevant term, [3] present anew, effective log-based approach to relevant term extractionand term suggestion.The goal of [2] is to develop an automated system thatdiscovers additional names for an entity given just one ofits names, using Latent semantic analysis (LSA) [1]. In theexample of authors, the city in India referred to as Bombayin some documents may be referred to as Mumbai in othersbecause its name officially changed from the former to thelatter in 1995.[5] is the introduction of an ontology-based skills man-agement system at Swiss Life and the lessons learned fromthe utilisation of the methodology, present a methodologyfor application-driven development of ontologies that isinstantiated by a case study.III. WORD2VEC ARCHITECTUREWord2Vec is a group of models proposed by Mikolov et alin 2013 [6]. It consists of 2 models: continuous bag-of-wordsand continuous Skip-gram, both are shallow neural networksthat try to learn distributed representations of words withthe target is to maximize the accuracy while minimizing thecomputational complexity. In the continuous bag-of-wordsarchitecture, the model predicts the current word from awindow of surrounding context words. On the other hand,Skip-gram model try to predict surrounding context wordsbased on the current word. In this work, we focus on Skip-gram model as it is known to be better with infrequent wordsand it also give slightly better result in our experiment.The model consists of three layers: input layer, one hiddenlayer and output layer. The input layer take a word encodedusing 1-of-V encoding (also known as one-hot encoding),where V is size of the dictionary. The word is then fedthrough the linear hidden layer to the output layer, whichis a softmax classifier. The output layer will try to predictwords within window size before and after current word.Using stochastic gradient descent and back propagation, wetrain the model until it converges.This model takes vector dimensionality and window sizeas parameters. The author found that increasing the windowsize improves the quality of the word vector, and yet itincreases the computational complexity.IV. METHODOLOGYA. Data collecting and processingChoosing a universal data set for the model is extremelyimportant, the data should be large enough and should havebalanced distributions over words (i.e. skills).arXiv:1707.09751v3 [cs.CL] 9 Oct 2019There are two dataset we need to concern, one (1) is thestandard skills dictionary for the parser and another (2) isskills for training model; follow the figure 1.Careerwebsites(1) Standard skillscollectcleaningCareerwebsitesJob descriptionscollect(*) parser(2) TrainingdataFig. 1. Pipeline of data collecting and processingFirst, we collected and prepared a large dictionary ofskills. With this dictionary, we can extract a set of skillsfrom raw job descriptions. Skillss need to be cleaned intounique skills because there are many way to present a skill injob description (i.e. OOP or Object-oriented programming).Figure 2 briefly depicts the concept of the cleaning process.Rawskill(1) Removepunctuation(2) Dictionaryreplace(3) Regularexpression(4) Lemmatization,StemmingCleanedskillFig. 2. Pipeline cleaning skillsAfter that, we had the dictionary of skills ready forparsing. We collected a huge number of job descriptionsfrom Dice.com - one of the most popular career websiteabout Tech jobs in USA. From these job descriptions, weextract skills for each one by using our skills dictionary(1). Now, the dataset is presented by a list of collectionsof skills based on job descriptions. After crawling, we gota total of 5GB with more than 1,400,000 job descriptions.From these data, we extracted skills and performed as alist of skills in the same context, the context here includesskills in the same job description. The dataset is publishedat https://github.com/duyetdev/skill2vec-datasetThe data structure is shown in table I.TABLE IDATA STRUCTUREJob description Context skillsJD1 Java, Spark, Hadoop, PythonJD2 Python, HiveJD3 Python, Flask, SQL* * * * * *B. Skill2vec architectureIn this paper, for training the dataset, we used a neuralnetwork inspired by Word2Vec model as mentioned above.Here we treated our skills as words in Word2Vec model.In this study, with the documents contain only the skills,we chose the maximum window size, implied that everyskills in the same job description are related to each other.For the vector dimensions, after some point, adding moredimensions provides diminishing improvements, so we chosethis parameter empirically. To honour the work of Word2Vecmodel as it holds a big part in our study, we name our modelSkill2Vec. Figure 3 briefly describes our Skill2Vec model.Fig. 3. Skill2vec architectureV. EXPERIMENTAL SETUPTo evaluate our method, we have an expert team assessesthe result following these steps:1) Pick 200 skills randomly from our dictionary.2) Our system will return top 5 "nearest" skills for each.3) The expert team will check if these top 5 "nearest"skills are relevant or not.The experiment showed that 78% of skills returned byour model is truly relevant to the input skill. We present theexperimental results in table IITABLE IIQUERY TOP 5 RELEVANT SKILLSQuery skill Top relevant skillsHTML5css3bootstrapfront endangularresponsiveOOPOODObjectiveJavaMultithreadSoftware DebugHadoopPigHiveHBaseBig DataSparkScalaZookeeperSparkData SystemSqoopsolrcloudHivePigHDFSHadoopSparkImpala</corps>
	<conclusion>
	In this paper, we developed a relationship network betweenskills in recruitment domain by using the neural net inspiredby Word2vec model. We observed that it is possible totrain high quality word vectors using very simple modelarchitectures due to lower cost of computation. Moreover,it is possible to compute very accurate high dimensionalword vectors from a much larger dataset. Using Skip-gramarchitecture and an advanced technique for preprocessingdata, the result seems to be impressive. The result of ourwork can contribute to building the matching system betweencandidates and job post. In the other hand, candidates canfind the gap between the job post requirements and theirability, so they can find the suitable trainings.A direction we can follow in the future: adding domainin training model, for example: Between Python, Java, andR, in Data Science domain, Python and R are more relevantthan Java, however in Back End domain, Python and Javaare more relevant than R.</conclusion>
	<discussion>
	</discussion>
	<biblio>
	[1] Michael W Berry and Ricardo D Fierro. "Low-rankorthogonal decompositions for information retrieval ap-plications". In: Numerical linear algebra with applica-tions 3.4 (1996), pp. 301-327.[2] Vinay Bhat et al. "Finding aliases on the web usinglatent semantic analysis". In: Data & Knowledge Engi-neering 49.2 (2004), pp. 129-143.[3] Chien-Kang Huang, Lee-Feng Chien, and Yen-JenOyang. "Relevant term suggestion in interactive websearch based on contextual information in query sessionlogs". In: Journal of the Association for InformationScience and Technology 54.7 (2003), pp. 638-649.[4] Simon Hughes. How We Data-Mine Related Tech Skills.2015. URL: http : / / insights . dice . com /2015/03/16/how-we-data-mine-related-tech-skills/ (visited on 09/12/2017).[5] Thorsten Lau and York Sure. "Introducing ontology-based skills management at a large insurance com-pany". In: Proceedings of the Modellierung. 2002,pp. 123-134.[6] Tomas Mikolov et al. "Efficient Estimation ofWord Representations in Vector Space". In: CoRRabs/1301.3781 (2013). URL: http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781.[7] Christian Posse. Cloud Jobs API: machine learninggoes to work on job search and discovery. 2016. URL:https : / / cloud . google . com / blog/big -data/2016/11/cloud-jobs-api-machine-learning-goes-to-work-on-job-search-and-discovery (visited on 03/03/2018).
	</biblio>
</article>