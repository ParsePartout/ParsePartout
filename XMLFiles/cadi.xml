<article>
	<preamble>cadi.pdf</preamble>
	<titre>CADI: Contextual Anomaly Detection using an Isolation Forest</titre>
	<auteurs>
		<auteur>
			<name>Veronne Yepmo</name>
			<mail>veronne.yepmo-tchaghe@irisa.fr</mail>
            <affiliation>N/A</affiliation>
      </auteur>
		<auteur>
			<name>Universite de Rennes</name>
			<mail>gregory.smits@imt-atlantique.fr</mail>
            <affiliation>N/A</affiliation>
      </auteur>
		<auteur>
			<name>Symposium On Applied Computing</name>
			<mail>marie-jeanne.lesot@lip6.fr</mail>
            <affiliation>N/A</affiliation>
      </auteur>
		<auteur>
			<name>Gregory Smits</name>
			<mail>permissions@acm.org.</mail>
            <affiliation>N/A</affiliation>
      </auteur>
		<auteur>
			<name>Marie-Jeanne Lesot</name>
			<mail>olivier.pivert@irisa.fr</mail>
            <affiliation>N/A</affiliation>
      </auteur>
	</auteurs>
	<abstract>Reconstructing the data inner structure and identifying abnormal points are two major tasks in many data analysis processes. A step beyond the decomposition of a data set as inliers and outliers, that then may be interpreted as anomalies, is to distinguish local from global outliers. This paper introduces a unified approach based on a revised version of an isolation forest that allows for both the reconstruction of dense regions of points, the identification of anomalies and the generation of contextual explanations about the abnormality of these points. To make the anomaly detection more informative and reliable, anomalies are compared to the reconstructed partition of the inliers so as to explain why they are considered abnormal and from which local generation mechanism they could originate from. Relying on a common data property, namely the isolation of anomalies from dense groups of regularities, eases the understanding of the data set structure and makes the provided explanations more informative than those provided by two independent mechanisms, one for clustering and one for detecting anomalies. Conducted experimentations show the relevance of the structural knowledge extracted from the proposed isolation forest and the effectiveness and robustness of the approach thanks to the unified isolation-based data model to analyse different facets of the data.</abstract>
	<introduction>Detecting points that significantly deviate from the rest of thedata set is a crucial issue in many applicative contexts as theseoutliers may correspond to anomalies, frauds, attacks or suspiciousbehaviors. Many machine learning techniques are dedicated tothis task of separating outliers from the other points consideredas regularities (see e.g. [6, 33]). Most of these anomaly detectionapproaches focus on calculating an anomaly score for each point toquantify the extent to which the point globally deviates from therest of the data set. Other methods quantify how much the pointdeviates from its neighborhood. However, anomaly detectors areoften embedded into decision-aid systems where their end usersrequire explanations in addition to these scores to be convincedthat an action has to be taken to deal with these anomalies. A wayto increase the trust in the automatic mechanism that has built thisseparation of the data into two unbalanced classes (regular pointsvs. anomalies), is to provide end users with explanations about thereasons why a point is tagged as an anomaly [37].Many anomaly detection algorithms acknowledge the existenceof local anomalies, which deviate only from a subset of the dataset (red circles on Fig. 1), as opposed to global anomalies whichdeviate from all the other instances in the data set (red square onFig. 1). However, the distinction between local and global anomaliesis often forgotten in the outputs of anomaly detectors, and duringthe explanation process. On the data set in Figure 1, even if thedetector is able to flag all red instances as anomalies, no existinganomaly explanation method will intrinsically capture that 1 isan anomaly for the blue cluster because of its value on 1. Thisexplanation can be produced only if some knowledge about thegroups of regularities in the data set is available, and that knowledgeis leveraged.The COIN strategy introduced in [25] is an anomaly explanationmethod that makes a step towards the extraction of these so-calledcontextual explanations. The method identifies, for each outlier toexplain, its nearest groups of inliers and the outlying attributes wrt.that neighborhood. However, COIN relies on an external anomalydetector and an external clustering algorithm. It combines differentmachine learning approaches to detect anomalies first, second tolocally analyze the data distribution, and then to generate datastructure aware explanations about each found anomaly. With sucha pipeline, the accuracy of each component depends on specificSAC'24, April 8 -April 12, 2024, Avila, Spain Veronne Yepmo, Gregory Smits, Marie-Jeanne Lesot, and Olivier PivertFigure 1: Illustrative examplehyper-parameters that have to be tuned and often on distancesthat have to be carefully selected as well. The work proposed inthis paper suggests the encapsulation of these different steps in aunified approach.The main contribution of this work is to introduce a unifiedapproach leveraging an isolation-based data model both to identifyand explain local anomalies. This approach called CADI whichstands for Contextual Anomaly Detection using an Isolation for-est, capitalizes on the advantages of the initial Isolation Forest (IF)method introduced in [23], namely to be accurate and interpretablewith only few hyper-parameters to set and no distance measureto choose. To go beyond the detection of anomalies, a density con-straint is applied on the randomly generated separation lines toensure that they do not split dense regions whose identification is aprerequisite to the reconstruction of the data inner structure. Withthe identification of anomalies and the reconstruction of the datainner structure, each anomaly is then explained wrt. the identifiedclusters of regular data points, still using the knowledge embeddedin the IF.The rest of the paper is organized as follows. Section 2 exploresthe existing literature on the different components of CADI. Sec-tion 3 presents the approach whose relevance is then confirmed byexperimentations conducted in Section 4.</introduction>
	<body>2 RELATED WORKCADI aims at associating each detected anomaly with informativedescriptions about the strength of its abnormality and the reasonsfor its classification as an anomaly according to its neighborhood.This section thus positions CADI wrt. existing approaches in thefields of anomaly detection and, in particular, anomaly explanation.2.1 Anomaly DetectionMany machine learning strategies have been proposed to addressspecifically the issue of identifying anomalies from a data set [6].The unsupervised case is the most attractive one because of theunpredictability of anomalies and the difficulty of labeling data sets.Considering that regular observations are largely predominant in adata set, deep learning-based strategies identify as anomalies thosepoints that do not fit the concise representation of the trainingdata [38]. Local Outlier Factor (LOF) [4], One-Class Support VectorAlgorithm 1 Isolation Forest : _ [23]1: Inputs: a sample   D, the depth  of the current node; = 0 during the first call of the method2: Output: a node in an isolation tree3: if || = 1 or  &amp;gt; then4: Return (,, ,,,)  Leaf (terminalnode)5: else6:   (A)  Random attribute selection7:   ([min  ., max  .])  Randomvalue selection8:  {  /. &amp;lt; }9:  {  /.  }10: Return (_(, + 1),  Internal node11: _(, + 1), ,,, )12: end ifMachines [2] and IF [23] are also among the most popular unsuper-vised methods. They leverage the fact that anomalies are located inlow density subspaces and easily separable from the other pointsto identify them. The Isolation Forest algorithm is very appealingfor anomaly detection because it is fast, interpretable at the treelevel and makes no assumption regarding the distribution of thedata set. Plus, the efficiency and the robustness of the approachagainst the choice of the hyper-parameters have been confirmedthroughout the years by different benchmarks [7, 16]. As CADIrelies on a revisited version of an IF, a focus is now made on thisparticular technique.IF [23] identifies so called global anomalies corresponding topoints that can be easily separated from the rest of the data. It is anensemble-based algorithm as a forest is composed of  trees builton  randomly drawn subsets of the data set D. As summarizedin Algorithm 1, a tree stems from recursive splits of a data subset on randomly chosen attributes and randomly chosen values in the range of values observed in  for each chosen attribute .The points with a value lower than  on attribute  are transferredto the left child of the current node, and the others to the rightchild. This separation process is repeated recursively until one ofthe following two conditions is met:* the node is no longer separable (it contains a single point) ;* the depth limit, a predefined hyper-parameter of the method,is reached.The algorithm depends on three hyper-parameters: the numberof trees in the forest , the sample size  and the depth limit of atree. A node is formally defined by a sextuplet (, , ,,, ),where  and  are pointers to its left and right nodes respec-tively,   D,  N is its depth in the tree,   A and   ().A denotes the set of all attributes and for each   A, ()denotes its domain. In Algorithm 1, the method ( _,_, , , , ) returns a new node.Once the forest built, each point to evaluate is propagated to theleaves of each tree in the forest and an anomaly score, function ofthe average depth of the node containing the data point in eachtree, is computed as follows [23]:CADI: Contextual Anomaly Detection using an Isolation Forest SAC'24, April 8 -April 12, 2024, Avila, Spain() = 2- (()) () , (1)where (()) is the average depth of the data point over the t trees.() is a normalization factor corresponding to the average pathlength of unsuccessful searches in a binary tree with  nodes [23].Several variants of the IF method have been proposed in the liter-ature. Some focus on the calculation of the anomaly score, withoutmodifying the process of building the trees and the forest. Thisis the case of [28] where five new functions to compute anomalyscores are proposed. Others modify the construction of the trees butnot the calculation of scores. In [22] and [17], oblique separationsare used, but with different goals: detecting clusters of anomaliesfor the former and improving score consistency for the latter. In [9],the separations are not completely random and aim at minimiz-ing the weighted standard deviation of the subtrees depth inducedby each separation. More generally, the variants proposed in theliterature focus on the outlier detection task [35].The CADI approach proposed in this paper introduces a novelusage of an IF as a unified data model to identify anomalies, re-construct the inner structure of the regular points and provideexplanations about the found anomalies simultaneously.2.2 Anomaly Explanation and InterpretationIn many applicative contexts, end users expect more than a de-composition of the data into regular points vs. anomalies. Expla-nations about the reasons for the abnormality of each anomalycan indeed increase the trust and usability of the automaticallyextracted knowledge. Because of the diverse nature of anomalies,anomaly explanation deserves special treatment even though it hasbenefited from XAI works dedicated to the explanation of classi-fiers in general. A general categorization of explanation methodsin general and anomaly explanation methods in particular is themodel-agnostic vs. model-specific one. A model-agnostic methodprovides an explanation to any anomaly detection algorithm while amodel-specific method is tailored for a particular detector. Recently,more refined taxonomies of anomaly explanation methods havebeen proposed. In [37], four categories of explanations are identi-fied: attribute importance explanation, attribute value explanation,point comparison explanation, and intrinsic data structure analy-sis explanation. In [31], the following categories of explanationsare introduced: methods that rank anomalies, methods that revealcausal relationships between anomalies, and methods that identifyattributes responsible for the abnormality of points or groups ofpoints. In both cases, it is stated that techniques returning importantattributes are the most common in the literature [15, 29]. However,these techniques often ignore the local context of an outlier duringthe explanation generation. A way to remedy this omission and togo a step further is to link anomalies with the intrinsic structure ofthe data set. A few methods thus consider the anomaly detectionas a part of a clustering process [21, 26] or a complement of it [34].CADI shares a same objective with the COIN (Contextual OutlierINterpretation) approach [25], namely: to help end users understandwhy a given point is flagged as an anomaly considering its context.The additional information associated with each anomaly in theCOIN approach is a triplet containing i) an anomaly score, ii) a listof attributes supporting the decision of considering the point asabnormal and iii) a description of the local context of the anomaly.Whereas COIN has to be combined with an anomaly detector anda clustering method, the latter being applied on the neighborhoodof the anomalous point to explain, CADI provides a unified andself-contained approach to reach the same goal. Another anomalyexplanation method, Attention-guided Triplet deviation networkfor Outlier interpretatioN (ATON) [36] also includes some localinformation when producing the explanations. It learns the devi-ations between the outlier to explain and its regular neighbors inan embedded feature space. However, unlike COIN, it only outputsfeature importance weights.2.3 Outlier-Aware ClusteringBeing able to reconstruct the data inner structure without beingaffected by the presence of outliers makes CADI a robust cluster-ing approach. The clustering task aims at decomposing a data setinto homogeneous, i.e. compact, and distinct, i.e. well separated,subgroups in the data. As such, it can be seen as summarizing theunderlying data distribution and providing a legible overview ofthe data content. Yet most clustering algorithms suffer from thepresence of outliers: the points that do not conform with the globalstructure of the data most often hinder the identification of regularclusters. The so-called robust clustering methods aim at addressingthis issue, providing data partitions that are not perturbed by out-liers: they aim at outputting the same results as would be obtainedwithout outliers and without requiring to perform a preliminarystep of outlier detection and removal. Robust clustering can beroughly categorized into two types of methods [3]: some of themproceed by automatically down-weighting atypical data points [12],using several approaches to define these weights, e.g. includingnoise clustering [11], possibilistic clustering [30], replacing the tra-ditional normal distributions by multivariate t-distributions [27] ordedicated approaches [18]. Other methods propose to replace theclassical squared Euclidean distance, which is known to be highlysensitive to outliers, by other distances [14]. Along the same lines,some approaches are explicitly based on robust M-estimators in-corporated in the cost function [10], the possibilistic c-means [19]can be seen in this framework. These approaches define robustnessas the ability to ignore the outliers, possibly grouping them in aspecific cluster, as in the noise clustering approach for instance.By aiming at providing a rich overview of the whole dataset,including the regular points inner structure and the existing anom-alies, CADI is related to the approaches introduced in [20], [8]and [24] but leverages a common data structure (viz. an isolationforest) to extract these two types of knowledge.3 THE CADI APPROACHThis section details the CADI approach focusing first on the modi-fication made on the IF construction algorithm to avoid splittingdense subspaces that will then form clusters or parts of them. Theanomaly scoring function is also revisited to take into accountthe impact of the density constraint on the isolation process. Thesame IF is then used to reconstruct a partition of the regular datapoints, and identify local anomalies to the found clusters. Finally,the identified anomalies are explained wrt. the clusters of regulardata points.SAC'24, April 8 -April 12, 2024, Avila, Spain Veronne Yepmo, Gregory Smits, Marie-Jeanne Lesot, and Olivier PivertTable 1: Notations used throughout the paperNotation MeaningD Data set of  pointsA = {1, . . . , } Descriptive attributes() Domain of attribute A Set of tested intervals on feature   A  D Data point. Value for data point  on attribute  Size of the forest F = {1, . . . ,} Size of the sample used to build a treeDepth limit() Leaf containing  in the -th tree Margin width percentageC Partition of D in  clusters {1, . . . ,} () Tree containing the leaf ((),) Deepest common ancestor between thepaths from the root of  () to and ()3.1 Density-Aware Isolation ForestAs reminded in Sec. 2.1, in the classical IF approach, an isolation treeis built through recursive splits of a data subset . A split is a couple(, ), where  is a randomly chosen attribute   A and  a valuefrom its observed domain   [min  ., max  .]. CADIrevisits this completely random process to keep only the splits thatfall in low density regions. Anomalies being by definition detachedfrom regular phenomenon materialized by dense subspaces, the ob-jective of this revisited isolation algorithm is to find separation linesin low density areas surrounding dense regions. To do so, a density-based constraint is added and determines if a split is maintained ordiscarded. The hypothesis is the following: if a significant numberof points are found in the neighborhood of the split, it is potentiallyseparating a cluster. In that case, the split is discarded and anotherone is generated. The goal is to surround the clusters of regularpoints by the separations, so that some leaves may contain a cluster,or a significant portion of a cluster. One hyper-parameter denoted is introduced in addition to the IF hyper-parameters to controlthe size of the margin around the separation which represents itsneighborhood. Alg. 2 details how density-aware isolation trees areconstructed and Fig. 2 illustrates an example of split selection. Thesplit (1, 1) on Fig. 2 is discarded as many points are located inthe margin surrounding the separation. It is not the case for thesplit (2, 2) that is kept.Compared to the initial IF algorithm, a cost overhead is undeni-ably induced by this split selection, as in the original IF approachthe splits are randomly drawn. However, to learn from the discardedsplits and to avoid generating separations in intervals that havealready been discarded because they contain many data points,the set of tested intervals on each attribute J = {1, . . . ,  }( being the set of tested intervals on attribute ) is stored andpassed as a parameter through the recursive calls to the build_treefunction (line 20 in Alg. 2). If the method was not able to find avalid separation in the whole interval of values of an attribute (line10), this attribute is discarded (line 11). The discarded attributesare therefore also stored, in the variable . If the method is unableto find a valid separation on any attribute (line 3), then a terminalFigure 2: Example of a discarded separation line (1, 1)falling in a dense area (dashed line) and a validated sepa-ration (2, 2) (plain line)Algorithm 2 CADI : _1: Inputs: data sample   D, depth  of the current node,margin width percentage , set of tested intervals J ={1, . . . ,  }, set of covered attributes ; J and  are emptywhen the method is first called, and  = 02: Output: a node in an isolation tree3: if  = A or || = 1 or  &amp;gt; then4: Return (,, ,,,)  returns a leaf5: else6:   (A \ )  random attribute selection7:   ([ ., .] \ {  }) random value selection8:   12(max  . - min  .)9:     [ - ,  + ]10: if    . [min  ., max  .]   then11:     {}  A is entirely scanned12: end if13:  {  /.  [ - ,  + ]}  points inthe margin14: if ||   x || then15:  {  /. &amp;lt; }16:  {  /.  }17: Return (_(, + 1, , , ),18: _(, +1, , , ), ,,, ) return an internal node19: end if20: Return _(,, , J,)  select another split21: end ifnode is returned (line 4), the current set of points being consideredas inseparable.In a tree generated by CADI a leaf may be of three differenttypes depending on the termination condition that yields it. AnIsolation Node (IN) stores a data point that has been isolated fromthe rest of the dataset, it is generated when || = 1. A terminal nodeis called a Dense Node (DN) if it gathers a set of inseparable points,formally if || &amp;gt; 1 and  = A (l.3 in Alg. 2). Finally, a Depth-Limit Node (DLN) is such that  = and   A. Whereasthe classical IF algorithm yields only nodes of type IN and DLN,the nodes of type DN induced by the density constraint applied onCADI: Contextual Anomaly Detection using an Isolation Forest SAC'24, April 8 -April 12, 2024, Avila, Spainthe randomly generated splits are particularly informative in theprospect of reconstructing the data inner structure (Sec. 3.2).As compared to a classical IF, a CADI forest induces an additionalcost related to the storage of the excluded intervals. As they arestored only for one node at a time, this overhead is a constant. Thetime complexity differs from that of a classical IF by the selectionof the separations. This difference is, in the worst case, linear withrespect to the number of attributes: O(|A|).3.2 Clustering from an IFA CADI forest has three types of terminal nodes: IN, DN, and DLN.IN leaves contain potential anomalies, as the data points were iso-lated. DLN leaves are those containing points which have not beenseparated after a certain number of splits, just like in IF. DN leavescontain points that cannot be separated no matter the attribute.They therefore gather dense group of points, corresponding to clus-ters or portions of clusters. As a result, in order to obtain a partitionof the data set, these leaves need to be combined.The combination strategy of CADI's DN leaves is inspired bygrid-based clustering [1] where the feature space is first partitionedby a grid. Each cell of the grid is a combination of intervals onthe attributes in A, and contains some points. Then, contiguousdense cells are merged to form clusters. In practice, a graph G =(V, E) is built. Each vertex of the graph is a cell and there is anedge  between two vertices 1,2  V if the corresponding cellsare contiguous. The connected components of the graph are laterextracted, and each connected component is a cluster. Like cellsin grid-based clustering, DN leaves in CADI contain data pointsand delimitate dense regions of the data space. However, there aresome major differences between the two units. First, DN leavescoming from different trees may have some points in common(because of the sampling), whereas grid cells are disjoint in termsof points. Second, the subspaces delimitated by DN leaves mayoverlap. As a result, instead of adding an edge between two verticesif the corresponding leaves are contiguous, an edge  is createdif the two leaves are somewhat similar in terms of points. Thissimilarity is measured by the Jaccard index between the two leavesand corresponds to the weight of :=|1  2||1  2|,with  = (1,2). 1 and 2 are the sets of points contained isthe respective leaves. Each connected component is a cluster. Thisstrategy allows to automatically discover the number of clustersin the data set, just like in grid-based clustering. The data pointsnot assigned to a cluster, because they were not part of any sampleused to build the forest, are propagated through each tree untilthey reach a terminal node. A majority vote is then performedamong the clusters of the corresponding DN leaves. Before theextraction of the connected components, a preprocessing step isapplied: the leaves included in other leaves are deleted from V toremove redundancies.3.3 Anomaly InterpretationIn the COIN [25] approach, an outlier explanation is composed of:(1) a quantification of the point abnormality,(2) a local positioning wrt. its surrounding regularities, which isequivalent for the method to a set of the clustered neighborsof the point,(3) a subset of attributes weighted by their relative contributionto the abnormality of the suspicious point.Similarly to COIN, the CADI approach provides for each outlier:(1) a quantification of the point abnormality,(2) a quantification of its deviation wrt. each identified cluster,(3) a subset of attributes weighted both by their relative contri-bution to bringing the suspicious point closer to each clusterand their relative contribution to the abnormality of thesuspicious point wrt. each cluster.Anomaly Score. Leveraging the property that it is more likely toisolate anomalies than regularities using random splits, the anomalyscore of a given point is computed in the original IF approach asa function of its depth of isolation in the different trees of theforest. Using the CADI approach, and due to the density constraintimposed on the randomly generated splits, a dense region may becompletely scanned without increasing the depth of the tree, thusleading to a leaf of type DN containing a high number of inseparablepoints located at low depth. To differentiate leaves of type IN fromthose of type DN, it thus makes more sense to define an anomalyscore based on the cardinality of the set of points isolated in a sameleaf instead of its depth. Equation 2 is used to calculate an anomalyscore for a given point  that depends on the cardinality of the nodeit is isolated in:() = 1 -|()| - 1 , (2)where () is the node containing  in the -th tree. The score() varies in ]0, 1] taking its maximum value when  is isolatedalone in an IN leaf and is close to 0 when the whole data subsetends in a same leaf. The latter situation occurs when no separationline can be validated on the whole universe: the dataset consists ofa single indivisible cluster.The global anomaly score is the average over the whole forestcontaining  trees:() =1=1(). (3)Local Structure-Aware Anomalies. In its original version, an IF de-tects points that may be easily separated from the rest of the dataset,leading to so-called global anomalies. Local anomalies may also beidentified by an IF, but no distinction between local and global anom-alies is made in the output of the method. Many recent works (seeSec. 2.2) focus on providing users with more informative descrip-tions of the data set, especially through a local contextualizationof the found anomalies. It is now shown that a forest generated byCADI embeds all the necessary structural knowledge to identifypossible links between anomalies and clusters.Let  be a point whose anomaly score () is sufficiently highto be considered as an anomaly. The next step is to determine foreach cluster   C whether  can be considered as an abnormaldeviation of the regular phenomenon modelled by. Let {1, . . . ,}be the set of DNs making up , and  (), = 1 . . .  be the tree inwhich is found. Still using the structural knowledge embedded in () only, viz. without having to choose an appropriate distanceSAC'24, April 8 -April 12, 2024, Avila, Spain Veronne Yepmo, Gregory Smits, Marie-Jeanne Lesot, and Olivier Pivert(a) Clusters 1 and 2(b) Deepest common ancestors (blue octogons) between and thes (green squares). Partial trees are displayed.Figure 3: Contextual/Local anomaly detection: leveragingCADI trees and DN leavesmeasure, a contextual score denoted by (,) is computed as anaggregation of the comparisons between  and the s forming . Ina tree, e.g. (), the path from the root to the leafimplies differentseparations each narrowing the subspace originally enclosed by theroot. As a result, if  and the points in are found in the same nodedeep in the tree, they are more likely to be close from each other inthe feature space. In that case, if  is separated from the points in ,it is more likely to be deviating from . By applying this principle toall the s in a cluster , a score corresponding to the local deviationof  wrt. is computed. This contextual score depends on the depthof the deepest common ancestor () between the node containing in the -th tree (), and each in the corresponding  ():(,) =1=1(((),)), (4)where((),) refers to the deepest common ancestor of ()and leaf node , and (((),)) is its depth.The process described above is illustrated on Figure 3. On Fig. 3a,two clusters 1 and 2 have been identified: 1 = {1,2} and 2 ={3,4,5}. The contextual score of  with each cluster is computedusing the depth of the deepest common ancestor between  andeach (Fig. 3b). These contextual scores are therefore given by:(,1) = (5/+ 4/)/2 = 9/2and (,2) = (2/+1/+1/)/3 = 4/3. As a conclusion,  is a local anomalyof 1.Common and Discriminating Attributes. In addition to the contextualscores computed between an anomaly  and each cluster  C, it ispossible and particularly interesting to determine which attributesmake of  a contextual anomaly of a given cluster . Each attribute is thus associated with a couple of weights forming the vector(,,). The first item of the couple indicates how much thevalue possessed by  on  (.) is shared by other members of. The second item of the couple quantifies how much  makes an anomaly of . To compute these weights, the paths in each () from its root to () and respectively are analyzed likein Fig. 3b. In a tree  (), the attribute involved in ((),),attribute denoted by (((),)) in Equation 5, is consideredas an explanation of the reason why  is an anomaly, whereasthe other attributes involved in the path from the root of to((),) ( excluded) describe the context shared by and. The couple represented by the vector (,) quantifies thecontribution of attribute  to explain  as a local anomaly of thedense subset of points gathered in the leaf .(,) = ()1 if  = (((),)) and 0 otherwise(5)where () is the number of times attribute  is used as a sepa-ration attribute in the path from the root of  () to ((),)( excluded).At the forest and cluster levels, the weight vector attached toan attribute  to quantify its contribution to explain why  is ananomaly of  is computed as follows:(,,) =1=1(,). (6)4 EXPERIMENTSThe objective of this section is to evaluate the proposed CADIapproach. Answers to the following questions are sought: 1) IsCADI able to accurately detect anomalies in a data set? 2) Is CADIable to provide an accurate partition of the data set? 3) Is CADIable to identify contextual anomalies wrt. clusters of regular datapoints? 4) Is CADI able to provide accurate contextual explanationsfor the anomalies? Each component of CADI is therefore evaluated.4.1 Data Sets and Experimental SetupAs anomaly detection has been extensively explored in the liter-ature, several real-world data sets for the assessment of this taskexist. Eleven real-world and two synthetic data sets [32], describedin Table 2, are used to evaluate the anomaly detection step.Unfortunately, there is no information neither on the presenceof clusters in these data sets, the locality of anomalies, nor on theground-truth explanations. In general, evaluating an explanationon real-world data sets is not an easy task, because of this absenceof ground-truth. With some knowledge about the data, it is possibleto have an insight on these ground-truth explanations. In the anom-aly explanation literature, a common practice is to add controllednoise attributes [5, 25]. The hypothesis behind this practice is thattrue explanations should lie among the original (viz. not noise)attributes. We believe that an evaluation using this scheme is notfaithful enough, since the true outlying attributes must be part ofthe original ones. This scheme therefore only evaluates the abilityof a method to provide non-aberrant explanations. In [36], anothertechnique to generate ground-truth explanations on real-worldCADI: Contextual Anomaly Detection using an Isolation Forest SAC'24, April 8 -April 12, 2024, Avila, SpainTable 2: Real-world data setsData set D |A| |D| # anomaliesAnnthyroid 6 7200 534Arrhythmia 271 420 57Breast 9 683 239Cover 10 286048 2747HBK 4 75 14HTTP 3 567498 2213Ionosphere 32 351 126Mammography 6 11183 260Pima 8 768 268Satellite 36 6435 2036Shuttle 9 58000 3511SMTP 3 95156 30Wood 6 20 4data sets is proposed. The outlying degree/score of real outliers inevery possible subspace of the original feature space is computed.The ground-truth explanation is the subspace where the anomalyreceives the highest score. Three different anomaly detectors areused, among which IF. Depending on the detector used, there aredifferent ground-truth outlying attributes that are used separatelyduring the evaluation. With this scheme, there is no informationregarding the possible clusters of regular data points.In contrast to real-world data sets, the true outlying attributes areknown during the generation of synthetic data sets. Furthermore,since the generation process is known, data sets containing clustersand local anomalies can be produced. Since the best usage of CADIis this setting, only synthetic data sets are used for the evaluationof the subsequent components of the method. We follow a strategysimilar to the one described in [25] for the generation of syntheticdata sets. In the first synthetic data set, each anomaly is close toonly one cluster. The outlying attributes wrt. the correspondingcluster constitue the ground-truth. In the second data set, someanomalies share some attributes values with more than one cluster.The third data set contains anomalies that deviate from all theclusters. The last cluster contains, in addition to local and globalanomalies, a cluster of anomalies. To evaluate the ability of CADIto discover non-spherical clusters, the second and third data setscontain stretched clusters. In addition to that, the fourth data setis the moons data set generally used to evaluate clustering. It iscomposed of two interleaving half circles, to which we manuallyadded outliers. Information about the synthetic data sets generatedare summarized in Table 3.To enhance the reproducibility of our experimental results, thecode and the synthetic data sets along with the ground truths arepublicly available 1.The parameters of CADI are set to these default values:  = 100, = 256, = 8 and the margin size is  = 5% of the attribute'sinitial range. The default values of ,  and are the same asfor IF. The intuition behind a fixed value of  is the following :if two points are separated by less than that  x () on anattribute , they should remain together during the tree building1https://gitlab.com/yveronne/cadiprocess. However, the value of that parameter can be adjusted withsome knowledge about the data. For example, if the user wantsto keep together data points having a difference in values on aspecific attribute  less than a quantity , then the value of for this attribute can be set to /(). This fixed value of causes an update on l.14 in Alg. 2. If the data distribution is uni-form, then /(max  . - min  .) x || points shouldbe expected in the margin. Nevertheless,  is an upper bound ofthe quantity /(max  . - min  .), as the quantity(max  .-min  .) decreases with separation. l.14 is there-fore not modified in the implementation and the experiments.4.2 Anomaly DetectionThis part of the experiments aims at evaluating CADI in termsof anomaly detection. To this end, the real-world and syntheticdata sets are used. For each data set, the ground-truth anomaliesare known. The Area under the Receiver Operating Characteristiccurve (AUC/AUROC) is computed. AUC is an appealing method foranomaly detection evaluation because it is independent of the outly-ing degree threshold. It represents the probability that an anomalyreceives a higher score than a regular data point. CADI is comparedto the classical IF method in terms of AUC. For comparison betweenIF and other anomaly detection algorithms, some benchmarks, like[16], are available. The means and standard deviations of the AUCafter ten runs of CADI and IF are reported in Table 4.In general, there is no significant difference between CADI andIF. However, CADI performs much better than IF on two data setsin particular: mammography and the synthetic data set wood. Thatperformance increase is due to the fact that these data sets containregular data points located in sparse regions of the feature space. AsCADI combines the separability information and the local densityinformation during the split selection, the method is able to makea better distinction than IF between regular data points located insparse subspaces and isolated anomalies. IF in contrast uses onlythe separability information when computing the anomaly score,leading to a less precise distinction between regular data pointslocated in sparse subspaces and anomalies. Another observation isthat the standard deviations of the anomaly score with CADI aregenerally lower, implying that the results obtained are more stable.This stability is caused by the non-completely random selection ofthe separations in CADI.4.3 Clustering EvaluationTo evaluate the clustering component of CADI, a forest is builtfor each synthetic data set. Anomalies are identified by choosing ascore threshold of 0.95. Then, a partition of the regular data points isextracted by the procedure described in Sec. 3.2. For each syntheticdata set, the true clustering labels of each data point are known.As a result, the Adjusted Rand Index (ARI) is used as evaluationmetric. To have a ground-truth partition for computing the ARI,we assign all the known anomalies to a cluster, and the regularinstances are assigned to their respective true cluster. CADI is com-pared with the density-based clustering algorithm DBSCAN [13]and the robust clustering algorithm -means-- [8]. The choice ofDBSCAN for comparison is motivated by three main reasons: 1)DBSCAN identifies not only the clusters of regular data points butSAC'24, April 8 -April 12, 2024, Avila, Spain Veronne Yepmo, Gregory Smits, Marie-Jeanne Lesot, and Olivier PivertTable 3: Synthetic data setsData set D |A| # clusters |D| # anomalies DescriptionD1 2 2 900 25 Spherical clusters. Local anomalies only.D2 2 3 1508 8 Two spherical and one stretched clusters. Local and global anomalies.D3 3 4 408 8Four stretched clusters. Each pair of clusters located in only two dimensionsLocal and global anomalies.D4 2 2 517 17 Two moons. One anomaly cluster. Local and global anomalies.Table 4: Anomaly detection performance: AUCData set CADI IFAnnthyroid 0.772  0.014 0.819  0.013Arrhythmia 0.812  0.007 0.775  0.045Breast 0.994  0.001 0.981  0.004Cover 0.832  0.027 0.873  0.022HBK 1.0  0.0 1.0  0.0HTTP 0.998  0.002 0.999  0.001Ionosphere 0.827  0.009 0.855  0.005Mammography 0.844  0.011 0.645  0.037Pima 0.702  0.007 0.684  0.010Satellite 0.700  0.014 0.699  0.016Shuttle 0.992  0.002 0.995  0.001SMTP 0.883  0.011 0.889  0.008Wood 0.956  0.061 0.868  0.069D1 0.999  0.001 0.999  0.001D2 0.965  0.005 0.979  0.004D3 0.974  0.005 0.995  0.002D4 0.990  0.001 0.972  0.005Mean AUC 0.8965 0.8839also the anomalies. 2) DBSCAN, as CADI, automatically discoversthe number of clusters and therefore does not require it as an inputparameter. 3) DBSCAN is able to discover non-elliptical clusters.-means-- are a variant of the classic -means clustering algo-rithm. The approach has two parameters: the number  of clustersto discover and the number  of outliers in the data set. The  far-thest points are discarded during the cluster centers updates. Weset the values of  and  to their true values for each dataset. Thedefault parameters of DBSCAN are used on D1, D2 and D3. Themost important parameter of DBSCAN is  which controls the sizeof the neighborhood. On D4, the default value of  does not producegood results. It was fine-tuned. On that same data sets, the weakestedges were removed by CADI to obtain two clusters. The best ARIobtained when using CADI, DBSCAN and -means-- on the datasets are reported in Table 5.-means-- obtain the best ARI on D1 and D2. This behaviorwas expected, since the classic -means algorithm is efficient forextracting spherical clusters. On D3 and D4, -means-- struggles,just like the original -means struggles on these data sets in theabsence of anomalies. DBSCAN and CADI on the other hand donot struggle to discover non-elliptical clusters. CADI obtains thebest ARI in average.Table 5: Clustering performance: ARIData set CADI DBSCAN -means--D1 0.986 0.914 1.0D2 0.970 0.963 0.994D3 0.936 0.971 0.333D4 0.992 0.999 0.287Mean 0.971 0.962 0.653Table 6: Contextual anomaly detection performanceData set Precision RecallD1 1.0 1.0D2 1.0 1.0D3 0.875 0.875D4 0.941 1.04.4 Contextual Anomaly DetectionIn addition to identifying anomalies and groups of regular datapoints, CADI provides some insight about the cluster(s) from whicheach anomaly may be deviate. As, to the best of our knowledge, nomethod in the literature is able to do so, there is no baseline forcomparison. However, since the true cluster assignments are knownfor each anomaly in the generated data sets, the performance ofCADI regarding the contextual anomaly detection can be evaluated.To do so, for each outlier , let P be the set of predicted clusters for, i.e the set of clusters from which  may be deviating accordingto CADI. Let T be the set of ground-truth clusters for , i.e the setof clusters from which  is deviating. The precision and recall arecomputed as  = |P  T |/|P| and  = |P  T |/|T |.For each data set, the precisions and recalls are averaged over theoutliers. The results are shown in Table 6. CADI performs well onall the data sets, with perfect precision and recall on D1 and D2.The method is more challenged on D3 as this data set containsseveral data points deviating from more than one cluster.4.5 Contextual ExplanationsThe last part of CADI's assessment concerns the generated contex-tual explanations. CADI outputs for an outlier  a list of discriminat-ing feature weights wrt. each identified cluster (second items of thepairs in Eq. 6). COIN [25] also outputs a list of feature weights, butwith respect to the local context of  only, and not the set of clustersin the data set. ATON [36] on the other hand does not take the localCADI: Contextual Anomaly Detection using an Isolation Forest SAC'24, April 8 -April 12, 2024, Avila, Spaincontext of  into account, but still outputs a list of feature weights.Both COIN and ATON need as input the outlier  to explain. It isnot the case for CADI which performs anomaly detection prior tothe explanation. COIN and ATON will nonetheless serve as baselinefor the evaluation. In addition to these two, CADI is compared tothe ground-truth explanation extraction procedure introduced in[36] and detailed in Section 4.1. The IF is used as detector for thismethod called ATON-GT from here onwards. ATON-GT outputsfor each specified outlier , the subspace in which it receives thehighest anomaly score. The implementations of COIN, ATON andATON-GT were kindly made available by their respective authors.Considering all the above-mentioned differences across CADI,COIN, ATON and ATON-GT, we propose the following evaluationprocedure to provide a fair comparison:* Since COIN, ATON and ATON-GT do not identify outliersper se, explanations for true outliers only are requested fromall the four methods. This allows to also evaluate the abilityof CADI to provide accurate explanations even for outliersthe method was not able to identify as such.* The ground-truth explanations are a list of discriminatingattributes wrt. each cluster. As a result, for the methods out-putting feature importance scores (CADI, COIN and ATON),the top  discriminating features are retrieved, with  beingthe length of the true explanatory subspace.* For all four methods, the precision and recall of the expla-nations are computed in a similar manner as for contextualanomaly detection performance evaluation. This procedureis also used in [36]. For each data set, the precision and recallover all the outliers are computed.* As CADI should not only produce the good cluster(s) butalso the good explanatory subspaces wrt. these clusters, thetwo information should match. Consequently, during theprecision and recall computation, the predicted cluster iscompared to the ground-truth cluster first, before comparingthe explanatory subspaces.* For COIN, ATON and ATON-GT, the generated explanatorysubspace is compared with the explanatory subspace of wrt. the true cluster(s) from which it is deviating, as thesemethods do not indicate from which cluster  may be devi-ating.The precision and recall for each data set and each method areshown in Table 7.CADI has a high precision and recall on all the data sets, meaningthat it is able not only to identify the cluster(s) from which aninstance is deviating, but also to provide a faithful explanation wrt.these clusters in terms of discriminating attributes. COIN performsbetter than ATON on D1, D2 and D4. This may be because of theclustering step of COIN that allows to mitigate the influence ofdifferent groups of points on the attributes importance. ATON andATON-GT perform better than COIN on D3. In this data set, clustersare located in different subspaces and local anomalies can alsobe identified in these subspaces. As ATON-GT explores differentsubspaces during the explanation generation process, it has a slightadvantage. CADI is also able to discover clusters (and consequentlyoutliers) in subspaces because of the split generation procedure. Asa result, it does not fall far behind ATON-GT in terms of precisionFigure 4: Data set D1. For the outlier represented by a redsquare, ATON-GT returns as explanatory subspace the fullfeature space.on D3. In general, ATON-GT has a high recall, because it tendsto overestimate the size of the explanatory subspace. For example,on data set D1, the explanatory subspace returned by ATON-GTfor the red square outlier on Figure 4 is the full attribute space.Although feature 1 is sufficient (regardless of the local contextor not), that outlier is more easily isolated in the full feature spacethan in 1.</body>
	<conclusion>This paper presented a unified model for contextual and inter-pretable anomaly detection. The proposed method, called CADIproposes a revised version of the IF as a basis to identify outliers,groups of regular data points and to provide explanations of bothglobal and local anomalies wrt. clusters, without relying on externalalgorithms to perform the different tasks. Conducted experimentsshow that CADI is indeed able not only to identify the anomaliesas well as classical IF on real-world data sets, but also to providemeaningful and accurate explanations regarding the abnormality ofan instance wrt. a group of points in synthetic data sets containingclusters and local anomalies. The main limitation of this work is theabsence of evaluation of the explanation component on real-worlddata and higher dimensional data sets. This will require some workto provide adequate ground-truths and is the main direction forfurther research.</conclusion>
	<discussion>N/A</discussion>
	<biblio>[1] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopulos, and Prabhakar Ragha-van. 1998. Automatic Subspace Clustering of High Dimensional Data for DataMining Applications. In Proceedings of the 1998 ACM SIGMOD International Con-ference on Management of Data. Association for Computing Machinery, NewYork, NY, USA, 94-105. https://doi.org/10.1145/276304.276314[2] Mennatallah Amer, Markus Goldstein, and Slim Abdennadher. 2013. EnhancingOne-Class Support Vector Machines for Unsupervised Anomaly Detection. InProceedings of the ACM SIGKDD Workshop on Outlier Detection and Description.Association for Computing Machinery, New York, NY, USA, 8-15. https://doi.org/10.1145/2500853.2500857[3] Christian Borgelt, Christian Braune, Marie-Jeanne Lesot, and Rudolf Kruse. 2015.Handling Noise and Outliers in Fuzzy Clustering. In Fifty Years of Fuzzy LogicSAC'24, April 8 -April 12, 2024, Avila, Spain Veronne Yepmo, Gregory Smits, Marie-Jeanne Lesot, and Olivier PivertTable 7: Outlier interpretation performanceData set Precision RecallCADI COIN ATON ATON-GT CADI COIN ATON ATON-GTD1 1.0 0.76 0.76 0.76 1.0 0.76 0.76 1.0D2 1.0 0.81 0.60 0.81 1.0 0.81 0.69 1.0D3 0.89 0.69 0.87 1.0 0.89 0.69 0.87 0.75D4 1.0 1.0 0.94 0.67 1.0 1.0 0.94 0.91and its Applications. Springer International Publishing, Cham, 315-335. https://doi.org/10.1007/978-3-319-19683-1_17[4] Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jorg Sander. 2000.LOF: Identifying Density-Based Local Outliers. In Proceedings of the 2000 ACMSIGMOD International Conference on Management of Data. Association for Com-puting Machinery, 93-104. https://doi.org/10.1145/342009.335388[5] Mattia Carletti, Matteo Terzi, and Gian Antonio Susto. 2023. InterpretableAnomaly Detection with DIFFI: Depth-based feature importance of IsolationForest. Engineering Applications of Artificial Intelligence 119 (2023). https://doi.org/10.1016/j.engappai.2022.105730[6] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly Detection:A Survey. ACM Comput. Surv. 41, 3, Article 15 (jul 2009), 58 pages. https://doi.org/10.1145/1541880.1541882[7] Chun-Hao Chang, Jinsung Yoon, Sercan O Arik, Madeleine Udell, and TomasPfister. 2023. Data-efficient and interpretable tabular anomaly detection. InProceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery andData Mining. Association for Computing Machinery, 190-201. https://doi.org/10.1145/3580305.3599294[8] Sanjay Chawla and Aristides Gionis. 2013. -means--: A unified approach to clus-tering and outlier detection. In Proceedings of the 2013 SIAM International Confer-ence on Data Mining (SDM). 189-197. https://doi.org/10.1137/1.9781611972832.21[9] David Cortes. 2021. Revisiting randomized choices in isolation forests. arXivpreprint arXiv:2110.13402 (2021).[10] R.N. Dave and R. Krishnapuram. 1997. Robust clustering methods: a unified view.IEEE Transactions on Fuzzy Systems 5, 2 (1997), 270-293. https://doi.org/10.1109/91.580801[11] Rajesh N Dave. 1991. Characterization and detection of noise in clustering.Pattern Recognition Letters 12, 11 (1991), 657-664. https://doi.org/10.1016/0167-8655(91)90002-4[12] Francesco Dotto, Alessio Farcomeni, Luis Angel Garcia-Escudero, and AgustinMayo-Iscar. 2018. A reweighting approach to robust clustering. Statistics andComputing 28, 2 (2018), 477-493. https://doi.org/10.1007/s11222-017-9742-x[13] Martin Ester, Hans-Peter Kriegel, Jorg Sander, Xiaowei Xu, et al. 1996. A density-based algorithm for discovering clusters in large spatial databases with noise. Inkdd, Vol. 96. 226-231.[14] Patrick J.F. Groenen and Krzysztof Jajuga. 2001. Fuzzy clustering with squaredMinkowski distances. Fuzzy Sets and Systems 120, 2 (2001), 227-237. https://doi.org/10.1016/S0165-0114(98)00403-5[15] Nikhil Gupta, Dhivya Eswaran, Neil Shah, Leman Akoglu, and Christos Faloutsos.2019. Beyond Outlier Detection: LookOut for Pictorial Explanation. In MachineLearning and Knowledge Discovery in Databases. Springer International Publish-ing, 122-138. https://doi.org/10.1007/978-3-030-10925-7_8[16] Songqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. 2022.ADBench: Anomaly Detection Benchmark. In Advances in Neural InformationProcessing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, andA. Oh (Eds.), Vol. 35. Curran Associates, Inc., 32142-32159.[17] Sahand Hariri, Matias Carrasco Kind, and Robert J. Brunner. 2021. ExtendedIsolation Forest. IEEE Transactions on Knowledge and Data Engineering 33, 4(2021), 1479-1489. https://doi.org/10.1109/TKDE.2019.2947676[18] Frank Klawonn and Frank Hoppner. 2003. What Is Fuzzy about Fuzzy Cluster-ing? Understanding and Improving the Concept of the Fuzzifier. In Advancesin Intelligent Data Analysis V. Springer Berlin Heidelberg, Berlin, Heidelberg,254-264.[19] R. Krishnapuram and J.M. Keller. 1996. The possibilistic C-means algorithm:insights and recommendations. IEEE Transactions on Fuzzy Systems 4, 3 (1996),385-393. https://doi.org/10.1109/91.531779[20] M.-J. Lesot and B. Bouchon-Meunier. 2004. Descriptive concept extraction withexceptions by hybrid clustering. In 2004 IEEE International Conference on FuzzySystems, Vol. 1. 389-394. https://doi.org/10.1109/FUZZY.2004.1375756[21] Marie-Jeanne Lesot and Adrien Revault d'Allonnes. 2012. Credit-Card FraudProfiling Using a Hybrid Incremental Clustering Methodology. In Scalable Uncer-tainty Management. Springer Berlin Heidelberg, 325-336.[22] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2010. On Detecting ClusteredAnomalies Using SCiForest. In Machine Learning and Knowledge Discovery inDatabases. Springer Berlin Heidelberg, Berlin, Heidelberg, 274-290.[23] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2012. Isolation-Based AnomalyDetection. ACM Trans. Knowl. Discov. Data 6 (mar 2012). https://doi.org/10.1145/2133360.2133363[24] Hongfu Liu, Jun Li, Yue Wu, and Yun Fu. 2021. Clustering With Outlier Removal.IEEE Transactions on Knowledge and Data Engineering 33, 6 (2021), 2369-2379.https://doi.org/10.1109/TKDE.2019.2954317[25] Ninghao Liu, Donghwa Shin, and Xia Hu. 2018. Contextual outlier interpretation.In Proceedings of the 27th International Joint Conference on Artificial Intelligence.AAAI Press, 2461-2467. https://doi.org/10.5555/3304889.3305002[26] Meghanath Macha and Leman Akoglu. 2018. Explaining anomalies in groupswith characterizing subspace rules. Data Mining and Knowledge Discovery 32, 5(2018), 1444-1480. https://doi.org/10.1007/s10618-018-0585-7[27] Geoffrey J. McLachlan and David Peel. 1998. Robust cluster analysis via mixturesof multivariate t-distributions. In Advances in Pattern Recognition. Springer BerlinHeidelberg, Berlin, Heidelberg, 658-666.[28] Antonella Mensi and Manuele Bicego. 2021. Enhanced anomaly scores for isola-tion forests. Pattern Recognition 120 (2021), 108-115. https://doi.org/10.1016/j.patcog.2021.108115[29] Tshepiso Mokoena, Turgay Celik, and Vukosi Marivate. 2022. Why is this ananomaly? Explaining anomalies using sequential explanations. Pattern Recogni-tion 121 (2022). https://doi.org/10.1016/j.patcog.2021.108227[30] N.R. Pal, K. Pal, J.M. Keller, and J.C. Bezdek. 2005. A possibilistic fuzzy c-meansclustering algorithm. IEEE Transactions on Fuzzy Systems 13, 4 (2005), 517-530.https://doi.org/10.1109/TFUZZ.2004.840099[31] Egawati Panjei, Le Gruenwald, Eleazar Leal, Christopher Nguyen, and ShejutiSilvia. 2022. A survey on outlier explanations. The VLDB Journal 31, 5 (2022),977-1008. https://doi.org/10.1007/s00778-021-00721-1[32] Shebuti Rayana. 2016. ODDS Library. http://odds.cs.stonybrook.edu[33] Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gregoire Montavon,Wojciech Samek, Marius Kloft, Thomas G. Dietterich, and Klaus-Robert Muller.2021. A unifying review of deep and shallow anomaly detection. Proc. IEEE 109,5 (2021), 756-795. https://doi.org/10.1109/JPROC.2021.3052449[34] Amit K Shukla, Gregory Smits, Olivier Pivert, and Marie-Jeanne Lesot. 2020.Explaining Data Regularities and Anomalies. In 2020 IEEE International Conferenceon Fuzzy Systems (FUZZ-IEEE). IEEE, 1-8. https://doi.org/10.1109/FUZZ48607.2020.9177689[35] Haolong Xiang, Hongsheng Hu, and Xuyun Zhang. 2022. DeepiForest: A DeepAnomaly Detection Framework with Hashing Based Isolation Forest. In 2022IEEE International Conference on Data Mining (ICDM). IEEE, 1251-1256. https://doi.org/10.1109/ICDM54844.2022.00163[36] Hongzuo Xu, Yijie Wang, Songlei Jian, Zhenyu Huang, Yongjun Wang, Ning Liu,and Fei Li. 2021. Beyond Outlier Detection: Outlier Interpretation by Attention-Guided Triplet Deviation Network. In Proceedings of the Web Conference 2021.Association for Computing Machinery, New York, NY, USA, 1328-1339. https://doi.org/10.1145/3442381.3449868[37] Veronne Yepmo, Gregory Smits, and Olivier Pivert. 2022. Anomaly explanation:A review. Data &amp; Knowledge Engineering 137 (2022). https://doi.org/10.1016/j.datak.2021.101946[38] Chong Zhou and Randy C. Paffenroth. 2017. Anomaly Detection with RobustDeep Autoencoders. In Proceedings of the 23rd ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining. Association for Computing Ma-chinery, New York, NY, USA, 665-674. https://doi.org/10.1145/3097983.3098052</biblio>
</article>