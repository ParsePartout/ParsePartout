<article>
	<preamble>surveyTermExtraction.pdf</preamble>
	<titre>AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS*</titre>
	<auteurs>
        <auteur>
            <name>M. Teresa Cabre Castellvi</name>
            <mail>teresa.cabre@trad.upf.es,</mail>
            <affiliation>Institut Universitari de Linguistica Aplicada. Universitat Pompeu Fabra</affiliation>
      </auteur>
        <auteur>
            <name>Rosa Estopa Bagot</name>
            <mail>rosa.estopa@trad.upf.es,</mail>
            <affiliation>Institut Universitari de Linguistica Aplicada. Universitat Pompeu Fabra</affiliation>
      </auteur>
        <auteur>
            <name>Jordi Vivaldi Palatresi</name>
            <mail>jorge.vivaldi@info.upf.es</mail>
            <affiliation>Institut Universitari de Linguistica Aplicada. Universitat Pompeu Fabra</affiliation>
      </auteur>
	</auteurs>
	<abstract>In this paper we account for the main characteristics and performance of a number of recently developed term extraction systems. The analysed tools represent the main strategies followed by researchers in this area. All systems are analysed and compared against a set of technically relevant characteristics.</abstract>
	<introduction>In the late 80s there was an acute need, from different disciplines and goals, toautomatically extract terminological units from specialised texts. In the 90s largecomputerised textual corpora have been constructed resulting in the firstprograms for terminology extraction1 (henceforth TE) which have showedencouraging results.Throughout the current decade computational linguists, applied linguists,translators, interpreters, scientific journalists and computer engineers have beeninterested in automatically isolating terminology from texts. There are manygoals that have led these different professional groups to design software tools soas to directly extract terminology from texts: building of glossaries, vocabulariesand terminological dictionaries; text indexing; automatic translation; building of* In Bourigault, D.; Jacquemin, C.; L'Homme, M-C. (2001) Recent Advances in ComputationalTerminology, 53-88.1 In order to give a broader view of TE we use both extractor and detector to refer to the same notion.However, we are aware of the fact that some scholars attribute different meanings to these words.2 Automatic Term Detection: a Review of Current Systemsknowledge databases; construction of hypertext systems; construction of expertsystems and corpus analysis.From the appearance of TERMINO (the first broadly known term detector) in1990 until today a number of projects to design different types of automaticterminology detectors have been carried out to assist terminological work.However, despite the large number of studies in progress, the automatisation ofthe terminological extraction phase is still fraught with problems. The mainproblems encountered by term extractors are: (1) identification of complexterms, that is, determining where a terminological phrase begins and ends; (2)recognition of complex terms, that is, deciding whether a discursive unitconstitutes a terminological phrase or a free unit; (3) identification of theterminological nature of a lexical unit, that is, knowing whether in a specialisedtext a lexical unit has a terminological nature or belongs to general language and(4) appropriateness of a terminological unit to a given vocabulary (this hasscarcely been addressed from the point of view of automatization).Systems for TE are based on three types of knowledge: (a) linguistic; (b)statistical; (c) hybrid (statistical and linguistic). Hence, there are differentapproaches to automatic term detection. All systems analyse a corpus ofspecialised texts in electronic form and extract lists of word chunks (i.e.candidate terms) that are to be confirmed by the terminologist. To make theterminologist's task easier the candidate term is provided with its context and,when available, with any other further information (frequency, relationshipbetween terms, etc.)Two relevant aspects regarding the nature of terms are termhood andunithood2; TE systems may be designed based on only one of these two aspects.Some practical experiments following each scheme for ranking a set termsextracted from Japanese texts are presented in (Nakagawa &amp; Mori, 1998). Theyshow that results in precision and recall are very close but the set of termsextracted are a somewhat different. This is still a research issue.Alongside term detection we find the task of automatic document indexing (i.e.information retrieval, IR). This applied field of natural language processing(NLP) techniques has an interesting common point with automatic termdetection, that is, word chunks that index a given document are oftenterminological units. This same goal explains why many extraction systems arerooted on IR as well as on the analysis of a specific IR system with noapplication whatsoever to TE.2 (Kageura &amp; Umino, 1996) refer to unithood as the degree of stability of syntagmatic combinations(collocations) and termhood as the degree in that a linguistic unit is related to a domain-specificconcept.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 3The difference between these two approaches lies in the fact that a tool for TEshould extract all terminological units from a text, whereas IR focuses on theextraction of only words or word sequences that better describe the contents ofthe document regardless of their grammatical features.The standard approach to IR consists in processing documents so as to extractthe so-called indexing terms. These terms are usually isolated words containingenough semantic load to provide information about its goodness when describingdocuments. Queries are processed in a similar fashion to extract query terms.With regard to queries the relevance of documents is based exclusively on theirrepresenting terms. This is the reason why their choice is crucial.Often these indexing terms are single words although it is known that isolatedwords are seldom relevant enough to decide the semantic value of a documentwith regard to the query. This fact has given rise to the ever-growing appearance,in the TREC3 assessments, of word and word-sequence indexing systems usingNLP techniques.Statistically based systems function by detecting two or more lexical units whoseoccurrence is higher than a given level. This is not a random situation, but it isrelated to a particular usage of these lexical units. This principle, called MutualInformation, also applies to other science domains such as telecommunicationsand physics. Term detectors based on hybrid knowledge tend to use this ideaprior to a linguistic-based processing.The problem with this kind of approach is that there are low-frequency termsdifficult to be managed by extraction systems. Here it is important to note thatthese systems use basically numerical information and thus are prone to belanguage-independent. The two most frequently used measures in the assessmentof these systems are found in IR: recall and precision. Recall is defined as therelationship between the sum of retrieved terms and the sum of existing terms inthe document that is being explored. In contrast precision accounts for therelationship between those extracted terms that are really terms and the aggregateof candidate terms that are found. These measures can be interpreted as thecapacity of the detection system to extract all terms from a document (recall) andthe capacity to discriminate between those units detected by the system whichare terms and those which are not (precision). The fact that recall accounts for allterms from a document implies that it is a figure much more difficult to estimateand improve than precision.In contrast with this traditional approach, other approaches attempt to solve theproblem by using linguistic knowledge, which may include two types ofinformation:3 TREC (Text Retrieval Engineering Conference) refers to a series of conferences supported by NISTand DARPA (U.S. agencies). Further information can be found at: http://trec.nist.gov/.4 Automatic Term Detection: a Review of Current Systemsa) Term specific: it consists in the detection of the recurrent patterns fromcomplex terminological units such as noun-adjective and noun-preposition-noun.This calls for the use of regular expressions and techniques of finite stateautomata.b) Language generic: it consists in the use of more complex systems of NLP thatstart with the detection of more basic linguistic structures: noun phrase (NP),prepositional phrase (PP), etc.In both approaches each word is associated to a morphological category. In orderto do so different strategies are proposed: from coarse systems that do not makeuse of any dictionary to complex systems that have an extremely detailedmorphological analysis and a final phase of disambiguation.Systems that harness structural information resort to techniques of partialanalysis to detect potentially terminological phrasal structures. There are alsosystems that benefit from their understanding of what is a non-term so they are atsome point in between those systems already mentioned. Other systems try toreutilize current terminological databases to find terms, variants or new terms.Systems based on linguistic knowledge tend to use noise and silence as ameasure of its efficiency. Noise attempts to assess the rate between discardedcandidates and accepted ones; silence attempts to assess those terms contained inan analysed text that are not detected by the system. Noise is common problemof those systems using this approach. Errors in the assignation of morphologicalcategory are also shared by these systems.The type of knowledge used leads to language-specific systems and therefore itrequires a prior linguistic analysis and probably a redesign of many parts of thesystem. Knowledge in artificial intelligence has been traditionally obtained fromexperts in each domain. This has yielded several difficulties so that somescholars have focused on automatization and systematisation in knowledgeacquisition. This strategy seems to show the benefits of a terminologicalapproach. Thus some researchers (e.g. Condamines, 1995) have proposed theconstruction of terminological knowledge databases so as to include linguisticknowledge in traditional databases. Although this is a recent approach, there isno database yet containing all the features that could be used in TE, i.e. there ishardly any semantic information. Thus closed lists of words containing sparsesemantic information within a given specialised domain have been proposed.In this paper we attempt to analyse the main systems of terminology extraction inorder to describe its current status and thus be able to enrich them. This paper isdivided up into two main parts: firstly, the largest part is devoted to describevarious systems of terminology extraction together with a short evaluation inwhich weak and strong points have been outlined. Secondly, the terminologyextraction systems have been classified according to some parameters.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 5</introduction>
	<body>2. Description of some terminology extraction systemsIn the following sections we offer a critical description of number ofsemiautomatic terminology extraction systems. In all cases, the followinginformation is given:a) The reference data of the system, that is, the author and the publication wherethe tool is first mentioned and the system goal.b) A brief description of the system.c) A short evaluation of the most relevant aspects. This evaluation is mainlybased on papers, oral presentations in congresses and working papers, etc.2.1. ANAReference publication: Enguehard and Pantera (1994)Main goal: Term extractionANA (Automatic Natural Acquisition) has been developed in accordance withthe following design principles: non-utilisation of linguistic knowledge, dealingwith written and oral texts (interview transcripts) and non-concern aboutsyntactic errors.According to the current trend of harnessing statistical techniques in the study ofnatural language, scholars use Mutual Information as a measure of lexicalassociation4. In order to avoid the involvement of linguistic knowledge theconcept of "flexible string recognition" is created, which generates amathematical function so as to determine the degree of similarity between words.Thus, no tool for morphological analysis is needed. For instance, the stringcolour of painting represents other similar strings like: colour of paintings,colour of this painting, colour of any painting, etc. The system has neither adictionary nor a grammar.The architecture of ANA is composed of 2 modules: a familiarity module and adiscovery module. The first module determines the following 3 groups of words,which constitute the only required knowledge for term detection:a. function words (i.e. empty words): a, any, for, in, is, of, to...b. scheme words (i.e. words establishing semantic relationships) such as box ofnails, where the preposition shows some kind of relationship between box andnails.c. bootstrap (i.e. set of terms that constitutes the kernel of the system and thestarting point for term detection).The second module consists in a gradual acquisition process of new terms fromexisting ones. Further, links between detected terms are automatically generated4 Remarkable examples of the use of these techniques are the works of Church &amp; Hanks (1989) onword association and Smadja (1991) on collocation extraction from large corpora.6 Automatic Term Detection: a Review of Current Systemsto build a semantic network. This module is based on word co-occurrence thatcan have 3 types of interpretations:* expression: high-frequency existing terms (TEXP) in the same window. The newword is considered a new term and thus is included in the semantic network. Forinstance if the system has diesel and engine as a known terms and findssequences like: ... the diesel engine is... or ... this diesel engine has... Then thesequence diesel engine is accepted as a new term and is included in the semanticnetwork as a new node with links to diesel and engine (see figure below).* candidate: an existing term appears frequently (TCAND) together with anotherword and a scheme word as in: ... any shade of wood... or ... this shade ofcolour... Here shade becomes a new term and is placed in a new node of thesemantic network (see figure below).* expansion: an existing term appears frequently (TEXPA) in the same wordsequence, without including any scheme word: ... use any soft woods to... or ...this soft woods or... As a result, soft wood is incorporated into the term list andthe semantic network as a new node with a link to woods (see fig. 1 below).The system keeps on recursivelyseeking elements with the threeinterpretations already mentioneduntil a new term is found.Enguehard and Pantera (1994)tested it by processing a documentin English of around 25,000 wordsand 29 reference terms. The systemmanaged to extract 200 terms withan error rate of 25%.shadeSoftwoodwoodDieselengineenginedieselFigure 1 Term candidates interpretationEvaluationMinimising linguistic resources is an extremely interesting issue, since it isdifficult to compile them. Likewise flexible string recognition may well apply toactual texts.A negative aspect of the system is that those terminological units added to the listof valid terms after each cycle are not validated. Thus ANA allows for theinclusion of non-valid terms that add up to the term list. However, no data aboutthe efficiency of this proposal are reported.2.2. CLARIT5Reference publication: Evans and Zhai (1996)Main goal: Document indexing5 Further information can be found at: http://www.clarit.com.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 7Document indexing for IR is an important field of application of NLPtechniques. This branch holds common points with term detection since the wordsequences that help in document indexing are normally terminological units too.CLARIT belongs to the group of systems that advocate an elaborated textualprocessing to detect complex terms in order to reach a more appropriatedescription of documents. This is the reason why we have included this systemamongst terminology detectors.Evans and Zhai (1996) propose the following kind of phrases for indexationpurposes:1. lexical atoms (hot dog, stainless steel, data base, on line, ...)2. head modifier pairs (treated strip, ...)3. subcompounds (stainless steel strip, ...)4. cross-preposition modification pairs (quality surface vs. quality of surface)The methodology starts with the morphological analysis of words and thedetection of noun phrases (NPs). The system distinguishes simplex noun phrasesfrom cross-preposition simplex phrases.What is behind this is the introduction of statistics to corpus linguistics. Statisticshere focuses on documents, that is, there is no prior training corpus. Linguisticknowledge facilitates the calculation weeding out irrelevant structures, improvesthe reliability of statistical decisions and adjusts the statistical parameters.The whole process is showed in the figure below:First, the raw text is parsed so as toextract NPs. Then each NP isrecursively parsed with the purposeof finding the most securegroupings. In this phase lexicalatoms are also detected and NPsare structured. Finally at thegeneration phase the remainingcompounds are obtained.Lexical atoms are defined assequences of two or more wordsconstituting a single semantic unitsuch as space shuttle, part ofspeech and hot dog. Since thedetection of these units is fraughtwith problems two heuristical rules are proposed:Figure 2 Whole process in CLARITNPsmeaningful subcompoundsRaw TextLexical atomsAttested termsSubcompoundgeneratorStructuredNPsSimplex NPParserCLARITNP Extractora. The words that constitute a lexical atom establish a close relationship and tendto lexicalise as if they were a single-word lexical unit.b. When acting as NP, lexical atoms hardly allow the insertion of words.8 Automatic Term Detection: a Review of Current SystemsThe first condition takes place if the frequency of the target pair W1W2is higherthan any other pair from the NP that is being processed. In the second conditionthe frequencies of grouped and separated occurrences are compared and there isa threshold beyond which the association is weeded out. This threshold isvariable according to the function of sentence morphological category. InEnglish texts, the most favoured sequence is that of noun-noun.NP analysis is also a recursive process. At every new phase the most recentlexical atoms are used for finding new associations that will be used in thefollowing phase. The process keeps going until the whole NP is analysed. Let usconsider the example below:general purpose high performance computergeneral purpose [high performance] computer[general purpose] [high performance] computer[general purpose] [[high performance] computer][[general purpose] [[high performance] computer]]The grouping order shows those sequences with a more reliable associationscore. In order to determine the association score a number of rules are taken intoaccount:* Lexical atoms are given score 0 as well as adverb combination withadjective, past participles and progressive verbs,* Syntactically impossible pairs are given score 100 (noun-adjective, noun-adverb, adjective-adjective, etc.).* As to the remaining pairs, there is a formula that account for the frequencyof each word, the association score of this word with other words from theNP and of two random parameters.To increase its reliability the association score is recomputed after everyassignation association. The system has been tested in an actual retrieval task ofdocument indexing substituting the default NLP module in the CLARIT system.The corpus and the queries were the standards used in the TREC conferences.There have been noticed some improvements in recall as well as in precision,which, in the author's opinion, justifies the use of these techniques. Then in theTREC-5 report a more detailed evaluation of the system is made (Zhai et al.1996). All in all it is concluded that the use of these techniques is effective,which enforces the similarities between term indexing and terminologyextraction.EvaluationThis seems to be an interesting system and the applicability of some basic ideasto terminology detection appears to be feasible. Actually CLARIT holdssimilarities with the Daille's (1994) proposal (a linguistically-driven statistics).AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS 9It should be borne in mind, however, that problems of terminology extractionand document indexing are similar but no identical so that many decisionsshould be re-considered strictly from the point of view of term detection. It isalso noteworthy that this system only extracts NP terminological units and thedata provided about how this system works are related to the application forwhich it has been designed.2.3. Daille-94 (ACABIT)Reference publication: Daille (1994)Main goal: Term extractionThe main idea behind this system is to combine linguistic knowledge withstatistical measures. Here the corpus should contain all the morphologicalinformation. Then a list of candidate terms is created according to text sequencesthat provide syntactic patterns of term formation. This information usesstatistical methods to filter out this list. This final process is different from othersystems in that it only uses linguistic resources.Assuming the fact that all terminological banks are basically composed ofcompound nouns, the program focuses on the detection of binary compoundnouns and disregards other co-occurring categories. This assumption lies in thefact that there is a large number of this kind of nouns in specialised languages.Further, most of these compounds of 3 or more constituents can be treated in abinary form.Those patterns considered relevant for French are N1PREP (DET) N2and NADJ PREP a (DET) N2, together with right and left coordination. Statisticalalgorithms are applied to these patterns. The author is aware of the fact that theapplication of statistical measures leads to some noise rate, that is, low-frequencyterms will not be recognised.The technique used for pattern recognition is that of finite state automata.Automata are represented by a subset of grammatical tags to which somelemmas, inflected forms and a punctuation mark are added. Thus we can regardautomata as linguistic filters that select defined patterns and also determine theiroccurrence frequency, distance and variation. Each morphosyntactic pattern isassociated with a specific finite automaton.The corpus is given a statistical treatment based on a large number of statisticalmeasures, which are grouped in the following classes: frequency measures,association criteria, diversity criteria and distance measures. The starting point isconsidering the two lemmas that constitute a pair within a pattern as twovariables on which the dependence degree is measured. Data are represented in astandard contingency table:10 Automatic Term Detection: a Review of Current SystemsL2Lnwhere a = L1L2occurrencesL1A b b = L1+Ln(n2) occurrencesLmC d c = Lm+L2(m1) occurrencesd = Lm+Ln(m1 and n2)Eighteen measures are applied with the aim of establishing the degree ofindependence of the variables in the contingency table. The analysis of theresults shows that only four of these measures are relevant to the purpose:frequency, cubed association criterion6 (IM3), likelihood criterion,Fager/MacGowan criterion.EvaluationUnlike in other systems, in ACABIT frequency has turned out to be one of themost important measures for term detection from a given area. However, theclassification resulting from the application of this frequency shows an importantnumber of frequent sequences that are not terms and, in contrast, does notsuggest the low-frequency terms.Daille (1994) believes that the best measure is the likelihood criterion, since it isa real statistical test, it proposes a classification that accounts for frequency, itbehaves adequately with large and medium size corpora and it is not defined inthose cases that are not to be considered. In any case, this measure yields somenoise due to several reasons:a. Errors in the morphological mark-up.b. Some combinations that are never of a compounding nature: ko bits (kilobits),a titre d'exemple (as an example)... ...c. Combinations of 3 or more elements, related to the problems of compositionand modification: bande laterale -unique- (-single- side band), service fixe -parsatellite- (-satellite- fixed service), etc.2.4. FASTR7Reference publication: Jacquemin (1996)Main goal: Term variation detectionThe aim of this tool is to detect terms variants from a set of previously knownterms. These terms may be available from a reference database or a termacquisition software. What is crucial in this system that it is not needed to startfrom scratch every time. Optionally Fastr can also be used for TE.The first step for applying Fastr is to obtain and analyse a set of existing termsand thus having a set of rules of a given grammar. The FASTR grammatical6 The formula was experimentally obtained by the autor from the association number described inBrown et al. (1988) in the aim of favouring the most frequent pairs: IM3=log2(a3/(a+b)(a-b))7 Further information can be obtained at http://www.limsi.fr/Individu/jacquemi/FASTR/index.htmlAUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS11formalism is an extension of that of PATR-II (Shieber, 1986). A partial parserbased on the unification mechanism is responsible for the application of theserules. Term variants are obtained through a metarule mechanism that isdynamically calculated.For instance, the term serum albumin corresponds to the Noun-Noun sequenceand is associated with the following rule:rule 1: N1 N2N3&amp;lt;N1lexicalization&amp;gt;= `N2'&amp;lt;N2lemma&amp;gt;=serum&amp;lt;N3lemma&amp;gt;=albumin.The value indicated by the feature "lexicalization" will be use just before partialparsing to selectively activate the target rules. Thus the above rule is linked to theword serum and so is activated when this word occurs in the sentence that isbeing parsed.At a different level several metarules generate new rules in order to describe allpossible variations of each term from the reference list. Each metarule presents aparticular structure and a specific pattern type. For instance, the followingmetarule can be applied to the previous rule:Metarule Coor(X1AE X2X3) = X1AE X2C4X5X3which leads to the new rule: N1AE N2C4X5N3This latter rule allows new constructions that substitute C4for a conjunction andX5for an isolated word such as serum and egg albumin. The candidate term isnot the whole new construction but the coordinated term (i.e., egg albumin). Thewords that have given way to the new rule (egg and albumin) maintain theirfunction of constricted equations of the original rule. Further, they are theanchoring point for the application of the metarule. A metarule can be associatedwith specific restrictions, as for instance: (&amp;lt;C4lemma&amp;gt;but) or (&amp;lt;X5cat&amp;gt; Dd).In this way, those sequences with no lexical relationship such as serum and thealbumin are rejected.The above rule is a coordination rule and it should be noted that there are alsoother types of rules that account for different kinds of variations:1. insertion rules: medullary carcinoma I medullary thyroid carcinoma2. permutation rules: control center I center for disease controlThe FASTR metagrammar for English contains 73 metarules altogether: 25coordination rules, 17 insertion rules and 31 permutation rules. In any case, forefficiency reasons the new rules are dynamically generated. Each rule is linkedto a pattern extractor that permits a very quick acquisition of information. As hasbeen pointed out, the FASTR grammatical formalism is a PATR-II extension(Shieber, 1986). This language allows to write grammars using feature structures.The rules describing terms are composed of a free-context part (N1 N2N3) and12 Automatic Term Detection: a Review of Current Systemsa number of restriction equations (e.g. &amp;lt;N2lemma&amp;gt;=serum). First, the systemfilters the rules that are to be applied according to the given text and then ananalysis take place.When Fastr is applied for term acquisition the process is gradual: from a givenset of terms the system detects new ones, which allows the beginning of a newcycle and the detection of new candidates. The loop goes on until new termscannot be detected. The author presents an experiment carried out on a medicinecorpus of 1,5 million words and a reference list of 70,000 terms from differentspecialised domains. After 15 cycles 17,000 terms were detected of which 5,000were new. The text was processed at a 2,562 word/minute speed.However, the number of recognised terms decreases when the reference list hasfewer items. For instance, if the reference sublist of medicine drops to 6,000terms, then only 3,800 new terms are recognised.The author also postulates the existence of a conceptual relation. between thenew terms and the term that has led to their recognition. This relationship isvariable in accordance with the type of rule that is applied i.e., insertion orcoordination rule. Permutation does not allow any relationship due to the phrasalnature of the relationship.All the language dependent data used by Fastr is stored in separated text files.This feature facilitates the use of the system in other languages as showed by therecent application of Fastr to Japanese, German and Spanish/Catalan.Recently Jacquemin has developed the detection of semantic variation usingresources like WordNet or the Microsoft Word97 thesaurus (Jacquemin, 1999).EvaluationThe main characteristic of FASTR is its ability for detecting term variants, anaspect often not considered by other systems. The fact of using alreadyrecognised and accepted terms is very useful, although, as the author admits, itplaces restrictions on the acquisition of new terms that are not related to thesource terms.TE in Fastr implies that terms that are added to the list of valid terms after eachcycle are not validated. Thus, a non-valid term may be added to the list so it islikely that in forecoming cycles more non-valid terms are added. Jacquemin(1996) believes that this is not an important error source because the system, insome way, corrects itself since "normally" non-correct candidates do not giveway to new potential candidate terms.Actually this technique should not be isolately applied. Rather, it should becoordinated with other strategies as in (Jacquin &amp; Liscouet 1996) and (Daille1998).AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS132.5. HEIDReference publication: Heid et al. (1996)Main goal: Term extractionHeid et al. (1996) believe that automatic TE has various applications anddictionary or glossary construction would be the major one. In dictionaryconstruction from computerized corpora two phases are distinguished: linguisticpre-analysis and a term identification tool. Each of these phases requires specificcomputer tools.In the linguistic pre-processing phase the following processes are required8:a) tokenizing, which identifies word and sentence boundaries.b) morphosyntactic analysis, which identifies grammatical categories as well asdistributional and morphosyntactic features.c) POS tagging, which disambiguate morphosyntactic hypotheses.d) lemmatization, which identifies the lemma candidates.For term identification the system has a general corpus retrieval interface thatincludes a corpus query processor (CQP), a macroprocessor for the CQP querylanguage and a key word in context (KWIC) program, to extract and sortconcordances and lists of absolute and relative frequency of search items.TE is linked to a complex query language. The queries will be differentaccording to the types of candidate terms searched for. Thus, for instance,queries about single-word terms are made from morphemes or typicalcomponents of compound or derived words (derivatives). In these queries it isassumed that NP affixed terms from specialised languages use more specificaffixes and/or prefixes than others. All the word sequence extracted (N-A, N-N,N-V), are based on POS patterns.Heid et al. (1996) have applied these tools to technical texts on automobileengineering in German, which amounts to 35,000 words. The sample has beenmanually analysed before the application of the above procedures. The resultsare as follows:* With regard to single-word terms, there has been found a 90% of candidateterms and a 10% of silence. This rate varies from one scheme to another.* With regard to multiword terms, there are no concluding results. The resultsare less satisfactory and that the same problems as linguistic based arefound: POS patterns do not constrain enough the context and produce toomuch noise. Heid et al. (1996) believe that by using a syntactic parser, as itis the case in English, noise would diminish.8Heid et al. (1996) note that a broad coverage morphosyntactic parser for German is not attained.Thus parser results are simulated using POS patterns.14 Automatic Term Detection: a Review of Current Systems* Finally, collocation extraction is shown to produce noise but not silence,since Heid et al. (1996) consider the frequency criterion.The Ahmad's statistical measure (Ahmad et al, 1992) of relative frequency incorpora of specialised and general language is applied to this corpus of 35,000words. They show that the results produced by linguistic corpus query areincluded in the output of statistical methods. However noise in statisticalmethods is higher than in linguistic methods.EvaluationTo tackle this system it should be taken into account the morphosyntacticfeatures of the German language. Unlike Romance languages, German prefers toform compounds in a synthesising manner. It means that what other languagesexpress via terminological phrases in German is expressed with a single-wordterm (by word is meant any segment found between two gaps). Thus it can beseen that in German automatic term detection does not depend much on termdelimitation but on the terminological nature of a word. This is the reason whywe need parameters to distinguish a term from a word of the general language,both having the same morphosyntactic structure.Like most of the reviewed programs, Heid focuses on NP terms although it canalso extract collocations combining nouns and verbs. In this case Heid et al.(1996) note that the results are much worse. We do not have specific data aboutthe performance and the results of this system.2.6. LEXTERReference publication: Bourigault (1994)Main goal: Term extractionThis system has been developed in the need of the EDF (Electricite de France)society for improving their indexation system. LEXTER aims at locatingboundaries among which potentially terminological NPs could be isolated.LEXTER carries a superficial analysis and makes use of the text heuristics inorder to obtain those NPs of maximum length that it regards as candidate terms.The program is composed of several modules and works as follows:1. Morphological analysis and disambiguation module. Texts receive informationabout the POS and the lemma assigned to every word.2. Delimitation module. At this stage a local syntactic analysis is carried so as tosplit the text into maximal-length NPs. For example: alimentation en eau (watersupply), pompe d'extraction (extraction pump), alimentation electrique de lapompe de refoulement (electric supply of the forcing back supply). Here thesystem takes advantage of the negative knowledge about the parts of complexterms. Thus those patterns of a potential term -finite verbs, pronouns andconjunctions- that will never become part of a term are identified and consideredAUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS15as boundaries. Some of these patterns are simple whereas others are complex(sequences of preposition + determiner).A French example of the latter would be SUR (prep) + LE (definite article): themost common analysis is to propose that this sequence establishes a boundarybetween NPs as in: on raccorde le cable d'alimentation du banc sur le coffret dedecharge batterie. However there is a rate (10%) in which this sequence is partof the term: action sur le bouton poussoir de rearmement or action sur lesysteme d'alimentation de secoursTo solve this and other similar situations, the system uses an endogenouslearning strategy of the patterns sub-categorisation. This strategy consists inlooking at the corpus to find those sequences of (noun) + sur + le havingdifferent contexts on the right hand side. Then non-productive nouns are weededout. Then sequences such as sur + le are considered sentence boundaries, exceptfor those cases wherein sequences are preceded by the productive noun locatedin the learning phase. To see how this system works let us suppose that at a firstanalysis the sequences below are found.Le protection contreProtection contreil s'agit de maintenir la teneur en oxygene de cette eau danson procede a l'injection d'eau danson procede a l'injection d'eau dansle systeme permet l'aiguillage des automates surle gel est assuree parles grands froidsles limites fixeesles limites fixeesles generateurs de vapeurle prelevement effectueThen productive sequences are not regarded as term boundaries whereas non-productive sequences are viewed as external boundaries of the candidate term. Inthe example above protection contre and eau dans do not become boundarieswhereas automates sur does.This strategy permits to detect a considerable amount of complex nouns whichotherwise would have been lost. Unfortunately it also allows a great deal ofundesirable material (between 10% and 50%).3. Splitting module. NPs are analysed and their constituents are divided into headand expansion. For example the term candidate pompe d'extraction (extractionpump) is splitted into: pompe -head- (pump) + extraction -expansion-(extraction).At this point the system may find ambiguous situations such as "Noun Adj1Adj2" and "Noun1Prep Noun2Adj" whose analysis is uncertain. To solve thesecases an endogenous learning process is followed which is similar to thatpresented in the delimitation module.4. Structuring module. The list of term candidates is organised in aterminological network. This network can be produced only by looking at a listof candidate terms and recognising the different parts of each candidate term,like in the following example:16 Automatic Term Detection: a Review of Current SystemsE expansionE expansionN headelectricforcing backhead N'electric supplyE expansionE' expansionN headN headsupplyextractionelectric supply of theforcing back pumpforcing back pumpextraction pumppumpAdditionally Lexter calculate some productivity figures based on links typeoccurrences. These coefficients do not become filters, but are passed on to theterminologist as a piece of data so as to facilitate the evaluation of candidateterms.5. Navigation module. A consulting interface is built (called terminologicalhypertext) from the source corpus, the candidate term network and the above-mentioned coefficients and lists.Although LEXTER is exclusively based on linguistic techniques it produceshighly satisfying results and is currently used to exploit different corpora fromEDF and different research projects. Besides it has been proved helpful in: textindexation, hypertextual consulting of technical documentation, knowledgeacquisition and construction of terminological databases.LEXTER is also used as a terminology extractor in the terminologicalknowledge base designed by the Terminologie et Intelligence Artificielle(Terminology &amp; Artificial Intelligence) terminology group. SYCLADE (Habert,1996), a tool for word classification also makes use of LEXTER.EvaluationLEXTER was born in an industrial environment and from the very beginning itsought a robust, accurate and domain-independent tools. These objectives werebasically attained although mark-up and disambiguation errors weaken thecapacity of the system. Some scholars note that this system (like those whichmake use of symbolic techniques) produce a considerable amount of noise. Thusof a corpus of 200,000 words there are obtained 20,000 candidate terms which,after the validation stage, amount to 10,000. Also, Bourigault stresses the silenceproblem, which he estimated around 5% of the total valid terms. Like the vastmajority of systems, LEXTER only focuses on NPs since verbs are believed tobe term boundary and so they are never part of candidate terms.One of the most remarkable achievements of this system is the endogenouslearning mechanism that allow to work autonomously and so there is no need fora complex and large dictionary. In a similar vein it should be highlighted theusefulness of presenting the results hypertextually, since it facilitates theterminologist's task.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS172.7. NAULLEAUReference publication: Naulleau (1998)Main goal: Noun phrase filtering systemThe model designed by Naulleau is a NP extraction system that proposes as termcandidates those sequences that comply with certain user tailored profile. Thewhole process can be divided in two main stages: profile acquisition and profileapplication.To define its own profile the user chooses the set of phrases that s/he considersrelevant for his task and discards the ones that s/he does not consider useful atthat time. The data collected in such way is generalised according to theirmorphological, syntactical and semantic characteristics dynamically creating aset of positive and negative filters. A simple example of positive and negativefilters is the following:(1) positive filter: metallic/automatic/nuclear/industrial taps(2) negative filter: important/recent/necessary/unreliable tapsThen, those filters produced in the learning stage are applied to new sequencesanalysed. As a result, some noun arguments and/or PPs can be eliminated. Thus aNP can be divided or reduced and the resulting sequences are passed on to anexpert to be evaluated.In doing so the author acknowledges the sociolinguistic nature of the term. Itimplies that there is no linguistic model that can tell whether a NP is a term ornot beyond the scope of a field or even the application. Also, this procedureintroduces the idea of how relevant a phrase is in relation to the interest profile ofthe user and assumes that such relevance may be evaluated on linguistic grounds.This is a fully symbolic approach that uses the AlethIP engine that producessentences fully lemmatised, tagged and syntactically parsed. Then nouns andadjectives are semantically tagged according to both suffix information andsemantic data from AlethIP and using a set of contextual rules for the morefrequent and ambiguous words. The whole strategy is based on the evaluation ofthe relevance of simple syntactic dependencies. Such relevance is only based onthe data provided by the user.According to the author, the results are encouraging. However it is difficult toevaluate due to the practical problem posed by such a detailed evaluation. Someadditional experiments are described in (Nalleau, 1999).EvaluationThis system may be considered the first one to use semantic data as a specificresource for proposing term candidates. Also, as far as we know, is the first timesince the very beginning in the design of a TE systems that the user and the ideaof relevance to an application are taken into account.18 Automatic Term Detection: a Review of Current SystemsIn this way the user may adapt the system to its specific needs but also itsintervention may crucially affect the performance of the system. The loss ofspecific data makes difficult to evaluate the tool behaviour in an actual context.2.8. NEURALReference publication: Frantzi and Ananiadou (1995)Main goal: Term extractionNeural is a system for TE of a hybrid nature, that is it uses both linguistic(morphosyntactic patterns and a list of suffixes specific to the domain) andstatistically knowledge (frequency and mutual information). Frantzi &amp;Ananiadou (1995) pays special attention to two different problems: detection ofnested terms and detection of low frequency terms using statistical methods.The test bench is a corpus of 55,000 words in the domain of medicine(ophthalmology). The structures analysed are Noun-Noun and Adjective-Nounthat are identified using a standard tagger. The list of suffixes includes thosefrequently found in terminological units in the field of ophthalmology like -oid, -oma, -ium. The system is implemented using a Back-Propagation (BP) twolayers neural network. The threshold has been set to .5 but this may vary. The BPneural network has been trained with a set of 300 compounds and the tests weremade with another set of 300 words. It obtained a success rate of 70 %.The author and other scholars from the Manchester Metropolitan University havebeen active since 1995 developing specific statistical figures for TE. In this wayit is necessary to mention those tasks related to the adding of context information(Frantzi, 1997, Maynard &amp; Ananiadou, 1999). Usually the context is discardedor, alternatively, considered as a bag of words although its relevance is signalledby many scholars. Here the basic assumption is that terms tend to appeargrouped in real text, so the termhood figure of a candidate would increase ifthere are other terms (or candidates highly ranked) in the context.Both Frantzi, 1997 and Maynard &amp; Ananiadou, 1999 propose a similarity figurebased on the distance between the candidate and the context words (nouns,adjectives and verbs). This figure is calculated by Frantzi (1997) using statisticaland syntactic information while Maynard &amp; Ananiadou (1999) include alsosemantic information from a specialised thesaurus (UMLS semantic Network).In Maynard &amp; Ananiadou (1999) this similarity figure may also be used to takeinto account some kind of semantic disambiguation for the sense that gets abetter value. A context factor (CF) is added to the figure already used to rank thecandidates (Cvalue) and thus reordering the set of candidates as follows:SNCvalue(a) = 0.8*Cvalue(a) + 0.2*CF(a). The authors report improvements inthe ranking of term candidates from his eye pathology corpus.EvaluationAUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS19The original system can be seen as a standard hybrid system. The linguisticknowledge includes Greek and Latin affixes and morphosyntactic patterns. Theincorporation of this kind of suffixes should be highly productive. However thechosen patterns may well apply to English but not to Romance Languages.The incorporation of the context as part of the data available for evaluating thetermhood of a candidate is a very interesting contribution to the behaviour ofterms in real texts. It should also serve to increase the relevance of low frequencycandidates but no specific figure is given.It is necessary to mention the use of semantic information as a kind of resourcethat is increasingly used in the TE field.2.9. NODALIDA-95Reference publication: Arppe (1995)Main goal: Term extractionNODALIDA, a product designed by the Lingsoft firm, is based on an enhancedversion of NPtool that is a program developed at the Department of GeneralLinguistics at the Helsinki University (Finland). NPtool (Voutilanen 1993)generates lists of NPs occurring in the sentences of a text and provides anassessment about whether these phrases are candidates terms or not (ok/?). Fromthese lists all the acceptable sub-chains are obtained. Besides, the source list ismultiplied. Let us see an actual example, for the sentence: "exact form of thecorrect theory of quantum gravity" NPtool proposes the following additionalNPs:form of the correct theory of quantum gravity form correct theoryexact form of the correct theory exact form gravityform of the correct theory theory quantum gravitySimultaneously there are a number of premises that become the first filter like inthe following: "Those NPs preceded by a determiner, adjective or prefixedsentence (kind of, some, one, ...) are weeded out."As for the remaining NPs, their occurrence frequency is calculated. Further, theyare ordered and grouped according to their grammatical head and are presentedto the terminologist together with their context. The NPtool module (Voutilanen,1993) is at the heart of the system. It is a NP detector largely based on theconstraint grammar formalism (Karlsson, 1990). Its main features are: (1)Morphological/syntactical descriptions are based on a large set of hand-codedlinguistic rules, (2) both the grammar and the lexicon allow a corpus analysiswith non-controlled text and (3) disambiguation is made according to onlylinguistic criteria. As a result, between 3% and 6% of the words remainambiguous.20 Automatic Term Detection: a Review of Current SystemsThe text goes through a previous process so as to determine sentence boundaries,idiomatic expressions, compound forms, typographical signs, etc. Then it ismorphologically analysed and a result like this is obtained9:("&amp;lt;*the&amp;gt;" ("the" DET CENTRAL ART SG/PL (@&amp;gt;N)))("&amp;lt;inlet&amp;gt;" ("inlet" N NOM SG))("&amp;lt;and&amp;gt;" ("and" CC (@CC)))("&amp;lt;exhaust&amp;gt;" ("exhaust" &amp;lt;SVO&amp;gt; V SUBJUNCTIVE VFIN (@V))("exhaust" &amp;lt;SVO&amp;gt; V IMP VFIN (@V))("exhaust" &amp;lt;SVO&amp;gt; V INF)("exhaust" &amp;lt;SVO&amp;gt; V PRES -SG3 VFIN (@V))("exhaust" N NOM SG))("&amp;lt;manifold&amp;gt;" ("manifold" N NOM PL))At this moment disambiguation takes place. For example in the sentence: "Theinlet and exhaust manifolds are mounted on opposite sides of the cylinder head"two analyses are obtained:(1) on/@AH opposite/@N sides/@NH of/@N&amp;lt; the/@&amp;gt;N cylinder/@NH head/@V(2) on/@AH opposite/@N sides/@NH of/@N&amp;lt; the/@&amp;gt;N cylinder/@&amp;gt;N head/@NHWhat distinguishes these two analyses is the consideration of whether the finalsequence (cylinder head) is a NP or not. The ongoing process gives only twopossible analyses for each sentence. First, those NPs of a maximal length arepreferred (NP-friendly) and, second, those NPs of a minimal length are preferred(NP-hostile). Then the system compares both strategies and labels each NP asok/? by considering whether the analysis is shared or not by both strategiesThus the last sentence gets this analysis below:(3) ok : inlet and exhaust manifolds ?: opposite sides of the cylinderok: exhaust manifolds ?: opposite sides of the cylinder headIn order to validate this additional information the terminologist is provided witha list of candidate terms. The results reported by the NPtool module are prettygood (precision=95-98% and recall=98.5-100%) with a text of about 20 Kwords.EvaluationNODALIDA is based on the use of linguistic knowledge through a structuralapproach (i.e., detection of phrasal structures and structural disambiguation).Arppe (1995) presents high-quality results. However, the corpus should beenlarged, since so far tests have been made on quite small corpora. It is not clearhow precision and recall figures are calculated, particularly how to determinewhich terms are deemed to be correct (i.e., those which have the ok signal or allof them). Also it should be stressed that NODALIDA has not been tested usingthe NPtool enhanced version in an actual situation of terminology problems.9 The meaning of the syntactic function tags are: @&amp;gt;N = pre-modifier; @&amp;lt;N = post-modifier; @CCand @CS= coordination and subordination conjunction; @V = Verb; @NH = nominal head. Finally,"&amp;gt;" and "&amp;lt;" indicate the direction of the phrasal head.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS21Taking into account that the disambiguator is one of the main error sources inthis kind of systems, Arppe (1995) believes that a high-degree quality is achieveddespite the fact that there are no data about terminology extraction in realsituations. Besides, to achieve this quality NODALIDA proposes a great deal ofrules, which yields management and control overhead.The list that is passed on to the terminologist to be validated comprises thosecandidates signalled with ok and ?. The way in which potential NPs are obtainedby the system leads us to suspect that there are many candidate terms in thevalidation list that the terminologist has to analyse.2.10. TERMIGHTReference publication: Dagan and Church (1994)Main goal: Translation aidTermight is currently used by A&amp;T Business Translation Systems. It was createdto be a tool for automating some stages of the professional translatorterminological research.To do so it starts with a tagged and disambiguated text as well as a list ofpredetermined syntactic patterns that could be adjusted to every document. Thus,a list of candidate terms is obtained comprising one or more words. Single-wordcandidates are defined as all those words that are not included in a previouslydetermined list of empty words (i.e. stop list). Multiword terms are referred toone of the predetermined syntactic patterns via regular expressions. Dagan andChurch (1994) considered only noun sequences patterns.Candidate terms are grouped and classified according to their lemma (i.e. theright hand side noun) and frequency. Those candidates sharing the same lemmaare classified alphabetically in accordance with the inverse order of theircompounding words. Thus it is showed the order of changes of the Englishsimple NPs.For each candidate term the corresponding concordances are obtained, which arealphabetically classified according to their context. This information enables theterminologist to evaluate whether each candidate is appropriate or not.Dagan and Church (1994) note that the rate of term list construction is of 150and 200 terms per hour, which is twice faster than the average. As for theextraction quality, they state that, unlike exclusively statistical methods,Termight permits to extract low-frequency terms.Moreover this system has a bilingual module which, via statistical methods,obtains a word-level alignment from texts. Thus terms found in language A arereferred to their counterparts in language B. This well-ordered list of candidateterms is again passed on to the terminologist to be evaluated.22 Automatic Term Detection: a Review of Current SystemsThe Termight bilingual module does not seem to be developed and tested as thebasic one. Tests have been made on 192 terms from a technical manual inEnglish and German. The correct translation is found in the first suggestedsolution in 40% of the cases, whereas only 7% corresponds to the correcttranslation suggested in the second place. As for the remaining, the correcttranslation was in other places of the proposal list.EvaluationTermight is a remarkable system in that there is an accurate classification andpresentation of candidate terms and it does not attempt to become an automaticsystem. Rather, it helps the translator.However, it presents a number of shortcomings: (1) The only syntactic patternconsidered is very simple: noun sequences. This pattern may well be valid forEnglish but not for Romance languages and (2) no numerical information aboutthe recognition quality is given. The type of pattern considered may supposehigh precision but low recall2.11. TERMINOReference publication: Plante and Dumas (1989)Main goal: Facilitation of the term extraction terminographer's task.The TERMINO program is composed of several tools to facilitate TE in French.It is a help for the terminologist insofar as the identification of those discourseunits that denominate notions or objects. Besides it provides every unit with theimmediate context from which data relevant to the notions denominated bytheses units can be obtained. There are a number of TERMINO versions whichimprove in some ways previous ones.This tool is based mainly on linguistic knowledge and it comprises 3 sub-systems: a pre-editor, which separates texts into words and sentences andidentifies proper nouns, a morphosyntactic parser and a record-drafting facility.The text does not get any special treatment: it is only required to be codified inASCII form.With regard to term delimitation and extraction the more interesting sub-systemis the morphosyntactic parser. It consists of 3 modules: a morphological parser; asyntactic parser and a synapsy detector.The morphological parser has two functions: a) automatic categorisation; b)lemma and tag identification. According to Plante and Dumas, 30% of words inFrench can be attributed to more than one category. This has led to the tagging ofall the possible categories for each word. As a result, there is an overproductionof words with different tags. Categorisation and lemmatisation are obtained fromthe application of the LCML program, it is not a dictionary but a morphologicalparser of lexical forms so it can correctly lemmatise and tag new lexical forms.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS23The syntactic parser is responsible for weeding out the vast majority ofambiguities generated in previous stages. It is managed through the constructionof a syntactic structure for each sentence.Finally, the synapsy detector (MRSF) selects, among the syntactic units from theparser, those lexical noun units that are likely to be terms. S. David (David andPlante, 1991) created MRSF especially for TERMINO. MRSF is based onprinciples of noun group construction. David's understanding of synapsy is thatof a polylexical unit of a syntactic nature that is the head of the NP. Thus,synapsies are only NPs groups: some of them will become terms and some ofthem will not. Further, some of them will only be "topics" that will enable theterminologist to know different concepts or grasp an overview of the text topics.The MRSF module comprises 5 sub-modules: (1) head hunter module; (2)expansion recogniser module; (3) categorisation module; (4) synapsy generatormodule and (5) representation and evaluation module.TERMINO has a set of software tools, which is much larger and comprisesdifferent modules that allow to manipulate terminological data. These tools helpthe terminologist decide whether a synapsy is a term or not, elaborateterminological filing forms and create terminological databases.TERMINO recognises between 70% and 74% of the complex terms. The factthat 30% of terms are not recognised by TERMINO can be explained bycoordination (it is a signal of segment breaking), acronyms and common nounsin capital letters. Moreover, there is 28% of noise, of which 47% is due to awrong mark-up and a 53% is due to synapsies belonging to general language.EvaluationTERMINO is one of the first candidate term extractors that worked and it is alinguistically-based extractor, composed of different independent modules. Thissystem is based on the concept of synapsy. The synapsy detector is based on theestablishment of a number of heuristic rules that may well be increased providedthe corpus is delimited.There is a need to improve this system taking into account that it is still too noisy(28%), which could be improved, for example, with a different treatment ofcapital letters and acronyms.2.12. TERMSReference publication: Justeson and Katz (1995)Main goal: Term extractionJusteson and Katz (1995) hold the following views about terms:a) Terminological noun phrases (TNP) are different from non-terminologicalnoun phrases (nTNP) in that the modifiers of the first ones are much shorter thanthose of the second ones.24 Automatic Term Detection: a Review of Current Systemsb) An entity introduced by a nTNP can be later referred to only by the head of theNP and often by other NP (synonyms, hyponyms, hyperonyms). By contrast, anentity introduced by a TNP is normally repeated identically in a given document,as a single omission of a modifier could yield a change of the referred entity.c) In technical texts lexical NPs are almost exclusively terminological.d) Multiword technical terms are nearly always composed of nouns andadjectives (97%) and some prepositions (3%) between two NPs.e) The average length of a TNP is of 1.91 words.The proposed filter finds strings with a frequency equal or higher than two.These strings follow with this regular expression: ((A|N)+ | ((A|N)*(NP)?)(A|N)*N. Those candidate terms of a length of 2 (2 patterns: AN and NA)and 3 (5 patterns: AAN, ANN, NAN, NNN and NPN) are by far the mostcommonly encountered.The purpose of this algorithm is to combine good coverage of the usualterminology from technical texts with high quality in the extraction phase. Thealgorithm prefers quality to coverage, since if it only made use of thegrammatical constraints then the system would propose many irrelevant NPs.The vast majority of relevant NPs overcome the frequency constraint.Selection of grammatical patterns also affects quality. If prepositions areadmitted within the pattern many candidates are introduced, although few will bevalid. As a result, quality decreases whereas quantity increases and, accordingly,Justeson and Katz (1995) prefer not to take prepositions into consideration.The implementation of grammatical patterns also affects the quality/coveragetrade-off. There are two ways in which a given linguistic unit is attributed to agrammatical category: disambiguation and filtering. The first one is rejectedbecause disambiguators are not totally reliable yet.Filtering consist in parsing and lemmatising each word of the text. Then thosesequences following the pattern are considered. If a word is not identified as anoun, adjective or preposition, it is discarded. Thus each word maintains itsnominal, adjectival and prepositional values and in this order. The chain isweeded out if more than one word can be identified as a preposition or if it doesnot follow the pattern (e.g. if the pattern ends with a noun and there is more thanone preposition then the word following the preposition is not a noun).Filtering has a coverage at least as good as what can be attained by a standardtagger. However, quality is not that good (e.g. fixed is only identified as anadjective -bug fixed-, but it can also become a verb: fixed disk drive). Incontrast, filtering is much faster than parsing.In any case, Justeson and Katz (1995) suggest to control the patterns, the list ofgrammatical words and the frequency to adjust the performance of the system toeach type of text.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS25This system has been applied to different domains (metallurgy, spatialengineering and nuclear energy) and it is used at IBM Translation Center. TheTERMS results are presented on the basis of 3 technical texts (statisticalclassification of patterns, lexical semantics and chromatography). Coverage hasonly been estimated for one of the text and it is of 71%. Quality has beenestimated between 77% and 96% of the instances.EvaluationAlthough Justeson and Katz (1995) present a detailed study on the performanceof terminological units (wherein there are some overstatements), the proposedfilter does not seem to take advantage of these previous analyses of terms.Further, it should be noted that this type of filtering based on quite simplepatterns would not be so efficient if they were applied to languages other thanEnglish such as Romance languages. Also, this kind of patterns produces a lot ofnoise.3. Contrastive AnalysisHere we will contrast the systems' main features, according to six relevantaspects when designing a new detection system of terminological units:linguistic resources, strategies of term delimitation, strategies of term filtering,classification of recognised terms and obtained results. For some of these criteriawe have created a table containing the most significant data so as to make thesystem comparison easier.3.1. Linguistic resourcesIt has been observed that the vast majority of the reviewed systems make use ofsome sort of linguistic information, at least a list of empty words taken asboundaries. The standard process includes a morphological analysis followed bysome kind of disambiguation system. The systems altering this procedure are thefollowing:a. ANA: does not use any linguistic resource, just a list of auxiliary wordsb. TERMS: use its own disambiguation system: POS filteringc. Naulleau: introduces semantic informationAdditionally, for the systems that use an incremental strategy, like ANA andFastr, it is necessary a set of initial terms to bootstrap the process.3.2. Strategies of term delimitationAll systems of terminology extraction have to determine at some point thebeginning and the end of the candidate term, that is, delimit the potentialterminological unit. The reviewed programs have different strategies to delimit26 Automatic Term Detection: a Review of Current Systemsterms: word-boundary elements, structural patterns, syntactic parser, textdistribution, typographical elements, term lists, structure disambiguation. Belowwe show a summary of the different options adopted by each system:Table 1: Strategies of term telimitationSystem term delimitation structure disamb.Name /Author boundaries Patterns Parser Other learning Other1 ANA X -2 CLARIT X statistical3 Daille X -4 FASTR X X X -5 Heid X -6 LEXTER X X7 Naulleau X -8 NEURAL X -9 NODALIDA-95 X -10 Termight X -11 TERMINO X -12 TERMS X -3.3. Strategies of term filteringTerm filtering is a key stage of any term detection system. This means that thelist of candidates is reduced as much as possible. The following table shows thestrategies found in all the reviewed systems:Table 2: Strategies of term filteringSystem Term FilteringName /Author Freq.10 Linguisticstatistical +linguisticlinguistic +statisticalreferencetermsuserdefined1 ANA X2 CLARIT X X3 Daille X4 FASTR X5 Heid X6 LEXTER X7 Naulleau X8 NEURAL X9 NODALIDA-95 X10 Termight X X11 TERMINO X12 TERMS X X10 The technique of term filtering through frequency terms has been considered something in betweenthose methods based on linguistic knowledge and those methods based on extralinguistic knowledge.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS273.4. Classification of recognised termsSome of the analysed systems classify recognised terms by grouping themaccording to some criteria. Thus, the related terms stay close to each other. EvenFASTR attempts to infer an ontology from the recognised terms. Those systemswhich show some classification of recognised terms are the following:a. ANA: it builds a semantic network from the detected terms.b. FASTR: it builds a graph to relate recognised terms. Also it proposes theconstruction of partial ontologies for some terms.c. LEXTER: it builds a terminological network splitting terms into head andexpansion.3.5. ResultsThe table below summarises for each system the type of corpus used for the testsand the results attained:Table 3: ResultsSystem Test corpora Terms %Name /Author Domain Language Size.[Kw.] precision recall1 ANA Aviation engineeringAcousticsFrenchEnglish12025 ???2 CLARIT11 News English 240 Mb - 81.63 Daille Telecommunications French 800 ? ?4 FASTR Medicine (abstracts) French 1.560 86.7 74.95 Heid Engineering German 35 ? ?6 LEXTER Engineering French 3.250 95 ?7 Naulleau Technical French ? ? ?8 NEURAL Medicine English 55 ? 709 NODALIDA-95 CosmologyTechnical textEnglish 20 95-98 98.5-10010 Termight Computer science English ? ? ?11 TERMINO Medicine French ? 72 70-7412 TERMS StatisticsSemanticsChromatographyEnglish 2.36.314.9778696</body>
	<conclusion>We can reach some conclusions after having analysed and evaluated some of themain systems of TE designed in the last decade:11 The system has been intensively tested with regard to the indexing frequency, but not in relation tothe quality of the extracted terms.28 Automatic Term Detection: a Review of Current Systemsa) The efficiency of the extraction presents a high degree of variation from oneto another. Broadly speaking, there is neither clear nor measurable explanation ofthe final results. Besides, we have to bear in mind that these systems are testedwith small and highly specialised corpora. This lack of data makes it difficult toevaluate and compare them. However, it does not prevent pinpointing thosesolutions, which are considered valid to solve specific problems.b) None of the systems is entirely satisfactory due to two main reasons. First, allsystems produce too much silence, especially statistically-based systems.Second, all of them generate a great deal of noise, especially linguistically-basedsystems.c) Taking into account the noise generated, all systems propose large lists ofcandidate terms, which at the end of the process have to be manually accepted orrejected.d) Most of the TE systems are related to only one language: French or English.Usually the language specific data is embedded in the tool. This makes difficultto use the system in a language other than the original.e) As has been already pointed out, training corpora tend to be small (from 2.3 to12 Kwords) and highly specialised with regard to the topic as well as thespecialisation degree. This allows for a quite precise patterns and lexicosemantic,formal and morphosyntactic heuristics albeit this only applies to highlyspecialised corpora.f) All systems focus entirely on NPs and none of them deals with verbal phrases.This is because there is a high rate of terminological NPs in specialised texts.This rate can vary according to the topic and the specialisation degree. Despitewhat has just been noted, it is noteworthy that all specialised languages havetheir own verbs (or specific combinations of a verbal nature), no matter how lowthe ratio is in comparison with nouns.g) As a result, none of the systems refers to the distinction between nominalcollocations and nominal terminological units of a syntactic nature. Nor do theyrefer to phraseology.h) Many of the systems make use of a number of morphosyntactic patterns toidentify complex terms. However they account for most of the terminologicalunits they are still too few and also not very constraining. Thus, for English areAN and NN, for French NA and N prep N. Some terms present structures otherthan these ones and they are never detected. Those systems based only on thesetypes of linguistic techniques generate too much noise.i) It is generally agreed that frequency is a good criterion to indicate that acandidate term is actually a terminological unit. However, frequency is not on itsown a sufficient criterion, as it yields a great deal of noise.j) Only a few recent systems use semantic information to recognise and delimitterminological units although its use takes place at different levels.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS29k) None of the systems uses extensively the combinatory features of terms fromspecialised languages in relation to a given domain. It is needed more studiesabout the type of constraints that terminological units present with regard toconceptual field and text type.l) Only one of the analysed systems take profit of the possibilities given by thealignment of specialised text.i) Most of the authors consider the POS disambiguation as one of the mostimportant error sources. However, they do not provide exact figures about itsincidence degree.To improve these systems of terminology extraction and lessen the noise andsilence that are generated, two type of studies should be encouraged. First, it isrequired more linguistic oriented studies on the semantic relationships amongterms, the semantic relationships among constituents of a terminological unit,semantico-lexical representation, constraints of terminological units within agiven specialised domain and in a given text type, all the grammatical categoriesthat are likely to become terms in specialised domains, the influence of thesyntactic function of terminological phrases on texts, the relationships betweenterms and their arrangement in texts.Second, we should focus on software systems that: combine in a more activemanner statistical and linguistic methods; improve statistical measures; combinemore than one strategy; are easily applicable to more than one language; improveinterfaces to facilitate the machine-user interaction. Also it should be very useful,as suggested in Kageura et al. (1998), the development of a common test benchfor aiding the evaluation/comparison of extracting methods.In sum, should we progress in the field of automatic terminology extraction,statistical and linguistic methods have to actively be combined. It means thatthey are not either-or approaches but complementary ones. The final goal is toreduce the amount of silence and noise so that the process of terminologicalextraction becomes as automatic and precise as possible. In the future, webelieve that any current terminology extractor, apart from accounting for themorphological, syntactic and structural aspects of terminological units, has tonecessarily include semantic aspects if the efficiency of the system is to beimproved with regard to the existing ones.</conclusion>
	<discussion>N/A</discussion>
	<biblio>Arppe, A. 1995. "Term extraction from unrestricted text". Lingsoft Web Site:http://www.lingsoft.comAhmad, K., Davies, A., Fulford, H. and Rogers, M. 1992. "What is a term? Thesemiautomatic extraction of terms from text". Translation Studies - aninterdiscipline. Amsterdam: John Benjamins.Bourigault, D. 1994. LEXTER, un Logiciel d'EXtraction de TERminologie.Application a l'acquisition des connaissances a partir de textes. PhD Thesis.Paris: Ecole des Hautes Etudes en Sciences Sociales.Bourigault, D., Gonzalez-Mullier, I. and Gros, C. 1996. "LEXTER, a NaturalLanguage Processing Tool for Terminology Extraction". Proceedings of the7th EURALEX International Congress. Goteborg.Brown, P. F., Cocke, F., Pietra, S., Felihek. F., Merces, R. and Rossin, P. (1988)A statistical approach to language translation. Procedings of 12th InternationalConference of Computational Linguistic (Coling-88). Budapest, Hungary.Cabre, M.T. 1999. Terminology. Theory, methods and applications. Amsterdam:John Benjamins.Church, K. 1989. "Word association norms, mutual information andlexicography". Proceedings of the 27th annual meeting of the ACL. Vancouver,76-83.Condamines, A. 1995. "Terminology: new needs, new perspectives".Terminology, 2, 2: 219-238.Dagan, I. and Church, K. 1994. "Termight: Identifying and translating technicalterminology". Proceedings of the Fourth Conference on Applied NaturalLanguage Processing, 34-40.Daille, B. 1994. Approche mixte pour l'extraction de terminologie: statistiquelexicale et filtres linguistiques. PhD dissertation. Paris: Universite Paris VII.Daille, B. and Jacquemin, C 1998. "Lexical database and information access: afruitfull association?". First International Conference on LREC. Granada.AUTOMATIC TERM DETECTION: A REVIEW OF CURRENT SYSTEMS31David, S. and Plante, P. 1991. "Le progiciel TERMINO: de la necessite d'uneanalyse morphosyntaxique pour le depouillement terminologique des textes".Proceedings of the Montreal Colloquium Les industries de la langue:perspectives des annees 1990, 1: 71-88.Enguehard, C. and Pantera, L. 1994. "Automatic Natural Acquisition of aTerminology". Journal of Quantitative Linguistics, 2, 1: 27-32.Estopa, R. 1999. Extraccio de terminologia: elements per a la construccio d'unSEACUSE (Sistema d'extraccio automatica de candidats a unitats designificacio especialitzada). PhD thesis, Barcelona: Universitat Pompeu Fabra.Estopa, R. and Vivaldi, J. 1998. "Systemes de detection automatique de(candidats a) termes: vers une proposition integratrice". Actes des 7emesJournees ERLA-GLAT, Brest, 385-410Evans, D.A. and Zhai, C. 1996. "Noun-phrase Analysis in Unrestricted Text forinformation retrieval". Proceedings of ACL, Santa Cruz, University ofCalifornia, 17-24.Frantzi, K. and Ananiadou, S. 1995. Statistical measures for terminologicalextraction. Working paper of the Department of Computing of ManchesterMetropolitan University.Frantzi, K. T. 1997. "Incorporating context information for extraction of terms".Proceedings of ACL/EACL, Madrid, 501-503.Habert, B., Naulleau, E. and Nazarenko, A. 1996. "Symbolic word clustering formedium-size corpora". Proceedings of Coling'96: 490-495.Heid, U., Jauss, S., Kruger, K. and Hohmann, A. 1996. "Term extraction withstandard tools for corpus exploration. Experience from German". In: TKE `96:Terminology and Knowledge Engineering,, 139-150. Berlin: Indeks Verlag.Jacquemin, C. 1994. "Recycling Terms into a Partial Parser". Proceedings ofANLP'94, 113-118.Jacquemin, C. 1999. "Syntagmatic and paradigmatic representations of termvariation". Proceedings of ACL'99, University of Maryland, 341-348.Jacquin, C. and Liscouet, M. 1996. "Terminology extraction from texts corpora:application to document keeping via Internet". In: TKE `96: Terminology andKnowledge Engineering, 74-83. Berlin: Indeks Verlag.Justeson, J. and Katz, S. 1995. "Technical terminology: some linguisticproperties and an algorithm for identification in text". Natural LanguageEngineering, 1, 1: 9-27.Kageura, K. and Umino, B. 1996. "Methods of Automatic Term Recognition".Papers of the National Center for Science Information Systems, 1-22.32 Automatic Term Detection: a Review of Current SystemsKageura, K., Yoshioka, M., Koyama, T. and Nozue, T. 1998. "Towards acommon testbed for corpus-based computational terminology". Proceedings ofComputerm `98, Montreal, 81-85.Karlsson, F. 1990. "Constraint grammar as a framework for parsing runningtext". Proceedings of the 13th International conference on computationallinguistic, 3: 168-173.Lauriston, A. 1994. "Automatic recognition of complex terms: Problems and theTERMINO solution". Terminology, 1, 1: 147-170.Maynard, D. and Ananiadou, S. 1999. "Identifying contextual information formulti-word term extraction". In: TKE `99: Terminology and KnowledgeEngineering, 212-221. Vienna: TermNet.Nakagawa, H. and Mori , T. 1998. "Nested collocation and Compound Noun forTerm Extraction". Proceedings of Computerm '98, Montreal, 64-70.Naulleau, E 1998. Apprentissage et filtrage syntaxico-semantique de syntagmesnominaux pertinents pour la recherche documentaire. PhD thesis. Paris:Universite Paris 13.Naulleau, E. 1999. "Profile-guided terminology extraction". In: TKE`99:Terminology and Knowledge Engineering. 222-240. Vienna: TermNet.Plante, P. and Dumas, L. 1998. "Le Depoulliment terminologique assiste parordinateur". Terminogramme, 46, 24-28.Shieber, S.N. 1986. "An Introduction to Unification-Based Approaches togrammar". CSLI Lecture Notes of University Press, 4.Smadja, F. 1991. Extracting collocations from text. An application : languagegeneration. Columbia: Columbia University. Department of ComputerScience. [Unpublished doctoral dissertation]Voutilainen, A. 1993. "NPtool, a detector of English noun phrases". Proceedingsof the Workshop on Very Large Corpora.Zhai, C., Tong, X., Milic-Frayling, N. and Evans, D.A. 1996. "Evaluation ofsyntactic phrase indexing CLARIT. NLP track report". Proceedings of theTREC-5. TREC Web Site: http://trec.nist.gov/pubs/trec5/t5_proceedings.html</biblio>
</article>