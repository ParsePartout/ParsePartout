<article>
	<preamble>BLESS.pdf</preamble>
	<titre>How we BLESSed distributional semantic evaluation</titre>
	<auteurs>
		<auteur>
			<name>Marco Baroni </name>
			<mail>marco.baroni@unitn.it</mail>
            <affiliation>University of Trento</affiliation>
      </auteur>
		<auteur>
			<name>Alessandro Lenci </name>
			<mail>alessandro.lenci@ling.unipi.it</mail>
            <affiliation>University of Pisa</affiliation>
      </auteur>
	</auteurs>
	<abstract>We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models.</abstract>
	<introduction>In NLP, it is customary to distinguish between in-trinsic evaluations, testing a system in itself, andextrinsic evaluations, measuring its performance insome task or application (Sparck Jones and Galliers,1996). For instance, the intrinsic evaluation of a de-pendency parser will measure its accuracy in identi-fying specific syntactic relations, while its extrinsicevaluation will focus on the impact of the parser ontasks such as question answering or machine trans-lation. Current approaches to the evaluation of Dis-tributional Semantic Models (DSMs, also knownas semantic spaces, vector-space models, etc.; seeTurney and Pantel (2010) for a survey) are task-oriented. Model performance is evaluated in "se-mantic tasks", such as detecting synonyms, recog-nizing analogies, modeling verb selectional prefer-ences, ranking paraphrases, etc. Measuring the per-formance of DSMs on such tasks represents an in-direct test of their ability to capture lexical mean-ing. The task-oriented benchmarks adopted in dis-tributional semantics have not specifically been de-signed to evaluate DSMs. For instance, the widelyused TOEFL synonym detection task was designedto test the learners' proficiency in English as a sec-ond language, and not to investigate the structure oftheir semantic representations (cf. Section 2).To gain a real insight into the abilities of DSMs toaddress lexical semantics, existing benchmarks mustbe complemented with a more intrinsically orientedapproach, to perform direct tests on the specific as-pects of lexical knowledge captured by the models.In order to achieve this goal, three conditions mustbe met: (i) to single out the particular aspects ofmeaning that we want to focus on in the evaluationof DSMs; (ii) to design a data set that is able to ex-plicitly and reliably encode the target semantic infor-mation; (iii) to specify the evaluation criteria of thesystem performance on the data set, in order to getan estimate of the intrinsic ability of DSMs to copewith the selected semantic aspects. In this paper, weaddress these three conditions by presenting BLESS(Baroni and Lenci Evaluation of Semantic Spaces),a new data set specifically geared towards the in-trinsic evaluation of DSMs, downloadable from:http://clic.cimec.unitn.it/distsem.</introduction>
	<body>2 Distributional semantics benchmarksThere are several benchmarks that have been widelyadopted for the evaluation of DSMs, all of them cap-turing interesting challenges a DSM should meet.We briefly review here some commonly used andrepresentative benchmarks, and discuss why we felt1the need to add BLESS to the set. We notice at theoutset of this discussion that we want to carve out aspace for BLESS, and not to detract from the impor-tance and usefulness of other data sets. We furtherremark that we focus on data sets that, like BLESS,are monolingual English and, while task-oriented,not aimed at a specific application setting (such asmachine translation or ontology population).Probably the most commonly used benchmark indistributional semantics is the TOEFL synonym de-tection task introduced to computational linguis-tics by Landauer and Dumais (1997). It consists of80 multiple-choice questions, each made of a targetword (a noun, verb, adjective or adverb) and 4 re-sponse words, 1 of them a synonym of the target.For example, given the target levied, the matchedwords are imposed, believed, requested, correlated,the first one being the correct choice. The task fora system is then to pick the true synonym amongthe responses. The TOEFL task focuses on a singlesemantic relation, namely synonymy. Synonymy isactually not a common semantic relation and one ofthe hardest to define, to the point that many lexi-cal semanticists have concluded that true synonymydoes not exist (Cruse, 1986). Just looking at a fewexamples of synonym pairs from the TOEFL set willillustrate the problem: discrepancy/difference, pro-lific/productive, percentage/proportion, to market/tosell, color/hue. Moreover, the criteria adopted tochoose the distractors (probably motivated by thelanguage proficiency testing purposes of TOEFL)are not known. By looking at the set, it is hardto discern a coherent pattern. In certain cases, thedistractors are semantically close to the target word(volume, sample and profit for percentage), whereasin other cases they are not (home, trail, and song forannals). It it thus not clear whether we are asking themodels to distinguish a semantically related word(the synonym) from random elements, or a moretightly related word (the synonym, again) from otherrelated words. The TOEFL task, finally, is based ona discrete choice (either you get the right word, oryou don't), with the result that evaluation is "quan-tized", leading to large accuracy gains for small ac-tual differences (one model that guesses one moresynonym right than another gets 1.25% more pointsin percentage accuracy).The WordSim 353 data set (Finkelstein et al.,2002) is a widely used example of semantic simi-larity rating set (see also Rubenstein and Goode-nough (1965) and Miller and Charles (1991)). Sub-jects were asked to rate a set of 353 word pairs on a"similarity" scale and average ratings for each pairwere computed. Models are then evaluated in termsof correlation of their similarity scores with aver-age ratings across pairs. From the point of viewof assessing the performance of a DSM, the Word-Sim (and related) similarity ratings are a mixed bag,in two senses. First, the data set contains a vari-ety of different semantic relations. In a recent se-mantic annotation of the WordSim performed byAgirre et al. (2009) we find that, among the 174pairs with above-median score (and thus presum-ably related), there is 1 identical pair, 17 synonympairs, 28 hyper-/hyponym pairs, 30 coordinate pairs,6 holo-/meronym pairs and 92 (more than half) pairsthat are "topically related, but none of the above".Second, the scores are a mixture of intuitions aboutwhich of these relations are more semantically tightand intuitions about more or less connected pairswithin each of the relations. For example, amongthe top-rated scores we find synonyms such as jour-ney/voyage and coordinate concepts (king/queen).If we look at the relations characterizing pairsaround the median rating, we find both less "per-fect" synonyms (monk/brother, that are synonymousonly under an unusual sense of brother) and lessclose coordinates (skin/eye), as well as pairs in-stantiating other, less taxonomically tight relations,such as many syntagmatically connected items (fam-ily/planning, disaster/area, bread/butter). Appar-ently, a single scale is merging intuitions about se-mantic similarity of specific pairs and semantic sim-ilarity of different relations.A perhaps more principled way to evaluate DSMsthat has recently gained some popularity is the con-cept categorization task, where a DSM has to clus-ter a set of nouns expressing basic-level conceptsinto gold standard categories. A particularly care-fully constructed example is the Almuhareb-Poesio(AP) set of 402 concepts introduced in Almuhareb(2006). Concept categorization sets also include theBattig (Baroni et al., 2010) and ESSLLI 2008 (Ba-roni et al., 2008) lists. The AP concepts must beclustered into 21 classes, each represented by be-tween 13 and 21 nouns. Examples include the ve-2hicle class (helicopter, motorcycle. . . ), the motiva-tion class (ethics, incitement, . . . ), and the socialunit class (platoon, branch). The concepts are bal-anced in terms of frequency and ambiguity, so that,e.g., the tree class contains a common concept suchas pine but also the casuarina tree, as well as thesamba tree, that is not only an ambiguous term, butone where the non-arboreal sense dominates.Concept categorization data sets, while interest-ing to simulate one of the basic aspects of humancognition, are limited to one kind of semantic re-lation (discovering coordinates). More importantly,the quality of the results will depend not only on theunderlying DSMs, but also on the clustering algo-rithm being used (and on how this interacts with theoverall structure of the DSM), thus making it hardto interpret the performance of DSMs. The forced"hard" category choice is also problematic, and ex-aggerates performance differences between modelsespecially in the presence of ambiguous terms (amodel that puts samba in the occasion class withdance and ball might be penalized as much as amodel that puts it in the monetary currency class).A more general issue with all benchmarks is thattasks are based on comparing a single quality scorefor each considered model (accuracy for TOEFL,correlation for WordSim, a clustering quality mea-sure for AP, etc.). This gives little insight into howand why the models differ. Moreover, there is nowell-established statistical procedure to assess sig-nificance of differences for most commonly usedmeasures. Finally, either because the data sets werenot originally intended as standard benchmarks, oreven on purpose, they all are likely to cause coverageproblems even for DSMs trained on very large cor-pora. Think of the presence of extremely rare nounslike casuarina in AP, of proper nouns in WordSim (itis not clear to us that DSMs are adequate semanticmodels for referring expressions - at the very leastthey should not be mixed up lightly with commonnouns), or multi-word expressions in other data sets.3 How we intend to BLESS distributionalsemantic evaluationDSMs measure the distributional similarity betweenwords, under the assumption that proximity in distri-butional space models semantic relatedness, includ-ing, as a special case, semantic similarity (Budanit-sky and Hirst, 2006). However, semantically relatedwords in turn differ for the type of relation hold-ing between them: e.g., dog is strongly related toboth animal and tail, but with different types of re-lations. Therefore, evaluating the intrinsic ability ofDSMs to represent the semantic space of a word en-tails both (i) determining to what extent words closein semantic space are actually semantically related,and (ii) analyzing, among related words, which typeof semantic relation they tend to instantiate. Twomodels can be equally very good in identifying se-mantically related words, while greatly differing forthe type of related pairs they favor.The BLESS data set complies with both theseconstraints. The set is populated with tuples ex-pressing a relation between a target concept (hence-forth referred to as concept) and a relatum concept(henceforth referred to as relatum). For instance, inthe BLESS tuple coyote-hyper-animal, the conceptcoyote is linked to the relatum animal via the hy-pernymy relation (the relatum is a hypernym of theconcept). BLESS focuses on a coherent set of basic-level nominal concrete concepts and a small but ex-plicit set of semantic relations, each instantiated bymultiple relata. Depending on the type of relation,relata can be nouns, verbs or adjectives. Moreover,BLESS also contains, for each concept, a number ofrandom "relatum" words that are not semanticallyrelated to the concept. Thus, it also allows to evalu-ate a model in terms of its ability to harvest relatedwords given a concept (by comparing true and ran-dom relata), and to identify specific types of relata,both in terms of semantic relation and part of speech.A data set intending to represent a gold standardfor evaluation should include tests items that are aslittle controversial as possible. The choice of re-stricting BLESS to concrete concepts is motivatedby the fact that they are by far the most studied ones,and there is better agreement about the relations thatcharacterize them (Murphy, 2002; Rogers and Mc-Clelland, 2004).As for the types of relation to include, we arefaced with a dilemma. On the one hand, there iswide evidence that taxonomic relations, the best un-derstood type, only represent a tiny portion of therich spectrum covered by semantic relatedness. Onthe other hand, most of these wider semantic rela-3tions are also highly controversial, and may easilylead to questionable classifications. For instance,concepts are related to events, but often it is not clearhow to distinguish the events expressing a typicalfunction of nominal concepts (e.g., car and trans-port), from those events that are also strongly re-lated to them but without representing their typicalfunction sensu stricto (e.g., car and fix). As will beshown in Section 4, the BLESS data set tries to over-come this dilemma by attempting a difficult com-promise: Semantic relations are not limited to tax-onomic types and also include attributes and eventsstrongly related to a concept, but in these cases wehave resorted to underspecification, rather than com-mitting ourselves to questionable granular relations.BLESS strives to capture those differences andsimilarities among DSMs that do not depend oncoverage, processing choices or lexical preferences.BLESS has been constructed using a publicly avail-able collection of corpora for reference (see Section4.4 below), which means that anybody can train aDSM on the same data and be sure to have perfectcoverage (but this is not strictly necessary). For eachconcept and relation, we pick a variety of relata (seenext section) in order to abstract away from inciden-tal gaps of models or different lexical/topical prefer-ences. For example, the concept robin has 7 hyper-nyms including the very general and non-technicalanimal and bird and the more specific and techni-cal passerine. A model more geared toward techni-cal terminology might assign a high similarity scoreto the latter, whereas a commonsense-knowledge-oriented DSM might pick bird. Both models havecaptured similarity with a hypernym, and we haveno reason, in general semantic terms, to penalize oneor the other. To maximize coverage, we also makesure that, for each concept and relation, a reason-able number of relata are frequently attested in ourreference corpora (see statistics below), we only in-clude single-word relata and, where appropriate, weinclude multiple forms for the same relatum (bothsock and socks as coordinates of scarf - as discussedin Section 4.1, we avoided similar ambiguous itemsas target concepts).Currently, distributional models for attributionalsimilarity and relational similarity (Turney, 2006)are tested on different data sets, e.g., TOEFL andSAT respectively (briefly, attributional similaritypertains to similarity between a pair of concepts interms of shared properties, whereas relational sim-ilarity measures the similarity of the relations in-stantiated by couples of concept pairs). Conversely,BLESS is not biased towards any particular type ofsemantic similarity and thus allows both families ofmodels to be evaluated on the same data set. Givena concept, we can analyze the types of relata that areselected by a model as more attributionally similarto the target. Alternatively, given a concept-relatumpair instantiating a specific semantic relation (e.g.,hypernymy) we can evaluate a model ability to iden-tify analogically similar pairs, i.e., others concept-relatum pairs instantiating the same relation (we donot illustrate this possibility here).Finally, by collecting distributions of 200 similar-ity values for each relation, BLESS allows reliablestatistical testing of the significance of differencesin similarity within a DSM (for example, using theprocedure we present in Section 5 below), as wellas across DSMs (for example, via a linear/ANOVAmodel with relations and DSMs as factors - not il-lustrated here).4 Construction4.1 ConceptsBLESS includes 200 distinct English concretenouns as target concepts, equally divided be-tween living and non-living entities. Conceptshave been grouped into 17 broader classes: AM-PHIBIAN REPTILE (including amphibians and rep-tiles: alligator), APPLIANCE (toaster), BIRD(crow), BUILDING (cottage), CLOTHING (sweater),CONTAINER (bottle), FRUIT (banana), FURNI-TURE (chair), GROUND MAMMAL (beaver), IN-SECT (cockroach), MUSICAL INSTRUMENT (vio-lin), TOOL (i.e., manipulable tools or devices: ham-mer), TREE (birch), VEGETABLE (cabbage), VEHI-CLE (bus), WATER ANIMAL (including fish and seamammals: herring), WEAPON (dagger).All 200 BLESS concepts are single-word nounsin the singular form (we avoided concepts such assocks whose surface form might change dependingon lemmatization choices). The major source weused to select the concepts were the McRae Norms(McRae et al., 2005), a collection of living and non-living basic-level concepts described by 725 sub-4jects with semantic features, each tagged with itsproperty type. As further constraints guiding ourselection, we wanted concepts with a reasonablyhigh frequency (cf. Section 4.4), we avoided am-biguous or highly polysemous concepts and we bal-anced inter- and intra-class composition. Classes in-clude both prototypical and atypical instances (e.g.,robin and penguin for BIRD), and have a wide spec-trum of internal variation (e.g., the class VEHICLEcontains wheeled, air and sea vehicles). 175 BLESSconcepts are attested in the McRae Norms, while theremnants were selected by the authors according tothe above constraints. The average number of con-cepts per class is 11.76 (median 11; min. 5 AMPHIB-IAN REPTILE; max. 21 GROUND MAMMAL).4.2 RelationsFor each concept noun, BLESS includes severalrelatum words, linked to the concept by one ofthe following 5 relations. COORD: the relatumis a noun that is a co-hyponym (coordinate) ofthe concept, i.e., they belong to the same (nar-rowly or broadly defined) semantic class: alligator-coord-lizard; HYPER: the relatum is a noun thatis a hypernym of the concept: alligator-hyper-animal; MERO: the relatum is a noun referringto a part/component/organ/member of the concept,or something that the concept contains or is madeof: alligator-mero-mouth; ATTRI: the relatum isan adjective expressing an attribute of the concept:alligator-attri-aquatic; EVENT: the relatum is averb referring to an action/activity/happening/eventthe concept is involved in or is performed by/withthe concept: alligator-event-swim. BLESS alsoincludes the relations RAN.N, RAN.J and RAN.V,which relate the target concepts to control tupleswith random noun, adjective and verb relata, respec-tively.The BLESS relations cover a wide spectrum ofinformation useful to describe a target concept andto qualify the notion of semantic relatedness: taxo-nomically related entities (hyper and coord), typicalattributes (attri), components (mero), and associatedevents (event). However, except for hyper and co-ord (corresponding to the standard relations of classinclusion and co-hyponymy respectively), the otherBLESS relations are highly underspecified. For in-stance, mero corresponds to a very broad notion ofmeronymy, including not only parts (dog-tail), butalso the material (table-wood) as well as the mem-bers (hospital-patient) of the entity the target con-cept refers to (Winston et al., 1987); event is used torepresent the behaviors of animals (dog-bark), typi-cal functions of instruments (violin-play), and eventsthat are simply associated with the target concept(car-park); attri captures a large range of attributes,from physical (elephant-big) to evaluative ones (car-expensive). As we said in section 3, we did not at-tempt to further specify these relations to avoid anycommitment to controversial ontologies of propertytypes. Note that we exclude synonymy both becauseof the inherent problems in this very notion (Cruse,1986), and because it is impossible to find convinc-ing synonyms for 200 concrete concepts.In BLESS, we have adopted the simplifying as-sumption that each relation type has relata belongingto the same part of speech: nouns for hyper, coordand mero, verbs for event, and adjectives for attri.Therefore, we abstract away from the fact that thesame semantic relation can be realized with differentparts of speech, e.g., a related event can be expressedby a verb (transport) or by a noun (transportation).4.3 RelataThe relata of the non-random relations are Englishnouns, verbs and adjectives selected and validatedby both authors using two types of sources: se-mantic sources (the McRae Norms (McRae et al.,2005), WordNet (Fellbaum, 1998) and ConceptNet(Liu and Singh, 2004)) and text sources (Wikipediaand the Web-derived ukWaC corpus, see Section 4.4below). These resources greatly differ in dimension,origin and content and therefore provide comple-mentary views on relata. Their relative contributionto BLESS also depends on the type of relation andthe target concept. For instance, the rich taxonomicstructure of WordNet has been the main source of in-formation for many technical hypernyms (e.g. gym-nosperm, oscine), which instead are missing frommore commonsense-oriented resources such as theMcRae Norms and ConceptNet. Meronyms arerarer in WordNet, and were collected mainly fromthe latter two resources, with many technical terms(e.g., parts of ships, weapons) harvested from theWikipedia entries for the target concepts.Attributes and events were collected from McRae5Norms, ConceptNet and ukWaC. In the McRaeNorms, the number of features per concept is fairlylimited, but they correspond to highly distinctive,prototypical and cognitively salient properties. Con-ceptNet instead provides a much wider array of as-sociated events and attributes that are part of ourcommonsense knowledge about the target concepts(e.g., the events park, steal and break, etc. for car).ConceptNet relations such as Created by, Used for,Capable of etc. have been analyzed to identify po-tential event relata, while the Has property relationhas been inspected to look for attributes. The mostsalient adjectival and verbal collocates of the tar-get nouns in the ukWaC corpus were also used toidentify associated attributes and events. For in-stance, the target concept elephant is not attested inthe McRae Norms and has few properties in Con-ceptNet. Thus, many of its related events have beenharvested from ukWaC. They include verbs such ashunt, kill, etc. which are quite salient and frequentwith respect to elephants, although they can hardlybe defined as prototypical properties of this animal.As a result of the combined use of such differenttypes of sources, the BLESS relata are representativeof a wide spectrum of semantic information aboutthe target concepts: they include domain-specificterms side by side to commonsense ones, very dis-tinctive features of a concept (e.g., hoot for owl)together with attributes and events that are insteadshared by a whole class of concepts (e.g., all animalshave relata such as eat, feed, and live), prototypicalfeatures as well as events and attributes that are sta-tistically salient for the target, etc.In many cases, the concept properties containedin semantic sources are expressed with phrases, e.g.,lay eggs, eat grass, live in Africa, etc. We decided,however, to keep only single-word relata in BLESS,because DSMs are typically populated with singlewords, and, when they are not, they differ in thekinds of multi-word elements they store. There-fore, phrasal relata have always been reduced totheir head: a verb for properties expressed by a verbphrase, and a noun for properties expressed by anoun phrase. For instance, from the property layeggs, we derived the event relatum lay.To extract the random relata, we adopted the fol-lowing procedure. For each relatum that instantiatesa true relation with the concept, we also randomlypicked from our combined corpus (cf. Section 4.4)another lemma with the same part of speech, andfrequency within 1 absolute logarithmic unit fromthe frequency of the corresponding true relatum.Since picking a random term does not guaranteethat it will not be related to the concept, we filteredthe extracted list by crowdsourcing, using the Ama-zon Mechanical Turk via the CrowdFlower interface(CF).1 We presented CF workers with the list ofabout 15K concept+random-term pairs selected withthe procedure we just described, plus a manuallychecked validation set (a "gold set" in CF terminol-ogy) comprised of 500 concept+true-relatum pairsand 500 concept+random-term pairs (these elementsare used by CF to determine the reliability of work-ers, and discard the ratings of unreliable ones), plus afurther set of 1.5K manually checked concept+true-relatum pairs to make the random-true distributionless skewed. The workers' task was, for each pair,to check a YES radio button if they thought there isa relation between the words, NO otherwise. Thewords were annotated with their part of speech, andworkers were instructed to pay attention to this in-formation when making their choices. Extensivecommented examples of both related pairs and un-related ones were also provided in the instructionpage. A minimum of 2 CF workers rated each pair,and, conservatively, we preserved only those items(about 12K) that were unanimously rated as unre-lated to their concept by the judges. See Table 1 forsummary statistics about the preserved random sets(nouns: RAND.N, adjectives: RAN.J, verbs:RAN.V).4.4 BLESS statisticsFor frequency information, we rely on the combi-nation of the freely available ukWaC and Wackype-dia corpora (size: 1.915B and 820M tokens, respec-tively).2 The data set contains 200 concepts thathave a mean corpus frequency of 53K occurrences(min. 1416 chisel, max. 793K car). The relata ofthese concepts (26,554 in total) are distributed as re-ported in Table 1.Note that the distributions reflect certain "natural"differences between relations (hypernyms tend to bemore frequent words than coordinates, but there are1http://crowdflower.com/2http://wacky.sslmit.unibo.it/6frequency cardinalityrelation min avg max min avg maxCOORD 0 37K 1.7M 6 17.1 35HYPER 31 138K 1.9M 2 6.7 15MERO 0 133K 2M 2 14.7 53ATTRI 0 501K 3.7M 4 13.6 27EVENT 0 517K 5.4M 6 19.1 40RAN.N 0 92K 2.4M 16 32.9 67RAN.J 1 472K 4.5M 3 10.9 24RAN.V 1 508K 7.7M 4 16.3 34Table 1: Distribution (minimum, mean and maximum) ofthe relata of all BLESS concepts: the frequency columnsreport summary statistics for corpus counts across relatainstantiating a relation; the cardinality columns reportsummary statistics for number of relata instantiating arelation across the 200 concepts, only considering relatawith corpus frequency  100.more coordinates than hypernyms, etc.). Instead oftrying to artificially control for these differences, weassess their impact in Section 5 by looking at thebehavior of baselines that exploit the frequency andcardinality of relations as proxies to semantic simi-larity (such factors could also be entered as regres-sors in a linear model).5 EvaluationThis section illustrates one possible way to useBLESS to explore and evaluate DSMs. Given thesimilarity scores provided by a model for a conceptwith all its relata across all relations, we pick the re-latum with the highest score (nearest neighbour) foreach relation (see discussion in Section 3 above onwhy we allow models to pick their favorite from aset of relata instantiating the same relation). In thisway, for each of the 200 BLESS concepts, we obtain8 similarity scores, one per relation. In order to fac-tor out concept-specific effects that might add to theoverall score variance (for example, a frequent con-cept might have a denser neighborhood than a rarerone, and consequently the nearest relatum scores ofthe former are trivially higher than those of the lat-ter), we transform the 8 similarity scores of eachconcept onto standardized z scores (mean: 0; s.d: 1)by subtracting from each their mean, and dividing bytheir standard deviation. After this transformation,we produce a boxplot summarizing the distributionof scores per relation across the 200 concepts (i.e.,each box of the plot summarizes the distribution ofthe 200 standardized scores picked for each rela-tion). Our boxplots (see examples in Fig. 1 below)display the median of a distribution as a thick hori-zontal line within a box extending from the first tothe third quartile, with whiskers covering 1.5 of theinterquartile range in each direction from the box,and values outside this extended range - extremeoutliers - plotted as circles (these are the defaultboxplotting option of the R statistical package).3While the boxplots are extremely informative aboutthe relation types that are best captured by models,we expect some degree of overlap among the distri-butions of different relations, and in such cases wemight want to ask whether a certain model assignssignificantly higher scores to one relation rather thananother (for example, to coordinates rather than ran-dom nouns). It is difficult to decide a priori whichpairwise statistical comparisons will be interesting.We thus take a conservative approach in which weperform all pairwise comparisons using the TukeyHonestly Significant Difference test, that is simi-lar to the standard t test, but accounts for the greaterlikelihood of Type I errors when multiple compar-isons are performed (Abdi and Williams, 2010). Weonly report the Tukey test results for those com-parisons that are of interest in the analysis of theboxplots, using the standard  = 0.05 significancethreshold.5.1 ModelsOccurrence and co-occurrence statistics for all mod-els are extracted from the combined ukWaC andWackypedia corpora (see Section 4.4 above). We ex-ploit the automated morphosyntactic annotation ofthe corpora by building our DSMs out of lemmas(instead of inflected words), and relying on part ofspeech information.Baselines. The RelatumFrequency baseline usesthe frequency of occurrence of a relatum as a sur-rogate of its cosine with the concept. With this ap-proach, we want to verify that the unequal frequencydistribution across relations (see Table 1 above) isnot trivially sufficient to differentiate relation classesin a semantically interesting way. For our secondbaseline, we assign a random number as cosine sur-3http://www.r-project.org/7rogate to each relatum (to smooth these random val-ues, we generate them by first sampling, for eachrelatum, 10K random variates from a uniform distri-bution, and then averaging them). If the set of relatainstantiating a certain relation is larger, it is morelikely that it will contain the highest random value.Thus, this RelationCardinality baseline will favorrelations that tend to have large relata set across con-cepts, controlling for effects due to different cardi-nalities across semantic relations (again, see Table 1above).DSMs. We choose a few ways to construct DSMsfor illustrative purposes only. All the models containvector representations for the same words, namely,approximately, the top 20K most frequent nouns, 5Kmost frequent adjectives and 5K most frequent verbsin the combined corpora. All the models use LocalMutual Information (Evert, 2005; Baroni and Lenci,2010) to weight raw co-occurrence counts (this asso-ciation measure is obtained by multiplying the rawcount by Pointwise Mutual Information, and it is aclose approximation to the Log-Likelihood Ratio).Three DSMs are based on counting co-occurrenceswith collocates within a window of fixed width,in the tradition of HAL (Lund and Burgess, 1996)and many later models. The ContentWindow2model records sentence-internal co-occurrence withthe nearest 2 content words to the left and rightof each target concept (the same 30K target nouns,verbs and adjectives are also employed as contextcontent words). ContentWindow20 is like Con-tentWindow2, but considers a larger window of 20words to the left and right of the target. AllWin-dow2 adopts the same window of ContentWindow2,but considers all co-occurrences, not only those withcontent words. The Document model, finally, isbased on a (Local-Mutual-Information transformed)word-by-document matrix, recording the distribu-tion of the 30K target words across the documents inthe concatenated corpus. This DSM is thus akin totraditional Latent Semantic Analysis (Landauer andDumais, 1997), without dimensionality reduction.The content-window-based models have, by con-struction, about 30K dimensions. The other modelsare much larger, and for practical reasons we onlykeep 1 million dimensions (those that account, cu-mulatively, for the largest proportion of the overallLocal Mutual Information mass).5.2 ResultsThe concept-by-concept z-normalized distributionsof cosines of relata instantiating each of our rela-tions are presented, for each of the example mod-els, in Fig. 1. The RelatumFrequency baselineshows a preference for adjectives and verbs in gen-eral, independently of whether they are meaningful(attributes, events) or not (random adjectives andverbs), reflecting the higher frequencies of adjec-tives and verbs in BLESS (Table 1). The Relation-Cardinality baseline produces even less interestingresults, with a strong preference for random nouns,followed by coordinates, events and random verbs(as predicted by the distribution in Table 1). We canconclude that the semantically meaningful patternsproduced by the other models cannot be explainedby trivial differences in relatum frequency or rela-tion cardinality in the BLESS data set.Moving then to the real DSMs, ContentWindow2essentially partitions the relations into 3 groups: co-ordinates are the closest relata, which makes sensesince they are, taxonomically, the most similar en-tities to target concepts. They are followed by (butsignificantly closer to the concept than) events, hy-pernyms and meronyms (events and hypernyms sig-nificantly above meronyms). Next come the at-tributes (significantly lower cosines than all relationtypes above). All the meaningful relata are signif-icantly closer to the concepts than the random re-lata. Similar patterns can be observed in the Con-tentWindow20 distribution, however in this case theevents, while still significantly below the coordi-nates, are significantly above the (statistically in-distinguishable) hypernym, meronym and attributeset. Again, all meaningful relata are above the ran-dom ones. Both content-window-based models pro-vide reasonable results, with ContentWindow2 be-ing probably closer to our "ontological" intuitions.The high ranking of events is probably explainedby the fact that a nominal concept will often ap-pear as subject or object of verbs expressing asso-ciated events (dog barks, fishing tuna), and thus thecorresponding verbs will share even relatively nar-row context windows with the concept noun. TheAllWindow2 distribution probably reflects the factthat many contexts picked by this DSM are function8COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2RelatumFrequencyCOORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2RelationCardinality COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2ContentWindow2COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2ContentWindow20COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2AllWindow2COORD HYPER MERO ATTRI EVENT RAN.N RAN.J RAN.V-2 -1 0 1 2DocumentFigure 1: Distribution of relata cosines across concepts (values on ordinate are cosines after concept-by-concept z-normalization).words, and thus they capture syntactic, rather thansemantic distributional properties. As a result, ran-dom nouns are as high (statistically indistinguish-able from) hypernyms and meronyms. Interestingly,attributes also belong to this subset of relations -probably due to the effect of determiners, quantifiersand other DP-initial function words, that will oftenoccur both before nouns and before adjectives. In-deed, even random adjectives, although significantlybelow the other relations we discussed, are signif-icantly above both random and meaningful verbs(i.e., events). For the Document model, all mean-ingful relations are significantly above the randomones. However, coordinates, while still the nearestneighbours (significantly closer than all other rela-tions) are much less distinct than in the window-based models. Note that we cannot say a priori thatContentWindow2 is better than Document becauseit favors coordinates. However, while they are bothable to sort out true and random relata, the lattershows a weaker ability to discriminate among differ-ent types of semantic relations (co-occurring withina document is indeed a much looser cue to similaritythan specifically co-occurring within a narrow win-dow). Traditional DSM tests, based on a single qual-ity measure, would not have given us this broad viewof how models are behaving.</body>
	<conclusion>We introduced BLESS, the first data set specificallydesigned for the intrinsic evaluation of DSMs. Thedata set contains tuples instantiating different, ex-plicitly typed semantic relations, plus a number ofcontrolled random tuples. Thus, BLESS can be usedto evaluate both the ability of DSMs to discriminatetruly related word pairs, and to perform in-depthanalyses of the types of semantic relata that differentmodels tend to favor among the nearest neighbors ofa target concept. Even a simple comparison of theperformance of a few DSMs on BLESS - like theone we have shown here - is able to highlight inter-esting differences in the semantic spaces producedby the various models. The success of BLESS willobviously depend on whether it will become a refer-ence model for the evaluation of DSMs, somethingthat can not be foreseen a priori. Whatever its des-tiny, we believe that the BLESS approach can boostand innovate evaluation in distributional semantics,as a key condition to get at a deeper understandingof its potentialities as a viable model for meaning.9</conclusion>
	<discussion>N/A</discussion>
	<biblio>Herv Abdi and Lynne Williams. 2010. Newman-Keulsand Tukey test. In N.J. Salkind, D.M. Dougherty, andB. Frey, editors, Encyclopedia of Research Design.Sage, Thousand Oaks, CA.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa. 2009. Astudy on similarity and relatedness using distributionaland WordNet-based approaches. In Proceedings ofHLT-NAACL, pages 19-27, Boulder, CO.Abdulrahman Almuhareb. 2006. Attributes in LexicalAcquisition. Phd thesis, University of Essex.Marco Baroni and Alessandro Lenci. 2010. Dis-tributional Memory: A general framework forcorpus-based semantics. Computational Linguistics,36(4):673-721.Marco Baroni, Stefan Evert, and Alessandro Lenci, edi-tors. 2008. Bridging the Gap between Semantic The-ory and Computational Simulations: Proceedings ofthe ESSLLI Workshop on Distributional Lexical Se-mantic. FOLLI, Hamburg.Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-simo Poesio. 2010. Strudel: A distributional semanticmodel based on properties and types. Cognitive Sci-ence, 34(2):222-254.Alexander Budanitsky and Graeme Hirst. 2006. Evalu-ating wordnet-based measures of lexical semantic re-latedness. Computational Linguistics, 32:13-47.D. A. Cruse. 1986. Lexical Semantics. Cambridge Uni-versity Press, Cambridge.Stefan Evert. 2005. The Statistics of Word Cooccur-rences. Dissertation, Stuttgart University.Christiane Fellbaum, editor. 1998. WordNet: An Elec-tronic Lexical Database. MIT Press, Cambridge, MA.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin. 2002. Placing search in context: The conceptrevisited. ACM Transactions on Information Systems,20(1):116-131.Thomas Landauer and Susan Dumais. 1997. A solu-tion to Plato's problem: The latent semantic analysistheory of acquisition, induction, and representation ofknowledge. Psychological Review, 104(2):211-240.Hugo Liu and Push Singh. 2004. ConceptNet: A prac-tical commonsense reasoning toolkit. BT TechnologyJournal, pages 211-226.Kevin Lund and Curt Burgess. 1996. Producinghigh-dimensional semantic spaces from lexical co-occurrence. Behavior Research Methods, 28:203-208.Ken McRae, George Cree, Mark Seidenberg, and ChrisMcNorgan. 2005. Semantic feature production normsfor a large set of living and nonliving things. BehaviorResearch Methods, 37(4):547-559.George Miller and Walter Charles. 1991. Contextual cor-relates of semantic similarity. Language and Cogni-tive Processes, 6(1):1-28.Gregory Murphy. 2002. The Big Book of Concepts. MITPress, Cambridge, MA.Timothy Rogers and James McClelland. 2004. Seman-tic Cognition: A Parallel Distributed Processing Ap-proach. MIT Press, Cambridge, MA.Herbert Rubenstein and John Goodenough. 1965. Con-textual correlates of synonymy. Communications ofthe ACM, 8(10):627-633.Karen Sparck Jones and Julia R. Galliers. 1996. Evaluat-ing Natural Language Processing Systems: An Analy-sis and Review. Springer Verlag, Berlin.Peter Turney and Patrick Pantel. 2010. From frequencyto meaning: Vector space models of semantics. Jour-nal of Artificial Intelligence Research, 37:141-188.Peter Turney. 2006. Similarity of semantic relations.Computational Linguistics, 32(3):379-416.Morton E. Winston, Roger Chaffin, and Douglas Her-rmann. 1987. A taxonomy of part-whole relations.Cognitive Science, 11:417-444.10</biblio>
</article>